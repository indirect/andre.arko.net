{
	"version": "https://jsonfeed.org/version/1",
	"title": "André.Arko.net",
	"icon": "https://micro.blog/indirect/avatar.jpg",
	"home_page_url": "https://andre.arko.net/",
	"feed_url": "https://andre.arko.net/feed.json",
	"items": [
		
			{
				"id": "http://indirect.micro.blog/2025/02/20/160321/",
				"title": "Housekeeping notes",
				"content_html": "<p>Hey, nice, my blog has a navbar now! In addition to the (previously hidden) pages for <a href=\"/talks\">Conference Talks</a> and <a href=\"/projects\">Personal Projects</a>, there is a completely new page for my <a href=\"/resume\">Professional Resumé</a>, which I have updated for the first time in uh&hellip; probably 12 years. So that&rsquo;s good.</p>\n<p>My personal about and splash page at <a href=\"https://arko.net\">arko.net</a> is also readable on phone-sized screens now. That only took 15 years or so.</p>\n<p>If you&rsquo;re seeing this, that also means I have successfully migrated from my legacy Netlify setup to my new setup on <a href=\"https://micro.blog\">Micro.blog</a>. Now that I&rsquo;m done moving all the existing posts, I can write new posts here without having to check out my git repo and create new markdown files and commit and push them. Hooray.</p>\n<p>Coming up next: still working on importing 18 years worth of tumblr posts into another micro.blog instance, and also planning to add linkblog-style posts to this blog so I can post smaller things even more easily.</p>\n",
				"content_text": "Hey, nice, my blog has a navbar now! In addition to the (previously hidden) pages for [Conference Talks](/talks) and [Personal Projects](/projects), there is a completely new page for my [Professional Resumé](/resume), which I have updated for the first time in uh... probably 12 years. So that's good.\n\nMy personal about and splash page at [arko.net](https://arko.net) is also readable on phone-sized screens now. That only took 15 years or so.\n\nIf you're seeing this, that also means I have successfully migrated from my legacy Netlify setup to my new setup on [Micro.blog](https://micro.blog). Now that I'm done moving all the existing posts, I can write new posts here without having to check out my git repo and create new markdown files and commit and push them. Hooray.\n\nComing up next: still working on importing 18 years worth of tumblr posts into another micro.blog instance, and also planning to add linkblog-style posts to this blog so I can post smaller things even more easily.\n\n",
				"date_published": "2025-02-20T16:03:21-08:00",
				"url": "https://andre.arko.net/2025/02/20/160321/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2024/10/10/updating-itunes-track-parser-scripts/",
				"title": "Updating iTunes Track Parser Scripts for Music.app",
				"content_html": "<p>Moving from my usual niche interests to a niche so small that I have only seen two people on the internet who care about this: I have some really great news if you still want to manage metadata tags like it’s 2010 and you’re ripping CDs into iTunes. I’ve updated the most useful iTunes track naming script to ever exist, so you can use it in Music.app on macOS 15.1 Sequoia in the year 2024.</p>\n<p>The scripts are named <code>Track Parser (Clipboard)</code> and <code>Track Parser (Song name)</code>, and they were written by <a href=\"https://www.danvk.org\">Dan Vanderkam</a> in 2004. He maintained them until 2009, put them into a public Google Code project, and eventually moved on with his life. I used both scripts hundreds or maybe even thousands of times between 2004 and 2014. That’s when I switched to Bandcamp purchases and Apple Music streaming, so I haven’t had much track renaming to automate since then. (Well, besides downloading <a href=\"https://www.dnalounge.com/webcast/mixtapes/\">jwz mixtapes</a>, but I wrote <a href=\"https://github.com/indirect/dotfiles/blob/main/dot_bin/executable_jwz-download\">a dedicated script</a> for that years ago.)</p>\n<p>Then I ran across <a href=\"https://www.youtube.com/playlist?list=PLAwkDjVcJePggQv6qM9cPWqrqoWkuD_h_\">the soundtrack to the recent official expansion for DOOM</a> (not for DOOM (2016), for the original DOOM! in 2024!). I downloaded the FLAC version, and slowly recollected my pipeline for batch converting audio files and dropping them into iTunes to add to my cloud library. I had the track listing in text, so I naturally expected I would run <code>Track Parser (Clipboard)</code> to take care of naming and numbering all of the songs for me.</p>\n<p>That’s when I discovered that the scripts had never been updated for Music.app. Several minutes of searching later, I found out that Dan is now the author of <a href=\"https://amzn.to/402nk5R\">Effective TypeScript</a>, and his undergraduate iTunes AppleScripts don’t seem to be on his GitHub profile anywhere. I eventually turned up <a href=\"https://code.google.com/archive/p/trackparser/\">the Google Code repo</a> where he published the scripts, which is happily still available in archive form even though Google Code shut down years ago.</p>\n<p>Armed with the AppleScript source files, I was able to puzzle my way through a bunch of error messages that mostly boiled down to “that’s a keyword now”. I eventually got the script working, and that’s when I realized… I messed up converting the FLAC files. As soon as I fixed the issue, the M4A metadata was  fully populated, and I didn’t need to parse the clipboard for track names after all.</p>\n<p>At least I had fun tracking down the history of what happened to these scripts that were really significant to me once in the past, and updated them so anyone weird enough to be mass-editing song names today can do that more easily.</p>\n<p>If that’s you, feel free to grab the <code>Track Parser</code> scripts from <a href=\"https://github.com/indirect/trackparser/releases\">the Releases page</a> in my new <a href=\"https://github.com/indirect/trackparser\">trackparser GitHub repo</a>.</p>\n",
				"content_text": "\nMoving from my usual niche interests to a niche so small that I have only seen two people on the internet who care about this: I have some really great news if you still want to manage metadata tags like it’s 2010 and you’re ripping CDs into iTunes. I’ve updated the most useful iTunes track naming script to ever exist, so you can use it in Music.app on macOS 15.1 Sequoia in the year 2024.\n\nThe scripts are named `Track Parser (Clipboard)` and `Track Parser (Song name)`, and they were written by [Dan Vanderkam](https://www.danvk.org) in 2004. He maintained them until 2009, put them into a public Google Code project, and eventually moved on with his life. I used both scripts hundreds or maybe even thousands of times between 2004 and 2014. That’s when I switched to Bandcamp purchases and Apple Music streaming, so I haven’t had much track renaming to automate since then. (Well, besides downloading [jwz mixtapes](https://www.dnalounge.com/webcast/mixtapes/), but I wrote [a dedicated script](https://github.com/indirect/dotfiles/blob/main/dot_bin/executable_jwz-download) for that years ago.)\n\nThen I ran across [the soundtrack to the recent official expansion for DOOM](https://www.youtube.com/playlist?list=PLAwkDjVcJePggQv6qM9cPWqrqoWkuD_h_) (not for DOOM (2016), for the original DOOM! in 2024!). I downloaded the FLAC version, and slowly recollected my pipeline for batch converting audio files and dropping them into iTunes to add to my cloud library. I had the track listing in text, so I naturally expected I would run `Track Parser (Clipboard)` to take care of naming and numbering all of the songs for me.\n\nThat’s when I discovered that the scripts had never been updated for Music.app. Several minutes of searching later, I found out that Dan is now the author of [Effective TypeScript](https://amzn.to/402nk5R), and his undergraduate iTunes AppleScripts don’t seem to be on his GitHub profile anywhere. I eventually turned up [the Google Code repo](https://code.google.com/archive/p/trackparser/) where he published the scripts, which is happily still available in archive form even though Google Code shut down years ago.\n\nArmed with the AppleScript source files, I was able to puzzle my way through a bunch of error messages that mostly boiled down to “that’s a keyword now”. I eventually got the script working, and that’s when I realized… I messed up converting the FLAC files. As soon as I fixed the issue, the M4A metadata was  fully populated, and I didn’t need to parse the clipboard for track names after all.\n\nAt least I had fun tracking down the history of what happened to these scripts that were really significant to me once in the past, and updated them so anyone weird enough to be mass-editing song names today can do that more easily.\n\nIf that’s you, feel free to grab the `Track Parser` scripts from [the Releases page](https://github.com/indirect/trackparser/releases) in my new [trackparser GitHub repo](https://github.com/indirect/trackparser).\n",
				"date_published": "2024-10-10T00:00:00-08:00",
				"url": "https://andre.arko.net/2024/10/10/updating-itunes-track-parser-scripts/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2023/11/24/homebrew-cask-formula-for-private/",
				"title": "Homebrew cask formula for private GitHub repo releases",
				"content_html": "<p>I try to use <a href=\"https://github.com/indirect/dotfiles\">my dotfiles</a> to install software for myself, mainly via <a href=\"https://brew.sh\">Homebrew</a>. This week, I ran into a new automation problem: I wanted to start using a program only available from a private GitHub repo, which requires authentication for downloads. To make things worse, new versions release frequently, so I can&rsquo;t use a static link to the GitHub Release entry&rsquo;s asset download link.</p>\n<p>I ended up doing a bunch of experimenting and searching, and I initially found <a href=\"https://blog.devgenius.io/create-homebrew-taps-for-private-github-repos-44daf2f4cff8\">several</a> <a href=\"https://gist.github.com/minamijoyo/3d8aa79085369efb79964ba45e24bb0e\">posts</a> <a href=\"https://dev.to/jhot/homebrew-and-private-github-repositories-1dfh\">about</a> private GitHub repos, mostly centered around the idea of setting a special environment variable with a GitHub API token and then writing a custom Homebrew download strategy class.</p>\n<p>I didn&rsquo;t want to have to keep track of another env var with another GitHub token, and I didn&rsquo;t want to have to maintain a custom download strategy class, so I kept looking. Eventually, I hit on <a href=\"https://github.com/Homebrew/brew/issues/15590\">this pull request to Homebrew itself</a> from a few months ago, adding support for setting HTTP headers needed to download private repo release assets.</p>\n<p>Unfortunately, the example in that PR had exactly the problem I mentioned, using a static URL containing a GitHub release. After carefully re-reading the Homebrew docs about creating casks, and then reading the source code of the class that powers the <code>url</code> stanza, I was finally able to craft something that works.</p>\n<p>In my particular case, the important bit was figuring out how to look up GitHub releases by tag name (since this repo keeps the same tag name for all releases), and then look up one specific asset on that release by filename, to get the correct archive for my OS.</p>\n<p>Putting it all together, here&rsquo;s the formula I wound up with that requires no special environment variable, and requires no special download strategy, just uses the options built into Homebrew already, with commentary about each part:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>cask <span style=\"color:#e6db74\">&#34;appname&#34;</span> <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#75715e\"># Use :latest to tell homebrew that this will always return the newest version, and there isn&#39;t a specific version number available.</span>\n</span></span><span style=\"display:flex;\"><span>  version <span style=\"color:#e6db74\">:latest</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#75715e\"># Use :no_check to tell Homebrew that it can&#39;t know the checksum in advance, and so it should not try to validate the checksum of the downloaded archive.</span>\n</span></span><span style=\"display:flex;\"><span>  sha256 <span style=\"color:#e6db74\">:no_check</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>  desc <span style=\"color:#e6db74\">&#34;some info&#34;</span>\n</span></span><span style=\"display:flex;\"><span>  hompage <span style=\"color:#e6db74\">&#34;https://github.com/username/appname&#34;</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#75715e\"># If there&#39;s no arguments and only a block, Homebrew will wait to run the block until it actually needs the URL to download the file at install-time.</span>\n</span></span><span style=\"display:flex;\"><span>  url <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#75715e\"># Homebrew has a built-in GitHub API client, conveniently able to provide the list of releases, converted from JSON to Ruby hashes.</span>\n</span></span><span style=\"display:flex;\"><span>    assets <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">GitHub</span><span style=\"color:#f92672\">.</span>get_release(<span style=\"color:#e6db74\">&#34;username&#34;</span>, <span style=\"color:#e6db74\">&#34;reponame&#34;</span>, <span style=\"color:#e6db74\">&#34;tagname&#34;</span>)<span style=\"color:#f92672\">.</span>fetch(<span style=\"color:#e6db74\">&#34;assets&#34;</span>)\n</span></span><span style=\"display:flex;\"><span>    latest <span style=\"color:#f92672\">=</span> assets<span style=\"color:#f92672\">.</span>find{<span style=\"color:#f92672\">|</span>a<span style=\"color:#f92672\">|</span> a<span style=\"color:#f92672\">[</span><span style=\"color:#e6db74\">&#34;name&#34;</span><span style=\"color:#f92672\">]</span> <span style=\"color:#f92672\">==</span> <span style=\"color:#e6db74\">&#34;appname-macos-universal.zip&#34;</span> }<span style=\"color:#f92672\">.</span>fetch(<span style=\"color:#e6db74\">&#34;url&#34;</span>)\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#75715e\"># The return value must match the arguments for the non-block version of `url`, first a URL, and then an options hash. The `header` option can take an array if you need to provide more than one header.</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#f92672\">[</span>latest, <span style=\"color:#e6db74\">header</span>: <span style=\"color:#f92672\">[</span>\n</span></span><span style=\"display:flex;\"><span>      <span style=\"color:#75715e\"># The GitHub API will return the binary content of an asset instead of JSON data about that asset if you set the Accept header to application/octet-stream.</span>\n</span></span><span style=\"display:flex;\"><span>      <span style=\"color:#e6db74\">&#34;Accept: application/octet-stream&#34;</span>,\n</span></span><span style=\"display:flex;\"><span>      <span style=\"color:#75715e\"># Homebrew also has a built-in helper that will return GitHub credentials, checking the keychain, config files, gh CLI tool, and other locations automatically. We can re-use those same credentials that Homebrew uses to make API requests for our own download by setting this header.</span>\n</span></span><span style=\"display:flex;\"><span>      <span style=\"color:#e6db74\">&#34;Authorization: bearer </span><span style=\"color:#e6db74\">#{</span><span style=\"color:#66d9ef\">GitHub</span><span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">API</span><span style=\"color:#f92672\">.</span>credentials<span style=\"color:#e6db74\">}</span><span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#f92672\">]]</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>  app <span style=\"color:#e6db74\">&#34;appname.app&#34;</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span>\n</span></span></code></pre></div><p>I hope that helps anyone with a similar problem! At this point I&rsquo;m just writing this down so that I can find this blog post later when I forget about it and need to create a formula for another private repo. 😅</p>\n",
				"content_text": "I try to use [my dotfiles](https://github.com/indirect/dotfiles) to install software for myself, mainly via [Homebrew](https://brew.sh). This week, I ran into a new automation problem: I wanted to start using a program only available from a private GitHub repo, which requires authentication for downloads. To make things worse, new versions release frequently, so I can't use a static link to the GitHub Release entry's asset download link.\n\nI ended up doing a bunch of experimenting and searching, and I initially found [several](https://blog.devgenius.io/create-homebrew-taps-for-private-github-repos-44daf2f4cff8) [posts](https://gist.github.com/minamijoyo/3d8aa79085369efb79964ba45e24bb0e) [about](https://dev.to/jhot/homebrew-and-private-github-repositories-1dfh) private GitHub repos, mostly centered around the idea of setting a special environment variable with a GitHub API token and then writing a custom Homebrew download strategy class.\n\nI didn't want to have to keep track of another env var with another GitHub token, and I didn't want to have to maintain a custom download strategy class, so I kept looking. Eventually, I hit on [this pull request to Homebrew itself](https://github.com/Homebrew/brew/issues/15590) from a few months ago, adding support for setting HTTP headers needed to download private repo release assets.\n\nUnfortunately, the example in that PR had exactly the problem I mentioned, using a static URL containing a GitHub release. After carefully re-reading the Homebrew docs about creating casks, and then reading the source code of the class that powers the `url` stanza, I was finally able to craft something that works.\n\nIn my particular case, the important bit was figuring out how to look up GitHub releases by tag name (since this repo keeps the same tag name for all releases), and then look up one specific asset on that release by filename, to get the correct archive for my OS.\n\nPutting it all together, here's the formula I wound up with that requires no special environment variable, and requires no special download strategy, just uses the options built into Homebrew already, with commentary about each part:\n\n```ruby\ncask \"appname\" do\n  # Use :latest to tell homebrew that this will always return the newest version, and there isn't a specific version number available.\n  version :latest\n  # Use :no_check to tell Homebrew that it can't know the checksum in advance, and so it should not try to validate the checksum of the downloaded archive.\n  sha256 :no_check\n\n  desc \"some info\"\n  hompage \"https://github.com/username/appname\"\n  # If there's no arguments and only a block, Homebrew will wait to run the block until it actually needs the URL to download the file at install-time.\n  url do\n    # Homebrew has a built-in GitHub API client, conveniently able to provide the list of releases, converted from JSON to Ruby hashes.\n    assets = GitHub.get_release(\"username\", \"reponame\", \"tagname\").fetch(\"assets\")\n    latest = assets.find{|a| a[\"name\"] == \"appname-macos-universal.zip\" }.fetch(\"url\")\n    # The return value must match the arguments for the non-block version of `url`, first a URL, and then an options hash. The `header` option can take an array if you need to provide more than one header.\n    [latest, header: [\n      # The GitHub API will return the binary content of an asset instead of JSON data about that asset if you set the Accept header to application/octet-stream.\n      \"Accept: application/octet-stream\",\n      # Homebrew also has a built-in helper that will return GitHub credentials, checking the keychain, config files, gh CLI tool, and other locations automatically. We can re-use those same credentials that Homebrew uses to make API requests for our own download by setting this header.\n      \"Authorization: bearer #{GitHub::API.credentials}\"\n    ]]\n  end\n\n  app \"appname.app\"\nend\n```\n\nI hope that helps anyone with a similar problem! At this point I'm just writing this down so that I can find this blog post later when I forget about it and need to create a formula for another private repo. 😅\n",
				"date_published": "2023-11-24T00:00:00-08:00",
				"url": "https://andre.arko.net/2023/11/24/homebrew-cask-formula-for-private/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2023/07/23/caddy-plus-pumadev-with-ssl/",
				"title": "Caddy plus puma-dev with SSL",
				"content_html": "<p><a href=\"/2023/03/05/caddy-puma-dev-for-local-development-with-custom-domains-and-https/\">Last time</a>, we talked about setting up Caddy as a reverse-proxy to puma-dev, providing automatically-managed local development Rails apps on their own dedicated <code>.test</code> domains. Either I missed this back then, or something inside puma-dev or Caddy changed in the meantime, but SSL requests stopped working inside Rails apps today, and it took me a while to figure out what was happening.</p>\n<p>I started getting the exception <code>HTTP Origin header (https://example.com) didn't match request.base_url (http://example.com)</code>, and tracked it down to the <code>HTTP_X_FORWARDED_PROTO</code> getting sent to Rails, which was being set to <code>http</code>. Unfortunately, just setting the header <code>X-Forwarded-Proto: https</code> doesn&rsquo;t work, because both Caddy and puma-dev strip it out and set their own value before sending the request on to the Rails app.</p>\n<p>After a bunch more fiddling around, I figured out how to get Caddy to generate an SSL certificate for the requesting domain, using its internal certificate authority, and make a request to puma-dev over SSL, without validating the (self-signed) certificate provided by puma-dev. With this slightly more complicated Caddy config block, SSL is again working:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-Caddyfile\" data-lang=\"Caddyfile\"><span style=\"display:flex;\"><span>*.test {\n</span></span><span style=\"display:flex;\"><span>\t<span style=\"color:#66d9ef\">tls</span> <span style=\"color:#66d9ef\">internal</span> {\n</span></span><span style=\"display:flex;\"><span>\t\t<span style=\"color:#66d9ef\">on_demand</span>\n</span></span><span style=\"display:flex;\"><span>\t}\n</span></span><span style=\"display:flex;\"><span>\t<span style=\"color:#66d9ef\">reverse_proxy</span> https://127.0.0.1:<span style=\"color:#ae81ff\">9283</span>  {\n</span></span><span style=\"display:flex;\"><span>\t\t<span style=\"color:#66d9ef\">transport</span> <span style=\"color:#e6db74\">http</span> {\n</span></span><span style=\"display:flex;\"><span>\t\t\t<span style=\"color:#66d9ef\">tls_insecure_skip_verify</span>\n</span></span><span style=\"display:flex;\"><span>\t\t}\n</span></span><span style=\"display:flex;\"><span>\t}\n</span></span><span style=\"display:flex;\"><span>\t<span style=\"color:#66d9ef\">log</span>\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div>",
				"content_text": "\n[Last time](/2023/03/05/caddy-puma-dev-for-local-development-with-custom-domains-and-https/), we talked about setting up Caddy as a reverse-proxy to puma-dev, providing automatically-managed local development Rails apps on their own dedicated `.test` domains. Either I missed this back then, or something inside puma-dev or Caddy changed in the meantime, but SSL requests stopped working inside Rails apps today, and it took me a while to figure out what was happening.\n\nI started getting the exception `HTTP Origin header (https://example.com) didn't match request.base_url (http://example.com)`, and tracked it down to the `HTTP_X_FORWARDED_PROTO` getting sent to Rails, which was being set to `http`. Unfortunately, just setting the header `X-Forwarded-Proto: https` doesn't work, because both Caddy and puma-dev strip it out and set their own value before sending the request on to the Rails app.\n\nAfter a bunch more fiddling around, I figured out how to get Caddy to generate an SSL certificate for the requesting domain, using its internal certificate authority, and make a request to puma-dev over SSL, without validating the (self-signed) certificate provided by puma-dev. With this slightly more complicated Caddy config block, SSL is again working:\n\n```Caddyfile\n*.test {\n\ttls internal {\n\t\ton_demand\n\t}\n\treverse_proxy https://127.0.0.1:9283  {\n\t\ttransport http {\n\t\t\ttls_insecure_skip_verify\n\t\t}\n\t}\n\tlog\n}\n```\n",
				"date_published": "2023-07-23T00:00:00-08:00",
				"url": "https://andre.arko.net/2023/07/23/caddy-plus-pumadev-with-ssl/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2023/03/05/caddy-pumadev-for-local-development/",
				"title": "Caddy + puma-dev for local development",
				"content_html": "<p>I develop a lot of webapps locally, often at the same time. For Ruby-only applications, <a href=\"https://github.com/puma/puma-dev\">puma-dev</a> is by far the most convenient way to handle the situation. A single setup command gets you a lot out of the box:</p>\n<ol>\n<li>DNS resolution for all <code>.test</code> domains to resolve to localhost</li>\n<li>A locally-generated SSL certificate root, so HTTPS works</li>\n<li>Automatic starting and stopping of Ruby processes on demand</li>\n<li>Adding apps easily: run <code>puma-dev link</code> then visit <code>https://appname.test</code>.</li>\n</ol>\n<p>The one downside of puma-dev is that there&rsquo;s no way to set a breakpoint and interact directly with the web server in a terminal. In those tricky cases, I would typically start a dev server myself just for that breakpoint and then go back to using the puma-dev server after.</p>\n<p>If I only ever used Ruby processes, I would have stopped there and been happy. But modern web development includes a lot of additional servers, like webpack, or esbuild, or tailwind, or other external services that have to run for local development to work. In those cases, I often use a Procfile and <a href=\"https://github.com/DarthSim/overmind\">overmind</a> to manage the set of processes needed for local development. The problem with using a Procfile is that it removes all of the benefits of puma-dev: no more custom domain, no more automatic process management, no more SSL certs.</p>\n<p>On top of that, even with a Procfile I would run into problems like a production app routing certain URLs to certain services. For example, I can never remember that <code>/admin</code> only exists on port 3001, while the rest of the app only exists on port 3000.</p>\n<p>At this point, I took to <a href=\"https://fiasco.social/@indirect/109927615725945076\">complaining online about it</a>, hoping someone else would have already solved the problem for me. Alas, none of the replies indicated there was anything already written that could do this out of the box.</p>\n<p>So how, I thought to myself, can I keep the custom domains and SSL certificates, but write my own config file that maps certain URLs to certain ports? Well, I have a program I use for that already, and it&rsquo;s <a href=\"https://caddyserver.com/\">Caddy</a>. Caddy is amazing and wonderful and a breath of extremely great fresh air in the HTTP server space, and you should use it if you aren&rsquo;t already.</p>\n<p>But both Caddy and puma-dev expect to take over port 80 and 443 on localhost, so they can do their magic, and Caddy doesn&rsquo;t offer the local TLD that puma-dev does, nor does it manage processes automatically. How can I get the best of both?</p>\n<p>After a frankly embarassing amount of time searching the internet and reading Caddy forum posts, I eventually concluded that there was no single thing that did everything I wanted. That&rsquo;s when I had a mildly deranged idea: what if I run puma-dev on a different port, and tell Caddy to reverse-proxy all <code>.test</code> domains to the puma-dev port? The puma-dev resolver will make sure that the domains point to Caddy running on localhost, and Caddy will make sure that the requests eventually reach puma-dev and from there reach my applications.</p>\n<p>It took an hour or two of fiddling around, but I actually got it working! The puma-dev change was to install it to the default userland ports by running <code>puma-dev -install -install-port 9280 -install-https-port 9283</code>. The Caddy change was to add this block to my <code>Caddyfile</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-Caddyfile\" data-lang=\"Caddyfile\"><span style=\"display:flex;\"><span>*.test {\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">tls</span> <span style=\"color:#66d9ef\">internal</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">reverse_proxy</span> 127.0.0.1:<span style=\"color:#ae81ff\">9280</span>\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><p>Shockingly, that was all I needed to do, and everything worked at that point. Better yet, I can add specific support individual mappings, too:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-Caddyfile\" data-lang=\"Caddyfile\"><span style=\"display:flex;\"><span>app-one.test {\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">reverse_proxy</span> <span style=\"color:#a6e22e\">/admin</span> 127.0.0.1:<span style=\"color:#ae81ff\">3001</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">reverse_proxy</span> 127.0.0.1:<span style=\"color:#ae81ff\">3000</span>\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><p>It doesn&rsquo;t handle automatic process management, but it can at least offer production-like routing to multiple processes running from a Procfile.</p>\n<p>This doesn&rsquo;t quite do everything that I want, since there are still hardcoded ports for the apps with custom routing, but it&rsquo;s closer than I&rsquo;ve ever had before. Maybe next time I can figure out how to wrap non-Ruby processes in a Rack wrapper and make puma-dev manage them for me.</p>\n",
				"content_text": "\nI develop a lot of webapps locally, often at the same time. For Ruby-only applications, [puma-dev][] is by far the most convenient way to handle the situation. A single setup command gets you a lot out of the box:\n\n1. DNS resolution for all `.test` domains to resolve to localhost\n1. A locally-generated SSL certificate root, so HTTPS works\n1. Automatic starting and stopping of Ruby processes on demand\n1. Adding apps easily: run `puma-dev link` then visit `https://appname.test`.\n\nThe one downside of puma-dev is that there's no way to set a breakpoint and interact directly with the web server in a terminal. In those tricky cases, I would typically start a dev server myself just for that breakpoint and then go back to using the puma-dev server after.\n\nIf I only ever used Ruby processes, I would have stopped there and been happy. But modern web development includes a lot of additional servers, like webpack, or esbuild, or tailwind, or other external services that have to run for local development to work. In those cases, I often use a Procfile and [overmind][] to manage the set of processes needed for local development. The problem with using a Procfile is that it removes all of the benefits of puma-dev: no more custom domain, no more automatic process management, no more SSL certs.\n\nOn top of that, even with a Procfile I would run into problems like a production app routing certain URLs to certain services. For example, I can never remember that `/admin` only exists on port 3001, while the rest of the app only exists on port 3000.\n\nAt this point, I took to [complaining online about it][1], hoping someone else would have already solved the problem for me. Alas, none of the replies indicated there was anything already written that could do this out of the box.\n\nSo how, I thought to myself, can I keep the custom domains and SSL certificates, but write my own config file that maps certain URLs to certain ports? Well, I have a program I use for that already, and it's [Caddy][]. Caddy is amazing and wonderful and a breath of extremely great fresh air in the HTTP server space, and you should use it if you aren't already.\n\nBut both Caddy and puma-dev expect to take over port 80 and 443 on localhost, so they can do their magic, and Caddy doesn't offer the local TLD that puma-dev does, nor does it manage processes automatically. How can I get the best of both?\n\nAfter a frankly embarassing amount of time searching the internet and reading Caddy forum posts, I eventually concluded that there was no single thing that did everything I wanted. That's when I had a mildly deranged idea: what if I run puma-dev on a different port, and tell Caddy to reverse-proxy all `.test` domains to the puma-dev port? The puma-dev resolver will make sure that the domains point to Caddy running on localhost, and Caddy will make sure that the requests eventually reach puma-dev and from there reach my applications.\n\nIt took an hour or two of fiddling around, but I actually got it working! The puma-dev change was to install it to the default userland ports by running `puma-dev -install -install-port 9280 -install-https-port 9283`. The Caddy change was to add this block to my `Caddyfile`:\n\n```Caddyfile\n*.test {\n  tls internal\n  reverse_proxy 127.0.0.1:9280\n}\n```\n\nShockingly, that was all I needed to do, and everything worked at that point. Better yet, I can add specific support individual mappings, too:\n\n```Caddyfile\napp-one.test {\n  reverse_proxy /admin 127.0.0.1:3001\n  reverse_proxy 127.0.0.1:3000\n}\n```\n\nIt doesn't handle automatic process management, but it can at least offer production-like routing to multiple processes running from a Procfile.\n\nThis doesn't quite do everything that I want, since there are still hardcoded ports for the apps with custom routing, but it's closer than I've ever had before. Maybe next time I can figure out how to wrap non-Ruby processes in a Rack wrapper and make puma-dev manage them for me.\n\n[1]: https://fiasco.social/@indirect/109927615725945076\n[puma-dev]: https://github.com/puma/puma-dev\n[overmind]: https://github.com/DarthSim/overmind\n[Caddy]: https://caddyserver.com/\n",
				"date_published": "2023-03-05T00:00:00-08:00",
				"url": "https://andre.arko.net/2023/03/05/caddy-pumadev-for-local-development/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2023/01/01/mastodon-missing-posts-with-empty/",
				"title": "Mastodon missing posts, with empty URL search results",
				"content_html": "<p><strong>tl;dr</strong> if you boot Mastodon without <code>LOCAL_DOMAIN</code> once, you might be unable to fetch posts from some other instances. If searching for a post URL returns nothing, try running <code>bin/rails r 'Account.representative.update!(username: ENV[&quot;LOCAL_DOMAIN&quot;])'</code>. That fixed things for me.</p>\n<p>So. Four hours of debugging missing posts later, here we are. The symptoms I noticed were:</p>\n<ul>\n<li>some posts were mysteriously missing on my server, even though they clearly existed on other servers</li>\n<li>posts that did appear were sometimes direct replies to posts that didn&rsquo;t appear</li>\n<li>searching for a missing post by full URL would return no results, even if that very post was visible in the federated feed</li>\n</ul>\n<p>Confused, I started reading logs, searching the internet fruitlessly, reading GitHub issues that contained the right keywords but were different problems, checking the Sidekiq admin page, and slowly reading every single page in the Mastodon admin section. I eventually found a typo in my sidekiq config, running the <em>schedule</em> queue instead of the <em>scheduler</em> queue. I concluded that must have been the problem, added the missing &ldquo;r&rdquo;, and went to bed.</p>\n<p>The next day, I realized that I needed to verify that my fix had actually resolved the problem, and went to search for a post by full URL. It still didn&rsquo;t work.</p>\n<p>Completely out of ideas about why a search by full post URL worked on any Mastodon instance except mine, I gave up and started treating Mastodon like I would any Rails app with confusing behavior: open a production console and slowly run all the code involved while looking for clues.</p>\n<p>I had already collected a line from the Rails log, telling me what controller handled my search request:</p>\n<pre tabindex=\"0\"><code>method=GET path=/api/v2/search format=html controller=Api::V2::SearchController action=index status=200 duration=1368.97 view=159.18 db=462.83\n</code></pre><p>Armed with that information, I browsed to <a href=\"https://github.com/mastodon/mastodon/blob/ef4d29c8791086b11f6e36aa121ff5c9b5fa0103/app/controllers/api/v2/search_controller.rb\">search_controller.rb</a> and discovered that search results are collected by invoking <a href=\"https://github.com/mastodon/mastodon/blob/ef4d29c8791086b11f6e36aa121ff5c9b5fa0103/app/services/search_service.rb\">SearchService</a>. At that point, I was ready to start running the code myself to find out what was going on.</p>\n<p>Step one: ssh to the VM running the Rails process.<br>\nStep two: edit the <code>Gemfile</code> to move <code>pry-rails</code> out of the development group.<br>\nStep three: run <code>bundle install</code>.<br>\nStep four: run <code>bin/rails c</code> to open a Rails console with Pry.<br>\nStep five: run <code>cd SearchService.new</code> to get into the right context.</p>\n<p>Once I was inside an instance of <code>SearchService</code>, I could run the code I was reading in the <code>call</code> method by hand.</p>\n<pre tabindex=\"0\"><code>&gt; @query = &#34;https://aus.social/@liam/109599234831423305&#34;\n&gt; url_query? \n=&gt; false\n</code></pre><p>Hmm. That&rsquo;s odd. Why would <code>url_query?</code> be false when the query is just a URL? Oh, looking at <a href=\"https://github.com/mastodon/mastodon/blob/ef4d29c8791086b11f6e36aa121ff5c9b5fa0103/app/services/search_service.rb#L75\">the <code>url_query?</code> method</a>, it seems like <code>@resolve</code> has to be <code>true</code>. We can do that.</p>\n<pre tabindex=\"0\"><code>&gt; @resolve = true\n&gt; url_query?\n=&gt; true\n&gt; url_resource\n=&gt; nil\n</code></pre><p>Wait, why is <a href=\"https://github.com/mastodon/mastodon/blob/ef4d29c8791086b11f6e36aa121ff5c9b5fa0103/app/services/search_service.rb#L82\"><code>url_resource</code></a> returning <code>nil</code>? That doesn&rsquo;t make sense either, public requests to this URL return reasonable responses. I guess that means we need to look inside <a href=\"https://github.com/mastodon/mastodon/blob/ef4d29c8791086b11f6e36aa121ff5c9b5fa0103/app/services/resolve_url_service.rb\"><code>ResolveURLService</code></a>.</p>\n<pre tabindex=\"0\"><code>&gt; cd ResolveURLService.new\n&gt; @url = &#34;https://aus.social/@liam/109599234831423305&#34;\n&gt; local_url?\n=&gt; false\n&gt; fetched_url\n=&gt; nil\n</code></pre><p>It&rsquo;s fetching from the URL, and getting <code>nil</code>? That&rsquo;s downright bizzare. Okay, I guess that means we need to look into what happens inside <a href=\"https://github.com/mastodon/mastodon/blob/ef4d29c8791086b11f6e36aa121ff5c9b5fa0103/app/services/fetch_resource_service.rb\"><code>FetchResourceService</code></a>.</p>\n<pre tabindex=\"0\"><code>&gt; cd FetchResourceService.new\n&gt; @url = &#34;https://aus.social/@liam/109599234831423305&#34;\n&gt; perform_request\n=&gt; nil\n</code></pre><p>Okay, that&rsquo;s just&hellip; what. Why is <code>perform_request</code> returning <code>nil</code>? What is this <code>Account.representative</code> thing that&rsquo;s getting added to this request?</p>\n<pre tabindex=\"0\"><code>&gt; Account.representative\n=&gt; #&lt;Account:0x00007ffa66d5cb50\n id: -99,\n username: &#34;localhost:3000&#34;,\n [...]&gt;\n</code></pre><p>Wait. The <code>username</code> attribute is set to <code>localhost:3000</code>? That&rsquo;s not the name of my instance, and that&rsquo;s not even the value set in the <code>LOCAL_DOMAIN</code> env var. I guess I should fix that, and then try again.</p>\n<pre tabindex=\"0\"><code>&gt; Account.representative.update!(username: ENV[&#34;LOCAL_DOMAIN&#34;])\n&gt; perform_request\n=&gt; nil\n</code></pre><p>Damn. I guess that didn&rsquo;t fix it.</p>\n<p>Wait. That method takes a block.</p>\n<pre tabindex=\"0\"><code>&gt; perform_request{|r| r }\n=&gt; #&lt;HTTP::Response/1.1 200 OK {&#34;Date&#34;=&gt;&#34;Sun, 01 Jan 2023 16:16:43 GMT&#34;, &#34;Content-Type&#34;=&gt;&#34;application/activity+json; charset=utf-8&#34;, [...]}&gt;\n</code></pre><p>Wait. That fixed it??? It works now???</p>\n<p>One quick detour to the web UI and a single search later&hellip; yes, that did in fact fix it.</p>\n<p>Some instances seem to refuse requests made on behalf of <code>localhost:3000</code>. Maybe it&rsquo;s even in an instance blocklist, which would be pretty reasonable&ndash;it doesn&rsquo;t make sense to try to exchange updates with a server running on localhost.</p>\n<p>Sure would be nice if there were some way to know this had happened, though. It turns out that any call to <a href=\"https://github.com/mastodon/mastodon/blob/ef4d29c8791086b11f6e36aa121ff5c9b5fa0103/app/models/concerns/account_finder_concern.rb#L18\"><code>Account.representative</code></a> creates a new database record if there isn&rsquo;t one, and saves the currently set local domain into the database forever.</p>\n<p>Where does the local domain come from, you ask? It is set by <a href=\"https://github.com/mastodon/mastodon/blob/ef4d29c8791086b11f6e36aa121ff5c9b5fa0103/config/initializers/1_hosts.rb#L4\">the <code>hosts</code> initializer</a>, which reads from the env var <code>LOCAL_DOMAIN</code>, with a fallback to <code>localhost:3000</code> if the env var is unset.</p>\n<p>I booted the Rails server one time before setting <code>LOCAL_DOMAIN</code>, and didn&rsquo;t even create an account. Everything seemed like it worked perfectly until I stumbled across an instance that refuses requests on behalf of <code>localhost:3000</code>. And then it took four hours to debug.</p>\n<p>It sure would be nice if Mastodon had some way to let you know that your <code>Account.representative</code> and your <code>LOCAL_DOMAIN</code> didn&rsquo;t match up, to save that four hours of debugging.</p>\n<p>Well, hopefully it won&rsquo;t take <em>you</em> four hours to debug, since you found this post. Good luck!</p>\n",
				"content_text": "\n**tl;dr** if you boot Mastodon without `LOCAL_DOMAIN` once, you might be unable to fetch posts from some other instances. If searching for a post URL returns nothing, try running `bin/rails r 'Account.representative.update!(username: ENV[\"LOCAL_DOMAIN\"])'`. That fixed things for me.\n\nSo. Four hours of debugging missing posts later, here we are. The symptoms I noticed were:\n\n- some posts were mysteriously missing on my server, even though they clearly existed on other servers\n- posts that did appear were sometimes direct replies to posts that didn't appear\n- searching for a missing post by full URL would return no results, even if that very post was visible in the federated feed\n\nConfused, I started reading logs, searching the internet fruitlessly, reading GitHub issues that contained the right keywords but were different problems, checking the Sidekiq admin page, and slowly reading every single page in the Mastodon admin section. I eventually found a typo in my sidekiq config, running the _schedule_ queue instead of the _scheduler_ queue. I concluded that must have been the problem, added the missing \"r\", and went to bed.\n\nThe next day, I realized that I needed to verify that my fix had actually resolved the problem, and went to search for a post by full URL. It still didn't work.\n\nCompletely out of ideas about why a search by full post URL worked on any Mastodon instance except mine, I gave up and started treating Mastodon like I would any Rails app with confusing behavior: open a production console and slowly run all the code involved while looking for clues.\n\nI had already collected a line from the Rails log, telling me what controller handled my search request:\n\n```\nmethod=GET path=/api/v2/search format=html controller=Api::V2::SearchController action=index status=200 duration=1368.97 view=159.18 db=462.83\n```\n\nArmed with that information, I browsed to [search_controller.rb](https://github.com/mastodon/mastodon/blob/ef4d29c8791086b11f6e36aa121ff5c9b5fa0103/app/controllers/api/v2/search_controller.rb) and discovered that search results are collected by invoking [SearchService](https://github.com/mastodon/mastodon/blob/ef4d29c8791086b11f6e36aa121ff5c9b5fa0103/app/services/search_service.rb). At that point, I was ready to start running the code myself to find out what was going on.\n\nStep one: ssh to the VM running the Rails process.  \nStep two: edit the `Gemfile` to move `pry-rails` out of the development group.  \nStep three: run `bundle install`.  \nStep four: run `bin/rails c` to open a Rails console with Pry.  \nStep five: run `cd SearchService.new` to get into the right context.\n\nOnce I was inside an instance of `SearchService`, I could run the code I was reading in the `call` method by hand.\n\n```\n> @query = \"https://aus.social/@liam/109599234831423305\"\n> url_query? \n=> false\n```\n\nHmm. That's odd. Why would `url_query?` be false when the query is just a URL? Oh, looking at [the `url_query?` method](https://github.com/mastodon/mastodon/blob/ef4d29c8791086b11f6e36aa121ff5c9b5fa0103/app/services/search_service.rb#L75), it seems like `@resolve` has to be `true`. We can do that.\n\n```\n> @resolve = true\n> url_query?\n=> true\n> url_resource\n=> nil\n```\n\nWait, why is [`url_resource`](https://github.com/mastodon/mastodon/blob/ef4d29c8791086b11f6e36aa121ff5c9b5fa0103/app/services/search_service.rb#L82) returning `nil`? That doesn't make sense either, public requests to this URL return reasonable responses. I guess that means we need to look inside [`ResolveURLService`](https://github.com/mastodon/mastodon/blob/ef4d29c8791086b11f6e36aa121ff5c9b5fa0103/app/services/resolve_url_service.rb). \n\n```\n> cd ResolveURLService.new\n> @url = \"https://aus.social/@liam/109599234831423305\"\n> local_url?\n=> false\n> fetched_url\n=> nil\n```\n\nIt's fetching from the URL, and getting `nil`? That's downright bizzare. Okay, I guess that means we need to look into what happens inside [`FetchResourceService`](https://github.com/mastodon/mastodon/blob/ef4d29c8791086b11f6e36aa121ff5c9b5fa0103/app/services/fetch_resource_service.rb).\n\n```\n> cd FetchResourceService.new\n> @url = \"https://aus.social/@liam/109599234831423305\"\n> perform_request\n=> nil\n```\n\nOkay, that's just... what. Why is `perform_request` returning `nil`? What is this `Account.representative` thing that's getting added to this request?\n\n```\n> Account.representative\n=> #<Account:0x00007ffa66d5cb50\n id: -99,\n username: \"localhost:3000\",\n [...]>\n```\n\nWait. The `username` attribute is set to `localhost:3000`? That's not the name of my instance, and that's not even the value set in the `LOCAL_DOMAIN` env var. I guess I should fix that, and then try again.\n\n```\n> Account.representative.update!(username: ENV[\"LOCAL_DOMAIN\"])\n> perform_request\n=> nil\n```\n\nDamn. I guess that didn't fix it.\n\nWait. That method takes a block.\n\n```\n> perform_request{|r| r }\n=> #<HTTP::Response/1.1 200 OK {\"Date\"=>\"Sun, 01 Jan 2023 16:16:43 GMT\", \"Content-Type\"=>\"application/activity+json; charset=utf-8\", [...]}>\n```\n\nWait. That fixed it??? It works now???\n\nOne quick detour to the web UI and a single search later... yes, that did in fact fix it.\n\nSome instances seem to refuse requests made on behalf of `localhost:3000`. Maybe it's even in an instance blocklist, which would be pretty reasonable--it doesn't make sense to try to exchange updates with a server running on localhost.\n\nSure would be nice if there were some way to know this had happened, though. It turns out that any call to [`Account.representative`](https://github.com/mastodon/mastodon/blob/ef4d29c8791086b11f6e36aa121ff5c9b5fa0103/app/models/concerns/account_finder_concern.rb#L18) creates a new database record if there isn't one, and saves the currently set local domain into the database forever.\n\nWhere does the local domain come from, you ask? It is set by [the `hosts` initializer](https://github.com/mastodon/mastodon/blob/ef4d29c8791086b11f6e36aa121ff5c9b5fa0103/config/initializers/1_hosts.rb#L4), which reads from the env var `LOCAL_DOMAIN`, with a fallback to `localhost:3000` if the env var is unset.\n\nI booted the Rails server one time before setting `LOCAL_DOMAIN`, and didn't even create an account. Everything seemed like it worked perfectly until I stumbled across an instance that refuses requests on behalf of `localhost:3000`. And then it took four hours to debug.\n\nIt sure would be nice if Mastodon had some way to let you know that your `Account.representative` and your `LOCAL_DOMAIN` didn't match up, to save that four hours of debugging.\n\nWell, hopefully it won't take _you_ four hours to debug, since you found this post. Good luck!\n",
				"date_published": "2023-01-01T00:00:00-08:00",
				"url": "https://andre.arko.net/2023/01/01/mastodon-missing-posts-with-empty/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2022/11/14/selfhosted-mastodon-smtp-configuration/",
				"title": "Self-hosted Mastodon SMTP configuration",
				"content_html": "<p>Since <a href=\"https://masto.host\">masto.host</a> has signups disabled at the moment, I tried deploying Mastodon to Fly.io using tmm1&rsquo;s <a href=\"https://github.com/tmm1/flyapp-mastodon/\">flyapp-mastodon</a> instructions. It worked surprisingly well, by which I mean about half of the documented commands were using removed option flags, I had to debug sidekiq restarting in a different random region constantly, and email sending didn&rsquo;t work at all.</p>\n<p>I eventually got email sending working via <a href=\"https://postmarkapp.com\">Postmark</a>, but the Mastodon documentation is <em>extremely</em> unhelpful about how to configure sending emails: it just lists all possible ENV vars and then moves on.</p>\n<p>Here&rsquo;s the env var configuration that worked for sending emails from Mastodon through Postmark, for me:</p>\n<pre tabindex=\"0\"><code>SMTP_SERVER=&#34;smtp.postmarkapp.com&#34;\nSMTP_PORT=&#34;587&#34;\nSMTP_ENABLE_STARTTLS=&#34;always&#34;\nSMTP_LOGIN=&#34;&lt;Postmark SMTP Token Access Key&gt;&#34;\nSMTP_PASSWORD=&#34;&lt;Postmark SMTP Token Secret Key&gt;&#34;\nSMTP_FROM_ADDRESS=&#34;&lt;Postmark Sender Signature verified email address&gt;&#34;\n</code></pre><p>Hopefully that helps someone (or at very least future me) set up outgoing emails on a self-hosted Mastodon server in the future.</p>\n",
				"content_text": "\nSince [masto.host](https://masto.host) has signups disabled at the moment, I tried deploying Mastodon to Fly.io using tmm1's [flyapp-mastodon](https://github.com/tmm1/flyapp-mastodon/) instructions. It worked surprisingly well, by which I mean about half of the documented commands were using removed option flags, I had to debug sidekiq restarting in a different random region constantly, and email sending didn't work at all.\n\nI eventually got email sending working via [Postmark](https://postmarkapp.com), but the Mastodon documentation is _extremely_ unhelpful about how to configure sending emails: it just lists all possible ENV vars and then moves on.\n\nHere's the env var configuration that worked for sending emails from Mastodon through Postmark, for me:\n\n```\nSMTP_SERVER=\"smtp.postmarkapp.com\"\nSMTP_PORT=\"587\"\nSMTP_ENABLE_STARTTLS=\"always\"\nSMTP_LOGIN=\"<Postmark SMTP Token Access Key>\"\nSMTP_PASSWORD=\"<Postmark SMTP Token Secret Key>\"\nSMTP_FROM_ADDRESS=\"<Postmark Sender Signature verified email address>\"\n```\n\nHopefully that helps someone (or at very least future me) set up outgoing emails on a self-hosted Mastodon server in the future.\n",
				"date_published": "2022-11-14T00:00:00-08:00",
				"url": "https://andre.arko.net/2022/11/14/selfhosted-mastodon-smtp-configuration/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2022/10/26/sharing-an-apple-pro-xdr/",
				"title": "Sharing an Apple XDR or Studio Display between a Mac and PC",
				"content_html": "<p>During the last episode of monitor shenanigans, I figured out how to <a href=\"/2022/10/24/apple-studio-display-from-a-pc-without-thunderbolt\">connect my PC’s Nvidia GeForce GPU to an Apple Display</a>. The new challenge began when I realized that I wanted to switch back and forth between my M1 Mac and my PC by pushing a button, instead of by standing up, moving the monitor, and changing the cable plugged in to the back.</p>\n<p>After an enormous amount of research, it seemed like I would probably be able to use the <a href=\"https://cklkvm.com/products/ckl-2-port-usb-3-0-kvm-switch-displayport-1-4-8k-30hz-for-2-computers-1-monitor-pc-screen-keyboard-mouse-peripheral-audio-sharing-selector-box-with-all-cables-62dp-4\">CKL-62DP-4</a> KVM switch, which explicitly supports the DisplayPort 1.4 standard needed for 5k or 6k screens at 60hz. So I ordered the KVM, waited two weeks for it to arrive, and connected everything up using my existing cables. It even almost worked!</p>\n<p>I was able to switch an Apple Pro XDR Display and an Apple Studio Display between a 14&quot; MacBook Pro (M1 Max) and and a Mac Studio (M1 Ultra) using <a href=\"https://cklkvm.com/products/ckl-2-port-usb-3-0-kvm-switch-displayport-1-4-8k-30hz-for-2-computers-1-monitor-pc-screen-keyboard-mouse-peripheral-audio-sharing-selector-box-with-all-cables-62dp-4\">this CKL KVM</a>, using two generic USB-C to DisplayPort cables from the Macs, and sending the output to the Studio Display with <a href=\"https://rads.stackoverflow.com/amzn/click/com/B08BY78C42\">this bi-directional DisplayPort to USB-C cable</a>.</p>\n<p>The problem arose with one Mac and one PC—the one configuration that I actually want to use. Plugging in the PC works great. Plugging in the Mac works great. Switching from the Mac to the PC works great. Switching from the PC to the Mac… doesn’t work at all. The screen never comes back on, and the monitor has to be unplugged and replugged to start working again.</p>\n<p>Since I was never able to get the KVM to work with one Mac and one PC, I kept scouring the internet for things that might meet my needs. I eventually found <a href=\"https://rads.stackoverflow.com/amzn/click/com/B092VHC166\">this unknown brand USB-C switch</a>, with a lone review mentioning that it had worked to switch between a PC and a Mac. When it arrived, I connected the switch to my Mac with <a href=\"https://rads.stackoverflow.com/amzn/click/com/B07X31FG6Z\">this USB-C cable</a> and to the GTX 3080 in my PC with <a href=\"https://rads.stackoverflow.com/amzn/click/com/B07R1NBCXK\">this DisplayPort to USB-C cable</a>.</p>\n<p>Shockingly, it worked perfectly. After the amount of cables, devices, and configurations that I tried while attempting to get the CKL KVM to work, I had thought maybe that was impossible, so it was a big relief.</p>\n<p>Unfortunately, I couldn’t figure out a way to get the fancy Belkin cable that adds a USB connection (in addition the the DisplayPort) to work through the switch—the PC says there is a USB error and only the display works, with no speakers or camera.</p>\n<p>Since the switch doesn’t include a true KVM, you might need to also add a <a href=\"%5Bhttps://www.amazon.com/UGREEN-Selector-Computers-Peripheral-One-Button/dp/B01MXXQKGM%5D\">regular USB switch like this one</a> if you want to share a single keyboard and mouse between the two machines.</p>\n<p>That was enough to meet my needs, and so I have actually settled on that as my permanent setup, with no camera and speakers from the PC, since I have a different camera and speakers that I prefer to use.</p>\n<p><strong>Update</strong> you can now buy a <a href=\"https://www.store.level1techs.com/products/p/dp-repeater-hdmi-splitter-6sha9-yznx5-zm58w\">USB-C Combiner</a> and a <a href=\"https://store.level1techs.com/products/kvm-switch-usbc-model\">USB-C KVM</a> from <a href=\"https://level1techs.com\">LevelOneTechs</a> in the US. I haven&rsquo;t bought one, but it seems like exactly what I was trying for, and they explicitly state that the combiner and the KVM work together.</p>\n<p><strong>Update again</strong> you can&rsquo;t buy it yet, but Sabrent claims they will sell a <a href=\"https://sabrent.com/products/sb-tb4k\">Thunderbolt 4 KVM</a> soon, thanks <a href=\"https://tapbots.social/@paul/110403339218941998\">@paul</a> for the link!</p>\n<p><strong>Update again again</strong> I bought the Sabrent Thunderbolt 4 KVM and built a new PC with a GTX 4080 and ASUS ROG Strix Z790-i with Thunderbolt. Switching my Thunderbolt display between Mac and PC is now reliable and consistent, hooray.</p>\n",
				"content_text": "\nDuring the last episode of monitor shenanigans, I figured out how to [connect my PC’s Nvidia GeForce GPU to an Apple Display](/2022/10/24/apple-studio-display-from-a-pc-without-thunderbolt). The new challenge began when I realized that I wanted to switch back and forth between my M1 Mac and my PC by pushing a button, instead of by standing up, moving the monitor, and changing the cable plugged in to the back.\n\nAfter an enormous amount of research, it seemed like I would probably be able to use the [CKL-62DP-4](https://cklkvm.com/products/ckl-2-port-usb-3-0-kvm-switch-displayport-1-4-8k-30hz-for-2-computers-1-monitor-pc-screen-keyboard-mouse-peripheral-audio-sharing-selector-box-with-all-cables-62dp-4) KVM switch, which explicitly supports the DisplayPort 1.4 standard needed for 5k or 6k screens at 60hz. So I ordered the KVM, waited two weeks for it to arrive, and connected everything up using my existing cables. It even almost worked!\n\nI was able to switch an Apple Pro XDR Display and an Apple Studio Display between a 14\" MacBook Pro (M1 Max) and and a Mac Studio (M1 Ultra) using [this CKL KVM](https://cklkvm.com/products/ckl-2-port-usb-3-0-kvm-switch-displayport-1-4-8k-30hz-for-2-computers-1-monitor-pc-screen-keyboard-mouse-peripheral-audio-sharing-selector-box-with-all-cables-62dp-4), using two generic USB-C to DisplayPort cables from the Macs, and sending the output to the Studio Display with [this bi-directional DisplayPort to USB-C cable](https://rads.stackoverflow.com/amzn/click/com/B08BY78C42).\n\nThe problem arose with one Mac and one PC—the one configuration that I actually want to use. Plugging in the PC works great. Plugging in the Mac works great. Switching from the Mac to the PC works great. Switching from the PC to the Mac… doesn’t work at all. The screen never comes back on, and the monitor has to be unplugged and replugged to start working again.\n\nSince I was never able to get the KVM to work with one Mac and one PC, I kept scouring the internet for things that might meet my needs. I eventually found [this unknown brand USB-C switch](https://rads.stackoverflow.com/amzn/click/com/B092VHC166), with a lone review mentioning that it had worked to switch between a PC and a Mac. When it arrived, I connected the switch to my Mac with [this USB-C cable](https://rads.stackoverflow.com/amzn/click/com/B07X31FG6Z) and to the GTX 3080 in my PC with [this DisplayPort to USB-C cable](https://rads.stackoverflow.com/amzn/click/com/B07R1NBCXK).\n\nShockingly, it worked perfectly. After the amount of cables, devices, and configurations that I tried while attempting to get the CKL KVM to work, I had thought maybe that was impossible, so it was a big relief.\n\nUnfortunately, I couldn’t figure out a way to get the fancy Belkin cable that adds a USB connection (in addition the the DisplayPort) to work through the switch—the PC says there is a USB error and only the display works, with no speakers or camera.\n\nSince the switch doesn’t include a true KVM, you might need to also add a [regular USB switch like this one]([https://www.amazon.com/UGREEN-Selector-Computers-Peripheral-One-Button/dp/B01MXXQKGM]) if you want to share a single keyboard and mouse between the two machines.\n\nThat was enough to meet my needs, and so I have actually settled on that as my permanent setup, with no camera and speakers from the PC, since I have a different camera and speakers that I prefer to use.\n\n**Update** you can now buy a [USB-C Combiner](https://www.store.level1techs.com/products/p/dp-repeater-hdmi-splitter-6sha9-yznx5-zm58w) and a [USB-C KVM](https://store.level1techs.com/products/kvm-switch-usbc-model) from [LevelOneTechs](https://level1techs.com) in the US. I haven't bought one, but it seems like exactly what I was trying for, and they explicitly state that the combiner and the KVM work together.\n\n**Update again** you can't buy it yet, but Sabrent claims they will sell a [Thunderbolt 4 KVM](https://sabrent.com/products/sb-tb4k) soon, thanks [@paul](https://tapbots.social/@paul/110403339218941998) for the link!\n\n**Update again again** I bought the Sabrent Thunderbolt 4 KVM and built a new PC with a GTX 4080 and ASUS ROG Strix Z790-i with Thunderbolt. Switching my Thunderbolt display between Mac and PC is now reliable and consistent, hooray.\n",
				"date_published": "2022-10-26T00:00:00-08:00",
				"url": "https://andre.arko.net/2022/10/26/sharing-an-apple-pro-xdr/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2022/10/24/apple-studio-display-from-a/",
				"title": "Apple Studio Display from a PC without Thunderbolt",
				"content_html": "<p>So maybe you have an Apple Studio Display (or possibly even an Apple Pro XDR Display), and you followed <a href=\"/2022/05/19/use-a-thunderbolt-3-display-from-a-windows-pc/\">my instructions from last time</a> to connect your display to your PC. You may have noticed a shortcoming: it’s impossible to adjust the brightness of the monitor from your Windows PC. Or maybe you noticed that the built-in webcam and speakers on your Studio Display don’t do anything.</p>\n<p>In theory, USB-C can provide both DisplayPort for a monitor and a USB channel for software brightness control, the monitor’s USB hub, camera, speakers, etc. In practice, there seems to be only one cable in the world that is a single combined USB-C on one end and DisplayPort 1.4 + USB on the other end. That cable is the <a href=\"https://www.belkin.com/us/support-article?articleNum=316883\">Belkin Charge and Sync Cable for Huawei VR Glass</a>. Since the Huawei VR Glass is only sold in China, that cable is also only sold in China.</p>\n<p>Several visits to Ali Express and other generic commerce websites later, I spent $90 on this cable plus shipping and waited just over three weeks for it to arrive. The outcome, though, is surprisingly solid.</p>\n<p>After I downloaded the latest Boot Camp image using <a href=\"https://github.com/ninxsoft/Bombardier\">Bombardier</a> and then followed <a href=\"https://nielsleenheer.com/articles/2022/using-the-apple-studio-display-on-a-windows-machine/\">these instructions to install the Boot Camp drivers</a>, the Apple Studio Display not only had working brightness controls, it worked perfectly as both speakers and camera for the PC.</p>\n<p>If your PC doesn’t have a Thunderbolt card, this seems like a genuinely good option to get full resolution and hardware support for Apple displays on your PC. It also seems like it&rsquo;s probably the only option?</p>\n<p><strong>Update</strong> there is now another option, the <a href=\"https://www.store.level1techs.com/products/p/dp-repeater-hdmi-splitter-6sha9-yznx5-zm58w\">USB-C Combiner from LevelOneTechs</a>. You can just buy a box that does the thing, shipped from the US, for $84. No need to wait a month for shipping from AliExpress!</p>\n<p><strong>Update again</strong> eventually, I bought a <a href=\"https://sabrent.com/products/sb-tb4k\">Sabrent Thunderbolt 4 KVM</a> and built a new PC with a GTX 4080 and ASUS ROG Strix Z790-i, which has internal GPU passthrough for the Thunderbolt ports. Switching my Thunderbolt display between Mac and PC is now reliable and consistent. Hooray.</p>\n",
				"content_text": "\nSo maybe you have an Apple Studio Display (or possibly even an Apple Pro XDR Display), and you followed [my instructions from last time](/2022/05/19/use-a-thunderbolt-3-display-from-a-windows-pc/) to connect your display to your PC. You may have noticed a shortcoming: it’s impossible to adjust the brightness of the monitor from your Windows PC. Or maybe you noticed that the built-in webcam and speakers on your Studio Display don’t do anything.\n\nIn theory, USB-C can provide both DisplayPort for a monitor and a USB channel for software brightness control, the monitor’s USB hub, camera, speakers, etc. In practice, there seems to be only one cable in the world that is a single combined USB-C on one end and DisplayPort 1.4 + USB on the other end. That cable is the [Belkin Charge and Sync Cable for Huawei VR Glass](https://www.belkin.com/us/support-article?articleNum=316883). Since the Huawei VR Glass is only sold in China, that cable is also only sold in China.\n\nSeveral visits to Ali Express and other generic commerce websites later, I spent $90 on this cable plus shipping and waited just over three weeks for it to arrive. The outcome, though, is surprisingly solid.\n\nAfter I downloaded the latest Boot Camp image using [Bombardier](https://github.com/ninxsoft/Bombardier) and then followed [these instructions to install the Boot Camp drivers](https://nielsleenheer.com/articles/2022/using-the-apple-studio-display-on-a-windows-machine/), the Apple Studio Display not only had working brightness controls, it worked perfectly as both speakers and camera for the PC.\n\nIf your PC doesn’t have a Thunderbolt card, this seems like a genuinely good option to get full resolution and hardware support for Apple displays on your PC. It also seems like it's probably the only option?\n\n**Update** there is now another option, the [USB-C Combiner from LevelOneTechs](https://www.store.level1techs.com/products/p/dp-repeater-hdmi-splitter-6sha9-yznx5-zm58w). You can just buy a box that does the thing, shipped from the US, for $84. No need to wait a month for shipping from AliExpress!\n\n**Update again** eventually, I bought a [Sabrent Thunderbolt 4 KVM](https://sabrent.com/products/sb-tb4k) and built a new PC with a GTX 4080 and ASUS ROG Strix Z790-i, which has internal GPU passthrough for the Thunderbolt ports. Switching my Thunderbolt display between Mac and PC is now reliable and consistent. Hooray.\n",
				"date_published": "2022-10-24T00:00:00-08:00",
				"url": "https://andre.arko.net/2022/10/24/apple-studio-display-from-a/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2022/05/24/jekyll-in-pumadev-with-live/",
				"title": "Jekyll in puma-dev with live reload",
				"content_html": "<p>It isn&rsquo;t a secret (and probably not even interesting) that this blog has used <a href=\"https://jekyllrb.com\">Jekyll</a> for many years. Almost as many years ago, I discovered <a href=\"https://github.com/puma/puma-dev\">puma-dev</a> and used it to set up all of the Rails and Sinatra apps that I work on. I don’t have to start or stop any local development servers, I just browse to <code>appname.local</code>, and a Puma worker will start up and let me see the app. It even handles SSL, which is very handy.</p>\n<p>Ever since I set up puma-dev for the apps I work on, it has annoyed me that I can’t do that for my blog. Instead, Jekyll requires me to actively start a server to see a local preview, and if I close that terminal window I’m just out of luck.</p>\n<p>I believe it was originally written to let you host your Jekyll blog on Heroku without a build step, but the <a href=\"https://github.com/adaoraul/rack-jekyll\"><code>rack-jekyll</code> gem</a> turns out to be exactly what I wanted: I can open <a href=\"https://blog.test\">https://blog.test</a> and see a freshly-rendered copy of my blog. The trick is setting up a <code>config.ru</code> for Puma to use:</p>\n<pre><code>require &quot;bundler/setup&quot;\nrequire &quot;rack/jekyll&quot;\nrun Rack::Jekyll.new(auto: true, future: true)\n</code></pre>\n<p>Once you have that file, run <code>bundle add rack-jekyll; bundle add puma; puma-dev link blog</code>, and you’re off to the races.</p>\n<p>Okay, great, I hear you saying, but what about the live reload you promised? That turns out to be a little bit trickier. I wound up having to write a Jekyll plugin to run the livereload server. But it works! Here’s the setup:</p>\n<ol>\n<li>\n<p>Run <code>bundle add rack-livereload</code>.</p>\n</li>\n<li>\n<p>Download <a href=\"/2022/05/24/jekyll-in-puma-dev-with-live-reload/live_reload_server.rb\"><code>live_reload_server.rb</code></a> and copy it into <code>_plugins</code>. You might have to <code>mkdir _plugins</code> first.</p>\n</li>\n<li>\n<p>Update your <code>config.ru</code> file to include <code>rack-livereload</code> and the Jekyll plugin:\nrequire &ldquo;bundler/setup&rdquo;\nrequire &ldquo;rack/jekyll&rdquo;\nrequire &ldquo;rack-livereload&rdquo;</p>\n<pre><code> require_relative &quot;_plugins/live_reload_server&quot;\n Jekyll::LiveReloadServer.start!\n\n use Rack::LiveReload\n run Rack::Jekyll.new(auto: true, future: true)\n</code></pre>\n</li>\n<li>\n<p>Restart Puma by running <code>touch tmp/restart.txt</code>.</p>\n</li>\n<li>\n<p>Reload your Jekyll site in your browser.</p>\n</li>\n<li>\n<p>Edit your markdown files and luxuriate in the bliss of watching them reload automatically in your browser as you save.</p>\n</li>\n</ol>\n",
				"content_text": "It isn't a secret (and probably not even interesting) that this blog has used [Jekyll](https://jekyllrb.com) for many years. Almost as many years ago, I discovered [puma-dev](https://github.com/puma/puma-dev) and used it to set up all of the Rails and Sinatra apps that I work on. I don’t have to start or stop any local development servers, I just browse to `appname.local`, and a Puma worker will start up and let me see the app. It even handles SSL, which is very handy.\n\nEver since I set up puma-dev for the apps I work on, it has annoyed me that I can’t do that for my blog. Instead, Jekyll requires me to actively start a server to see a local preview, and if I close that terminal window I’m just out of luck.\n\nI believe it was originally written to let you host your Jekyll blog on Heroku without a build step, but the [`rack-jekyll` gem](https://github.com/adaoraul/rack-jekyll) turns out to be exactly what I wanted: I can open https://blog.test and see a freshly-rendered copy of my blog. The trick is setting up a `config.ru` for Puma to use:\n\n\trequire \"bundler/setup\"\n\trequire \"rack/jekyll\"\n\trun Rack::Jekyll.new(auto: true, future: true)\n\nOnce you have that file, run `bundle add rack-jekyll; bundle add puma; puma-dev link blog`, and you’re off to the races.\n\nOkay, great, I hear you saying, but what about the live reload you promised? That turns out to be a little bit trickier. I wound up having to write a Jekyll plugin to run the livereload server. But it works! Here’s the setup:\n\n1. Run `bundle add rack-livereload`.\n2. Download <a href=\"/2022/05/24/jekyll-in-puma-dev-with-live-reload/live_reload_server.rb\">`live_reload_server.rb`</a> and copy it into `_plugins`. You might have to `mkdir _plugins` first.\n3. Update your `config.ru` file to include `rack-livereload` and the Jekyll plugin:\n\t\trequire \"bundler/setup\"\n\t\trequire \"rack/jekyll\"\n\t\trequire \"rack-livereload\"\n\t\t\n\t\trequire_relative \"_plugins/live_reload_server\"\n\t\tJekyll::LiveReloadServer.start!\n\t\t\n\t\tuse Rack::LiveReload\n\t\trun Rack::Jekyll.new(auto: true, future: true)\n4. Restart Puma by running `touch tmp/restart.txt`.\n5. Reload your Jekyll site in your browser.\n6. Edit your markdown files and luxuriate in the bliss of watching them reload automatically in your browser as you save.\n",
				"date_published": "2022-05-24T00:00:00-08:00",
				"url": "https://andre.arko.net/2022/05/24/jekyll-in-pumadev-with-live/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2022/05/19/use-a-thunderbolt-display-from/",
				"title": "Use a Thunderbolt 3 display from Windows",
				"content_html": "<p>Since 2016, when the <a href=\"https://www.anandtech.com/show/10798/lg-introduces-new-4k-and-5k-ultrafine-monitors\">LG UltraFine 5k was announced</a>, the only high-DPI 5k screens have been for (or inside) Macs. The external displays, all three of them, have used Thunderbolt 3. There have been some non-Thunderbolt 5k panels that are ultra widescreen, but none of them have been the high-quality high-DPI screen I wanted.</p>\n<p>With the field narrowed down to the UltraFine 5k, the <a href=\"https://www.apple.com/pro-display-xdr/\">Apple Pro Display XDR</a>, and the <a href=\"https://www.apple.com/studio-display/\">Apple Studio Display</a>, I spent a lot of time wildly searching the internet trying to figure out some way to also use the monitor with my gaming PC. Everything I could find from searching basically boiled down to either “lol it’s impossible” or “you have to add a PCIe Thunderbolt card to your PC”.</p>\n<p>I eventually found a passing reference somewhere on Twitter to “bi-directional” DisplayPort to USB-C cables, and that turned out to be the miracle I was looking for. With <a href=\"https://amzn.to/39WrlRp\">a bi-directionable cable that supports DisplayPort 1.4</a>, you can drive an UltraFine 5k, an Apple Studio Display, or even an Apple Pro Display XDR at 6k and 60hz, directly from your graphics card’s DisplayPort port. The magic is provided by Display Stream Compression (DSC), which reduces the bandwidth needed down to a level that USB-C can sustain.</p>\n<p>I turns out someone had <a href=\"https://ntk.me/2021/10/12/pro-display-xdr-on-windows-pc/\">already written a blog post about this</a>, but I wasn’t able to find it when I was searching. Unlike that blog post, my simple one-cable connection also doesn’t support using the displays’ internal USB hub, or using any speakers or cameras that might be attached. I’m happy with that because I already have other speakers and a camera set up.</p>\n<p>In the end, this post is partly a note so I can look this up later if I need to, and partly to try to spread the word that bidirectional DisplayPort cables exist and work with 5k and 6k displays. Hopefully future me, and others with this problem, can spend fewer days searching the internet and just <a href=\"https://amzn.to/39WrlRp\">buy the cable</a> and get on with their lives.</p>\n<p><strong>Update</strong> you can now just <a href=\"https://www.store.level1techs.com/products/p/dp-repeater-hdmi-splitter-6sha9-yznx5-zm58w\">buy a DisplayPort and USB to USB-C box</a> that not only works like a bi-directional cable, but also adds USB communication between your PC and monitor, for brightness control, speakers, and camera access.</p>\n",
				"content_text": "Since 2016, when the [LG UltraFine 5k was announced](https://www.anandtech.com/show/10798/lg-introduces-new-4k-and-5k-ultrafine-monitors), the only high-DPI 5k screens have been for (or inside) Macs. The external displays, all three of them, have used Thunderbolt 3. There have been some non-Thunderbolt 5k panels that are ultra widescreen, but none of them have been the high-quality high-DPI screen I wanted.\n\nWith the field narrowed down to the UltraFine 5k, the [Apple Pro Display XDR](https://www.apple.com/pro-display-xdr/), and the [Apple Studio Display](https://www.apple.com/studio-display/), I spent a lot of time wildly searching the internet trying to figure out some way to also use the monitor with my gaming PC. Everything I could find from searching basically boiled down to either “lol it’s impossible” or “you have to add a PCIe Thunderbolt card to your PC”.\n\nI eventually found a passing reference somewhere on Twitter to “bi-directional” DisplayPort to USB-C cables, and that turned out to be the miracle I was looking for. With [a bi-directionable cable that supports DisplayPort 1.4](https://amzn.to/39WrlRp), you can drive an UltraFine 5k, an Apple Studio Display, or even an Apple Pro Display XDR at 6k and 60hz, directly from your graphics card’s DisplayPort port. The magic is provided by Display Stream Compression (DSC), which reduces the bandwidth needed down to a level that USB-C can sustain.\n\nI turns out someone had [already written a blog post about this](https://ntk.me/2021/10/12/pro-display-xdr-on-windows-pc/), but I wasn’t able to find it when I was searching. Unlike that blog post, my simple one-cable connection also doesn’t support using the displays’ internal USB hub, or using any speakers or cameras that might be attached. I’m happy with that because I already have other speakers and a camera set up.\n\nIn the end, this post is partly a note so I can look this up later if I need to, and partly to try to spread the word that bidirectional DisplayPort cables exist and work with 5k and 6k displays. Hopefully future me, and others with this problem, can spend fewer days searching the internet and just [buy the cable](https://amzn.to/39WrlRp) and get on with their lives.\n\n**Update** you can now just [buy a DisplayPort and USB to USB-C box](https://www.store.level1techs.com/products/p/dp-repeater-hdmi-splitter-6sha9-yznx5-zm58w) that not only works like a bi-directional cable, but also adds USB communication between your PC and monitor, for brightness control, speakers, and camera access.\n",
				"date_published": "2022-05-19T00:00:00-08:00",
				"url": "https://andre.arko.net/2022/05/19/use-a-thunderbolt-display-from/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2022/05/15/automatic-dependabot-merges/",
				"title": "Automatic Dependabot merges",
				"content_html": "<p>I&rsquo;ve been using <a href=\"https://github.com/dependabot\">Dependabot</a> for a long time. Back before GitHub bought it and took away the web dashboard, there was an amazing, glorious, wonderful feature: you could check a checkbox, and Dependabot would merge the open PR as soon as your tests passed.</p>\n<p>Now that Dependabot has no web dashboard, and can&rsquo;t be added to a repo with one click, it has also lost the ability to automatically merge updates.</p>\n<p>After several days of copying and pasting from blog posts and then troubleshooting YAML syntax, I am here to report that one of those three things can be brought back! (If you run your tests in GitHub actions, anyway.)</p>\n<p>Here&rsquo;s what the automerge GitHub action looks like:</p>\n<p>{% raw %}</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-YAML\" data-lang=\"YAML\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># .github/workflows/merge-dependabot.yml</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#f92672\">name</span>: <span style=\"color:#e6db74\">&#34;Merge updates&#34;</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#f92672\">on</span>:\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#f92672\">workflow_run</span>:\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#f92672\">workflows</span>: [<span style=\"color:#e6db74\">&#34;CI&#34;</span>]\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#f92672\">types</span>: [<span style=\"color:#e6db74\">&#34;completed&#34;</span>]\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#f92672\">branches</span>: [<span style=\"color:#e6db74\">&#34;dependabot/**&#34;</span>]\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#f92672\">jobs</span>:\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#f92672\">merge</span>:\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#f92672\">name</span>: <span style=\"color:#e6db74\">&#34;Merge&#34;</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#f92672\">runs-on</span>: <span style=\"color:#e6db74\">&#34;ubuntu-latest&#34;</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#f92672\">if</span>: &gt;<span style=\"color:#e6db74\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">      github.event.workflow_run.event == &#39;pull_request&#39; &amp;&amp;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">      github.event.workflow_run.conclusion == &#39;success&#39; &amp;&amp;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">      github.actor == &#39;dependabot[bot]&#39;</span>      \n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#f92672\">steps</span>:\n</span></span><span style=\"display:flex;\"><span>      - <span style=\"color:#f92672\">name</span>: <span style=\"color:#e6db74\">&#34;Merge pull request&#34;</span>\n</span></span><span style=\"display:flex;\"><span>        <span style=\"color:#f92672\">uses</span>: <span style=\"color:#e6db74\">&#34;actions/github-script@v6&#34;</span>\n</span></span><span style=\"display:flex;\"><span>        <span style=\"color:#f92672\">with</span>:\n</span></span><span style=\"display:flex;\"><span>          <span style=\"color:#f92672\">github-token</span>: <span style=\"color:#e6db74\">&#34;${{ secrets.GITHUB_TOKEN }}&#34;</span>\n</span></span><span style=\"display:flex;\"><span>          <span style=\"color:#f92672\">script</span>: |<span style=\"color:#e6db74\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">            const pullRequest = context.payload.workflow_run.pull_requests[0]\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">            const repository = context.repo\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">            await github.rest.pulls.merge({\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">              merge_method: &#34;merge&#34;,\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">              owner: repository.owner,\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">              pull_number: pullRequest.number,\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">              repo: repository.repo,\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">            })</span>            \n</span></span></code></pre></div><p>{% endraw %}</p>\n<p>If your CI GitHub action is named something besides &ldquo;CI&rdquo;, you&rsquo;ll need to put your job&rsquo;s name in the fourth line.</p>\n<p>Happy automerging!</p>\n",
				"content_text": "I've been using [Dependabot](https://github.com/dependabot) for a long time. Back before GitHub bought it and took away the web dashboard, there was an amazing, glorious, wonderful feature: you could check a checkbox, and Dependabot would merge the open PR as soon as your tests passed.\n\nNow that Dependabot has no web dashboard, and can't be added to a repo with one click, it has also lost the ability to automatically merge updates.\n\nAfter several days of copying and pasting from blog posts and then troubleshooting YAML syntax, I am here to report that one of those three things can be brought back! (If you run your tests in GitHub actions, anyway.)\n\nHere's what the automerge GitHub action looks like:\n\n{% raw %}\n```YAML\n# .github/workflows/merge-dependabot.yml\nname: \"Merge updates\"\non:\n  workflow_run:\n    workflows: [\"CI\"]\n    types: [\"completed\"]\n    branches: [\"dependabot/**\"]\njobs:\n  merge:\n    name: \"Merge\"\n    runs-on: \"ubuntu-latest\"\n    if: >\n      github.event.workflow_run.event == 'pull_request' &&\n      github.event.workflow_run.conclusion == 'success' &&\n      github.actor == 'dependabot[bot]'\n    steps:\n      - name: \"Merge pull request\"\n        uses: \"actions/github-script@v6\"\n        with:\n          github-token: \"${{ secrets.GITHUB_TOKEN }}\"\n          script: |\n            const pullRequest = context.payload.workflow_run.pull_requests[0]\n            const repository = context.repo\n            await github.rest.pulls.merge({\n              merge_method: \"merge\",\n              owner: repository.owner,\n              pull_number: pullRequest.number,\n              repo: repository.repo,\n            })\n```\n{% endraw %}\n\nIf your CI GitHub action is named something besides \"CI\", you'll need to put your job's name in the fourth line.\n\nHappy automerging!\n",
				"date_published": "2022-05-15T00:00:00-08:00",
				"url": "https://andre.arko.net/2022/05/15/automatic-dependabot-merges/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2022/05/11/ruby-how-to-make-a/",
				"title": "Ruby55: how to make a gem in under 60 seconds",
				"content_html": "<p>Tell me if you&rsquo;ve heard this one before: a flying squirrel walks into a terminal, and shows you how to create a gem in 55 seconds.</p>\n<p>I&rsquo;ve been meaning to post about this for 18 months now, but my <a href=\"https://cloudcity.io\">Cloud City</a> coworker <a href=\"https://twitter.com/brendanpgh\">Brendan Miller</a> offered to turn a Bundler tutorial into a video containing me as an an animated flying squirrel. How could I refuse?</p>\n<p>Just in case you needed a video explaning how to use Bundler to create a new gem, here you go!</p>\n<br>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/NNBiwZ1Cbso\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
				"content_text": "\nTell me if you've heard this one before: a flying squirrel walks into a terminal, and shows you how to create a gem in 55 seconds.\n\nI've been meaning to post about this for 18 months now, but my [Cloud City](https://cloudcity.io) coworker [Brendan Miller](https://twitter.com/brendanpgh) offered to turn a Bundler tutorial into a video containing me as an an animated flying squirrel. How could I refuse?\n\nJust in case you needed a video explaning how to use Bundler to create a new gem, here you go!\n\n<br>\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/NNBiwZ1Cbso\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
				"date_published": "2022-05-11T00:00:00-08:00",
				"url": "https://andre.arko.net/2022/05/11/ruby-how-to-make-a/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2022/04/16/instant-rails-ci-for-github/",
				"title": "Instant Rails CI for GitHub Actions (finally)",
				"content_html": "<p>A couple of months ago, I <a href=\"/2022/02/17/feedyouremail/\">built a webapp from scratch using Rails 7</a>. In addition to leaving Node out of my Rails app, I wanted to use GitHub Actions as the only CI for a real webapp that other people use.</p>\n<p>GitHub Actions was <a href=\"https://techcrunch.com/2018/10/16/github-launches-actions-its-workflow-automation-tool/\">first introduced in 2018</a>, and <a href=\"https://github.blog/2021-12-17-getting-started-with-github-actions-just-got-easier/\">one-click CI for GitHub Actions</a> was introduced in 2021. Because that is literally years in the past, I assumed that it would be straightforward to set up a Rails app to automatically run tests inside GHA. Nothing could have been further from the truth.</p>\n<p>I started where you might expect, going to <code>/actions/new</code> inside my repo and searching the GitHub-provided starter actions for “Rails”. Somehow, even though Django and Laravel apps had one-click CI, the Rails starter workflow… runs Rubocop. And Brakeman. And no tests. I guess linting is better than nothing, but that is extremely not CI, and definitely does not help me know if it’s safe to deploy my app automatically.</p>\n<p>Annoyed, I went to search for a public GitHub Action file that I could copy and paste into my project. Somehow, all the blog posts I could find were about the beta version of GitHub Actions, and none of the YAML files actually worked. On top of that, every single blog post told me I would need to make changes to my app’s code so that my tests would run. I know enough about Rails to know that is absolutely not true, but apparently no one who knew that ever wrote about it somewhere easily findable with a search engine.</p>\n<p>At that point I was annoyed all the way into action: I frankenstiened together three different blog posts and then started deleting everything that seemed unneeded or would require changes to my code. By the time I was done, I had a <a href=\"https://github.com/actions/starter-workflows/blob/main/ci/rubyonrails.yml\">pretty concise YAML file</a> that 1) worked with any Rails app out of the box, 2) supported both minitest and rspec without any changes, and 3) ran security checks and lints in a second parallel job, so everything finished faster.</p>\n<p>Once I had a working action, my first thought was to simply post it here… but then I realized I could fix the one-click workflow offered by GitHub itself to actually run tests instead of just linters! So I went and <a href=\"https://github.com/actions/starter-workflows/pull/1353\">spent four days getting my pull request accepted</a>, and now there’s a gloriously straightforward two-click way to test your Rails application with GitHub Actions. Phew.</p>\n<img src=\"https://indirect.micro.blog/uploads/2025/b07ff1db8c.jpg\">\n",
				"content_text": "\nA couple of months ago, I [built a webapp from scratch using Rails 7](/2022/02/17/feedyouremail/). In addition to leaving Node out of my Rails app, I wanted to use GitHub Actions as the only CI for a real webapp that other people use.\n\nGitHub Actions was [first introduced in 2018](https://techcrunch.com/2018/10/16/github-launches-actions-its-workflow-automation-tool/), and [one-click CI for GitHub Actions](https://github.blog/2021-12-17-getting-started-with-github-actions-just-got-easier/) was introduced in 2021. Because that is literally years in the past, I assumed that it would be straightforward to set up a Rails app to automatically run tests inside GHA. Nothing could have been further from the truth.\n\nI started where you might expect, going to `/actions/new` inside my repo and searching the GitHub-provided starter actions for “Rails”. Somehow, even though Django and Laravel apps had one-click CI, the Rails starter workflow… runs Rubocop. And Brakeman. And no tests. I guess linting is better than nothing, but that is extremely not CI, and definitely does not help me know if it’s safe to deploy my app automatically.\n\nAnnoyed, I went to search for a public GitHub Action file that I could copy and paste into my project. Somehow, all the blog posts I could find were about the beta version of GitHub Actions, and none of the YAML files actually worked. On top of that, every single blog post told me I would need to make changes to my app’s code so that my tests would run. I know enough about Rails to know that is absolutely not true, but apparently no one who knew that ever wrote about it somewhere easily findable with a search engine.\n\nAt that point I was annoyed all the way into action: I frankenstiened together three different blog posts and then started deleting everything that seemed unneeded or would require changes to my code. By the time I was done, I had a [pretty concise YAML file](https://github.com/actions/starter-workflows/blob/main/ci/rubyonrails.yml) that 1) worked with any Rails app out of the box, 2) supported both minitest and rspec without any changes, and 3) ran security checks and lints in a second parallel job, so everything finished faster.\n\nOnce I had a working action, my first thought was to simply post it here… but then I realized I could fix the one-click workflow offered by GitHub itself to actually run tests instead of just linters! So I went and [spent four days getting my pull request accepted](https://github.com/actions/starter-workflows/pull/1353), and now there’s a gloriously straightforward two-click way to test your Rails application with GitHub Actions. Phew.\n\n<img src=\"https://indirect.micro.blog/uploads/2025/b07ff1db8c.jpg\">\n",
				"date_published": "2022-04-16T00:00:00-08:00",
				"url": "https://andre.arko.net/2022/04/16/instant-rails-ci-for-github/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2022/03/13/parsing-logs-faster-with-rust/",
				"title": "Parsing logs faster with Rust, revisited",
				"content_html": "<p>A few years ago, I <a href=\"/2018/10/25/parsing-logs-230x-faster-with-rust/\">wrote about parsing logs 230x faster with Rust</a>, and then <a href=\"/2019/01/11/parsing-logs-faster-with-rust-continued/\">followed up with some improvements</a>.</p>\n<p>Thanks to <a href=\"https://github.com/rubytogether/kirby/pull/20\">a contributor adding IP address deduplication</a>, I&rsquo;ve had to update, build, and deploy it again. As a result, I learned some new stuff about: <code>rustc</code> inside <code>qemu</code>, Rust LTO (link-time optimization) with musl, profile-guided optimizations, and just how fast M1 Max MacBook Pros are.</p>\n<h3 id=\"rustc-segfaults-in-dockers-qemu\"><code>rustc</code> segfaults in Docker&rsquo;s <code>qemu</code></h3>\n<p>The (somewhat hacky) AWS Lambda setup I am using predates support for Rust. Instead, I tell AWS it&rsquo;s a Go binary, and then upload a Rust binary. Binaries running in Lambda&rsquo;s &ldquo;Go&rdquo; mode need to be compiled against <a href=\"http://musl.libc.org\">musl libc</a> (probably because Lambda&rsquo;s runtime is built on top of Alpine Linux, but I don&rsquo;t know for sure). Back in 2018, I couldn&rsquo;t figure out how to get Rust to cross-compile from macOS to Linux with musl, so I used a Docker image with a cross-compiler built in.</p>\n<p>The first problem I ran into while updating the project was&hellip; not having an x86 machine anymore. My M1 Max MacBook Pro is really great, but the Docker image I was using to cross-compile only exists in x86 flavor. (Docker calls it &ldquo;amd64&rdquo;.)</p>\n<p>To run x86 images on the ARM-based M1 Macs, Docker ships a copy of <code>qemu</code> inside, to run the container on an emulated x86. Unfortunately, <a href=\"https://github.com/rust-lang/rust/issues/80346\"><code>rustc</code> segfaults inside Docker&rsquo;s <code>qemu</code></a>. With no idea how to resolve that almost-two-year-old problem, I gave up and decided to use an old Intel Mac to build the new version, at least for the time being. If I was able to get it working, I figured I could set up GitHub Actions to build it for me or something.</p>\n<h3 id=\"rust-link-time-optimization-with-musl\">Rust link-time optimization with musl</h3>\n<p>Once I had burned a day or two troubleshooting the insurmountable segfault and given up, I ran into a new problem. In 2018, a contributor <a href=\"https://github.com/rubytogether/kirby/commit/74b3d81b0827bd0674eba3ef32cf0223b5756e02\">enabled LTO for a huge 25% speedup</a>. It clearly worked at the time, but it seems  the Docker-based cross-compile had stopped working <a href=\"https://github.com/rubytogether/kirby/issues/16\">by November 2019</a>. As far as I can tell, the problem is exclusive to using both LTO and musl at the same time. Inside the Docker container, LTO works with glibc, and musl works without LTO&hellip; but you can&rsquo;t have both together.</p>\n<p>Fortunately, Rust cross-compilation has improved a lot in the last few years. Thanks to <a href=\"https://github.com/messense/homebrew-macos-cross-toolchains\">macos-cross-toolchains</a> and <a href=\"https://github.com/cross-rs/cross\">cross</a>, I was able to ditch the Docker setup entirely and compile for x86 Linux with musl directly from macOS on ARM. (Well, once I got it configured. Which required synthesizing together three separate blog posts, since there&rsquo;s no documentation that explained what I needed.) I&rsquo;m super impressed that <code>cross</code> works so well, and not just from macOS on my laptop but also from Linux on GitHub Actions.</p>\n<h3 id=\"profile-guided-optimizations\">Profile-guided optimizations</h3>\n<p>Finally, I learned that even the previously absurd speeds that I was getting can be soundly beaten via modern hardware and compilation techniques.</p>\n<p>At <a href=\"/2019/01/11/parsing-logs-faster-with-rust-continued/\">the beginning of 2019</a>, we had made it as far as 353,000 records/second on one CPU core. In 2022, just recompiling and running the test again on an M1 Max MacBook Pro with Rust 1.60 is already a kind of unbelievable improvement: 550,000 records/second, a 56% improvement (!). Those Apple hardware engineers really know what they&rsquo;re doing, apparently.</p>\n<p>There&rsquo;s another new cool optimization thing that works in Rust now, though: profile-guided optimization. PGO is actually a feature of LLVM that you can enable, compile your program, run it several times to gather profile data, and then compile again while feeding LLVM the profile data that you collected. <a href=\"https://doc.rust-lang.org/rustc/profile-guided-optimization.html\">The PGO docs from Rust</a> are quite readable, and I was able to <a href=\"https://github.com/rubytogether/kirby/blob/main/bin/bench#L15-L31\">copy and paste the example into my build script</a>.</p>\n<p>With PGO online, the a single M1 core jumped up from 535,000 to 638,000 records/second, another 19% improvement, bringing the total speedup to 76% faster than before.</p>\n<p>In terms of &ldquo;how fast can one whole laptop go&rdquo;, I also benchmarked in multi-file, multi-core mode, maxing out the entire machine. On a 2018 Intel MacBook Pro, the fastest possible speed was 1.1 million records/second. On the 2022 M1 MacBook Pro, it was 3.2 million, and adding profile-guided optimization brought it up to 3.6 million. That&rsquo;s a 3.3x speedup, and I didn&rsquo;t even make any changes to the program. 🤯</p>\n<p>Rust (and Apple Silicon) continues to impress.</p>\n",
				"content_text": "A few years ago, I [wrote about parsing logs 230x faster with Rust](/2018/10/25/parsing-logs-230x-faster-with-rust/), and then [followed up with some improvements](/2019/01/11/parsing-logs-faster-with-rust-continued/).\n\nThanks to [a contributor adding IP address deduplication](https://github.com/rubytogether/kirby/pull/20), I've had to update, build, and deploy it again. As a result, I learned some new stuff about: `rustc` inside `qemu`, Rust LTO (link-time optimization) with musl, profile-guided optimizations, and just how fast M1 Max MacBook Pros are.\n\n### `rustc` segfaults in Docker's `qemu`\n\nThe (somewhat hacky) AWS Lambda setup I am using predates support for Rust. Instead, I tell AWS it's a Go binary, and then upload a Rust binary. Binaries running in Lambda's \"Go\" mode need to be compiled against [musl libc](http://musl.libc.org) (probably because Lambda's runtime is built on top of Alpine Linux, but I don't know for sure). Back in 2018, I couldn't figure out how to get Rust to cross-compile from macOS to Linux with musl, so I used a Docker image with a cross-compiler built in.\n\nThe first problem I ran into while updating the project was... not having an x86 machine anymore. My M1 Max MacBook Pro is really great, but the Docker image I was using to cross-compile only exists in x86 flavor. (Docker calls it \"amd64\".)\n\nTo run x86 images on the ARM-based M1 Macs, Docker ships a copy of `qemu` inside, to run the container on an emulated x86. Unfortunately, [`rustc` segfaults inside Docker's `qemu`](https://github.com/rust-lang/rust/issues/80346). With no idea how to resolve that almost-two-year-old problem, I gave up and decided to use an old Intel Mac to build the new version, at least for the time being. If I was able to get it working, I figured I could set up GitHub Actions to build it for me or something.\n\n### Rust link-time optimization with musl\n\nOnce I had burned a day or two troubleshooting the insurmountable segfault and given up, I ran into a new problem. In 2018, a contributor [enabled LTO for a huge 25% speedup](https://github.com/rubytogether/kirby/commit/74b3d81b0827bd0674eba3ef32cf0223b5756e02). It clearly worked at the time, but it seems  the Docker-based cross-compile had stopped working [by November 2019](https://github.com/rubytogether/kirby/issues/16). As far as I can tell, the problem is exclusive to using both LTO and musl at the same time. Inside the Docker container, LTO works with glibc, and musl works without LTO... but you can't have both together.\n\nFortunately, Rust cross-compilation has improved a lot in the last few years. Thanks to [macos-cross-toolchains](https://github.com/messense/homebrew-macos-cross-toolchains) and [cross](https://github.com/cross-rs/cross), I was able to ditch the Docker setup entirely and compile for x86 Linux with musl directly from macOS on ARM. (Well, once I got it configured. Which required synthesizing together three separate blog posts, since there's no documentation that explained what I needed.) I'm super impressed that `cross` works so well, and not just from macOS on my laptop but also from Linux on GitHub Actions.\n\n### Profile-guided optimizations\n\nFinally, I learned that even the previously absurd speeds that I was getting can be soundly beaten via modern hardware and compilation techniques.\n\nAt [the beginning of 2019](/2019/01/11/parsing-logs-faster-with-rust-continued/), we had made it as far as 353,000 records/second on one CPU core. In 2022, just recompiling and running the test again on an M1 Max MacBook Pro with Rust 1.60 is already a kind of unbelievable improvement: 550,000 records/second, a 56% improvement (!). Those Apple hardware engineers really know what they're doing, apparently.\n\nThere's another new cool optimization thing that works in Rust now, though: profile-guided optimization. PGO is actually a feature of LLVM that you can enable, compile your program, run it several times to gather profile data, and then compile again while feeding LLVM the profile data that you collected. [The PGO docs from Rust](https://doc.rust-lang.org/rustc/profile-guided-optimization.html) are quite readable, and I was able to [copy and paste the example into my build script](https://github.com/rubytogether/kirby/blob/main/bin/bench#L15-L31).\n\nWith PGO online, the a single M1 core jumped up from 535,000 to 638,000 records/second, another 19% improvement, bringing the total speedup to 76% faster than before.\n\nIn terms of \"how fast can one whole laptop go\", I also benchmarked in multi-file, multi-core mode, maxing out the entire machine. On a 2018 Intel MacBook Pro, the fastest possible speed was 1.1 million records/second. On the 2022 M1 MacBook Pro, it was 3.2 million, and adding profile-guided optimization brought it up to 3.6 million. That's a 3.3x speedup, and I didn't even make any changes to the program. 🤯\n\nRust (and Apple Silicon) continues to impress.\n",
				"date_published": "2022-03-13T00:00:00-08:00",
				"url": "https://andre.arko.net/2022/03/13/parsing-logs-faster-with-rust/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2022/02/17/feedyouremail/",
				"title": "feedyour.email",
				"content_html": "<p>Confession time: I still use <a href=\"https://reeder.com\">a feed reader</a>. (Honestly not sure at this point if that makes me Just A Millenial™ or makes me a weird cranky holdout insisting that I don&rsquo;t want blog posts in my email.)</p>\n<p>To help myself cope with our brave new era of newsletters without blogs, I started using <a href=\"https://kill-the-newsletter.com\">Kill the Newsletter!</a>, a handy single-purpose website that gives you a random email address and an RSS feed for emails sent to that address. I found it so helpful that I shared it with my friends, and we used it to feed our addiction to <a href=\"https://www.bloomberg.com/opinion/authors/ARbTQlRLRjE/matthew-s-levine\">Money Stuff</a>.</p>\n<p>One friend reported a consistent problem, though: emails send in the morning not showing up in the feed until the afternoon, or sometimes even the next day. I suggested running our own copy so that we could debug things, but that friend isn’t specialized into web development, and I don’t want to run my own mailserver.</p>\n<p>I didn’t realize it yet, but I had already been nerdsniped.</p>\n<p>Over the next day, my brain automatically architected my own version in the background while I was in the shower or doing nothing in particular. By the time <a href=\"https://twitter.com/numist\">@numist</a> asked how hard it would be to build a copy, I was ready to guess “about an hour”.</p>\n<p>Somewhat predictably for writing software, I was both correct and off by 3x at the same time. In an hour, I had a proof of concept that ran end to end. To get it productionized and deployed took another two or three hours, though.</p>\n<p>A mere two weeks of occasionally tweaking the copy, design, and functionality later, it was ready! It’s honestly pretty handy, even if I do say so myself. Try it out at <a href=\"https://feedyour.email\">feedyour.email</a>.</p>\n",
				"content_text": "\nConfession time: I still use [a feed reader](https://reeder.com). (Honestly not sure at this point if that makes me Just A Millenial™ or makes me a weird cranky holdout insisting that I don't want blog posts in my email.)\n\nTo help myself cope with our brave new era of newsletters without blogs, I started using [Kill the Newsletter!](https://kill-the-newsletter.com), a handy single-purpose website that gives you a random email address and an RSS feed for emails sent to that address. I found it so helpful that I shared it with my friends, and we used it to feed our addiction to [Money Stuff](https://www.bloomberg.com/opinion/authors/ARbTQlRLRjE/matthew-s-levine).\n\nOne friend reported a consistent problem, though: emails send in the morning not showing up in the feed until the afternoon, or sometimes even the next day. I suggested running our own copy so that we could debug things, but that friend isn’t specialized into web development, and I don’t want to run my own mailserver.\n\nI didn’t realize it yet, but I had already been nerdsniped.\n\nOver the next day, my brain automatically architected my own version in the background while I was in the shower or doing nothing in particular. By the time [@numist](https://twitter.com/numist) asked how hard it would be to build a copy, I was ready to guess “about an hour”.\n\nSomewhat predictably for writing software, I was both correct and off by 3x at the same time. In an hour, I had a proof of concept that ran end to end. To get it productionized and deployed took another two or three hours, though.\n\nA mere two weeks of occasionally tweaking the copy, design, and functionality later, it was ready! It’s honestly pretty handy, even if I do say so myself. Try it out at [feedyour.email](https://feedyour.email). \n",
				"date_published": "2022-02-17T00:00:00-08:00",
				"url": "https://andre.arko.net/2022/02/17/feedyouremail/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2021/12/13/uses-this-interview/",
				"title": "uses this interview",
				"content_html": "<p>Back in September, I did <a href=\"https://usesthis.com/interviews/andre.arko/\">an interview</a> with the impressively long-running <a href=\"https://usesthis.com\">usesthis.com</a>. Since I like to archive my own writing, I&rsquo;ve reproduced it here.</p>\n<h3 id=\"who-are-you-and-what-do-you-do\">Who are you, and what do you do?</h3>\n<p>Hello! I’m <a href=\"https://arko.net\">André Arko</a>, although I mostly go by <a href=\"https://twitter.com/indirect\">@indirect</a> on the internet. I spend <a href=\"https://en.wikipedia.org/wiki/Extremely_online\">too much time online</a>, and I think that computers and programming are really neat. As for what I do, uh, it’s hard to pin down exactly but most of it could be called “web development”. Probably.</p>\n<p>For example, I artisanally hand-wrote all the HTML and CSS for <a href=\"https://www.cuberule.com\">The Cube Rule</a>, a website that exists primarily make everyone who reads it astounded and/or enraged while remaining indisputably correct. It will settle whether a hot dog is a sandwich (no) and countless other food controversies. The site has been <a href=\"https://mdcourts.gov/data/opinions/cosa/2019/2411s17.pdf\">cited in a Maryland state judicial ruling</a>, and was a top 5 finalist for <a href=\"https://www.webbyawards.com/news/21st-webby-winners-showcase-2/\">the 2019 Webby Awards</a>.</p>\n<p>I’ve loved <a href=\"https://ruby-lang.org\">the Ruby programming language</a> ever since I discovered it in 2003. Then I discovered <a href=\"https://rubyonrails.org\">Ruby on Rails</a> in 2004 or so, and that led to lots of involvement in the Ruby community. Notable highlights include:</p>\n<ul>\n<li>getting involved in open source in 2009 and ending up lead developer of <a href=\"https://bundler.io\">Bundler</a>, the Ruby language dependency manager</li>\n<li>presenting <a href=\"/talks/\">around 25 talks at 36 events in 14 countries</a></li>\n<li>writing <a href=\"https://therubyway.io\">the third edition of <em>The Ruby Way</em></a> (because I loved the first edition, which I used to learn Ruby in 2003!)</li>\n<li>founding the non-profit <a href=\"https://rubytogether.org\">Ruby Together</a>, a 501(c)(6) trade association that funds open source development work to benefit all Ruby developers</li>\n</ul>\n<p>Professionally, I’ve also had a bit of a roller-coaster, working for a <a href=\"http://irvine.com\">software defense contractor</a>, a <a href=\"http://blog.precipice.org/why-wesabe-lost-to-mint/\">personal finance startup</a>, a <a href=\"https://en.wikipedia.org/wiki/Engine_Yard\">web application hosting startup</a>, a <a href=\"https://plex.tv\">personal media streaming startup</a>, and then <a href=\"https://cloudcity.io\">Cloud City Development</a> since 2013. Cloud City is an agency, and I spend my work days helping <a href=\"https://www.cloudcity.io/work/\">clients</a> (who are mostly tech companies, and mostly in SF) develop their own web applications. There’s a set of stories from each of those places, but you’ll have to ask me about them at a conference or on the internet so this interview can stay a manageable length.</p>\n<p>Back on the personal side of things, and for once including no web development, I built <a href=\"https://witchy.co\">a lunar calendar for witches and werewolves</a> with my partner <a href=\"https://usesthis.com/interviews/amy.wibowo/\">@sailorhg</a>. It’s written in Swift, and lives <a href=\"https://itunes.apple.com/us/app/luna-lunar-calendar-for-witches/id1052484934?mt=8\">on the iPhone App Store</a>. It’s also the subject of my favorite <a href=\"/2018/05/24/everyone-knows-a-lunar-cycle-is-28-days/\">blog post</a> and <a href=\"https://speakerdeck.com/indirect/how-to-calculate-the-phase-of-the-moon-very-very-badly\">conference talk</a>, <a href=\"https://www.youtube.com/watch?v=syx4pWxu_sk\">”How to calculate the phase of the moon very, very badly!”</a>.</p>\n<p>I also curated <a href=\"https://actually.men\">actually.men</a>, a collection of single-serving websites attempting to fight against the misogyny rampant in tech: the seminal <a href=\"http://istechameritocracy.com\">istechameritocracy.com</a>, followed by <a href=\"http://isitapipelineproblem.com/\" title=\"it's not a pipeline problem\">isitapipelineproblem.com</a>, <a href=\"http://arewomenbadatcoding.com/\" title=\"women aren't bad at coding\">arewomenbadatcoding.com</a>, and <a href=\"http://dowomentalkmore.com/\">dowomentalkmore.com</a>.</p>\n<h3 id=\"what-hardware-do-you-use\">What hardware do you use?</h3>\n<p>Okay, now it’s starting to feel like a theme, but… it’s kind of complicated.</p>\n<p>When I’m out and about, I use a pacific blue <a href=\"https://www.apple.com/iphone-12-pro/key-features/\">iPhone 12 Pro</a>, an <a href=\"https://www.apple.com/apple-watch-series-6/\">Apple Watch Series 6</a>, and <a href=\"https://www.apple.com/airpods-pro/\">AirPods Pro</a>. Each one of those devices feels right on the the edge of magic to me. Today’s phones are absolutely the culmination of everything I could imagine while using a <a href=\"https://en.wikipedia.org/wiki/Handspring_(company)#Visor_and_Visor_Deluxe\">Handspring Visor Deluxe</a> and a <a href=\"https://en.wikipedia.org/wiki/Creative_NOMAD#NOMAD_Jukebox_Zen\">Creative NOMAD Jukebox</a> in 2000. The AirPods Pro ability to switch between transparency or noise cancellation, with quick connection when I change devices, makes them my favorite headphones of all time.</p>\n<p>My desk setup has an <a href=\"https://www.owcdigital.com/products/thunderbolt-3-dock-14-port\">OWC Thunderbolt 3 Dock</a>, which hosts an <a href=\"https://www.lg.com/us/monitors/lg-27md5kl-b-5k-uhd-led-monitor\">LG UltraFine 5K</a> display (discontinued 😢), <a href=\"https://www.whathifi.com/us/news/audyssey-lower-east-side-media-speakers-les-more\">Audessey Lower East Side speakers</a> (discontinued 😢), and <a href=\"https://shop.keyboard.io/products/model-01-keyboard?variant=30996744405065\">Keyboardio Model01</a> (discontinued 😢). For less miserable video calls, I’ve also added a boom-mounted <a href=\"http://en.rode.com/microphones/videomicntg\">RØDE VideoMic NTG</a>, a <a href=\"https://fujifilm-x.com/global/products/cameras/x-t30/\">FujiFilm X-T30</a> made into a webcam via <a href=\"https://www.genkithings.com/products/shadowcast\">Genki ShadowCast</a>, and an <a href=\"https://www.elgato.com/en/ring-light\">Elgato Ring Light</a>.</p>\n<p>I’m pretty sure <a href=\"/2020/03/14/keyboards-tell-me-more/\">I have a keyboard problem</a>. For one, I’ve been using the <a href=\"https://en.wikipedia.org/wiki/Dvorak_keyboard_layout\">Dvorak keyboard layout</a> since around 2003. (Somehow, Dvorak completely resolved my crippling RSI from computering for 16 hours a day). For another, I fully replaced the keyboard in my Mac laptop with a <a href=\"https://en.wikipedia.org/wiki/FingerWorks\">FingerWorks</a> <a href=\"https://www.macrumors.com/2009/02/26/practicality-of-multi-touch-and-an-early-powerbook-multi-touch-keyboard-design/\">MacNTouch</a> touch surface until that became physically impossible. Today, I am definitely a sucker for unusual keyboards, and I often use a <a href=\"https://kinesis-ergo.com/keyboards/advantage2-keyboard/\">Kinesis Advantage 2</a>, a <a href=\"https://shop.keyboard.io/products/keyboardio-atreus\">Keyboardio Atreus 2</a>, or my <a href=\"/2020/03/15/built-an-atreus-2/\">hand-built prototype Atreus 2</a>. (Turns out my <a href=\"https://technomancy.us\">college friend Phil</a> designed the Atreus! Small world.)</p>\n<p>My (hopefully back in use soon!) travel kit includes a  Hyper <a href=\"https://www.hypershop.com/collections/chargers/products/hyperjuice-100w-usb-c-gan-charger\">100W charger</a> and <a href=\"https://www.hypershop.com/products/hyperjuice-130w-usb-c-battery?variant=31304056963134\">battery pack</a>, a <a href=\"https://www.therooststand.com\">Roost laptop stand</a>, and an <a href=\"https://www.google.com/search?client=safari&amp;rls=en&amp;q=apple+magic+keyboard&amp;ie=UTF-8&amp;oe=UTF-8\" title=\"apple magic keyboard\">Apple Magic Keyboard</a> and <a href=\"https://www.apple.com/shop/product/MLA02LL/A/magic-mouse-2-silver\">Magic Mouse</a>. On trips to other countries, I add a <a href=\"https://zendure.com/collections/recommendations/products/pre-sale-passport-ii-pro?variant=32425252683850\">Passport II Pro</a> so I can use most plug types. If I bring my <a href=\"https://www.nintendo.com/switch/\">Nintendo Switch</a>, I’ll also pack a <a href=\"https://www.genkithings.com/products/covert-dock\">Covert Dock</a> so we can play games on big screens.</p>\n<p>At any given time, work means I have between one and three <a href=\"https://everymac.com/systems/apple/macbook_pro/specs/macbook-pro-core-i9-2.3-eight-core-16-2019-scissor-specs.html\">16” Intel MacBook Pros</a> that I connect to that desk setup. Those are pretty nice machines, to be honest. A laptop with 32GB of RAM is a godsend when you have to run a bunch of stuff in <a href=\"https://www.docker.com\">Docker</a>.</p>\n<p>In my personal life, though, I use an <a href=\"https://www.tomsguide.com/reviews/macbook-air-2020-m1\">M1 MacBook Air</a>. After almost 10 years of computers that aren’t noticeably faster than last year’s, the Apple Silicon Macs are honestly <a href=\"https://www.anandtech.com/show/16226/apple-silicon-m1-a14-deep-dive/2\">a revelation</a>. My tiny 2.8 pound 13” laptop has no fan (!) but can <a href=\"https://www.anandtech.com/show/16252/mac-mini-apple-m1-tested/2\">compete with desktop Intel PCs</a>… and has 12 hours of battery life. It “only” has 16GB of RAM, but the entire 1TB SSD is <a href=\"https://eclecticlight.co/2020/12/12/how-fast-is-the-ssd-inside-an-m1-mac/\">about as fast</a> as <a href=\"https://en.wikipedia.org/wiki/DDR2_SDRAM\">DDR2 RAM</a> from the early 2000s. I love being surprised by how fast a computer is, and I’ve had that feeling in spades.</p>\n<p>Speaking of fast computers, my desk also squeezes in a <a href=\"https://www.corsair.com/us/en/Categories/Products/Systems/CORSAIR-ONE/CORSAIR-ONE-Compact-Gaming-PC/p/CS-9020021-NA\">Corsair One a200</a> gaming PC, with an <a href=\"https://www.techradar.com/reviews/amd-ryzen-9-5900x\">AMD Ryzen 5900X CPU</a> and <a href=\"https://www.techradar.com/reviews/nvidia-geforce-rtx-3080\">Nvidia GTX 3080 GPU</a>. It’s one of the few machines that can definitely outperform my tiny MacBook Air, but I pretty much only use it to play games at 144fps with all the graphics settings turned up to “highest” on my 4k <a href=\"https://www.rtings.com/monitor/reviews/lg/27gn950-b\">LG 27GN950 monitor</a>.</p>\n<p>The keyboard problem also continues on the PC, where I use at <a href=\"https://www.keyboardco.com/keyboard/filco-majestouch-2-hakua-tenkeyless-nkr-silent-soft-linear-action-usa-keyboard.asp\">Filco Majestouch 2 HAKUA</a> with <a href=\"https://www.cherrymx.de/en/mx-original/mx-silent-red.html\">Cherry MX Silent Red</a> keyswitches and <a href=\"https://shop.norbauer.com/products/after-school-1992-vaporwave-keycaps\">After-school 1992</a> keycaps. Rounding out the gaming setup is a <a href=\"https://www.logitechg.com/en-us/products/gaming-mice/g203-lightsync-rgb-gaming-mouse.910-005791.html\">Logitech G203 Lightsync</a> mouse, <a href=\"https://www.hyperxgaming.com/unitedstates/us/headsets/cloud-flight-s-wireless-gaming-headset\">HyperX Cloud Flight S</a> headset, and an <a href=\"https://www.theverge.com/2019/4/30/18523000/oculus-quest-review-vr-headset-price-specs-features\">Oculus Quest</a> wired to the PC for VR. The final (and my favorite) accessory is the <a href=\"https://www.xbox.com/en-US/accessories/controllers/elite-wireless-controller-series-2\">Microsoft Elite 2</a> controller, the only piece of gaming hardware that feels to me like Apple could have made it.</p>\n<p>Next to the desk is my media center, with an <a href=\"https://www.lg.com/us/tvs/lg-OLED65C8PUA-oled-4k-tv\">LG OLED C8 TV</a>, an <a href=\"https://www.apple.com/apple-tv-4k/\">AppleTV 4k</a>, a <a href=\"https://www.playstation.com/en-us/ps5/\">Playstation 5</a>, and a <a href=\"https://www.nintendo.com/switch/\">Nintendo Switch</a> dock. They’re all connected to an <a href=\"https://www.intl.onkyo.com/products/av_components/av_receivers/tx-nr545/index.html\">Onkyo TX-NR545</a> that drives 7.1 surround speakers. I mainly use it for watching movies and TV shows, but console exclusive games are hard to resist.</p>\n<p>The movies and TV shows mostly comes from the other side of my desk, where my home server lives. That setup is a <a href=\"https://everymac.com/systems/apple/mac_mini/specs/mac-mini-core-i7-3.2-late-2018-specs.html\">2018 Mac Mini</a> with an <a href=\"https://eshop.macsales.com/item/OWC/TB36LRKIT0/\">OWC Thunderbay 6</a> with six 14TB hard drives, for 72TB of usable storage. There’s also a <a href=\"https://www.storagereview.com/review/synology-diskstation-ds216j-review\">Synology DS216j</a> for Time Machine backups, and a <a href=\"https://www.pcmag.com/reviews/fujitsu-scansnap-ix500\">Fujitsu ScanSnap iX500</a>.</p>\n<p>Getting my first ScanSnap back in 2004 might be the biggest quality of life improvement I have experienced as an adult. I can scan papers and then shred them in just a few seconds. 🎉 If I need it later, I can search by date, title, or even inside the OCRed text. (From my phone, if necessary.)</p>\n<h3 id=\"and-what-software\">And what software?</h3>\n<p>Oh boy. This’ll probably take a while. 😅 Thanks to consulting, I typically set up between 2 and 5 new machines every year. To make that easier, I’ve scripted a lot of the setup in <a href=\"https://github.com/indirect/dotfiles\">my dotfiles</a>, which use <a href=\"https://brew.sh\">homebrew</a> extensively to install applications and command-line tools.</p>\n<p>My programming work is mostly web development on macOS. My most common workdays involve a lot of <a href=\"https://macvim-dev.github.io/macvim/\">MacVim</a>, <a href=\"https://support.apple.com/guide/terminal/welcome/mac\">Terminal.app</a>, <a href=\"https://git-scm.com\">git</a> and <a href=\"https://github.com\">GitHub</a>, <a href=\"https://www.ruby-lang.org/en/\">Ruby</a>, <a href=\"https://rubyonrails.org\">Rails</a>, and <a href=\"https://rspec.info\">RSpec</a>. It doesn’t happen every day, but I’ve also spent a lot of time working with <a href=\"https://www.terraform.io\">Terraform</a>, <a href=\"https://www.docker.com\">Docker</a>, <a href=\"https://nodejs.org/en/\">Node</a>, and <a href=\"https://reactjs.org\">React</a>, as well as a host of other frameworks and tools.</p>\n<p>When I’m working directly in the terminal, I can’t live without the ruby switcher <a href=\"https://github.com/postmodern/chruby\">chruby</a>, the search tool <a href=\"https://github.com/BurntSushi/ripgrep\">ripgrep</a>, the directory jumping tool <a href=\"https://github.com/ajeetdsouza/zoxide\">zoxide</a>, and my <a href=\"/2019/01/20/git-in-as-fw-chrs-as-psbl/\">minimalist git shortcuts</a> (and <a href=\"/2020/02/19/git-golf-continued/\">magical git shortcuts</a>). For my shell, I run <a href=\"https://www.zsh.org\">zsh</a>, with a custom <a href=\"https://github.com/romkatv/powerlevel10k\">powerlevel10k</a> status line, displayed in 14pt <a href=\"https://en.wikipedia.org/wiki/Menlo_(typeface)\">Menlo</a> <a href=\"https://www.nerdfonts.com\">Nerd Font</a>.</p>\n<p>Over the years, I’ve built up a pile of related handy tools that I often use to make my work easier. My favorites right now are:</p>\n<ul>\n<li><a href=\"https://fork.dev\">Fork</a>, a great visual git client that lets me stage individual lines</li>\n<li><a href=\"https://kapeli.com/dash\">Dash</a>, to download and search API docs incredibly quickly</li>\n<li><a href=\"https://github.com/tmux/tmux/wiki\">tmux</a>, to run a CLI session shared by all my terminal windows and SSH sessions</li>\n<li><a href=\"https://github.com/puma/puma-dev\">puma-dev</a>, to run local dev servers with custom domains and HTTPS</li>\n<li><a href=\"https://github.com/claui/wishfish\">wishfish</a>, to run SSH connections over wifi so they don’t disconnect when I undock from my desk</li>\n<li><a href=\"https://github.com/withoutboats/bpb\">bpb</a>, which lets me <a href=\"/2021/02/06/signing-git-commits-without-gpg/\">sign my git commits without gpg</a></li>\n<li><a href=\"https://github.com/fabianishere/pam_reattach\">pam_reattach</a>, so I can <a href=\"/2020/07/10/sudo-with-touchid-and-apple-watch-even-inside-tmux/\"><code>sudo</code> via TouchID from tmux</a></li>\n</ul>\n<p>In an example of going probably too far for a joke, I wrote <a href=\"https://github.com/indirect/homebrew-tap/blob/master/Formula/horse_fortunes.rb\">my own homebrew formula</a> to install a new source for the <code>fortune</code> command. Now, every time I open a new shell, I see <a href=\"https://mylittlehorseebooks.tumblr.com\">a random text-art My Little Pony saying a quote</a> from <a href=\"https://en.wikipedia.org/wiki/Horse_ebooks\">@horse_ebooks</a>. It’s incredibly stupid, and I love the results so much I also wrote an <a href=\"https://www.alfredapp.com\">Alfred</a> shortcut to make it easier to screenshot and post funny examples to the Tumblr.</p>\n<p>When I’m not programming, I’m probably reading something. My biggest timesink is definitely <a href=\"https://twitter.com/home?lang=en\">Twitter</a>, where I have had an account for <em>checks notes</em> …14 years? 🤯 In a similar kind of probably going too far, I wrote <a href=\"https://github.com/indirect/twumblr\">my own custom web service</a> to format Tweets the way I want when posting them to my “main” Tumblr at <a href=\"https://indirect.io\">indirect.io</a>.</p>\n<p>When I’m not on Twitter, I (somehow, still, in 2021) read a lot of RSS feeds using the excellent Mac-and-iOS <a href=\"https://reederapp.com\">Reeder</a>. Everything not from an RSS feed goes into <a href=\"https://www.instapaper.com/u\">Instapaper</a>, where I like to think I might, one day, read things. (Unless it’s a recipe. Those go in <a href=\"https://www.paprikaapp.com\">Paprika</a>.)</p>\n<p>For typical web browsing, I am a big fan of <a href=\"https://www.apple.com/safari/\">Safari</a> with <a href=\"https://1blocker.com\">1Blocker</a> and <a href=\"https://1password.com\">1Password</a>. I’m not actively against any other browsers, and sometimes use <a href=\"https://www.google.com/chrome/\">Chrome</a> just for the <a href=\"https://developer.chrome.com/docs/devtools/\">excellent devtools</a>. But only Safari is optimized to the point where I can use my computer for multiple days on a single battery charge.</p>\n<p>When reading long-form, fiction or non-fiction, I mostly use the <a href=\"https://apps.apple.com/us/app/amazon-kindle/id302584613\">Kindle app for iOS</a>. I’ve been reading ebooks since back when that meant typo-ridden .txt files from the sketchier parts of the internet, and I’m both very happy and very upset with how ebooks have turned out today. The ability to buy almost any book I want, while the author gets paid, is absolutely amazing.</p>\n<p>On the other hand, the way <a href=\"https://www.nbcnews.com/technolog/you-dont-own-your-kindle-books-amazon-reminds-customer-1c6626211\">buying ebooks doesn’t mean you own them</a> is incredibly upsetting. My personal workaround is to combine <a href=\"https://www.amazon.com/Kindle-eBooks/b?ie=UTF8&amp;node=154606011\">Kindle ebooks</a> with <a href=\"https://calibre-ebook.com\">Calibre</a> and <a href=\"https://github.com/apprenticeharper/DeDRM_tools\">De-DRM</a>. It means I do a bunch of tedious manual work that doesn’t change anything now, but at least I know that I have copies of the ebooks I paid for that will keep working even if Amazon decides to terminate my account.</p>\n<p>When I’m writing my own long-form text, I mostly use <a href=\"https://ulysses.app\">Ulysses</a>. It’s good for focused writing, and the sync across Mac, iPad, and iPhone is very helpful when I want to squeeze in writing time around other things. When I’m working with others, I’m a big fan of <a href=\"https://draftin.com\">Draft</a> for the editing experience that includes versioned diffs.</p>\n<p>For anything with dated entries, like a work log, journal, or reading and watching diary, I’m a big fan of <a href=\"https://dayoneapp.com\">Day One</a>, thanks to the date-oriented entries and sync across all my devices. If I’m writing Markdown, I’ll often use <a href=\"https://marked2app.com\">Marked 2</a> for a live preview. It’s even customized the theme from my blog, for an accurate preview if I’m writing a blog post.</p>\n<p>I also spend a lot of time chatting online, mainly in <a href=\"https://support.apple.com/messages\">iMessage</a>, <a href=\"https://discord.com\">Discord</a>, and <a href=\"https://slack.com\">Slack</a>. After twenty years of intense online conversations, I’m glad messaging is available to almost everyone, almost everywhere… but I really miss being able to connect every chat backend to <a href=\"https://adium.im\">Adium</a>, where I could have all my conversations in one app, and a single searchable log of all my conversations in my own postgres database.</p>\n<p>Slack is a lot more approachable than IRC, and a lot more scalable than Campfire, but has repeatedly made clear that they aren’t interested in community users or moderation tools. Discord is deliberately made for communities, but it’s incredibly hard to find (or make) a middle ground between “basically empty” and “overwhelming firehose”. iMessage is very good at what it does, but it doesn’t help at all with the 8 other messaging apps I have to use to talk to people who won’t or can’t use iMessage. We made it to the future, and communication is amazing! …and also a huge bummer.</p>\n<p>My day to day productivity suite is fairly boring: I use <a href=\"https://support.apple.com/mail\">Apple Mail</a> connected to Google Apps G Suite Google Workspace on a custom domain. I calendar from <a href=\"https://flexibits.com/fantastical\">Fantastical</a>, due to the combination of natural language event creation and continuous new helpful features for years. I’ve also just started using <a href=\"https://flexibits.com/cardhop\">Cardhop</a> from the same folks as a superpowered contacts app. For calculations, I’m a long-time user of <a href=\"https://www.acqualia.com/soulver/\">Soulver</a>, which hits the perfect sweet spot between calculator and spreadsheet for me.</p>\n<p>To keep track of my own projects and todos, I’ve been using <a href=\"https://www.omnigroup.com/omnifocus/\">OmniFocus</a> for more than a decade. I spend more or less time with it as my personal task tracking waxes and wanes, but it’s definitely been the consistent place I know I can track anything that I need to get done. For more general “keep things in it”, I’ve been using <a href=\"https://www.notion.so\">Notion</a>. It’s better than any other personal wiki I’ve ever used, mostly because it does a pretty good job as both CMS and database, with excellent sharing and teams support.</p>\n<p>I’ll wrap up with shoutouts to the random set of utilities that I feel like I need on any computer I use, in no particular order. <a href=\"https://www.alfredapp.com\">Alfred</a> continues to be the best launcher, clipboard history, and automation tool. I have written several of my own workflows to act as a GUI for scripts, and I use the <a href=\"https://kapeli.com/dash\">Dash</a> integration to look up API docs constantly. <a href=\"https://karabiner-elements.pqrs.org\">Karabiner-Elements</a> is a must to force even badly-written programs to allow me to type in <a href=\"https://en.wikipedia.org/wiki/Dvorak_keyboard_layout\">Dvorak</a>, and to convert the capslock key into control if I hold it down and escape if I tap it. (Yes, I also have muscle memory for Emacs AND Vim. Oof.) I also use <a href=\"https://www.obdev.at/products/littlesnitch/index.html\">Little Snitch</a>, so that I can control what programs are allowed to use the network depending on what network I’m connected to. For example, if I’m tethering to my phone, I don’t want <a href=\"https://www.backblaze.com\">Backblaze</a>, <a href=\"https://www.dropbox.com/\">Dropbox</a>, or <a href=\"https://support.apple.com/en-us/HT204264\">iCloud Photos</a> to sync or back up my local changes.</p>\n<p>Less of a requirement but significant quality of life improvements include:</p>\n<ul>\n<li><a href=\"https://member.ipmu.jp/yuji.tachikawa/MenuMetersElCapitan/\">MenuMeters</a>, so I can see when network, disk, or CPU usage has gone wrong</li>\n<li><a href=\"https://www.dueapp.com\">Due</a>, which I use for any todo or reminder that has hard time limits</li>\n<li><a href=\"https://rogueamoeba.com/soundsource/\">SoundSource</a>, a replacement for the Sound menu that has superpowers, including per app volume controls and a menu for changing outputs</li>\n<li><a href=\"http://www.hammerspoon.org\">Hammerspoon</a>, which I use mainly for window management keyboard shortcuts</li>\n<li><a href=\"https://sindresorhus.com/shareful\">Shareful</a>, which adds a “Copy” item to the OS share menu (but why isn’t this built in, is what I really want to know)</li>\n<li><a href=\"https://peakhourapp.com\">PeakHour</a>, so I can see if my home internet connection is acting up</li>\n</ul>\n<h3 id=\"what-would-be-your-dream-setup\">What would be your dream setup?</h3>\n<p>I think my hardware is getting pretty close. If this interview was 18 months ago, I would have said adding a gaming PC. Today, I’ve pretty much spent the last 18 months shopping and buying and setting that up, and I’m really happy with how it turned out.</p>\n<p>In my dreams, I can afford a <a href=\"https://www.apple.com/pro-display-xdr/\">Pro Display XDR</a>, and it’s paired with some future Mac Pro that has Apple Silicon. I’d also have the upcoming <a href=\"https://www.kickstarter.com/projects/keyboardio/model-100\">Keyboardio Model100</a> with Cherry Blue keyswitches and translucent keycaps, as well as an Apple Magic Mouse that somehow auto-switches when I connect a machine to my dock.</p>\n<p>In my bigger dreams, all computers have error correction, and preserve human-entered data against crashes and physical or human accidents. It could be impossible to lose your work accidentally. We have the technology!</p>\n<p>In my biggest dreams, all of that is happening in a world with open borders, guaranteed housing, gay marriage, free public healthcare, trans rights, and universal income. We’re not particularly close to that world, and I’m honestly pretty worried about whether the world as a whole is even moving in that direction, but I’m doing what I can to help things along.</p>\n",
				"content_text": "Back in September, I did [an interview](https://usesthis.com/interviews/andre.arko/) with the impressively long-running [usesthis.com](https://usesthis.com). Since I like to archive my own writing, I've reproduced it here.\n\n### Who are you, and what do you do?\n\nHello! I’m [André Arko](https://arko.net), although I mostly go by [@indirect](https://twitter.com/indirect) on the internet. I spend [too much time online](https://en.wikipedia.org/wiki/Extremely_online), and I think that computers and programming are really neat. As for what I do, uh, it’s hard to pin down exactly but most of it could be called “web development”. Probably.\n\nFor example, I artisanally hand-wrote all the HTML and CSS for [The Cube Rule](https://www.cuberule.com), a website that exists primarily make everyone who reads it astounded and/or enraged while remaining indisputably correct. It will settle whether a hot dog is a sandwich (no) and countless other food controversies. The site has been [cited in a Maryland state judicial ruling](https://mdcourts.gov/data/opinions/cosa/2019/2411s17.pdf), and was a top 5 finalist for [the 2019 Webby Awards](https://www.webbyawards.com/news/21st-webby-winners-showcase-2/).\n\nI’ve loved [the Ruby programming language](https://ruby-lang.org) ever since I discovered it in 2003. Then I discovered [Ruby on Rails](https://rubyonrails.org) in 2004 or so, and that led to lots of involvement in the Ruby community. Notable highlights include:\n\n- getting involved in open source in 2009 and ending up lead developer of [Bundler](https://bundler.io), the Ruby language dependency manager\n- presenting [around 25 talks at 36 events in 14 countries](/talks/)\n- writing [the third edition of _The Ruby Way_](https://therubyway.io) (because I loved the first edition, which I used to learn Ruby in 2003!)\n- founding the non-profit [Ruby Together](https://rubytogether.org), a 501(c)(6) trade association that funds open source development work to benefit all Ruby developers\n\nProfessionally, I’ve also had a bit of a roller-coaster, working for a [software defense contractor](http://irvine.com), a [personal finance startup](http://blog.precipice.org/why-wesabe-lost-to-mint/), a [web application hosting startup](https://en.wikipedia.org/wiki/Engine_Yard), a [personal media streaming startup](https://plex.tv), and then [Cloud City Development](https://cloudcity.io) since 2013. Cloud City is an agency, and I spend my work days helping [clients](https://www.cloudcity.io/work/) (who are mostly tech companies, and mostly in SF) develop their own web applications. There’s a set of stories from each of those places, but you’ll have to ask me about them at a conference or on the internet so this interview can stay a manageable length.\n\nBack on the personal side of things, and for once including no web development, I built [a lunar calendar for witches and werewolves](https://witchy.co) with my partner [@sailorhg](https://usesthis.com/interviews/amy.wibowo/). It’s written in Swift, and lives [on the iPhone App Store](https://itunes.apple.com/us/app/luna-lunar-calendar-for-witches/id1052484934?mt=8). It’s also the subject of my favorite [blog post](/2018/05/24/everyone-knows-a-lunar-cycle-is-28-days/) and [conference talk](https://speakerdeck.com/indirect/how-to-calculate-the-phase-of-the-moon-very-very-badly), [”How to calculate the phase of the moon very, very badly!”](https://www.youtube.com/watch?v=syx4pWxu_sk).\n\nI also curated [actually.men](https://actually.men), a collection of single-serving websites attempting to fight against the misogyny rampant in tech: the seminal [istechameritocracy.com](http://istechameritocracy.com), followed by [isitapipelineproblem.com](http://isitapipelineproblem.com/ \"it's not a pipeline problem\"), [arewomenbadatcoding.com](http://arewomenbadatcoding.com/ \"women aren't bad at coding\"), and [dowomentalkmore.com](http://dowomentalkmore.com/).\n\n### What hardware do you use?\n\nOkay, now it’s starting to feel like a theme, but… it’s kind of complicated.\n\nWhen I’m out and about, I use a pacific blue [iPhone 12 Pro](https://www.apple.com/iphone-12-pro/key-features/), an [Apple Watch Series 6](https://www.apple.com/apple-watch-series-6/), and [AirPods Pro](https://www.apple.com/airpods-pro/). Each one of those devices feels right on the the edge of magic to me. Today’s phones are absolutely the culmination of everything I could imagine while using a [Handspring Visor Deluxe](https://en.wikipedia.org/wiki/Handspring_(company)#Visor_and_Visor_Deluxe) and a [Creative NOMAD Jukebox](https://en.wikipedia.org/wiki/Creative_NOMAD#NOMAD_Jukebox_Zen) in 2000. The AirPods Pro ability to switch between transparency or noise cancellation, with quick connection when I change devices, makes them my favorite headphones of all time.\n\nMy desk setup has an [OWC Thunderbolt 3 Dock](https://www.owcdigital.com/products/thunderbolt-3-dock-14-port), which hosts an [LG UltraFine 5K](https://www.lg.com/us/monitors/lg-27md5kl-b-5k-uhd-led-monitor) display (discontinued 😢), [Audessey Lower East Side speakers](https://www.whathifi.com/us/news/audyssey-lower-east-side-media-speakers-les-more) (discontinued 😢), and [Keyboardio Model01](https://shop.keyboard.io/products/model-01-keyboard?variant=30996744405065) (discontinued 😢). For less miserable video calls, I’ve also added a boom-mounted [RØDE VideoMic NTG](http://en.rode.com/microphones/videomicntg), a [FujiFilm X-T30](https://fujifilm-x.com/global/products/cameras/x-t30/) made into a webcam via [Genki ShadowCast](https://www.genkithings.com/products/shadowcast), and an [Elgato Ring Light](https://www.elgato.com/en/ring-light).\n\nI’m pretty sure [I have a keyboard problem](/2020/03/14/keyboards-tell-me-more/). For one, I’ve been using the [Dvorak keyboard layout](https://en.wikipedia.org/wiki/Dvorak_keyboard_layout) since around 2003. (Somehow, Dvorak completely resolved my crippling RSI from computering for 16 hours a day). For another, I fully replaced the keyboard in my Mac laptop with a [FingerWorks](https://en.wikipedia.org/wiki/FingerWorks) [MacNTouch](https://www.macrumors.com/2009/02/26/practicality-of-multi-touch-and-an-early-powerbook-multi-touch-keyboard-design/) touch surface until that became physically impossible. Today, I am definitely a sucker for unusual keyboards, and I often use a [Kinesis Advantage 2](https://kinesis-ergo.com/keyboards/advantage2-keyboard/), a [Keyboardio Atreus 2](https://shop.keyboard.io/products/keyboardio-atreus), or my [hand-built prototype Atreus 2](/2020/03/15/built-an-atreus-2/). (Turns out my [college friend Phil](https://technomancy.us) designed the Atreus! Small world.)\n\nMy (hopefully back in use soon!) travel kit includes a  Hyper [100W charger](https://www.hypershop.com/collections/chargers/products/hyperjuice-100w-usb-c-gan-charger) and [battery pack](https://www.hypershop.com/products/hyperjuice-130w-usb-c-battery?variant=31304056963134), a [Roost laptop stand](https://www.therooststand.com), and an [Apple Magic Keyboard](https://www.google.com/search?client=safari&rls=en&q=apple+magic+keyboard&ie=UTF-8&oe=UTF-8 \"apple magic keyboard\") and [Magic Mouse](https://www.apple.com/shop/product/MLA02LL/A/magic-mouse-2-silver). On trips to other countries, I add a [Passport II Pro](https://zendure.com/collections/recommendations/products/pre-sale-passport-ii-pro?variant=32425252683850) so I can use most plug types. If I bring my [Nintendo Switch](https://www.nintendo.com/switch/), I’ll also pack a [Covert Dock](https://www.genkithings.com/products/covert-dock) so we can play games on big screens.\n\nAt any given time, work means I have between one and three [16” Intel MacBook Pros](https://everymac.com/systems/apple/macbook_pro/specs/macbook-pro-core-i9-2.3-eight-core-16-2019-scissor-specs.html) that I connect to that desk setup. Those are pretty nice machines, to be honest. A laptop with 32GB of RAM is a godsend when you have to run a bunch of stuff in [Docker](https://www.docker.com).\n\nIn my personal life, though, I use an [M1 MacBook Air](https://www.tomsguide.com/reviews/macbook-air-2020-m1). After almost 10 years of computers that aren’t noticeably faster than last year’s, the Apple Silicon Macs are honestly [a revelation](https://www.anandtech.com/show/16226/apple-silicon-m1-a14-deep-dive/2). My tiny 2.8 pound 13” laptop has no fan (!) but can [compete with desktop Intel PCs](https://www.anandtech.com/show/16252/mac-mini-apple-m1-tested/2)… and has 12 hours of battery life. It “only” has 16GB of RAM, but the entire 1TB SSD is [about as fast](https://eclecticlight.co/2020/12/12/how-fast-is-the-ssd-inside-an-m1-mac/) as [DDR2 RAM](https://en.wikipedia.org/wiki/DDR2_SDRAM) from the early 2000s. I love being surprised by how fast a computer is, and I’ve had that feeling in spades.\n\nSpeaking of fast computers, my desk also squeezes in a [Corsair One a200](https://www.corsair.com/us/en/Categories/Products/Systems/CORSAIR-ONE/CORSAIR-ONE-Compact-Gaming-PC/p/CS-9020021-NA) gaming PC, with an [AMD Ryzen 5900X CPU](https://www.techradar.com/reviews/amd-ryzen-9-5900x) and [Nvidia GTX 3080 GPU](https://www.techradar.com/reviews/nvidia-geforce-rtx-3080). It’s one of the few machines that can definitely outperform my tiny MacBook Air, but I pretty much only use it to play games at 144fps with all the graphics settings turned up to “highest” on my 4k [LG 27GN950 monitor](https://www.rtings.com/monitor/reviews/lg/27gn950-b).\n\nThe keyboard problem also continues on the PC, where I use at [Filco Majestouch 2 HAKUA](https://www.keyboardco.com/keyboard/filco-majestouch-2-hakua-tenkeyless-nkr-silent-soft-linear-action-usa-keyboard.asp) with [Cherry MX Silent Red](https://www.cherrymx.de/en/mx-original/mx-silent-red.html) keyswitches and [After-school 1992](https://shop.norbauer.com/products/after-school-1992-vaporwave-keycaps) keycaps. Rounding out the gaming setup is a [Logitech G203 Lightsync](https://www.logitechg.com/en-us/products/gaming-mice/g203-lightsync-rgb-gaming-mouse.910-005791.html) mouse, [HyperX Cloud Flight S](https://www.hyperxgaming.com/unitedstates/us/headsets/cloud-flight-s-wireless-gaming-headset) headset, and an [Oculus Quest](https://www.theverge.com/2019/4/30/18523000/oculus-quest-review-vr-headset-price-specs-features) wired to the PC for VR. The final (and my favorite) accessory is the [Microsoft Elite 2](https://www.xbox.com/en-US/accessories/controllers/elite-wireless-controller-series-2) controller, the only piece of gaming hardware that feels to me like Apple could have made it. \n\nNext to the desk is my media center, with an [LG OLED C8 TV](https://www.lg.com/us/tvs/lg-OLED65C8PUA-oled-4k-tv), an [AppleTV 4k](https://www.apple.com/apple-tv-4k/), a [Playstation 5](https://www.playstation.com/en-us/ps5/), and a [Nintendo Switch](https://www.nintendo.com/switch/) dock. They’re all connected to an [Onkyo TX-NR545](https://www.intl.onkyo.com/products/av_components/av_receivers/tx-nr545/index.html) that drives 7.1 surround speakers. I mainly use it for watching movies and TV shows, but console exclusive games are hard to resist.\n\nThe movies and TV shows mostly comes from the other side of my desk, where my home server lives. That setup is a [2018 Mac Mini](https://everymac.com/systems/apple/mac_mini/specs/mac-mini-core-i7-3.2-late-2018-specs.html) with an [OWC Thunderbay 6](https://eshop.macsales.com/item/OWC/TB36LRKIT0/) with six 14TB hard drives, for 72TB of usable storage. There’s also a [Synology DS216j](https://www.storagereview.com/review/synology-diskstation-ds216j-review) for Time Machine backups, and a [Fujitsu ScanSnap iX500](https://www.pcmag.com/reviews/fujitsu-scansnap-ix500).\n\nGetting my first ScanSnap back in 2004 might be the biggest quality of life improvement I have experienced as an adult. I can scan papers and then shred them in just a few seconds. 🎉 If I need it later, I can search by date, title, or even inside the OCRed text. (From my phone, if necessary.)\n\n### And what software?\n\nOh boy. This’ll probably take a while. 😅 Thanks to consulting, I typically set up between 2 and 5 new machines every year. To make that easier, I’ve scripted a lot of the setup in [my dotfiles](https://github.com/indirect/dotfiles), which use [homebrew](https://brew.sh) extensively to install applications and command-line tools.\n\nMy programming work is mostly web development on macOS. My most common workdays involve a lot of [MacVim](https://macvim-dev.github.io/macvim/), [Terminal.app](https://support.apple.com/guide/terminal/welcome/mac), [git](https://git-scm.com) and [GitHub](https://github.com), [Ruby](https://www.ruby-lang.org/en/), [Rails](https://rubyonrails.org), and [RSpec](https://rspec.info). It doesn’t happen every day, but I’ve also spent a lot of time working with [Terraform](https://www.terraform.io), [Docker](https://www.docker.com), [Node](https://nodejs.org/en/), and [React](https://reactjs.org), as well as a host of other frameworks and tools.\n\nWhen I’m working directly in the terminal, I can’t live without the ruby switcher [chruby](https://github.com/postmodern/chruby), the search tool [ripgrep](https://github.com/BurntSushi/ripgrep), the directory jumping tool [zoxide](https://github.com/ajeetdsouza/zoxide), and my [minimalist git shortcuts](/2019/01/20/git-in-as-fw-chrs-as-psbl/) (and [magical git shortcuts](/2020/02/19/git-golf-continued/)). For my shell, I run [zsh](https://www.zsh.org), with a custom [powerlevel10k](https://github.com/romkatv/powerlevel10k) status line, displayed in 14pt [Menlo](https://en.wikipedia.org/wiki/Menlo_(typeface)) [Nerd Font](https://www.nerdfonts.com).\n\nOver the years, I’ve built up a pile of related handy tools that I often use to make my work easier. My favorites right now are:\n- [Fork](https://fork.dev), a great visual git client that lets me stage individual lines\n- [Dash](https://kapeli.com/dash), to download and search API docs incredibly quickly\n- [tmux](https://github.com/tmux/tmux/wiki), to run a CLI session shared by all my terminal windows and SSH sessions\n- [puma-dev](https://github.com/puma/puma-dev), to run local dev servers with custom domains and HTTPS\n- [wishfish](https://github.com/claui/wishfish), to run SSH connections over wifi so they don’t disconnect when I undock from my desk\n- [bpb](https://github.com/withoutboats/bpb), which lets me [sign my git commits without gpg](/2021/02/06/signing-git-commits-without-gpg/)\n- [pam\\_reattach](https://github.com/fabianishere/pam_reattach), so I can [`sudo` via TouchID from tmux](/2020/07/10/sudo-with-touchid-and-apple-watch-even-inside-tmux/)\n\nIn an example of going probably too far for a joke, I wrote [my own homebrew formula](https://github.com/indirect/homebrew-tap/blob/master/Formula/horse_fortunes.rb) to install a new source for the `fortune` command. Now, every time I open a new shell, I see [a random text-art My Little Pony saying a quote](https://mylittlehorseebooks.tumblr.com) from [@horse\\_ebooks](https://en.wikipedia.org/wiki/Horse_ebooks). It’s incredibly stupid, and I love the results so much I also wrote an [Alfred](https://www.alfredapp.com) shortcut to make it easier to screenshot and post funny examples to the Tumblr.\n\nWhen I’m not programming, I’m probably reading something. My biggest timesink is definitely [Twitter](https://twitter.com/home?lang=en), where I have had an account for *checks notes* …14 years? 🤯 In a similar kind of probably going too far, I wrote [my own custom web service](https://github.com/indirect/twumblr) to format Tweets the way I want when posting them to my “main” Tumblr at [indirect.io](https://indirect.io).\n\nWhen I’m not on Twitter, I (somehow, still, in 2021) read a lot of RSS feeds using the excellent Mac-and-iOS [Reeder](https://reederapp.com). Everything not from an RSS feed goes into [Instapaper](https://www.instapaper.com/u), where I like to think I might, one day, read things. (Unless it’s a recipe. Those go in [Paprika](https://www.paprikaapp.com).)\n\nFor typical web browsing, I am a big fan of [Safari](https://www.apple.com/safari/) with [1Blocker](https://1blocker.com) and [1Password](https://1password.com). I’m not actively against any other browsers, and sometimes use [Chrome](https://www.google.com/chrome/) just for the [excellent devtools](https://developer.chrome.com/docs/devtools/). But only Safari is optimized to the point where I can use my computer for multiple days on a single battery charge.\n\nWhen reading long-form, fiction or non-fiction, I mostly use the [Kindle app for iOS](https://apps.apple.com/us/app/amazon-kindle/id302584613). I’ve been reading ebooks since back when that meant typo-ridden .txt files from the sketchier parts of the internet, and I’m both very happy and very upset with how ebooks have turned out today. The ability to buy almost any book I want, while the author gets paid, is absolutely amazing.\n\nOn the other hand, the way [buying ebooks doesn’t mean you own them](https://www.nbcnews.com/technolog/you-dont-own-your-kindle-books-amazon-reminds-customer-1c6626211) is incredibly upsetting. My personal workaround is to combine [Kindle ebooks](https://www.amazon.com/Kindle-eBooks/b?ie=UTF8&node=154606011) with [Calibre](https://calibre-ebook.com) and [De-DRM](https://github.com/apprenticeharper/DeDRM_tools). It means I do a bunch of tedious manual work that doesn’t change anything now, but at least I know that I have copies of the ebooks I paid for that will keep working even if Amazon decides to terminate my account.\n\nWhen I’m writing my own long-form text, I mostly use [Ulysses](https://ulysses.app). It’s good for focused writing, and the sync across Mac, iPad, and iPhone is very helpful when I want to squeeze in writing time around other things. When I’m working with others, I’m a big fan of [Draft](https://draftin.com) for the editing experience that includes versioned diffs. \n\nFor anything with dated entries, like a work log, journal, or reading and watching diary, I’m a big fan of [Day One](https://dayoneapp.com), thanks to the date-oriented entries and sync across all my devices. If I’m writing Markdown, I’ll often use [Marked 2](https://marked2app.com) for a live preview. It’s even customized the theme from my blog, for an accurate preview if I’m writing a blog post. \n\nI also spend a lot of time chatting online, mainly in [iMessage](https://support.apple.com/messages), [Discord](https://discord.com), and [Slack](https://slack.com). After twenty years of intense online conversations, I’m glad messaging is available to almost everyone, almost everywhere… but I really miss being able to connect every chat backend to [Adium](https://adium.im), where I could have all my conversations in one app, and a single searchable log of all my conversations in my own postgres database.\n\nSlack is a lot more approachable than IRC, and a lot more scalable than Campfire, but has repeatedly made clear that they aren’t interested in community users or moderation tools. Discord is deliberately made for communities, but it’s incredibly hard to find (or make) a middle ground between “basically empty” and “overwhelming firehose”. iMessage is very good at what it does, but it doesn’t help at all with the 8 other messaging apps I have to use to talk to people who won’t or can’t use iMessage. We made it to the future, and communication is amazing! …and also a huge bummer.\n\nMy day to day productivity suite is fairly boring: I use [Apple Mail](https://support.apple.com/mail) connected to Google Apps G Suite Google Workspace on a custom domain. I calendar from [Fantastical](https://flexibits.com/fantastical), due to the combination of natural language event creation and continuous new helpful features for years. I’ve also just started using [Cardhop](https://flexibits.com/cardhop) from the same folks as a superpowered contacts app. For calculations, I’m a long-time user of [Soulver](https://www.acqualia.com/soulver/), which hits the perfect sweet spot between calculator and spreadsheet for me.\n\nTo keep track of my own projects and todos, I’ve been using [OmniFocus](https://www.omnigroup.com/omnifocus/) for more than a decade. I spend more or less time with it as my personal task tracking waxes and wanes, but it’s definitely been the consistent place I know I can track anything that I need to get done. For more general “keep things in it”, I’ve been using [Notion](https://www.notion.so). It’s better than any other personal wiki I’ve ever used, mostly because it does a pretty good job as both CMS and database, with excellent sharing and teams support.\n\nI’ll wrap up with shoutouts to the random set of utilities that I feel like I need on any computer I use, in no particular order. [Alfred](https://www.alfredapp.com) continues to be the best launcher, clipboard history, and automation tool. I have written several of my own workflows to act as a GUI for scripts, and I use the [Dash](https://kapeli.com/dash) integration to look up API docs constantly. [Karabiner-Elements](https://karabiner-elements.pqrs.org) is a must to force even badly-written programs to allow me to type in [Dvorak](https://en.wikipedia.org/wiki/Dvorak_keyboard_layout), and to convert the capslock key into control if I hold it down and escape if I tap it. (Yes, I also have muscle memory for Emacs AND Vim. Oof.) I also use [Little Snitch](https://www.obdev.at/products/littlesnitch/index.html), so that I can control what programs are allowed to use the network depending on what network I’m connected to. For example, if I’m tethering to my phone, I don’t want [Backblaze](https://www.backblaze.com), [Dropbox](https://www.dropbox.com/), or [iCloud Photos](https://support.apple.com/en-us/HT204264) to sync or back up my local changes.\n\nLess of a requirement but significant quality of life improvements include:\n- [MenuMeters](https://member.ipmu.jp/yuji.tachikawa/MenuMetersElCapitan/), so I can see when network, disk, or CPU usage has gone wrong\n- [Due](https://www.dueapp.com), which I use for any todo or reminder that has hard time limits\n- [SoundSource](https://rogueamoeba.com/soundsource/), a replacement for the Sound menu that has superpowers, including per app volume controls and a menu for changing outputs\n- [Hammerspoon](http://www.hammerspoon.org), which I use mainly for window management keyboard shortcuts\n- [Shareful](https://sindresorhus.com/shareful), which adds a “Copy” item to the OS share menu (but why isn’t this built in, is what I really want to know)\n- [PeakHour](https://peakhourapp.com), so I can see if my home internet connection is acting up\n\n### What would be your dream setup?\n\nI think my hardware is getting pretty close. If this interview was 18 months ago, I would have said adding a gaming PC. Today, I’ve pretty much spent the last 18 months shopping and buying and setting that up, and I’m really happy with how it turned out.\n\nIn my dreams, I can afford a [Pro Display XDR](https://www.apple.com/pro-display-xdr/), and it’s paired with some future Mac Pro that has Apple Silicon. I’d also have the upcoming [Keyboardio Model100](https://www.kickstarter.com/projects/keyboardio/model-100) with Cherry Blue keyswitches and translucent keycaps, as well as an Apple Magic Mouse that somehow auto-switches when I connect a machine to my dock.\n\nIn my bigger dreams, all computers have error correction, and preserve human-entered data against crashes and physical or human accidents. It could be impossible to lose your work accidentally. We have the technology!\n\nIn my biggest dreams, all of that is happening in a world with open borders, guaranteed housing, gay marriage, free public healthcare, trans rights, and universal income. We’re not particularly close to that world, and I’m honestly pretty worried about whether the world as a whole is even moving in that direction, but I’m doing what I can to help things along.\n",
				"date_published": "2021-12-13T00:00:00-08:00",
				"url": "https://andre.arko.net/2021/12/13/uses-this-interview/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2021/07/20/exclude-bundles-from-time-machine/",
				"title": "Exclude bundles from Time Machine and Spotlight",
				"content_html": "<p><strong>short version:</strong> run <code>cd; bundle plugin install bundler-mac</code>, never think about it again.</p>\n<p><strong>long version:</strong></p>\n<p>Last week I noticed that Time Machine was taking an incredibly long time to finish a backup. I investigated using <a href=\"https://eclecticlight.co/consolation-t2m2-and-log-utilities/\">The Time Machine Mechanic</a> and discovered that at least one of the slow things was many thousands of files from installed gems.</p>\n<p>I have a lot of Ruby projects checked out on my machine at any given time. To keep all of those projects completely isolated from each other, I use <code>bundle config set --global path .bundle</code>. That config means each bundle will be installed into the <code>.bundle</code> directory inside that project.</p>\n<p>It turns out that installing tens or hundreds of gems each into tens or hundreds of projects is&hellip; a lot of files. And all those files need to be backed up anytime I check out a new project or update a project and change the installed gems. Realizing that I don&rsquo;t actually need to back up those files, since I can reinstall the ones I need by running <code>bundle install</code>, I started looking for ways to automatically exclude bundled gems from backups.</p>\n<p>I was able to find <a href=\"https://gist.github.com/peterdemartini/4c918635208943e7a042ff5ffa789fc1\">a pretty good wrapper function</a> posted to Github. It can handle <code>npm</code>, <code>bundler</code>, and <code>cargo</code> fairly effectively, but replacing commands with shell functions always feels kind of hacky. Then I discovered that Cargo actually <a href=\"https://github.com/rust-lang/cargo/issues/3884\">excludes built files from Time Machine automatically</a>. If Cargo can do it, Bundler ought to be able to do it, right?</p>\n<p>This led me on a multi-week quest, wherein I discovered <a href=\"https://rubygems.org/gems/xattr\">the 12-year-old <code>xattr</code> gem</a>, then had to learn enough about <a href=\"https://github.com/ruby/fiddle\"><code>fiddle</code></a> to <a href=\"https://github.com/indirect/xattr\">update <code>xattr</code> to use it</a>, then <a href=\"https://github.com/indirect/xattr/blob/main/spec/xattr_spec.rb\">write tests until I was confident that it worked</a>. Once I had a new <code>xattr</code> gem, I was finally ready to tackle the original idea: making Bundler exclude gems from backups automatically.</p>\n<p>Somewhat ironically, I mentored the Google Summer of Code student who implemented Bundler plugins, but I had never written one myself. Fortunately, <a href=\"https://bundler.io/guides/bundler_plugins.html\">the docs for writing a Bundler plugin</a> were actually pretty good! It took much less time to actually make it work than it had to figure out what I wanted to do in the first place. In an afternoon of work, <a href=\"https://github.com/indirect/bundler-mac\">bundler-mac</a> was born.</p>\n<p>While researching Time Machine and how to exclude files from backups, I discovered the answer to something else that had annoyed me: Spotlight indexing. If I use the Finder to search for &ldquo;module&rdquo;, I don&rsquo;t want to see every file from every gem I have ever installed that defines a module&hellip; but that&rsquo;s what I get by default. So I added something to <code>bundler-mac</code> that also creates a magic dotfile to warn Spotlight away. No more 25 search results from 25 copies of the same gem!</p>\n<p>Bundler plugins have one somewhat surprising aspect: if <code>pwd</code> has a Gemfile, installing a plugin will only apply to the current application bundle. If there is no Gemfile present, installing a plugin will apply at the user level, to all application bundles. Because of that, you&rsquo;ll want to make sure that you <code>cd</code> into your home directory before installing this plugin.</p>\n<p>Maybe, if this works well, it can eventually be part of Bundler itself! If we&rsquo;re incredibly lucky, maybe this can eventually be part of every package manager, and not just Cargo and Bundler.</p>\n<p>With that, we&rsquo;ve made it through all the explanations to the very end! Run <code>cd</code> to get to your home directory, and then run <code>bundle plugin install bundler-mac</code> to get the plugin. Now anytime you run <code>bundle install</code> in the future, your bundled gems will be excluded from Time Machine backups and Spotlight search indexing. Hooray!</p>\n",
				"content_text": "\n**short version:** run `cd; bundle plugin install bundler-mac`, never think about it again.\n\n\n**long version:**\n\nLast week I noticed that Time Machine was taking an incredibly long time to finish a backup. I investigated using [The Time Machine Mechanic](https://eclecticlight.co/consolation-t2m2-and-log-utilities/) and discovered that at least one of the slow things was many thousands of files from installed gems.\n\nI have a lot of Ruby projects checked out on my machine at any given time. To keep all of those projects completely isolated from each other, I use `bundle config set --global path .bundle`. That config means each bundle will be installed into the `.bundle` directory inside that project.\n\nIt turns out that installing tens or hundreds of gems each into tens or hundreds of projects is... a lot of files. And all those files need to be backed up anytime I check out a new project or update a project and change the installed gems. Realizing that I don't actually need to back up those files, since I can reinstall the ones I need by running `bundle install`, I started looking for ways to automatically exclude bundled gems from backups.\n\nI was able to find [a pretty good wrapper function](https://gist.github.com/peterdemartini/4c918635208943e7a042ff5ffa789fc1) posted to Github. It can handle `npm`, `bundler`, and `cargo` fairly effectively, but replacing commands with shell functions always feels kind of hacky. Then I discovered that Cargo actually [excludes built files from Time Machine automatically](https://github.com/rust-lang/cargo/issues/3884). If Cargo can do it, Bundler ought to be able to do it, right?\n\nThis led me on a multi-week quest, wherein I discovered [the 12-year-old `xattr` gem](https://rubygems.org/gems/xattr), then had to learn enough about [`fiddle`](https://github.com/ruby/fiddle) to [update `xattr` to use it](https://github.com/indirect/xattr), then [write tests until I was confident that it worked](https://github.com/indirect/xattr/blob/main/spec/xattr_spec.rb). Once I had a new `xattr` gem, I was finally ready to tackle the original idea: making Bundler exclude gems from backups automatically.\n\nSomewhat ironically, I mentored the Google Summer of Code student who implemented Bundler plugins, but I had never written one myself. Fortunately, [the docs for writing a Bundler plugin](https://bundler.io/guides/bundler_plugins.html) were actually pretty good! It took much less time to actually make it work than it had to figure out what I wanted to do in the first place. In an afternoon of work, [bundler-mac](https://github.com/indirect/bundler-mac) was born.\n\nWhile researching Time Machine and how to exclude files from backups, I discovered the answer to something else that had annoyed me: Spotlight indexing. If I use the Finder to search for \"module\", I don't want to see every file from every gem I have ever installed that defines a module... but that's what I get by default. So I added something to `bundler-mac` that also creates a magic dotfile to warn Spotlight away. No more 25 search results from 25 copies of the same gem!\n\nBundler plugins have one somewhat surprising aspect: if `pwd` has a Gemfile, installing a plugin will only apply to the current application bundle. If there is no Gemfile present, installing a plugin will apply at the user level, to all application bundles. Because of that, you'll want to make sure that you `cd` into your home directory before installing this plugin.\n\nMaybe, if this works well, it can eventually be part of Bundler itself! If we're incredibly lucky, maybe this can eventually be part of every package manager, and not just Cargo and Bundler.\n\nWith that, we've made it through all the explanations to the very end! Run `cd` to get to your home directory, and then run `bundle plugin install bundler-mac` to get the plugin. Now anytime you run `bundle install` in the future, your bundled gems will be excluded from Time Machine backups and Spotlight search indexing. Hooray!\n",
				"date_published": "2021-07-20T00:00:00-08:00",
				"url": "https://andre.arko.net/2021/07/20/exclude-bundles-from-time-machine/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2021/02/22/never-bundle-exec-again/",
				"title": "Never `bundle exec` again",
				"content_html": "<p>If you work with a lot of Ruby and/or Rails codebases, you probably spend a significant amount of time using <code>bundle exec</code> to run commands. Over the years, I&rsquo;ve spent a lot of time explaining why <code>bundle exec</code> exists, what it does, and how to avoid it. I&rsquo;m writing this post now in hopes that it will spread across the Ruby community, and over time hopefully everyone will know the answers to those questions.</p>\n<h3 id=\"why-does-bundle-exec-exist\">Why does <code>bundle exec</code> exist?</h3>\n<p>The <code>bundle exec</code> command is needed to tell your computer when you want to run a command using a gem from the current application&rsquo;s Gemfile.</p>\n<p>When you <code>gem install</code>, it puts the gem in a ruby-wide location. When you <code>bundle install</code>, the gem is just for that codebase. That means running <code>rake</code> will get you the newest rake version installed with <code>gem install</code>, while <code>bundle exec rake</code> will get you the rake from your Gemfile.</p>\n<p>If you don&rsquo;t use <code>bundle exec</code>, you might happen to get exactly the same version from your Gemfile, and not even notice. You might happen to get a version close enough that everything still works. Sometimes, however, you  will eventually get a version of the command that doesn&rsquo;t work with your application, and suddenly everything is broken, confusing, and hard to debug.</p>\n<h3 id=\"how-to-type-it-less\">How to type it less</h3>\n<ol>\n<li>I have heard that <code>rvm</code> overwrites all gem commands to automatically load your Bundle, so you don&rsquo;t have to use <code>bundle exec</code>. I don&rsquo;t recommend doing that, but it seems to work for at least some people.</li>\n<li>Create a shell alias to make it shorter to type. I&rsquo;m partial to <code>alias b=&quot;bundle exec&quot;</code>.</li>\n</ol>\n<p>To be honest, neither one of these options strikes me as particularly good, and I wasn&rsquo;t ever very happy with them. Fortunately, there&rsquo;s another option.</p>\n<h3 id=\"binstubs-the-other-option\">Binstubs, the other option</h3>\n<p>For me, the way that makes sense to distinguish between ruby-wide commands and application commands is to use per-application executables. A binstub lives in your application&rsquo;s <code>bin/</code> directory, and contains just enough code to set up the application-specific environment before running the command you&rsquo;ve requested.</p>\n<p>Running <code>/path/to/app/bin/rails</code> is functionally equivalent to running <code>BUNDLE_GEMFILE=/path/to/app/Gemfile bundle exec rails</code>. The binstub itself knows where to look for the application Gemfile, so you don&rsquo;t have to <code>cd</code> into the app directory before running your command. Since the binstubs delegate actual execution to the installed gems, they don&rsquo;t need to be updated when gem versions change.</p>\n<p>On top of <code>bin/rake</code> making it clearer that you are running this application&rsquo;s version of rake, it&rsquo;s also faster than running <code>bundle exec rake</code>. The exec version has to run Bundler first, and then switch over to running Rake, while the binstub can jump directly to setting up and running the rake command for this specific application.</p>\n<h3 id=\"no-more-bundle-exec\">No more <code>bundle exec</code></h3>\n<p>So, how do you get this for other gems besides Rake and Rails? Bundler can create an application-specific binstub for any gem command, by running <code>bundle binstubs GEM</code>. Once you&rsquo;ve run that command, make sure to commit whichever commands Bundler added to <code>bin/</code> that you want to keep and use. For example, run <code>bundle binstubs rspec-core</code> and then commit your new <code>bin/rspec</code> file. Or <code>bundle binstubs puma</code> and then commit <code>bin/puma</code>.</p>\n<p>Once you have binstubs, it&rsquo;s always clear which gem you&rsquo;re going to get when you run a command: <code>rspec</code> will get you whatever the newest rspec gem is, while <code>bin/rspec</code> will get you the rspec gem that this particular application has in its Gemfile.lock.</p>\n<p>Use binstubs. That&rsquo;s the post!</p>\n",
				"content_text": "If you work with a lot of Ruby and/or Rails codebases, you probably spend a significant amount of time using `bundle exec` to run commands. Over the years, I've spent a lot of time explaining why `bundle exec` exists, what it does, and how to avoid it. I'm writing this post now in hopes that it will spread across the Ruby community, and over time hopefully everyone will know the answers to those questions.\n\n### Why does `bundle exec` exist?\n\nThe `bundle exec` command is needed to tell your computer when you want to run a command using a gem from the current application's Gemfile.\n\nWhen you `gem install`, it puts the gem in a ruby-wide location. When you `bundle install`, the gem is just for that codebase. That means running `rake` will get you the newest rake version installed with `gem install`, while `bundle exec rake` will get you the rake from your Gemfile.\n\nIf you don't use `bundle exec`, you might happen to get exactly the same version from your Gemfile, and not even notice. You might happen to get a version close enough that everything still works. Sometimes, however, you  will eventually get a version of the command that doesn't work with your application, and suddenly everything is broken, confusing, and hard to debug.\n\n### How to type it less\n\n1. I have heard that `rvm` overwrites all gem commands to automatically load your Bundle, so you don't have to use `bundle exec`. I don't recommend doing that, but it seems to work for at least some people.\n1. Create a shell alias to make it shorter to type. I'm partial to `alias b=\"bundle exec\"`.\n\nTo be honest, neither one of these options strikes me as particularly good, and I wasn't ever very happy with them. Fortunately, there's another option.\n\n### Binstubs, the other option\n\nFor me, the way that makes sense to distinguish between ruby-wide commands and application commands is to use per-application executables. A binstub lives in your application's `bin/` directory, and contains just enough code to set up the application-specific environment before running the command you've requested.\n\nRunning `/path/to/app/bin/rails` is functionally equivalent to running `BUNDLE_GEMFILE=/path/to/app/Gemfile bundle exec rails`. The binstub itself knows where to look for the application Gemfile, so you don't have to `cd` into the app directory before running your command. Since the binstubs delegate actual execution to the installed gems, they don't need to be updated when gem versions change.\n\nOn top of `bin/rake` making it clearer that you are running this application's version of rake, it's also faster than running `bundle exec rake`. The exec version has to run Bundler first, and then switch over to running Rake, while the binstub can jump directly to setting up and running the rake command for this specific application.\n\n### No more `bundle exec`\n\nSo, how do you get this for other gems besides Rake and Rails? Bundler can create an application-specific binstub for any gem command, by running `bundle binstubs GEM`. Once you've run that command, make sure to commit whichever commands Bundler added to `bin/` that you want to keep and use. For example, run `bundle binstubs rspec-core` and then commit your new `bin/rspec` file. Or `bundle binstubs puma` and then commit `bin/puma`.\n\nOnce you have binstubs, it's always clear which gem you're going to get when you run a command: `rspec` will get you whatever the newest rspec gem is, while `bin/rspec` will get you the rspec gem that this particular application has in its Gemfile.lock.\n\nUse binstubs. That's the post!\n",
				"date_published": "2021-02-22T00:00:00-08:00",
				"url": "https://andre.arko.net/2021/02/22/never-bundle-exec-again/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2021/02/11/homebrew-on-apple-silicon-macs/",
				"title": "Homebrew on Apple Silicon Macs",
				"content_html": "<p>A mere five and a half weeks after I ordered it, my M1 MacBook Air has finally arrived! Phew. Here are my notes from porting my dotfiles and setting it up.</p>\n<h3 id=\"homebrew\">Homebrew</h3>\n<p>Installing Homebrew on M1 Macs is blessedly straightforward: you go to <a href=\"https://brew.sh\">brew.sh</a> and copy and paste the install command into your terminal.</p>\n<p>However! On an M1 Mac, using the Homebrew installer puts your entire installation into <code>/opt/homebrew</code> instead of the previous usual <code>/usr/local</code>. You’ll want to check for that, and put it in your path ahead of <code>/usr/local</code>, if it exists. Here’s my snippet to make sure I always get the right <code>brew</code> on my path:</p>\n<pre><code>export BREW_PREFIX=$([[ &quot;$(arch)&quot; == &quot;arm64&quot; ]] &amp;&amp; echo &quot;/opt/homebrew&quot; || echo &quot;/usr/local&quot;)\n[[ &quot;$PATH&quot; != &quot;*$BREW_PREFIX/bin*&quot; ]] &amp;&amp; export PATH=&quot;$BREW_PREFIX/bin:$PATH&quot;\n</code></pre>\n<h3 id=\"strap\">Strap</h3>\n<p>If you use <a href=\"https://macos-strap.herokuapp.com\"><code>strap</code></a> to set up new computers, keep in mind that it will skip the Homebrew installer and instead copy it directly into <code>/usr/local</code>. You&rsquo;ll need to use the regular Homebrew install process above to get a copy installed into <code>/opt/homebrew</code> as well.</p>\n<h3 id=\"architectures\">architectures</h3>\n<p>If you need bottles that only exist for Intel (like <code>mas</code>), you can create shell aliases to help you access that stuff more easily:</p>\n<pre><code>alias ibrew=&quot;arch -x86_64 /usr/local/bin/brew&quot;\n</code></pre>\n<p>With that alias, you can use <code>ibrew</code> to manually force the Intel Homebrew installation, to install something you might not otherwise be able to install. The rest of the time, you can use regular <code>brew</code> to get native binaries that have been compiled for your machine.</p>\n<h3 id=\"brew-bundle-and-mas\"><code>brew bundle</code> and <code>mas</code></h3>\n<p>Speaking of things that you might not be able to install compiled for Apple Silicon, let’s talk about <code>mas</code>.</p>\n<p>I use the <code>brew bundle</code> command (run automatically by Strap) to install a bunch of Homebrew formulas, casks, and even Mac App Store apps. However, the <code>mas</code> CLI tool used to manage apps from the Mac App Store <a href=\"https://github.com/mas-cli/mas/issues/308\">hasn’t been ported to Apple Silicon yet</a>.</p>\n<p>For now, that means running <code>brew bundle</code> twice: first, comment out all the Mac App Store apps and run <code>/opt/homebrew/bin/brew bundle</code>. Then, comment out everything <em>but</em> the Mac App Store apps, and run <code>arch -x86_64 /usr/local/bin/brew bundle</code> to use the Intel version of <code>mas</code> to tell the OS to install the Mac App Store apps.</p>\n<p>You’ll still get Apple Silicon native versions of the apps from the App Store, since Hombrew has no control over that, and is just asking the App Store to install an app by ID number.</p>\n<h3 id=\"share-and-enjoy\">share and enjoy</h3>\n<p>and with that, you’re all caught up! I’ve really been enjoying the improved keyboard, absurdly fast compilation speeds, and even more absurdly long battery life. For my uses, switching from Intel to Apple silicon feels a lot like the first time I got an SSD—my computer is visibly faster for almost everything I do. Running for 10 hours on a single battery charge is just ridiculous gravy on top.</p>\n",
				"content_text": "\nA mere five and a half weeks after I ordered it, my M1 MacBook Air has finally arrived! Phew. Here are my notes from porting my dotfiles and setting it up.\n\n### Homebrew\n\nInstalling Homebrew on M1 Macs is blessedly straightforward: you go to [brew.sh](https://brew.sh) and copy and paste the install command into your terminal.\n\nHowever! On an M1 Mac, using the Homebrew installer puts your entire installation into `/opt/homebrew` instead of the previous usual `/usr/local`. You’ll want to check for that, and put it in your path ahead of `/usr/local`, if it exists. Here’s my snippet to make sure I always get the right `brew` on my path:\n\n\texport BREW_PREFIX=$([[ \"$(arch)\" == \"arm64\" ]] && echo \"/opt/homebrew\" || echo \"/usr/local\")\n\t[[ \"$PATH\" != \"*$BREW_PREFIX/bin*\" ]] && export PATH=\"$BREW_PREFIX/bin:$PATH\"\n\n### Strap\n\nIf you use [`strap`](https://macos-strap.herokuapp.com) to set up new computers, keep in mind that it will skip the Homebrew installer and instead copy it directly into `/usr/local`. You'll need to use the regular Homebrew install process above to get a copy installed into `/opt/homebrew` as well.\n\n### architectures\n\nIf you need bottles that only exist for Intel (like `mas`), you can create shell aliases to help you access that stuff more easily:\n\n\talias ibrew=\"arch -x86_64 /usr/local/bin/brew\"\n\nWith that alias, you can use `ibrew` to manually force the Intel Homebrew installation, to install something you might not otherwise be able to install. The rest of the time, you can use regular `brew` to get native binaries that have been compiled for your machine.\n\n### `brew bundle` and `mas`\n\nSpeaking of things that you might not be able to install compiled for Apple Silicon, let’s talk about `mas`.\n\nI use the `brew bundle` command (run automatically by Strap) to install a bunch of Homebrew formulas, casks, and even Mac App Store apps. However, the `mas` CLI tool used to manage apps from the Mac App Store [hasn’t been ported to Apple Silicon yet](https://github.com/mas-cli/mas/issues/308).\n\nFor now, that means running `brew bundle` twice: first, comment out all the Mac App Store apps and run `/opt/homebrew/bin/brew bundle`. Then, comment out everything _but_ the Mac App Store apps, and run `arch -x86_64 /usr/local/bin/brew bundle` to use the Intel version of `mas` to tell the OS to install the Mac App Store apps.\n\nYou’ll still get Apple Silicon native versions of the apps from the App Store, since Hombrew has no control over that, and is just asking the App Store to install an app by ID number.\n\n### share and enjoy\n\nand with that, you’re all caught up! I’ve really been enjoying the improved keyboard, absurdly fast compilation speeds, and even more absurdly long battery life. For my uses, switching from Intel to Apple silicon feels a lot like the first time I got an SSD—my computer is visibly faster for almost everything I do. Running for 10 hours on a single battery charge is just ridiculous gravy on top.\n",
				"date_published": "2021-02-11T00:00:00-08:00",
				"url": "https://andre.arko.net/2021/02/11/homebrew-on-apple-silicon-macs/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2021/02/06/signing-git-commits-without-gpg/",
				"title": "signing git commits without gpg",
				"content_html": "<p>Given the <a href=\"https://twitter.com/FiloSottile/status/1355225801172660224\">”incredibly perfect heap overflow”</a> in gpg that <a href=\"https://dev.gnupg.org/T5275\">dropped this week</a>, it seems worthwhile to write up my strategy for signing git commits.</p>\n<p>My strategy for securely signing git commits goes like this:</p>\n<ol>\n<li><a href=\"https://latacora.micro.blog/2020/02/19/stop-using-encrypted.html\">Stop using encrypted email</a></li>\n<li>Don’t let gpg touch your secret keys</li>\n<li>Don’t even install gpg onto your machine</li>\n</ol>\n<p>It&rsquo;s sad that the perfectly encrypted cyberpunk utopia we were promised devolved into user-hostile systems <a href=\"https://gist.github.com/rjhansen/67ab921ffb4084c865b3618d6955275f\">full of exploitable bugs</a>, but that&rsquo;s what we have. We can do infinitely better by realistically acknowledging how computers (and people!) actually work.</p>\n<p>With that disappointing but honest admission out of the way, let’s do this!</p>\n<p><strong>Step 1</strong> Install <a href=\"https://github.com/withoutboats/bpb/\">boats&rsquo; privacy barricade</a>.</p>\n<pre><code>brew install indirect/tap/bpb\n</code></pre>\n<p>The <code>bpb</code> project is written in Rust by <a href=\"https://twitter.com/withoutboats\">@withoutboats</a>, a long-time member of the Rust programming language design team. It contains just enough code to generate a private key and use it to sign git commits, and doesn’t do anything else.</p>\n<p>Because I am lazy, I don’t want to have to check out a repository and build a binary every time I set up a new machine, so I packaged up binaries for Intel and Apple Silicon Macs running Big Sur and made them available via my Homebrew tap. Feel free to check out the <a href=\"https://github.com/indirect/homebrew-tap/blob/master/Formula/bpb.rb\">homebrew formula</a> and <a href=\"https://github.com/indirect/homebrew-tap/releases/tag/bpb-v1.2.0\">release tarballs</a> if you’re interested.</p>\n<p><strong>Step 2</strong> Generate a new secret key.</p>\n<pre><code>bpb init &quot;André Arko &lt;andre@arko.net&gt;&quot;\n</code></pre>\n<p>You probably want the userid associated with the key to match your git configuration, but as far as I know it doesn’t actually matter what you use. I just make it exactly the same as my git name/email config.</p>\n<p><strong>Step 3</strong> Test <code>bpb</code> to make sure it’s able to sign things.</p>\n<p>If your setup worked, you’ll be able to use the <code>bpb</code> command to sign things via stdin. You can check to make sure by running something like this:</p>\n<pre><code>$ echo &quot;hello&quot; | bpb --sign\n</code></pre>\n<p>If it worked, you’ll see a PGP-formatted signature as the output. Here’s the output when I run the example above:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-plain\" data-lang=\"plain\"><span style=\"display:flex;\"><span>[GNUPG:] SIG_CREATED \n</span></span><span style=\"display:flex;\"><span>-----BEGIN PGP SIGNATURE-----\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>iQB1BAAWCAAdFiEE7U7DQTZs3fUOkr3Au+UhJSudFWoFAmAfBiAACgkQu+UhJSudFWoDRAD+OuSWJzN2FWemZKrlQgZ4rcp6YfjxhKsqfUrnn8M06gEA/2eqNf7/J3JPvSfEfVA44xVOOfni7utAa/+sP1CdbwsG\n</span></span><span style=\"display:flex;\"><span>=BZb0\n</span></span><span style=\"display:flex;\"><span>-----END PGP SIGNATURE-----\n</span></span></code></pre></div><p><strong>Step 4</strong> Configure <code>git</code> to automatically sign commits using <code>bpb</code>.</p>\n<p>Now the important part, making sure <code>git</code> will use <code>bpb</code> to sign your commits.  I use three git configuration settings to produce the signature setup that I want. Here’s the relevant section of my <code>.gitconfig</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-plain\" data-lang=\"plain\"><span style=\"display:flex;\"><span>[gpg]\n</span></span><span style=\"display:flex;\"><span>program = bpb\n</span></span><span style=\"display:flex;\"><span>[commit]\n</span></span><span style=\"display:flex;\"><span>gpgSign = true\n</span></span><span style=\"display:flex;\"><span>[tag]\n</span></span><span style=\"display:flex;\"><span>forceSignAnnotated = true\n</span></span></code></pre></div><p>Depending on how your <code>$PATH</code> is set up, you might need to give the full path to <code>bpb</code>. If you installed <code>bpb</code> via my Homebrew tap, that full path will be <code>/usr/local/bin/bpb</code> on Intel Macs, and <code>/opt/homebrew/bin/bpb</code> on Apple Silicon Macs.</p>\n<p>Setting <code>gpgSign</code> to true means that git will automatically try to sign any commit you make, and setting <code>forceSignAnnotated</code> to true means that git will automatically sign any tags you create that have annotations, which I find is usually what I want.</p>\n<p>I have one more bit of useful git config, an alias named <code>sign</code>. It will re-create all the commits in my current branch (by rebasing against the repo’s main branch), and sign every commit on the way.</p>\n<pre tabindex=\"0\"><code class=\"language-gitconfig\" data-lang=\"gitconfig\">[alias]\nsign = &#34;!f() { git rebase \\&#34;${1:-$((git branch | egrep &#39;main|master|development|latest|release&#39; || echo &#39;master&#39;) | sed &#39;s|* |origin/|&#39; | awk &#39;{print $1}&#39;)}\\&#34; --exec &#39;git commit --amend --no-edit -n -S&#39;; }; f&#34;\n</code></pre><p>If the signatures on my commits are somehow wrong or missing, running <code>git sign</code> and force-pushing is a quick way to clean things up.</p>\n<p><strong>Step 5</strong> Tell GitHub about your new signing key.</p>\n<p>Now that you have working git commit signatures, you probably want to tell GitHub that this new key you’re using belongs to your account. The entire reason you’re doing all this work is to get those little green “Verified” bubbles on your commits on GitHub, right?</p>\n<p>Copy your public key to the clipboard by running <code>bpb print | pbcopy</code>, and then navigate to <a href=\"https://github.com/settings/gpg/new\">your GitHub settings page to add a GPG key</a>. Paste your public key into the box and hit the “Add GPG Key” button.</p>\n<p>Now you have signed and verified git commits, without involving <code>gpg</code>. Congrats! 🎉</p>\n<p><strong>Step 6 (optional)</strong> Use gpg how to verify your signed commits.</p>\n<p>If you want to verify your own signed commits, just to know that it&rsquo;s working, or to debug an issue with GitHub&rsquo;s signature verification, you&rsquo;ll need to use <code>gpg</code>. Sorry. 😞\n(Feel free to uninstall <code>gpg</code> when you&rsquo;re done!) There are three steps:</p>\n<ol>\n<li>\n<p>Import the public key into <code>gpg</code></p>\n<pre><code> bpb print | gpg --import\n</code></pre>\n</li>\n<li>\n<p>Tell <code>gpg</code> to give the key ultimate trust</p>\n<pre><code> gpg --list-keys --fingerprint --with-colons | \\\n sed -E -n -e 's/^fpr:::::::::([0-9A-F]+):$/\\1:6:/p' | \\\n gpg --import-ownertrust\n</code></pre>\n</li>\n<li>\n<p>Check the signatures on your commits with <code>git log</code></p>\n<pre><code> git log --date=relative --pretty='format:%C(yellow)%h%C(reset) %G? %C(blue)%&gt;(14,trunc)%ad %C(green)%&lt;(19)%aN%C(reset)%s%C(red)% gD% D'\n</code></pre>\n</li>\n</ol>\n<p>The <code>%G?</code> bit is the signature check, printed into the second column after the commit sha. According to the man page it will</p>\n<blockquote>\n<p>show &ldquo;G&rdquo; for a good (valid) signature, &ldquo;B&rdquo; for a bad signature, &ldquo;U&rdquo; for a good signature with unknown validity, &ldquo;X&rdquo; for a good signature that has expired, &ldquo;Y&rdquo; for a good signature made by an expired key, &ldquo;R&rdquo; for a good signature made by a revoked key, &ldquo;E&rdquo; if the signature cannot be checked (e.g. missing key) and &ldquo;N&rdquo; for no signature</p>\n</blockquote>\n<p><strong>Step 7 (optional)</strong> Move your actual secret key out of your dotfiles and into the macOS Keychain.</p>\n<p>If you’re like me, you might keep your dotfiles in <a href=\"https://github.com/indirect/dotfiles\">a public git repository</a>. If your dotfiles are public, this new configuration file with a PGP key in it is a problem. You can’t commit the file and publish your secret key, but you want to have a single secret key that you share across whatever machines you happen to be working on.</p>\n<p>Happily, <code>bpb</code> has a solution for you! The config file supports replacing the secret with a program that <code>bpb</code> can run to get the secret, instead. On macOS, the easiest candidate for this is the <code>security</code> command, which can fetch secrets from the macOS Keychain.</p>\n<p>To set this up, open the Keychain Access application, and choose &ldquo;New Password Item&rdquo; from the &ldquo;File&rdquo; menu. Enter &ldquo;bpb key&rdquo; as the &ldquo;Keychain Item Name&rdquo;, and paste the secret from <code>~/.bpb_keys.toml</code> into the &ldquo;Password&rdquo; field.</p>\n<p>Next, create a new bash script that can fetch the secret from the keychain. Mine is lives in my PATH with the name <code>bpb-key</code>, with contents like this:</p>\n<pre><code>#!/bin/bash\n/usr/bin/security find-generic-password -l 'bpb key' -w | tr -d '\\n'\n</code></pre>\n<p>Finally, edit <code>~/.bpb_keys.toml</code> to invoke the script anytime it needs access to the secret. Here&rsquo;s what mine looks like.</p>\n<pre><code>❯ cat ~/.bpb_keys.toml\n[public]\nkey = &quot;5373b1ccc46af267b8e7dab5392eecdea13de78b03e5cb21e2f956d891b20939&quot;\nuserid = &quot;André Arko &lt;andre@arko.net&gt;&quot;\ntimestamp = 1534364503\n[secret]\nprogram = &quot;bpb-key&quot;\n</code></pre>\n<p>Test <code>bpb</code> to make sure that it still works by running <code>echo &quot;test&quot; | bpb --sign</code>, and you&rsquo;re all set!</p>\n<p><strong>Step 8 (optional)</strong> Copy your secret key into iCloud Keychain or 1Password</p>\n<p>Now that you have all of that set up, it would be really great if there was some way to automatically copy that secret onto new machines, wouldn&rsquo;t it?</p>\n<p>I was really hoping that iCloud Keychain would be the secret ingredient here, and I would have access to my bpb key on every machine as long as I had iCloud Keychain syncing turned on.</p>\n<p>Unfortunately, iCloud Keychain is a completely different thing from regular macOS keychains, and the <code>security</code> program can&rsquo;t interact with iCloud Keychains at all. 😞</p>\n<p>The least-bad thing I have figured out how to do is to manually copy the secret into the iCloud Keychain, so that new machines can be set up by manually copying from iCloud Keychain into the macOS Keychain for <code>security</code> to read.</p>\n<p>If you have any better ideas, <a href=\"mailto:andre+bpb@arko.net\">let me know!</a></p>\n",
				"content_text": "Given the [”incredibly perfect heap overflow”](https://twitter.com/FiloSottile/status/1355225801172660224) in gpg that [dropped this week](https://dev.gnupg.org/T5275), it seems worthwhile to write up my strategy for signing git commits.\n\nMy strategy for securely signing git commits goes like this:\n\n1. [Stop using encrypted email](https://latacora.micro.blog/2020/02/19/stop-using-encrypted.html)\n2. Don’t let gpg touch your secret keys\n3. Don’t even install gpg onto your machine\n\nIt's sad that the perfectly encrypted cyberpunk utopia we were promised devolved into user-hostile systems [full of exploitable bugs](https://gist.github.com/rjhansen/67ab921ffb4084c865b3618d6955275f), but that's what we have. We can do infinitely better by realistically acknowledging how computers (and people!) actually work.\n\nWith that disappointing but honest admission out of the way, let’s do this!\n\n**Step 1** Install [boats' privacy barricade](https://github.com/withoutboats/bpb/).\n\n    brew install indirect/tap/bpb\n\nThe `bpb` project is written in Rust by [@withoutboats](https://twitter.com/withoutboats), a long-time member of the Rust programming language design team. It contains just enough code to generate a private key and use it to sign git commits, and doesn’t do anything else.\n\nBecause I am lazy, I don’t want to have to check out a repository and build a binary every time I set up a new machine, so I packaged up binaries for Intel and Apple Silicon Macs running Big Sur and made them available via my Homebrew tap. Feel free to check out the [homebrew formula](https://github.com/indirect/homebrew-tap/blob/master/Formula/bpb.rb) and [release tarballs](https://github.com/indirect/homebrew-tap/releases/tag/bpb-v1.2.0) if you’re interested.\n\n**Step 2** Generate a new secret key.\n\n    bpb init \"André Arko <andre@arko.net>\"\n\nYou probably want the userid associated with the key to match your git configuration, but as far as I know it doesn’t actually matter what you use. I just make it exactly the same as my git name/email config.\n\n**Step 3** Test `bpb` to make sure it’s able to sign things.\n\nIf your setup worked, you’ll be able to use the `bpb` command to sign things via stdin. You can check to make sure by running something like this:\n\n    $ echo \"hello\" | bpb --sign\n\nIf it worked, you’ll see a PGP-formatted signature as the output. Here’s the output when I run the example above:\n\n```plain\n[GNUPG:] SIG_CREATED \n-----BEGIN PGP SIGNATURE-----\n\niQB1BAAWCAAdFiEE7U7DQTZs3fUOkr3Au+UhJSudFWoFAmAfBiAACgkQu+UhJSudFWoDRAD+OuSWJzN2FWemZKrlQgZ4rcp6YfjxhKsqfUrnn8M06gEA/2eqNf7/J3JPvSfEfVA44xVOOfni7utAa/+sP1CdbwsG\n=BZb0\n-----END PGP SIGNATURE-----\n```\n\n**Step 4** Configure `git` to automatically sign commits using `bpb`.\n\nNow the important part, making sure `git` will use `bpb` to sign your commits.  I use three git configuration settings to produce the signature setup that I want. Here’s the relevant section of my `.gitconfig`:\n\n```plain\n[gpg]\nprogram = bpb\n[commit]\ngpgSign = true\n[tag]\nforceSignAnnotated = true\n```\n\nDepending on how your `$PATH` is set up, you might need to give the full path to `bpb`. If you installed `bpb` via my Homebrew tap, that full path will be `/usr/local/bin/bpb` on Intel Macs, and `/opt/homebrew/bin/bpb` on Apple Silicon Macs.\n\nSetting `gpgSign` to true means that git will automatically try to sign any commit you make, and setting `forceSignAnnotated` to true means that git will automatically sign any tags you create that have annotations, which I find is usually what I want.\n\nI have one more bit of useful git config, an alias named `sign`. It will re-create all the commits in my current branch (by rebasing against the repo’s main branch), and sign every commit on the way.\n\n```gitconfig\n[alias]\nsign = \"!f() { git rebase \\\"${1:-$((git branch | egrep 'main|master|development|latest|release' || echo 'master') | sed 's|* |origin/|' | awk '{print $1}')}\\\" --exec 'git commit --amend --no-edit -n -S'; }; f\"\n```\n\nIf the signatures on my commits are somehow wrong or missing, running `git sign` and force-pushing is a quick way to clean things up.\n\n**Step 5** Tell GitHub about your new signing key.\n\nNow that you have working git commit signatures, you probably want to tell GitHub that this new key you’re using belongs to your account. The entire reason you’re doing all this work is to get those little green “Verified” bubbles on your commits on GitHub, right?\n\nCopy your public key to the clipboard by running `bpb print | pbcopy`, and then navigate to [your GitHub settings page to add a GPG key](https://github.com/settings/gpg/new). Paste your public key into the box and hit the “Add GPG Key” button.\n\nNow you have signed and verified git commits, without involving `gpg`. Congrats! 🎉\n\n**Step 6 (optional)** Use gpg how to verify your signed commits.\n\nIf you want to verify your own signed commits, just to know that it's working, or to debug an issue with GitHub's signature verification, you'll need to use `gpg`. Sorry. 😞\n(Feel free to uninstall `gpg` when you're done!) There are three steps:\n\n1. Import the public key into `gpg`\n\n        bpb print | gpg --import\n\n2. Tell `gpg` to give the key ultimate trust\n\n        gpg --list-keys --fingerprint --with-colons | \\\n        sed -E -n -e 's/^fpr:::::::::([0-9A-F]+):$/\\1:6:/p' | \\\n        gpg --import-ownertrust\n\n3. Check the signatures on your commits with `git log`\n\n        git log --date=relative --pretty='format:%C(yellow)%h%C(reset) %G? %C(blue)%>(14,trunc)%ad %C(green)%<(19)%aN%C(reset)%s%C(red)% gD% D'\n\nThe `%G?` bit is the signature check, printed into the second column after the commit sha. According to the man page it will\n> show \"G\" for a good (valid) signature, \"B\" for a bad signature, \"U\" for a good signature with unknown validity, \"X\" for a good signature that has expired, \"Y\" for a good signature made by an expired key, \"R\" for a good signature made by a revoked key, \"E\" if the signature cannot be checked (e.g. missing key) and \"N\" for no signature\n\n**Step 7 (optional)** Move your actual secret key out of your dotfiles and into the macOS Keychain.\n\nIf you’re like me, you might keep your dotfiles in [a public git repository](https://github.com/indirect/dotfiles). If your dotfiles are public, this new configuration file with a PGP key in it is a problem. You can’t commit the file and publish your secret key, but you want to have a single secret key that you share across whatever machines you happen to be working on.\n\nHappily, `bpb` has a solution for you! The config file supports replacing the secret with a program that `bpb` can run to get the secret, instead. On macOS, the easiest candidate for this is the `security` command, which can fetch secrets from the macOS Keychain.\n\nTo set this up, open the Keychain Access application, and choose \"New Password Item\" from the \"File\" menu. Enter \"bpb key\" as the \"Keychain Item Name\", and paste the secret from `~/.bpb_keys.toml` into the \"Password\" field.\n\nNext, create a new bash script that can fetch the secret from the keychain. Mine is lives in my PATH with the name `bpb-key`, with contents like this:\n\n    #!/bin/bash\n    /usr/bin/security find-generic-password -l 'bpb key' -w | tr -d '\\n'\n\nFinally, edit `~/.bpb_keys.toml` to invoke the script anytime it needs access to the secret. Here's what mine looks like.\n\n    ❯ cat ~/.bpb_keys.toml\n    [public]\n    key = \"5373b1ccc46af267b8e7dab5392eecdea13de78b03e5cb21e2f956d891b20939\"\n    userid = \"André Arko <andre@arko.net>\"\n    timestamp = 1534364503\n    [secret]\n    program = \"bpb-key\"\n\nTest `bpb` to make sure that it still works by running `echo \"test\" | bpb --sign`, and you're all set!\n\n**Step 8 (optional)** Copy your secret key into iCloud Keychain or 1Password\n\nNow that you have all of that set up, it would be really great if there was some way to automatically copy that secret onto new machines, wouldn't it?\n\nI was really hoping that iCloud Keychain would be the secret ingredient here, and I would have access to my bpb key on every machine as long as I had iCloud Keychain syncing turned on.\n\nUnfortunately, iCloud Keychain is a completely different thing from regular macOS keychains, and the `security` program can't interact with iCloud Keychains at all. 😞\n\nThe least-bad thing I have figured out how to do is to manually copy the secret into the iCloud Keychain, so that new machines can be set up by manually copying from iCloud Keychain into the macOS Keychain for `security` to read.\n\nIf you have any better ideas, [let me know!](mailto:andre+bpb@arko.net)\n",
				"date_published": "2021-02-06T00:00:00-08:00",
				"url": "https://andre.arko.net/2021/02/06/signing-git-commits-without-gpg/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2020/12/08/github-apps-a-highwtf-introduction/",
				"title": "GitHub Apps, a high-wtf introduction",
				"content_html": "<p>So maybe you&rsquo;ve heard about those <a href=\"https://docs.github.com/en/free-pro-team@latest/developers/apps\">GitHub App things</a> that can programmatically interact with GitHub, and do useful stuff. &ldquo;I want to build useful stuff!&rdquo;, you might say to yourself.</p>\n<p>Well, there&rsquo;s good news and bad news. The good news is that it&rsquo;s a huge step up to interact with GitHub as an app, rather than trying to borrow a GitHub user&rsquo;s credentials. Creating a dedicated GitHub user account for every installation of a bot service was technically against the terms of service, as well as fiddly, error-prone, and typically a giant pain in the butt.</p>\n<p>The bad news is that GitHub Apps are <em>also</em> fiddly, error-prone, and incredibly complicated and confusing in ways that &ldquo;pretend to be a single user&rdquo; could never be complicated or confusing. Brace yourself, here we go.</p>\n<p>To build a GitHub app, you need to use FIVE different GitHub API clients.</p>\n<ol>\n<li>\n<p>What GitHub calls an &ldquo;app&rdquo;. These are available in the <a href=\"https://github.com/marketplace\">GitHub Marketplace</a>, and can be installed into one or more repos belonging to a user or org. Every &ldquo;app&rdquo; also has its own GitHub account with a &ldquo;bot&rdquo; label.</p>\n<p>When you first create your &ldquo;app&rdquo;, GitHub generates a .pem certificate file that you must download and use to generate a JWT for the <code>Authentication</code> header in API calls to the GitHub App API, which lives at <code>api.github.com</code>. You&rsquo;ll also need to add the <code>Accept</code> header <code>application/vnd.github.doctor-strange-preview+json</code>, because this API still lives behind a feature flag.</p>\n</li>\n<li>\n<p>What GitHub calls an &ldquo;installation&rdquo;. After an &ldquo;app&rdquo; has been installed by a user or org, you can make API call that returns a short-lived per-installation access token. The Installation API is more or less the same as if you created a dedicated GitHub account for your bot to use and gave that account read/write permission on the repo in question.</p>\n<p>Use that per-installation access token in the <code>Authentication</code> header to interact with the GitHub Installation API at <code>api.github.com</code>. You&rsquo;ll also need to add the <code>Accept</code> header <code>application/vnd.github.machine-man-preview+json</code>, because this API is feature-flagged.</p>\n</li>\n<li>\n<p>What GitHub calls an &ldquo;oauth app&rdquo;. When you create an &ldquo;app&rdquo;, GitHub also creates linked &ldquo;oauth app&rdquo; at the same time, with a generated Client ID and a Client Secret.</p>\n<p>You use the Client ID and Client Secret as your username and password to interact with the GitHub OAuth App API at <code>api.github.com</code>.</p>\n<p>As an &ldquo;oauth app&rdquo;, you can also provide a &ldquo;Log in with GitHub&rdquo; flow, which allows your app to not just authenticate users but also call the regular GitHub API as if you are that user. To do this, redirect a user to GitHub to log in and authorize your app, after which GitHub will redirect back to your app with a code.</p>\n</li>\n<li>\n<p>Use the code provided by GitHub&rsquo;s redirect back to your site to request a short-lived per-user access token. The access token API does not use any HTTP authentication, and instead requires both your Client ID and Client Secret provided as query parameters.</p>\n<p>Extra confusingly, for <em>only</em> these API calls that fetch user access tokens, you <em>must</em> use <code>github.com</code> instead of <code>api.github.com</code>. Don&rsquo;t spend an hour debugging permissions errors only to discover you&rsquo;re calling the wrong domain like I did.</p>\n</li>\n<li>\n<p>Finally, we made it: the regular user-facing GitHub API. These API responses will contain content as if you are the user who has authorized your app, allowing you to check permissions and (if you have been granted the right permissions) take actions on GitHub as that user.</p>\n<p>You use the short-lived access token mentioned above in the <code>Authorization</code> header, and send your GitHub API requests to <code>api.github.com</code>.</p>\n</li>\n</ol>\n<h3 id=\"tldr\">tl;dr</h3>\n<ol>\n<li>GitHub App: auth with JWT, <code>Accept: doctor-strange-preview</code>, <code>api.github.com</code>.</li>\n<li>GitHub App Installation: auth with installation access token, <code>Accept: man-machine-preview</code>, <code>api.github.com</code>.</li>\n<li>GitHub OAuth App: auth with client ID and client secret, <code>api.github.com</code>.</li>\n<li>GitHub OAuth Access Tokens: don&rsquo;t auth, include client ID and client secret in params, <code>github.com</code>.</li>\n<li>GitHub User: auth with user access token, <code>api.github.com</code>.</li>\n</ol>\n<p>Now you know everything I learned about GitHub Apps this week. Good luck building yours!</p>\n",
				"content_text": "So maybe you've heard about those [GitHub App things](https://docs.github.com/en/free-pro-team@latest/developers/apps) that can programmatically interact with GitHub, and do useful stuff. \"I want to build useful stuff!\", you might say to yourself.\n\nWell, there's good news and bad news. The good news is that it's a huge step up to interact with GitHub as an app, rather than trying to borrow a GitHub user's credentials. Creating a dedicated GitHub user account for every installation of a bot service was technically against the terms of service, as well as fiddly, error-prone, and typically a giant pain in the butt.\n\nThe bad news is that GitHub Apps are _also_ fiddly, error-prone, and incredibly complicated and confusing in ways that \"pretend to be a single user\" could never be complicated or confusing. Brace yourself, here we go.\n\nTo build a GitHub app, you need to use FIVE different GitHub API clients.\n\n1. What GitHub calls an \"app\". These are available in the [GitHub Marketplace](https://github.com/marketplace), and can be installed into one or more repos belonging to a user or org. Every \"app\" also has its own GitHub account with a \"bot\" label.\n\n    When you first create your \"app\", GitHub generates a .pem certificate file that you must download and use to generate a JWT for the `Authentication` header in API calls to the GitHub App API, which lives at `api.github.com`. You'll also need to add the `Accept` header `application/vnd.github.doctor-strange-preview+json`, because this API still lives behind a feature flag.\n\n1. What GitHub calls an \"installation\". After an \"app\" has been installed by a user or org, you can make API call that returns a short-lived per-installation access token. The Installation API is more or less the same as if you created a dedicated GitHub account for your bot to use and gave that account read/write permission on the repo in question.\n\n    Use that per-installation access token in the `Authentication` header to interact with the GitHub Installation API at `api.github.com`. You'll also need to add the `Accept` header `application/vnd.github.machine-man-preview+json`, because this API is feature-flagged.\n\n1. What GitHub calls an \"oauth app\". When you create an \"app\", GitHub also creates linked \"oauth app\" at the same time, with a generated Client ID and a Client Secret.\n\n    You use the Client ID and Client Secret as your username and password to interact with the GitHub OAuth App API at `api.github.com`.\n\n    As an \"oauth app\", you can also provide a \"Log in with GitHub\" flow, which allows your app to not just authenticate users but also call the regular GitHub API as if you are that user. To do this, redirect a user to GitHub to log in and authorize your app, after which GitHub will redirect back to your app with a code.\n\n1. Use the code provided by GitHub's redirect back to your site to request a short-lived per-user access token. The access token API does not use any HTTP authentication, and instead requires both your Client ID and Client Secret provided as query parameters.\n\n    Extra confusingly, for _only_ these API calls that fetch user access tokens, you _must_ use `github.com` instead of `api.github.com`. Don't spend an hour debugging permissions errors only to discover you're calling the wrong domain like I did.\n\n1. Finally, we made it: the regular user-facing GitHub API. These API responses will contain content as if you are the user who has authorized your app, allowing you to check permissions and (if you have been granted the right permissions) take actions on GitHub as that user.\n\n    You use the short-lived access token mentioned above in the `Authorization` header, and send your GitHub API requests to `api.github.com`.\n\n### tl;dr\n\n1. GitHub App: auth with JWT, `Accept: doctor-strange-preview`, `api.github.com`.\n1. GitHub App Installation: auth with installation access token, `Accept: man-machine-preview`, `api.github.com`.\n1. GitHub OAuth App: auth with client ID and client secret, `api.github.com`.\n1. GitHub OAuth Access Tokens: don't auth, include client ID and client secret in params, `github.com`.\n1. GitHub User: auth with user access token, `api.github.com`.\n\nNow you know everything I learned about GitHub Apps this week. Good luck building yours!\n",
				"date_published": "2020-12-08T00:00:00-08:00",
				"url": "https://andre.arko.net/2020/12/08/github-apps-a-highwtf-introduction/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2020/08/23/rails-containers-but-really-quickly/",
				"title": "Rails 6 containers, but really quickly",
				"content_html": "<p>Running <code>docker build</code> always feels too slow. Most of the time, new builds still have to download and install something that every previous build also had to download and install—whether apt, yum, npm, or gem, there a lot of options for something that has to be done slowly, over and over.</p>\n<p>Using layer caching helps, but it&rsquo;s still sort of infuriating. A single change (for example, adding just one gem) means Docker has to start completely from scratch for that step. Have 400 gems already built into an image, and add a single new gem? Now Docker has to install not 1 gem, but 401 gems.</p>\n<p>Even worse, whatever steps come after that step are no longer cached either. Now that you&rsquo;ve added your one gem, you also have to wait through reinstalling 8,000 npm packages from scratch, even though they haven&rsquo;t changed at all.</p>\n<p>I felt frustrated by this for years of using Docker, but never really had a good solution. Usually I would just reorder my Dockerfile to put whatever I was changing last, so that edits at least wouldn&rsquo;t mean re-running any unchanged steps.</p>\n<p>Then, earlier this year, I ran across <a href=\"https://ledermann.dev/blog/2020/01/29/building-docker-images-the-performant-way/\">a novel approach using a relatively new Docker feature called ONBUILD</a>. Excitingly, it offers an actual solution to the problem: create a base image that has all current gems installed, and use that base to build a per-commit image that updates only new gems. That might change over time, but if you also use scheduled builds, you can guarantee that the build only has to install gems added since the last scheduled build. With this tactic, per-commit images build in a little as 1-2 minutes!</p>\n<p>To set it up, you create two Dockerfiles. The first Dockerfile (I usually call it <code>Dockerfile-base</code>) installs the OS packages you&rsquo;ll need to build gems from source, and installs gems and packages. Here&rsquo;s a simplified version of the logic from <code>Dockerfile-base</code>.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-dockerfile\" data-lang=\"dockerfile\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># Dockerfile-base</span><span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#960050;background-color:#1e0010\"></span><span style=\"color:#66d9ef\">FROM</span><span style=\"color:#e6db74\"> ruby:alpine</span><span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#960050;background-color:#1e0010\"></span><span style=\"color:#75715e\"># Install base app gems into the base image</span><span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#960050;background-color:#1e0010\"></span><span style=\"color:#66d9ef\">COPY</span> Gemfile* .ruby-version /app/<span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#960050;background-color:#1e0010\"></span><span style=\"color:#66d9ef\">RUN</span> bundle install --deployment --path /app/vendor/bundle<span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#960050;background-color:#1e0010\"></span><span style=\"color:#75715e\"># In builds using this as a base, install new gems and remove obsolete gems</span><span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#960050;background-color:#1e0010\"></span><span style=\"color:#66d9ef\">ONBUILD</span> <span style=\"color:#66d9ef\">COPY</span> Gemfile* .ruby-version /app/<span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#960050;background-color:#1e0010\"></span><span style=\"color:#66d9ef\">ONBUILD</span> <span style=\"color:#66d9ef\">RUN</span> bundle install --clean<span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#960050;background-color:#1e0010\"></span><span style=\"color:#75715e\"># After updating gems for the child image, copy in the latest app code</span><span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#960050;background-color:#1e0010\"></span><span style=\"color:#66d9ef\">ONBUILD</span> <span style=\"color:#66d9ef\">COPY</span> . /app<span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span></code></pre></div><p>Note that the ONBUILD steps <em>do not</em> run when this image is built. Instead, those steps run when another image uses this image as a base.</p>\n<p>The base needs to be rebuilt periodically, but it&rsquo;s not super important—each individual change to the underlying gems or packages typically adds just a few seconds to the build. I typically set GitHub Actions to rebuild the base image and push once each night, rolling up all the changes from the previous day and speeding up builds for the next day. Here&rsquo;s an example GitHub Action to rebuild the base image each night.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-yaml\" data-lang=\"yaml\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># .github/workflows/daily-build-base.yml</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#f92672\">name</span>: <span style=\"color:#ae81ff\">Build base image</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#f92672\">on</span>:\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#f92672\">schedule</span>:\n</span></span><span style=\"display:flex;\"><span>    - <span style=\"color:#f92672\">cron</span>: <span style=\"color:#e6db74\">&#34;3 8 * * *&#34;</span> <span style=\"color:#75715e\"># 8am UTC is 1am PST</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#f92672\">steps</span>:\n</span></span><span style=\"display:flex;\"><span>  - <span style=\"color:#f92672\">name</span>: <span style=\"color:#ae81ff\">Checkout code</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#f92672\">uses</span>: <span style=\"color:#ae81ff\">actions/checkout@v2</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>  - <span style=\"color:#f92672\">name</span>: <span style=\"color:#ae81ff\">Build and push Docker images</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#f92672\">uses</span>: <span style=\"color:#ae81ff\">docker/build-push-action@v1</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#f92672\">with</span>:\n</span></span><span style=\"display:flex;\"><span>      <span style=\"color:#f92672\">username</span>: <span style=\"color:#ae81ff\">${{ secrets.DOCKER_USERNAME }}</span>\n</span></span><span style=\"display:flex;\"><span>      <span style=\"color:#f92672\">password</span>: <span style=\"color:#ae81ff\">${{ secrets.DOCKER_PASSWORD }}</span>\n</span></span><span style=\"display:flex;\"><span>      <span style=\"color:#f92672\">repository</span>: <span style=\"color:#ae81ff\">myorg/myrepo</span>\n</span></span><span style=\"display:flex;\"><span>      <span style=\"color:#f92672\">tags</span>: <span style=\"color:#ae81ff\">latest</span>\n</span></span></code></pre></div><p>Then, the main Dockerfile uses that image as a base. The especial genius of this move is that Bundler and npm/yarn no longer start from nothing, but install on top of a complete set of packages from the recent past. If you add a new gem, the base image already has every gem except that one, and the only work Bundler has to do at build time is add that one gem. Here&rsquo;s what a <code>Dockerfile</code> might look like with this strategy.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-dockerfile\" data-lang=\"dockerfile\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">FROM</span><span style=\"color:#e6db74\"> myorg/myrepo-base:latest AS base</span><span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#960050;background-color:#1e0010\"></span><span style=\"color:#66d9ef\">FROM</span><span style=\"color:#e6db74\"> ruby:alpine</span><span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#960050;background-color:#1e0010\"></span><span style=\"color:#66d9ef\">COPY</span> --from<span style=\"color:#f92672\">=</span>base /app /app<span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#960050;background-color:#1e0010\"></span><span style=\"color:#66d9ef\">CMD</span> [<span style=\"color:#e6db74\">&#34;bin/puma&#34;</span>, <span style=\"color:#e6db74\">&#34;-p&#34;</span> <span style=\"color:#e6db74\">&#34;$PORT&#34;</span>]<span style=\"color:#960050;background-color:#1e0010\">\n</span></span></span></code></pre></div><p>You can&rsquo;t see it in this Dockerfile, but the ONBUILD steps from <code>Dockerfile-base</code> will update the base image to have the latest gems and app files before the COPY steps add those files to the final image.</p>\n<p>Using this technique, it&rsquo;s possible to build Rails 6 production containers, including running webpacker to generate assets, in as little as 2-3 minutes—even when there are changes to gems or node modules.</p>\n",
				"content_text": "Running `docker build` always feels too slow. Most of the time, new builds still have to download and install something that every previous build also had to download and install—whether apt, yum, npm, or gem, there a lot of options for something that has to be done slowly, over and over.\n\nUsing layer caching helps, but it's still sort of infuriating. A single change (for example, adding just one gem) means Docker has to start completely from scratch for that step. Have 400 gems already built into an image, and add a single new gem? Now Docker has to install not 1 gem, but 401 gems.\n\nEven worse, whatever steps come after that step are no longer cached either. Now that you've added your one gem, you also have to wait through reinstalling 8,000 npm packages from scratch, even though they haven't changed at all.\n\nI felt frustrated by this for years of using Docker, but never really had a good solution. Usually I would just reorder my Dockerfile to put whatever I was changing last, so that edits at least wouldn't mean re-running any unchanged steps.\n\nThen, earlier this year, I ran across [a novel approach using a relatively new Docker feature called ONBUILD](https://ledermann.dev/blog/2020/01/29/building-docker-images-the-performant-way/). Excitingly, it offers an actual solution to the problem: create a base image that has all current gems installed, and use that base to build a per-commit image that updates only new gems. That might change over time, but if you also use scheduled builds, you can guarantee that the build only has to install gems added since the last scheduled build. With this tactic, per-commit images build in a little as 1-2 minutes!\n\nTo set it up, you create two Dockerfiles. The first Dockerfile (I usually call it `Dockerfile-base`) installs the OS packages you'll need to build gems from source, and installs gems and packages. Here's a simplified version of the logic from `Dockerfile-base`.\n\n```dockerfile\n# Dockerfile-base\nFROM ruby:alpine\n\n# Install base app gems into the base image\nCOPY Gemfile* .ruby-version /app/\nRUN bundle install --deployment --path /app/vendor/bundle\n\n# In builds using this as a base, install new gems and remove obsolete gems\nONBUILD COPY Gemfile* .ruby-version /app/\nONBUILD RUN bundle install --clean\n\n# After updating gems for the child image, copy in the latest app code\nONBUILD COPY . /app\n```\n\nNote that the ONBUILD steps _do not_ run when this image is built. Instead, those steps run when another image uses this image as a base.\n\nThe base needs to be rebuilt periodically, but it's not super important—each individual change to the underlying gems or packages typically adds just a few seconds to the build. I typically set GitHub Actions to rebuild the base image and push once each night, rolling up all the changes from the previous day and speeding up builds for the next day. Here's an example GitHub Action to rebuild the base image each night.\n\n```yaml\n# .github/workflows/daily-build-base.yml\nname: Build base image\n\non:\n  schedule:\n    - cron: \"3 8 * * *\" # 8am UTC is 1am PST\n\nsteps:\n  - name: Checkout code\n    uses: actions/checkout@v2\n\n  - name: Build and push Docker images\n    uses: docker/build-push-action@v1\n    with:\n      username: ${{ secrets.DOCKER_USERNAME }}\n      password: ${{ secrets.DOCKER_PASSWORD }}\n      repository: myorg/myrepo\n      tags: latest\n```\n\nThen, the main Dockerfile uses that image as a base. The especial genius of this move is that Bundler and npm/yarn no longer start from nothing, but install on top of a complete set of packages from the recent past. If you add a new gem, the base image already has every gem except that one, and the only work Bundler has to do at build time is add that one gem. Here's what a `Dockerfile` might look like with this strategy.\n\n```dockerfile\nFROM myorg/myrepo-base:latest AS base\nFROM ruby:alpine\n\n\nCOPY --from=base /app /app\n\nCMD [\"bin/puma\", \"-p\" \"$PORT\"]\n```\n\nYou can't see it in this Dockerfile, but the ONBUILD steps from `Dockerfile-base` will update the base image to have the latest gems and app files before the COPY steps add those files to the final image.\n\nUsing this technique, it's possible to build Rails 6 production containers, including running webpacker to generate assets, in as little as 2-3 minutes—even when there are changes to gems or node modules.\n",
				"date_published": "2020-08-23T00:00:00-08:00",
				"url": "https://andre.arko.net/2020/08/23/rails-containers-but-really-quickly/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2020/07/10/codesudocode-with-touchid-and-apple/",
				"title": "\u003ccode\u003esudo\u003c/code\u003e with TouchID and Apple Watch, even inside \u003ccode\u003etmux\u003c/code\u003e",
				"content_html": "<p>Ever since TouchID was introduced in the 2016 MacBook Pro, I wondered why it could replace user authentication dialogs in the GUI, like System Preferences or Installer, but not in the command line, for things like <code>sudo</code>. Perhaps predictably, many nerds on the internet had the same idea, and for a while you could <a href=\"https://github.com/mattrajca/sudo-touchid\">install a fork of sudo</a> (!!) or a <a href=\"https://github.com/hamzasood/pam_touchid\">custom PAM module</a> to get TouchID support.</p>\n<p>It turns out that none of that was actually needed, though, and a (somewhat obscure) built-in way to enable it was <a href=\"https://twitter.com/cabel/status/931292107372838912\">shared on Twitter in 2017</a>. Surprisingly, Apple actually ships a PAM module named <a href=\"https://opensource.apple.com/source/pam_modules/pam_modules-173.1.1/modules/pam_tid/pam_tid.c.auto.html\"><code>pam_tid.so</code></a> in every copy of macOS. If you <a href=\"https://apple.stackexchange.com/a/306324\">configure <code>sudo</code> to use it</a>, you can skip typing your password and just TouchID instead, without having to completely destroy the security of your machine.</p>\n<p>Unfortunately, it never worked for me. 😭 Some debugging later, I figured out that it worked inside a regular shell, but not inside shells opened by <code>tmux</code> or <code>ssh</code>. Since I pretty much exclusively use shells opened by <code>tmux</code> and <code>ssh</code>, I spent some time frantically googling around for how to fix them, and found pretty much nothing. It worked outside of tmux, it didn&rsquo;t work inside tmux, and that was that.</p>\n<p>Fast forward three years to today, and while griping to a friend about how it didn&rsquo;t work inside tmux, I discovered that technology has advanced and there is now <a href=\"https://github.com/fabianishere/pam_reattach\">a fix, named <code>pam_reattach</code></a>! It&rsquo;s a PAM module that you configure to run before the built-in <code>pam_tid.so</code>, and it makes the <code>sudo</code> command able to find and use the TouchID module to authenticate, even from inside <code>tmux</code>.</p>\n<p>Amazingly, I was even able to find a written explanation of the thought process that produced the PAM module, in the form of this <a href=\"https://superuser.com/a/1348180\">Stack Overflow answer</a>. Based on that answer and the linked discussion, it seems the steps were:</p>\n<ol>\n<li>Apple patches <code>screen</code> to stay attached to a user&rsquo;s GUI login session, so that CLI tools like <code>pbcopy</code>, <code>security</code>, and system calls like TouchID checks will continue to work.</li>\n<li><a href=\"https://github.com/ChrisJohnsen\">@ChrisJohnsen</a> uses some of the undocumented functions called by the <code>screen</code> patch to implement the now-ubiquitous <a href=\"https://github.com/ChrisJohnsen/tmux-MacOSX-pasteboard/\"><code>reattach-to-user-namespace</code></a> command that allows <code>pbcopy</code> and <code>pbpaste</code> to continue working inside <code>tmux</code> or non-Apple <code>screen</code>.</li>\n<li>Once TouchID MacBooks have emerged, <a href=\"https://twitter.com/cabel/\">@Cabel</a> reveals the existence of <code>pam_tid.so</code> and TouchID authentication for <code>sudo</code>. Unfortunately, it doesn&rsquo;t work in non-GUI processes like SSH, tmux, or homebrewed screen.</li>\n<li><a href=\"https://github.com/fabianishere\">@fabianishere</a>, again inspired by Apple&rsquo;s patches to <code>screen</code>, sends pull requests that re-enable TouchID to both <a href=\"https://github.com/ChrisJohnsen/tmux-MacOSX-pasteboard/pull/70\">reattach-to-user-namespace</a> and <a href=\"https://github.com/tmux/tmux/pull/1434\">tmux</a>.</li>\n<li>After some discussion of unfortunate tradeoffs, both PRs are closed, and they create a new PAM module, analogous to <code>reattach-to-user-namespace</code> but just for the PAM flow: <a href=\"https://github.com/fabianishere/pam_reattach\"><code>pam_reattach</code></a>. This enables TouchID for <code>sudo</code> in both <code>tmux</code> and homebrewed <code>screen</code>.</li>\n<li>In macOS Catalina, Apple adds Apple Watch confirmations to the &ldquo;TouchID&rdquo; system. If you have an Apple Watch configured to unlock your Mac, you can also double-tap the watch button to confirm a TouchID prompt instead of scanning your finger.</li>\n<li>Enterprising Swift coder <a href=\"https://github.com/Reflejo\">@Reflejo</a> wrote a new PAM module to enable TouchID support named <a href=\"https://github.com/Reflejo/pam-touchID\"><code>pam-touchid</code></a>. Implemented in Swift, it is 1/3 shorter and (in my opinion) about 1000x easier to understand than the <a href=\"https://opensource.apple.com/source/pam_modules/pam_modules-173.1.1/modules/pam_tid/pam_tid.c.auto.html\">straight C module from Apple</a>.</li>\n<li><a href=\"https://github.com/biscuitehh/\">@biscuitehh</a>, not content with Apple Watch <code>sudo</code> exclusively on Macs with TouchID hardware, forked <code>pam-touchid</code> into <a href=\"https://github.com/biscuitehh/pam-watchid\"><code>pam-watchid</code></a>, a PAM module that allows <code>sudo</code> via Apple Watch on any Mac.</li>\n</ol>\n<p>I&rsquo;m pretty excited that I can finally <code>sudo</code> using my fingerprint or my watch, and a little bit in awe of the way determined nerds manage to figure things out eventually. Nice work, everyone.</p>\n",
				"content_text": "\nEver since TouchID was introduced in the 2016 MacBook Pro, I wondered why it could replace user authentication dialogs in the GUI, like System Preferences or Installer, but not in the command line, for things like `sudo`. Perhaps predictably, many nerds on the internet had the same idea, and for a while you could [install a fork of sudo](https://github.com/mattrajca/sudo-touchid) (!!) or a [custom PAM module](https://github.com/hamzasood/pam_touchid) to get TouchID support.\n\nIt turns out that none of that was actually needed, though, and a (somewhat obscure) built-in way to enable it was [shared on Twitter in 2017](https://twitter.com/cabel/status/931292107372838912). Surprisingly, Apple actually ships a PAM module named [`pam_tid.so`](https://opensource.apple.com/source/pam_modules/pam_modules-173.1.1/modules/pam_tid/pam_tid.c.auto.html) in every copy of macOS. If you [configure `sudo` to use it](https://apple.stackexchange.com/a/306324), you can skip typing your password and just TouchID instead, without having to completely destroy the security of your machine.\n\nUnfortunately, it never worked for me. 😭 Some debugging later, I figured out that it worked inside a regular shell, but not inside shells opened by `tmux` or `ssh`. Since I pretty much exclusively use shells opened by `tmux` and `ssh`, I spent some time frantically googling around for how to fix them, and found pretty much nothing. It worked outside of tmux, it didn't work inside tmux, and that was that.\n\nFast forward three years to today, and while griping to a friend about how it didn't work inside tmux, I discovered that technology has advanced and there is now [a fix, named `pam_reattach`](https://github.com/fabianishere/pam_reattach)! It's a PAM module that you configure to run before the built-in `pam_tid.so`, and it makes the `sudo` command able to find and use the TouchID module to authenticate, even from inside `tmux`.\n\nAmazingly, I was even able to find a written explanation of the thought process that produced the PAM module, in the form of this [Stack Overflow answer](https://superuser.com/a/1348180). Based on that answer and the linked discussion, it seems the steps were:\n\n1. Apple patches `screen` to stay attached to a user's GUI login session, so that CLI tools like `pbcopy`, `security`, and system calls like TouchID checks will continue to work.\n1. [@ChrisJohnsen](https://github.com/ChrisJohnsen) uses some of the undocumented functions called by the `screen` patch to implement the now-ubiquitous [`reattach-to-user-namespace`](https://github.com/ChrisJohnsen/tmux-MacOSX-pasteboard/) command that allows `pbcopy` and `pbpaste` to continue working inside `tmux` or non-Apple `screen`.\n1. Once TouchID MacBooks have emerged, [@Cabel](https://twitter.com/cabel/) reveals the existence of `pam_tid.so` and TouchID authentication for `sudo`. Unfortunately, it doesn't work in non-GUI processes like SSH, tmux, or homebrewed screen.\n1. [@fabianishere](https://github.com/fabianishere), again inspired by Apple's patches to `screen`, sends pull requests that re-enable TouchID to both [reattach-to-user-namespace](https://github.com/ChrisJohnsen/tmux-MacOSX-pasteboard/pull/70) and [tmux](https://github.com/tmux/tmux/pull/1434).\n1. After some discussion of unfortunate tradeoffs, both PRs are closed, and they create a new PAM module, analogous to `reattach-to-user-namespace` but just for the PAM flow: [`pam_reattach`](https://github.com/fabianishere/pam_reattach). This enables TouchID for `sudo` in both `tmux` and homebrewed `screen`.\n1. In macOS Catalina, Apple adds Apple Watch confirmations to the \"TouchID\" system. If you have an Apple Watch configured to unlock your Mac, you can also double-tap the watch button to confirm a TouchID prompt instead of scanning your finger.\n1. Enterprising Swift coder [@Reflejo](https://github.com/Reflejo) wrote a new PAM module to enable TouchID support named [`pam-touchid`](https://github.com/Reflejo/pam-touchID). Implemented in Swift, it is 1/3 shorter and (in my opinion) about 1000x easier to understand than the [straight C module from Apple](https://opensource.apple.com/source/pam_modules/pam_modules-173.1.1/modules/pam_tid/pam_tid.c.auto.html).\n1. [@biscuitehh](https://github.com/biscuitehh/), not content with Apple Watch `sudo` exclusively on Macs with TouchID hardware, forked `pam-touchid` into [`pam-watchid`](https://github.com/biscuitehh/pam-watchid), a PAM module that allows `sudo` via Apple Watch on any Mac.\n\nI'm pretty excited that I can finally `sudo` using my fingerprint or my watch, and a little bit in awe of the way determined nerds manage to figure things out eventually. Nice work, everyone.\n",
				"date_published": "2020-07-10T00:00:00-08:00",
				"url": "https://andre.arko.net/2020/07/10/codesudocode-with-touchid-and-apple/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2020/07/09/rails-with-webpack-in-appassets/",
				"title": "Rails 6 with Webpack in app/assets (and no Sprockets)",
				"content_html": "<p>The first version of Rails with Sprockets to manage JS and CSS assets shipped in May 2011. Generating a new Rails app today includes not only Sprockets, but an entire second JS and CSS asset pipeline that uses Webpack. It&hellip; sort of&hellip; makes sense to do this, for legacy reasons, but it&rsquo;s confusing.</p>\n<p>Putting CSS into <code>app/javascript/css</code> doesn&rsquo;t really make sense. Wouldn&rsquo;t it make more sense to put CSS into <code>app/assets/css</code>? It would make a lot more sense, but Webpack is stuck in <code>app/javascript</code> is because Sprockets already owns <code>app/assets</code>.</p>\n<p>Despite that, I have good news! It is possible to use <code>app/assets</code> for the JS, CSS, images, fonts, and other&hellip; assets&hellip; that are managed by Webpack. All you have to do is completely rip out Sprockets (giving up all gem-based JS and CSS) and then strategically reconfigure Webpack. Shall we get started?</p>\n<p>If you&rsquo;re starting a new Rails 6 app, you can use <code>rails new --no-sprockets</code> to avoid most of Sprockets. If you have an existing app, you&rsquo;re going to have to yank it out by hand.</p>\n<h3 id=\"remove-sprockets\">Remove Sprockets</h3>\n<p>(Skip this step if you generated a new Rails app with the <code>--no-sprockets</code> option.)</p>\n<ol>\n<li>\n<p><code>bundle remove sass-rails</code></p>\n</li>\n<li>\n<p><code>rm config/initalizers/assets.rb</code></p>\n</li>\n<li>\n<p>Replace <code>require 'rails/all'</code> in <code>config/application.rb</code> with these lines instead:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#34;rails&#34;</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># Pick the frameworks you want:</span>\n</span></span><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#34;active_model/railtie&#34;</span>\n</span></span><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#34;active_job/railtie&#34;</span>\n</span></span><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#34;active_record/railtie&#34;</span>\n</span></span><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#34;active_storage/engine&#34;</span>\n</span></span><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#34;action_controller/railtie&#34;</span>\n</span></span><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#34;action_mailer/railtie&#34;</span>\n</span></span><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#34;action_mailbox/engine&#34;</span>\n</span></span><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#34;action_text/engine&#34;</span>\n</span></span><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#34;action_view/railtie&#34;</span>\n</span></span><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#34;action_cable/engine&#34;</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># require &#34;sprockets/railtie&#34;</span>\n</span></span><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#34;rails/test_unit/railtie&#34;</span>\n</span></span></code></pre></div></li>\n<li>\n<p>Remove these lines from <code>config/application/development.rb</code></p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># Debug mode disables concatenation and preprocessing of assets.</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># This option may cause significant delays in view rendering with a large</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># number of complex assets.</span>\n</span></span><span style=\"display:flex;\"><span>config<span style=\"color:#f92672\">.</span>assets<span style=\"color:#f92672\">.</span>debug <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">true</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># Suppress logger output for asset requests.</span>\n</span></span><span style=\"display:flex;\"><span>config<span style=\"color:#f92672\">.</span>assets<span style=\"color:#f92672\">.</span>quiet <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">true</span>\n</span></span></code></pre></div></li>\n<li>\n<p>Remove these lines from <code>config/application/production.rb</code></p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># Compress CSS using a preprocessor.</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># config.assets.css_compressor = :sass</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># Do not fallback to assets pipeline if a precompiled asset is missed.</span>\n</span></span><span style=\"display:flex;\"><span>config<span style=\"color:#f92672\">.</span>assets<span style=\"color:#f92672\">.</span>compile <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">false</span>\n</span></span></code></pre></div></li>\n</ol>\n<p>Once you have Sprockets completely removed, make sure you <a href=\"https://github.com/rails/webpacker#installation\">have Webpacker installed</a>. If you generated a fresh Rails 6 app, you already have it.</p>\n<h3 id=\"move-webpack-to-appassets\">Move Webpack to app/assets/</h3>\n<p>Now that you&rsquo;ve gotten rid of Sprockets, you can configure the Webpacker gem to tell Webpack to use the <code>app/assets</code> directory to store your Webpack-organized CSS and JS.</p>\n<p>For some reason, Rails 6 still generates a Sprockets-style <code>app/assets</code> directory, even if you explicitly disable Sprockets, so we&rsquo;ll have to remove that first.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span>rm -rf app/assets\n</span></span></code></pre></div><p>Then, move the existing Webpack assets directory over there instead.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span>mv app/javascript app/assets\n</span></span></code></pre></div><p>Create an application stylesheet in <code>packs</code>.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span>touch app/assets/packs/application.scss\n</span></span></code></pre></div><p>(If you generate controllers, Rails will create stylesheets like <code>app/assets/stylesheets/controller.css</code>, and you can import those files with lines like <code>@import &quot;../stylesheets/controller.css&quot;;</code> in the pack.)</p>\n<p>Update <code>app/views/layouts/application.html.erb</code> to use the pack instead of the now-gone Sprockets stylesheet.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span>sed -i <span style=\"color:#e6db74\">&#39;&#39;</span> s/stylesheet_link_tag/stylesheet_pack_tag/ app/views/layouts/application.html.erb\n</span></span></code></pre></div><p>Finally, update the Webpacker gem config so that both Rails and Webpack will know where to look for your assets.</p>\n<p>In <code>config/webpacker.yml</code>, change <code>source_path: app/javascript</code> to <code>source_path: app/assets</code>.</p>\n<p>That&rsquo;s it! Welcome to the confusing and somewhat terrifying world of modern frontend dependencies. You can&rsquo;t use any Javascript provided via RubyGems anymore, but you can use any Javascript or CSS provided by npm packages. Just run <code>yarn add package-name</code>, and import the package in your <code>application.js</code> or <code>application.scss</code>. Enjoy it.</p>\n",
				"content_text": "The first version of Rails with Sprockets to manage JS and CSS assets shipped in May 2011. Generating a new Rails app today includes not only Sprockets, but an entire second JS and CSS asset pipeline that uses Webpack. It... sort of... makes sense to do this, for legacy reasons, but it's confusing.\n\nPutting CSS into `app/javascript/css` doesn't really make sense. Wouldn't it make more sense to put CSS into `app/assets/css`? It would make a lot more sense, but Webpack is stuck in `app/javascript` is because Sprockets already owns `app/assets`.\n\nDespite that, I have good news! It is possible to use `app/assets` for the JS, CSS, images, fonts, and other... assets... that are managed by Webpack. All you have to do is completely rip out Sprockets (giving up all gem-based JS and CSS) and then strategically reconfigure Webpack. Shall we get started?\n\nIf you're starting a new Rails 6 app, you can use `rails new --no-sprockets` to avoid most of Sprockets. If you have an existing app, you're going to have to yank it out by hand.\n\n### Remove Sprockets\n\n(Skip this step if you generated a new Rails app with the `--no-sprockets` option.)\n\n1. `bundle remove sass-rails`\n2. `rm config/initalizers/assets.rb`\n3. Replace `require 'rails/all'` in `config/application.rb` with these lines instead:\n\n    ```ruby\n    require \"rails\"\n    # Pick the frameworks you want:\n    require \"active_model/railtie\"\n    require \"active_job/railtie\"\n    require \"active_record/railtie\"\n    require \"active_storage/engine\"\n    require \"action_controller/railtie\"\n    require \"action_mailer/railtie\"\n    require \"action_mailbox/engine\"\n    require \"action_text/engine\"\n    require \"action_view/railtie\"\n    require \"action_cable/engine\"\n    # require \"sprockets/railtie\"\n    require \"rails/test_unit/railtie\"\n    ```\n\n4. Remove these lines from `config/application/development.rb`\n\n    ```ruby\n    # Debug mode disables concatenation and preprocessing of assets.\n    # This option may cause significant delays in view rendering with a large\n    # number of complex assets.\n    config.assets.debug = true\n\n    # Suppress logger output for asset requests.\n    config.assets.quiet = true\n    ```\n\n5. Remove these lines from `config/application/production.rb`\n\n    ```ruby\n    # Compress CSS using a preprocessor.\n    # config.assets.css_compressor = :sass\n\n    # Do not fallback to assets pipeline if a precompiled asset is missed.\n    config.assets.compile = false\n    ```\n\nOnce you have Sprockets completely removed, make sure you [have Webpacker installed](https://github.com/rails/webpacker#installation). If you generated a fresh Rails 6 app, you already have it.\n\n### Move Webpack to app/assets/\n\nNow that you've gotten rid of Sprockets, you can configure the Webpacker gem to tell Webpack to use the `app/assets` directory to store your Webpack-organized CSS and JS.\n\nFor some reason, Rails 6 still generates a Sprockets-style `app/assets` directory, even if you explicitly disable Sprockets, so we'll have to remove that first.\n\n```bash\nrm -rf app/assets\n```\n\nThen, move the existing Webpack assets directory over there instead.\n\n```bash\nmv app/javascript app/assets\n```\n\nCreate an application stylesheet in `packs`.\n\n```bash\ntouch app/assets/packs/application.scss\n```\n\n(If you generate controllers, Rails will create stylesheets like `app/assets/stylesheets/controller.css`, and you can import those files with lines like `@import \"../stylesheets/controller.css\";` in the pack.)\n\nUpdate `app/views/layouts/application.html.erb` to use the pack instead of the now-gone Sprockets stylesheet.\n\n```bash\nsed -i '' s/stylesheet_link_tag/stylesheet_pack_tag/ app/views/layouts/application.html.erb\n```\n\nFinally, update the Webpacker gem config so that both Rails and Webpack will know where to look for your assets. \n\nIn `config/webpacker.yml`, change `source_path: app/javascript` to `source_path: app/assets`.\n\nThat's it! Welcome to the confusing and somewhat terrifying world of modern frontend dependencies. You can't use any Javascript provided via RubyGems anymore, but you can use any Javascript or CSS provided by npm packages. Just run `yarn add package-name`, and import the package in your `application.js` or `application.scss`. Enjoy it.\n",
				"date_published": "2020-07-09T00:00:00-08:00",
				"url": "https://andre.arko.net/2020/07/09/rails-with-webpack-in-appassets/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2020/06/30/building-ruby-on-arm-macos/",
				"title": "Building Ruby on arm64 macOS",
				"content_html": "<p>As part of my <a href=\"/2020/06/29/symbol-_ffi_prep_closure-not-found/\">new computer project</a>, I&rsquo;ve been trying to compile Ruby for arm64-darwin20 from source. The bad news is that if you don&rsquo;t do anything in particular, it won&rsquo;t work:</p>\n<pre tabindex=\"0\"><code>compiling closure.c\nclosure.c:264:14: error: implicit declaration of function &#39;ffi_prep_closure&#39; is invalid in C99 [-Werror,-Wimplicit-function-declaration]\n    result = ffi_prep_closure(pcl, cif, callback, (void *)self);\n             ^\n1 error generated.\nmake[2]: *** [closure.o] Error 1\nmake[1]: *** [ext/fiddle/all] Error 2\nmake: *** [build-ext] Error 2\n</code></pre><p>The good news is that it&rsquo;s possible to get it to work anyway. If you use <a href=\"https://github.com/postmodern/ruby-install\">ruby-install</a>, there are two steps. First, you need to <a href=\"https://github.com/indirect/ruby-install/commit/e0079f5354bb373bbd7ce361f72ffae9deba836f\">remove <code>libffi</code> from the list of packages</a>. Then, you need to pass a couple of flags to the install process.</p>\n<p>It looks like the ruby-core team is <a href=\"https://github.com/ruby/ruby/commit/7cb8904a12c850ee30dcd67817fa2f9dc3fee813\">already starting to merge explicit support for arm64</a>, so hopefully it won&rsquo;t be long before none of the extra flags need to be passed in at all.</p>\n<p>I&rsquo;ve already removed the <code>libffi</code> brew package from my fork, so you can build Ruby 2.7.1 and 2.6.6 like this:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span>git clone https://github.com/indirect/ruby-install.git\n</span></span><span style=\"display:flex;\"><span>cd ruby-install\n</span></span><span style=\"display:flex;\"><span>./bin/ruby-install ruby 2.7.1 -c -- --with-arch<span style=\"color:#f92672\">=</span>arm64 CFLAGS<span style=\"color:#f92672\">=</span>-DUSE_FFI_CLOSURE_ALLOC<span style=\"color:#f92672\">=</span><span style=\"color:#ae81ff\">1</span> \n</span></span><span style=\"display:flex;\"><span>./bin/ruby-install ruby 2.6.6 -c -- --with-arch<span style=\"color:#f92672\">=</span>arm64 CFLAGS<span style=\"color:#f92672\">=</span>-DUSE_FFI_CLOSURE_ALLOC<span style=\"color:#f92672\">=</span><span style=\"color:#ae81ff\">1</span> \n</span></span></code></pre></div><p>Assuming you&rsquo;ve already followed the <a href=\"/2020/06/29/symbol-_ffi_prep_closure-not-found/\">steps from yesterday</a> to fix system Ruby and Homebrew, that should be all you need!</p>\n<pre tabindex=\"0\"><code>$ ~/.rubies/ruby-2.7.1/bin/ruby -v\nruby 2.7.1p83 (2020-03-31 revision a0c7c23c9c) [arm64-darwin20]\n$ ~/.rubies/ruby-2.6.6/bin/ruby -v \nruby 2.6.6p146 (2020-03-31 revision 67876) [arm64-darwin20]\n</code></pre>",
				"content_text": "As part of my [new computer project](/2020/06/29/symbol-_ffi_prep_closure-not-found/), I've been trying to compile Ruby for arm64-darwin20 from source. The bad news is that if you don't do anything in particular, it won't work:\n\n```\ncompiling closure.c\nclosure.c:264:14: error: implicit declaration of function 'ffi_prep_closure' is invalid in C99 [-Werror,-Wimplicit-function-declaration]\n    result = ffi_prep_closure(pcl, cif, callback, (void *)self);\n             ^\n1 error generated.\nmake[2]: *** [closure.o] Error 1\nmake[1]: *** [ext/fiddle/all] Error 2\nmake: *** [build-ext] Error 2\n```\n\nThe good news is that it's possible to get it to work anyway. If you use [ruby-install](https://github.com/postmodern/ruby-install), there are two steps. First, you need to [remove `libffi` from the list of packages](https://github.com/indirect/ruby-install/commit/e0079f5354bb373bbd7ce361f72ffae9deba836f). Then, you need to pass a couple of flags to the install process.\n\nIt looks like the ruby-core team is [already starting to merge explicit support for arm64](https://github.com/ruby/ruby/commit/7cb8904a12c850ee30dcd67817fa2f9dc3fee813), so hopefully it won't be long before none of the extra flags need to be passed in at all.\n\nI've already removed the `libffi` brew package from my fork, so you can build Ruby 2.7.1 and 2.6.6 like this:\n\n```bash\ngit clone https://github.com/indirect/ruby-install.git\ncd ruby-install\n./bin/ruby-install ruby 2.7.1 -c -- --with-arch=arm64 CFLAGS=-DUSE_FFI_CLOSURE_ALLOC=1 \n./bin/ruby-install ruby 2.6.6 -c -- --with-arch=arm64 CFLAGS=-DUSE_FFI_CLOSURE_ALLOC=1 \n```\n\nAssuming you've already followed the [steps from yesterday](/2020/06/29/symbol-_ffi_prep_closure-not-found/) to fix system Ruby and Homebrew, that should be all you need!\n\n```\n$ ~/.rubies/ruby-2.7.1/bin/ruby -v\nruby 2.7.1p83 (2020-03-31 revision a0c7c23c9c) [arm64-darwin20]\n$ ~/.rubies/ruby-2.6.6/bin/ruby -v \nruby 2.6.6p146 (2020-03-31 revision 67876) [arm64-darwin20]\n```\n",
				"date_published": "2020-06-30T00:00:00-08:00",
				"url": "https://andre.arko.net/2020/06/30/building-ruby-on-arm-macos/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2020/06/29/symbol-ffiprepclosure-not-found/",
				"title": "symbol '_ffi_prep_closure' not found",
				"content_html": "<p>I got a new computer today! It was very exciting. Unfortunately, when I tried to install <a href=\"https://brew.sh\">homebrew</a>, I discovered that the copy of Ruby included in macOS had a small problem that manifested in the form of this somewhat inscrutable error:</p>\n<pre tabindex=\"0\"><code>/System/Library/Frameworks/Ruby.framework/Versions/2.6/usr/lib/ruby/2.6.0/rubygems/core_ext/kernel_require.rb:54:in `require&#39;: dlopen(/System/Library/Frameworks/Ruby.framework/Versions/2.6/usr/lib/ruby/2.6.0/universal-darwin20/fiddle.bundle, 0x0009): symbol &#39;_ffi_prep_closure&#39; not found, expected in flat namespace by &#39;/System/Library/Frameworks/Ruby.framework/Versions/2.6/usr/lib/ruby/2.6.0/universal-darwin20/fiddle.bundle&#39; - /System/Library/Frameworks/Ruby.framework/Versions/2.6/usr/lib/ruby/2.6.0/universal-darwin20/fiddle.bundle (LoadError)\n</code></pre><p>I&rsquo;m not 100% sure why the <code>fiddle</code> gem that came with macOS ruby wasn&rsquo;t able to find the symbols it needed, but <a href=\"https://github.com/ffi/ffi/pull/746\">it seems like libffi changed that function name recently</a>, so maybe that was the cause of the disconnect? Unfortunately, reinstalling the latest version of <code>fiddle</code> (1.0.0 at this writing) didn&rsquo;t do anything to help. Happily, installing the latest commit from the git repository worked great.</p>\n<p>To fix your <code>fiddle</code> gem, and thereby Ruby, allowing you to use Homebrew to slowly compile the universe from scratch, you can build and install the latest <code>fiddle</code> like this:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span>$ git clone https://github.com/ruby/fiddle\n</span></span><span style=\"display:flex;\"><span>$ cd fiddle\n</span></span><span style=\"display:flex;\"><span>$ bundle install --path vendor/bundle\n</span></span><span style=\"display:flex;\"><span>$ bundle exec rake build\n</span></span><span style=\"display:flex;\"><span>$ sudo gem install pkg/fiddle-1.0.1.gem\n</span></span></code></pre></div><p>All set! Brew your way to greatness.</p>\n<p>P.S. There&rsquo;s one other error you might hit, but it also has a quick solution:</p>\n<pre tabindex=\"0\"><code>xcrun: error: unable to load libxcrun (dlopen(/Library/Developer/CommandLineTools/usr/lib/libxcrun.dylib, 0x0005): could not use &#39;/Library/Developer/CommandLineTools/usr/lib/libxcrun.dylib&#39; because it is not a compatible arch).\n</code></pre><p>If you see this, run <code>sudo rm -rf /Library/Developer/CommandLineTools; sudo xcode-select --switch /Application/Xcode-beta.app</code>. The Homebrew installer really really wants to install those Command Line Tools, but they&rsquo;re the wrong architecture and won&rsquo;t ever run. Use the devtools built into Xcode-beta instead.</p>\n<p>P.P.S. If you also want to build Ruby yourself, to get the latest version, check out <a href=\"/2020/06/30/building-ruby-on-arm64-macos/\">this follow-up post</a>.</p>\n",
				"content_text": "I got a new computer today! It was very exciting. Unfortunately, when I tried to install [homebrew](https://brew.sh), I discovered that the copy of Ruby included in macOS had a small problem that manifested in the form of this somewhat inscrutable error:\n\n```\n/System/Library/Frameworks/Ruby.framework/Versions/2.6/usr/lib/ruby/2.6.0/rubygems/core_ext/kernel_require.rb:54:in `require': dlopen(/System/Library/Frameworks/Ruby.framework/Versions/2.6/usr/lib/ruby/2.6.0/universal-darwin20/fiddle.bundle, 0x0009): symbol '_ffi_prep_closure' not found, expected in flat namespace by '/System/Library/Frameworks/Ruby.framework/Versions/2.6/usr/lib/ruby/2.6.0/universal-darwin20/fiddle.bundle' - /System/Library/Frameworks/Ruby.framework/Versions/2.6/usr/lib/ruby/2.6.0/universal-darwin20/fiddle.bundle (LoadError)\n```\n\nI'm not 100% sure why the `fiddle` gem that came with macOS ruby wasn't able to find the symbols it needed, but [it seems like libffi changed that function name recently](https://github.com/ffi/ffi/pull/746), so maybe that was the cause of the disconnect? Unfortunately, reinstalling the latest version of `fiddle` (1.0.0 at this writing) didn't do anything to help. Happily, installing the latest commit from the git repository worked great.\n\nTo fix your `fiddle` gem, and thereby Ruby, allowing you to use Homebrew to slowly compile the universe from scratch, you can build and install the latest `fiddle` like this:\n\n```bash\n$ git clone https://github.com/ruby/fiddle\n$ cd fiddle\n$ bundle install --path vendor/bundle\n$ bundle exec rake build\n$ sudo gem install pkg/fiddle-1.0.1.gem\n```\n\nAll set! Brew your way to greatness.\n\nP.S. There's one other error you might hit, but it also has a quick solution:\n\n```\nxcrun: error: unable to load libxcrun (dlopen(/Library/Developer/CommandLineTools/usr/lib/libxcrun.dylib, 0x0005): could not use '/Library/Developer/CommandLineTools/usr/lib/libxcrun.dylib' because it is not a compatible arch).\n```\n\nIf you see this, run `sudo rm -rf /Library/Developer/CommandLineTools; sudo xcode-select --switch /Application/Xcode-beta.app`. The Homebrew installer really really wants to install those Command Line Tools, but they're the wrong architecture and won't ever run. Use the devtools built into Xcode-beta instead.\n\nP.P.S. If you also want to build Ruby yourself, to get the latest version, check out [this follow-up post](/2020/06/30/building-ruby-on-arm64-macos/).\n",
				"date_published": "2020-06-29T00:00:00-08:00",
				"url": "https://andre.arko.net/2020/06/29/symbol-ffiprepclosure-not-found/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2020/06/26/automatically-update-every-github-default/",
				"title": "Automatically update every GitHub default branch",
				"content_html": "<p>When I wrote about <a href=\"/2020/06/06/changing-git-and-githubs-default-branch-name/\">how to change git and GitHub&rsquo;s default branches</a>, I was thinking entirely in terms of fixing the problem one repo at a time. The next day, GitHub announced they would be changing the default, so I thought my script wouldn&rsquo;t even be necessary, since surely GitHub would offer a bulk change tool as part of changing the default.</p>\n<p>Unfortunately, it&rsquo;s been 20 days, and GitHub not only hasn&rsquo;t shipped a tool to bulk-change default branches, it hasn&rsquo;t shipped the default branch name change at all. In the spirit of solving the problem as quickly as possible, here&rsquo;s a script that will change the default branch for every repository in a particular user or organization on GitHub. You&rsquo;ll need <a href=\"https://hub.github.com\">hub</a> installed. On macOS, that means running <code>brew install hub</code>, and then <code>hub api user</code> to trigger authentication.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\">#!/bin/bash\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#75715e\"></span>NAME<span style=\"color:#f92672\">=</span><span style=\"color:#e6db74\">&#34;</span>$1<span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>BRANCH<span style=\"color:#f92672\">=</span><span style=\"color:#e6db74\">&#34;</span><span style=\"color:#e6db74\">${</span>2<span style=\"color:#66d9ef\">:-</span>main<span style=\"color:#e6db74\">}</span><span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>HUB<span style=\"color:#f92672\">=</span><span style=\"color:#66d9ef\">$(</span>which hub <span style=\"color:#f92672\">||</span> echo /usr/local/bin/hub<span style=\"color:#66d9ef\">)</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># get repos that belong to the given user/org, are not archived, and are not forks</span>\n</span></span><span style=\"display:flex;\"><span>repos<span style=\"color:#f92672\">=(</span><span style=\"color:#66d9ef\">$(</span>$HUB api --paginate --obey-ratelimit --flat graphql -f query<span style=\"color:#f92672\">=</span><span style=\"color:#e6db74\">&#39;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">  query($endCursor: String) {\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">    repositoryOwner(login: &#34;&#39;</span><span style=\"color:#e6db74\">&#34;</span>$NAME<span style=\"color:#e6db74\">&#34;</span><span style=\"color:#e6db74\">&#39;&#34;) {\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">      repositories(isLocked: false, isFork: false, first: 100, after: $endCursor) {\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">        nodes { nameWithOwner }\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">        pageInfo { hasNextPage, endCursor }\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">      }\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">    }\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">  }\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">&#39;</span> | grep nameWithOwner | cut -f2 | grep <span style=\"color:#e6db74\">&#34;</span>$NAME<span style=\"color:#e6db74\">/&#34;</span><span style=\"color:#66d9ef\">)</span><span style=\"color:#f92672\">)</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>count<span style=\"color:#f92672\">=</span><span style=\"color:#66d9ef\">$(</span>echo <span style=\"color:#66d9ef\">$(</span>echo <span style=\"color:#e6db74\">&#34;</span><span style=\"color:#e6db74\">${</span>repos[*]<span style=\"color:#e6db74\">}</span><span style=\"color:#e6db74\">&#34;</span> | wc -w<span style=\"color:#66d9ef\">))</span>\n</span></span><span style=\"display:flex;\"><span>echo <span style=\"color:#e6db74\">&#34;found </span>$count<span style=\"color:#e6db74\"> repos belonging to </span>$NAME<span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">for</span> repo in <span style=\"color:#e6db74\">${</span>repos[@]<span style=\"color:#e6db74\">}</span>; <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>  echo <span style=\"color:#e6db74\">&#34;</span>$repo<span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#75715e\"># look for a branch with the right name</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">if</span> $HUB api --flat <span style=\"color:#e6db74\">&#34;repos/</span>$repo<span style=\"color:#e6db74\">/git/refs/heads/</span>$BRANCH<span style=\"color:#e6db74\">&#34;</span> | grep <span style=\"color:#e6db74\">&#34;.object.sha&#34;</span> 1&gt; /dev/null; <span style=\"color:#66d9ef\">then</span>\n</span></span><span style=\"display:flex;\"><span>    echo <span style=\"color:#e6db74\">&#34;  found branch </span>$BRANCH<span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">else</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#75715e\"># create the branch we need if it doesn&#39;t exist</span>\n</span></span><span style=\"display:flex;\"><span>    SHA<span style=\"color:#f92672\">=</span><span style=\"color:#66d9ef\">$(</span>$HUB api --flat <span style=\"color:#e6db74\">&#34;repos/</span>$repo<span style=\"color:#e6db74\">/git/refs/heads/master&#34;</span> | grep <span style=\"color:#e6db74\">&#34;.object.sha&#34;</span> | cut -f2<span style=\"color:#66d9ef\">)</span>\n</span></span><span style=\"display:flex;\"><span>    $HUB api <span style=\"color:#e6db74\">&#34;repos/</span>$repo<span style=\"color:#e6db74\">/git/refs&#34;</span> -F <span style=\"color:#e6db74\">&#34;ref=refs/heads/</span>$BRANCH<span style=\"color:#e6db74\">&#34;</span> -F <span style=\"color:#e6db74\">&#34;sha=</span>$SHA<span style=\"color:#e6db74\">&#34;</span> 1&gt; /dev/null\n</span></span><span style=\"display:flex;\"><span>    echo <span style=\"color:#e6db74\">&#34;  created branch </span>$BRANCH<span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">fi</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#75715e\"># now that the branch exists, update the repo default branch</span>\n</span></span><span style=\"display:flex;\"><span>  $HUB api <span style=\"color:#e6db74\">&#34;repos/</span>$repo<span style=\"color:#e6db74\">&#34;</span> -X PATCH -F default_branch<span style=\"color:#f92672\">=</span><span style=\"color:#e6db74\">&#34;</span>$BRANCH<span style=\"color:#e6db74\">&#34;</span> --flat 1&gt; /dev/null\n</span></span><span style=\"display:flex;\"><span>  echo <span style=\"color:#e6db74\">&#34;  set default branch to </span>$BRANCH<span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#75715e\"># check how close we are to the rate limit</span>\n</span></span><span style=\"display:flex;\"><span>  ratelimit<span style=\"color:#f92672\">=</span><span style=\"color:#66d9ef\">$(</span>$HUB api rate_limit --flat | grep .core<span style=\"color:#66d9ef\">)</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">if</span> <span style=\"color:#f92672\">[[</span> <span style=\"color:#66d9ef\">$(</span>echo <span style=\"color:#e6db74\">&#34;</span>$ratelimit<span style=\"color:#e6db74\">&#34;</span> | grep .remaining | cut -f2<span style=\"color:#66d9ef\">)</span> &lt; <span style=\"color:#ae81ff\">4</span> <span style=\"color:#f92672\">]]</span>; <span style=\"color:#66d9ef\">then</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#75715e\"># if we have less than 4 API calls left, sleep until the limit resets</span>\n</span></span><span style=\"display:flex;\"><span>    sleep <span style=\"color:#66d9ef\">$(</span>expr <span style=\"color:#66d9ef\">$(</span>echo <span style=\"color:#e6db74\">&#34;</span>$ratelimit<span style=\"color:#e6db74\">&#34;</span> | grep .reset | cut -f2<span style=\"color:#66d9ef\">)</span> - <span style=\"color:#66d9ef\">$(</span>date +%s<span style=\"color:#66d9ef\">))</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">fi</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">done</span>\n</span></span></code></pre></div><p>If you just want to do the thing, you can run it directly like this:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span>curl -L https://git.io/JJeCZ | bash -s USERNAME <span style=\"color:#f92672\">[</span>BRANCH<span style=\"color:#f92672\">]</span>\n</span></span></code></pre></div><p>Replace <code>USERNAME</code> with your GitHub username, or the name of an organization whose repos you want to update. The second argument <code>BRANCH</code> is optional, and defaults to <code>main</code>. Keep in mind that you might not have permission to change the default branch on every repo in an organization unless you have &ldquo;Owner&rdquo; permissions in that org.</p>\n<p>If anything goes wrong, it shouldn&rsquo;t hurt anything to run the script more than once&ndash;repos that have already updated will get processed faster, with less API calls.</p>\n<p>Depending on how many repos you want to update, this might take a couple of hours. GitHub only allows 5000 API requests per hour, and this script needs 2-4 requests per repo. If the script hits the rate limit, it will sleep until the time GitHub said the limit would reset and then keep going.</p>\n<p>While you&rsquo;re waiting for all of your repos to update, why not donate to <a href=\"https://thelovelandfoundation.org/loveland-therapy-fund/\">The Loveland Foundation</a>, <a href=\"https://www.theokraproject.com\">The Okra Project</a>, or <a href=\"https://www.innocenceproject.org\">The Innocence Project</a>?</p>\n",
				"content_text": "\nWhen I wrote about [how to change git and GitHub's default branches](/2020/06/06/changing-git-and-githubs-default-branch-name/), I was thinking entirely in terms of fixing the problem one repo at a time. The next day, GitHub announced they would be changing the default, so I thought my script wouldn't even be necessary, since surely GitHub would offer a bulk change tool as part of changing the default.\n\nUnfortunately, it's been 20 days, and GitHub not only hasn't shipped a tool to bulk-change default branches, it hasn't shipped the default branch name change at all. In the spirit of solving the problem as quickly as possible, here's a script that will change the default branch for every repository in a particular user or organization on GitHub. You'll need [hub](https://hub.github.com) installed. On macOS, that means running `brew install hub`, and then `hub api user` to trigger authentication.\n\n```bash\n#!/bin/bash\nNAME=\"$1\"\nBRANCH=\"${2:-main}\"\nHUB=$(which hub || echo /usr/local/bin/hub)\n\n# get repos that belong to the given user/org, are not archived, and are not forks\nrepos=($($HUB api --paginate --obey-ratelimit --flat graphql -f query='\n  query($endCursor: String) {\n    repositoryOwner(login: \"'\"$NAME\"'\") {\n      repositories(isLocked: false, isFork: false, first: 100, after: $endCursor) {\n        nodes { nameWithOwner }\n        pageInfo { hasNextPage, endCursor }\n      }\n    }\n  }\n' | grep nameWithOwner | cut -f2 | grep \"$NAME/\"))\n\ncount=$(echo $(echo \"${repos[*]}\" | wc -w))\necho \"found $count repos belonging to $NAME\"\n\nfor repo in ${repos[@]}; do\n  echo \"$repo\"\n\n  # look for a branch with the right name\n  if $HUB api --flat \"repos/$repo/git/refs/heads/$BRANCH\" | grep \".object.sha\" 1> /dev/null; then\n    echo \"  found branch $BRANCH\"\n  else\n    # create the branch we need if it doesn't exist\n    SHA=$($HUB api --flat \"repos/$repo/git/refs/heads/master\" | grep \".object.sha\" | cut -f2)\n    $HUB api \"repos/$repo/git/refs\" -F \"ref=refs/heads/$BRANCH\" -F \"sha=$SHA\" 1> /dev/null\n    echo \"  created branch $BRANCH\"\n  fi\n\n  # now that the branch exists, update the repo default branch\n  $HUB api \"repos/$repo\" -X PATCH -F default_branch=\"$BRANCH\" --flat 1> /dev/null\n  echo \"  set default branch to $BRANCH\"\n\n  # check how close we are to the rate limit\n  ratelimit=$($HUB api rate_limit --flat | grep .core)\n  if [[ $(echo \"$ratelimit\" | grep .remaining | cut -f2) < 4 ]]; then\n    # if we have less than 4 API calls left, sleep until the limit resets\n    sleep $(expr $(echo \"$ratelimit\" | grep .reset | cut -f2) - $(date +%s))\n  fi\ndone\n```\n\nIf you just want to do the thing, you can run it directly like this:\n\n```bash\ncurl -L https://git.io/JJeCZ | bash -s USERNAME [BRANCH]\n```\n\nReplace `USERNAME` with your GitHub username, or the name of an organization whose repos you want to update. The second argument `BRANCH` is optional, and defaults to `main`. Keep in mind that you might not have permission to change the default branch on every repo in an organization unless you have \"Owner\" permissions in that org.\n\nIf anything goes wrong, it shouldn't hurt anything to run the script more than once--repos that have already updated will get processed faster, with less API calls.\n\nDepending on how many repos you want to update, this might take a couple of hours. GitHub only allows 5000 API requests per hour, and this script needs 2-4 requests per repo. If the script hits the rate limit, it will sleep until the time GitHub said the limit would reset and then keep going.\n\nWhile you're waiting for all of your repos to update, why not donate to [The Loveland Foundation](https://thelovelandfoundation.org/loveland-therapy-fund/), [The Okra Project](https://www.theokraproject.com), or [The Innocence Project](https://www.innocenceproject.org)?\n",
				"date_published": "2020-06-26T00:00:00-08:00",
				"url": "https://andre.arko.net/2020/06/26/automatically-update-every-github-default/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2020/06/06/changing-git-and-githubs-default/",
				"title": "Changing git and GitHub's default branch name",
				"content_html": "<p>First off: <a href=\"https://blacklivesmatter.com\">Black lives matter</a>. Go support Black activism in your community right now. I&rsquo;ll wait. If you&rsquo;re in the bay area, try <a href=\"https://www.instagram.com/peoplesbreakfastoakland/\">People&rsquo;s Breakfast Oakland</a>, the <a href=\"http://www.tgijp.org\">Transgender Gender-Variant &amp; Intersex Justice Project</a>, the <a href=\"https://antirepressionbayarea.com\">Bay area anti-repression committee</a>, and the <a href=\"https://nlgsf.org\">National Lawyer&rsquo;s Guild SF</a>.</p>\n<p>Okay, now that you&rsquo;re back, let&rsquo;s talk about a tiny way you can avoid referencing the incredibly fucked up history of racist oppression in the US while writing software: stop naming branches <code>master</code>. It&rsquo;s surprisingly hard, since neither git nor GitHub let you set a default for all new repos. These are some scripts I have cobbled together to work around that for my preferred primary branch name <code>main</code>.</p>\n<p>To work around the way <code>master</code> is literally hardcoded into <code>git</code>, you&rsquo;ll need to replace <code>git init</code>. Git doesn&rsquo;t let you override subcommands with aliases, so this has to be a shell function. That said, this function should work just fine in either bash or zsh.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">function</span> git<span style=\"color:#f92672\">()</span> <span style=\"color:#f92672\">{</span>\n</span></span><span style=\"display:flex;\"><span>  command git <span style=\"color:#e6db74\">&#34;</span>$@<span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">if</span> <span style=\"color:#f92672\">[[</span> <span style=\"color:#e6db74\">&#34;</span>$1<span style=\"color:#e6db74\">&#34;</span> <span style=\"color:#f92672\">==</span> <span style=\"color:#e6db74\">&#34;init&#34;</span> <span style=\"color:#f92672\">&amp;&amp;</span> <span style=\"color:#e6db74\">&#34;</span>$@<span style=\"color:#e6db74\">&#34;</span> !<span style=\"color:#f92672\">=</span> *<span style=\"color:#e6db74\">&#34;--help&#34;</span>* <span style=\"color:#f92672\">]]</span>; <span style=\"color:#66d9ef\">then</span>\n</span></span><span style=\"display:flex;\"><span>    git symbolic-ref HEAD refs/heads/main\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">fi</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#f92672\">}</span>\n</span></span></code></pre></div><p>The somewhat trickier part is changing the GitHub default branch, which you can&rsquo;t do by pushing branches. If (and only if) your very first push to the empty repo is on a different branch, that branch will become your default. Assuming you used the modified <code>git init</code> listed above, you can create a repo with <code>hub create</code>, and push directly using <code>git push</code>. (<a href=\"https://github.com/github/hub\">The <code>hub</code> command</a> is a very useful CLI tool for interacting with GitHub.)</p>\n<p>If you&rsquo;ve already pushed, or used the web UI, however, the default branch has automatically been set. The only official way to change a default branch is using the website, going to Settings, and clicking a bunch. I don&rsquo;t want to do that over and over, so I also created a wrapper for <code>hub</code> that adds a subcommand to change the default branch on GitHub for the repo in the current directory. This function should also work with either bash or zsh.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">function</span> hub<span style=\"color:#f92672\">()</span> <span style=\"color:#f92672\">{</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">if</span> <span style=\"color:#f92672\">[[</span> <span style=\"color:#e6db74\">&#34;</span>$1<span style=\"color:#e6db74\">&#34;</span> <span style=\"color:#f92672\">==</span> <span style=\"color:#e6db74\">&#34;default-branch&#34;</span> <span style=\"color:#f92672\">&amp;&amp;</span> <span style=\"color:#e6db74\">&#34;</span>$@<span style=\"color:#e6db74\">&#34;</span> !<span style=\"color:#f92672\">=</span> *<span style=\"color:#e6db74\">&#34;--help&#34;</span>* <span style=\"color:#f92672\">]]</span>; <span style=\"color:#66d9ef\">then</span>\n</span></span><span style=\"display:flex;\"><span>    local BRANCH<span style=\"color:#f92672\">=</span><span style=\"color:#e6db74\">&#34;</span><span style=\"color:#e6db74\">${</span>2<span style=\"color:#66d9ef\">:-</span>main<span style=\"color:#e6db74\">}</span><span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>    git checkout -b <span style=\"color:#e6db74\">&#34;</span>$BRANCH<span style=\"color:#e6db74\">&#34;</span> 2&gt;/dev/null <span style=\"color:#f92672\">||</span> git checkout <span style=\"color:#e6db74\">&#34;</span>$BRANCH<span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>    git push origin <span style=\"color:#e6db74\">&#34;</span>$BRANCH<span style=\"color:#e6db74\">:</span>$BRANCH<span style=\"color:#e6db74\">&#34;</span> 1&gt;/dev/null\n</span></span><span style=\"display:flex;\"><span>    hub api repos/<span style=\"color:#f92672\">{</span>owner<span style=\"color:#f92672\">}</span>/<span style=\"color:#f92672\">{</span>repo<span style=\"color:#f92672\">}</span> -X PATCH -F default_branch<span style=\"color:#f92672\">=</span><span style=\"color:#e6db74\">&#34;</span>$BRANCH<span style=\"color:#e6db74\">&#34;</span> 1&gt; /dev/null\n</span></span><span style=\"display:flex;\"><span>    git branch -D master 2&gt;/dev/null\n</span></span><span style=\"display:flex;\"><span>    git push origin :master 2&gt;/dev/null\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">else</span>\n</span></span><span style=\"display:flex;\"><span>    command hub <span style=\"color:#e6db74\">&#34;</span>$@<span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">fi</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#f92672\">}</span>\n</span></span></code></pre></div><p>To use it, run <code>hub default-branch [NAME]</code> in a checkout of the repo you want to change. If you pass an argument, that will be used as the branch name. Otherwise, the branch name <code>main</code> will be used.</p>\n<p>Now that your git repos are slightly less bad, why not spend some time looking for actions you can take to oppose racism today?</p>\n",
				"content_text": "\nFirst off: [Black lives matter](https://blacklivesmatter.com). Go support Black activism in your community right now. I'll wait. If you're in the bay area, try [People's Breakfast Oakland](https://www.instagram.com/peoplesbreakfastoakland/), the [Transgender Gender-Variant & Intersex Justice Project](http://www.tgijp.org), the [Bay area anti-repression committee](https://antirepressionbayarea.com), and the [National Lawyer's Guild SF](https://nlgsf.org).\n\nOkay, now that you're back, let's talk about a tiny way you can avoid referencing the incredibly fucked up history of racist oppression in the US while writing software: stop naming branches `master`. It's surprisingly hard, since neither git nor GitHub let you set a default for all new repos. These are some scripts I have cobbled together to work around that for my preferred primary branch name `main`.\n\nTo work around the way `master` is literally hardcoded into `git`, you'll need to replace `git init`. Git doesn't let you override subcommands with aliases, so this has to be a shell function. That said, this function should work just fine in either bash or zsh.\n\n```bash\nfunction git() {\n  command git \"$@\"\n  if [[ \"$1\" == \"init\" && \"$@\" != *\"--help\"* ]]; then\n    git symbolic-ref HEAD refs/heads/main\n  fi\n}\n```\n\nThe somewhat trickier part is changing the GitHub default branch, which you can't do by pushing branches. If (and only if) your very first push to the empty repo is on a different branch, that branch will become your default. Assuming you used the modified `git init` listed above, you can create a repo with `hub create`, and push directly using `git push`. ([The `hub` command](https://github.com/github/hub) is a very useful CLI tool for interacting with GitHub.)\n\nIf you've already pushed, or used the web UI, however, the default branch has automatically been set. The only official way to change a default branch is using the website, going to Settings, and clicking a bunch. I don't want to do that over and over, so I also created a wrapper for `hub` that adds a subcommand to change the default branch on GitHub for the repo in the current directory. This function should also work with either bash or zsh.\n\n```bash\nfunction hub() {\n  if [[ \"$1\" == \"default-branch\" && \"$@\" != *\"--help\"* ]]; then\n    local BRANCH=\"${2:-main}\"\n    git checkout -b \"$BRANCH\" 2>/dev/null || git checkout \"$BRANCH\"\n    git push origin \"$BRANCH:$BRANCH\" 1>/dev/null\n    hub api repos/{owner}/{repo} -X PATCH -F default_branch=\"$BRANCH\" 1> /dev/null\n    git branch -D master 2>/dev/null\n    git push origin :master 2>/dev/null\n  else\n    command hub \"$@\"\n  fi\n}\n```\n\nTo use it, run `hub default-branch [NAME]` in a checkout of the repo you want to change. If you pass an argument, that will be used as the branch name. Otherwise, the branch name `main` will be used.\n\nNow that your git repos are slightly less bad, why not spend some time looking for actions you can take to oppose racism today?\n",
				"date_published": "2020-06-06T00:00:00-08:00",
				"url": "https://andre.arko.net/2020/06/06/changing-git-and-githubs-default/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2020/05/19/migrating-off-cloudapp-to-dropbox/",
				"title": "Migrating off CloudApp (to Dropbox + Dropshare)",
				"content_html": "<p>I&rsquo;ve been using <a href=\"https://getcloudapp.com/\">CloudApp</a> since 2010. It was a pioneer in a service category that&rsquo;s incredibly busy today, full of companies like Droplr, Jumpshare, Dropshare, and honestly way more than I could possibly name. The concept is pretty simple: you get a keyboard shortcut and a menu bar icon that let you upload a screenshot or file, and your clipboard fills with a URL you can paste.</p>\n<p>The screenshot taking and markup is more or less copied from an earlier app, Skitch. (RIP Skitch, you were amazing until Evernote bought you.) The appeal of &ldquo;you take your screenshots and there is always a permanent link directly to them in the clipboard&rdquo; made me feel like it was worth paying for.</p>\n<h3 id=\"the-good\">the good</h3>\n<p>When I started, it had a cleverly short url (<code>cl.ly</code>), <a href=\"https://github.com/cloudapp/api\">an open API</a>, a native Mac app, multiple RubyGems. Later, I discovered that I was friends with someone who would listen to my complaints and feature requests (&lt;3 @lmarburger), and that made it even better.</p>\n<p>Eventually, the service changed hands, sold to a holding company that believed they could turn it into a dramatically more profitable business. That&rsquo;s completely understandable, especially given my understanding that the tiny team wasn&rsquo;t even making a living from it at the time.</p>\n<h3 id=\"the-bad\">the bad</h3>\n<p>Sadly, over the past few years, CloudApp has been incredibly pushy and aggressive about how much I should be using &ldquo;CloudApp Teams&rdquo;. I don&rsquo;t need a team, and it absolutely sucks that you are showing me ads even though I pay $10/mo. 😠</p>\n<p>After a while, teams wasn&rsquo;t even enough. The Mac app started aggressively telling me that I was using an obsolete version of CloudApp and I &ldquo;need to upgrade soon&rdquo;. Calling it an upgrade was absolutely a lie: it was a completely different app, and much, much worse.</p>\n<h3 id=\"the-ugly\">the ugly</h3>\n<p>The new Mac app lost the ability to copy direct links to images to the clipboard, and didn&rsquo;t get it back for months. To add insult to injury, it also gained prominent buttons telling me to sign up for a team. Even today, every time I open the Mac app menu to see my uploaded files, the tiny modal window includes a prominent button that says &ldquo;Need a TEAM? Sign up today!&rdquo;.</p>\n<p>The new web app is&hellip; really, really bad. The old web app had search by file name, date, upload type, and even the colors present in the image. The new webapp has&hellip; nothing. It shows me 221 pages of uploads, in chronological order. I can&rsquo;t sort them, I can&rsquo;t search them, I can only click through all 221 pages hoping my eyes find the old upload I am looking for.</p>\n<p>The old web app had an API, making me feel comfortable about keeping my files in someone else&rsquo;s service. If something went wrong, I could easily pull all my data out using a Ruby library maintained by the company itself, which is very reassuring. But this week, I found out that the new webapp doesn&rsquo;t have an API! They threw it away, and then said &ldquo;Curious what you used the API for?&rdquo; when I complained about it being gone on Twitter.</p>\n<p>Even though the new web app launched 6 months ago, and still doesn&rsquo;t have search or an API, it has new features that CloudApp has put enormous effort into promoting: call to action buttons directly on uploads! So if I want to&hellip; sell someone something&hellip; from my uploaded screenshot&hellip; I can do that. This is the exact opposite of what I want in a personal file sharing service.</p>\n<p>In a final bit of horror to me, as a web developer, I just noticed that the entire contents of every page on the new website is rendered directly from a Javascript string full of HTML. Apparently, adding a toggle between grid and list view requires sending two JS strings full of an entire page worth of HTML, and using an if/else statement to decide which HTML string to dump into the page to be visible. I shivered with horror just writing that. 😬</p>\n<p>So today I&rsquo;m screen-scraping the new webapp to get a copy of my uploads, to take them somewhere else. Unfortunately, &ldquo;somewhere else&rdquo; turns out to be a huge problem, too.</p>\n<h3 id=\"the-cloudapp-bundle\">the cloudapp bundle</h3>\n<p>CloudApp provides, in a bundle, these four things:</p>\n<ul>\n<li>Screenshot/video capture and annotation</li>\n<li>File uploads from a Mac menu bar app</li>\n<li>Permanent cloud storage</li>\n<li>URL shortening on my own domain</li>\n</ul>\n<p>There don&rsquo;t seem to be any competitors that hit all four, which just&hellip; ugh, of course not, why would there be.</p>\n<p>Droplr only allows custom domains on their enterprise plan. Jumpshare has been promising an API &ldquo;very soon&rdquo; for <em>eight years</em>. Dropshare has unusably bad annotation tools. CleanShot X doesn&rsquo;t offer a cloud service (yet).</p>\n<h3 id=\"build-a-bundle\">build-a-bundle</h3>\n<p>After giving up on the entire bundle, I started with Dropbox: I already pay for Dropbox, so can I get a Mac app that uploads to Dropbox and copies a share URL to the clipboard? Confusingly, the answer is Dropshare: the Mac app is also a standalone purchase, and works with any cloud storage, including Dropbox.</p>\n<p>CleanShot X is by far the best capture and annotation tool, surpassing even CloudApp in my estimation. It is also available as a standalone purchase, so I can drag directly from CleanShot to Dropshare and have something that seems pretty good, I guess?</p>\n<h3 id=\"oh-no-url-shortening\">oh no, url shortening</h3>\n<p>With that, I have annotation, menu bar uploads, and cloud storage, so I just need URL shortening for the share links. Amazingly, Dropshare also supports URL shortening, with built-in support for Rebrand, Bitly or custom services. As great as that is, Rebrand and Bitly charge $29/mo for their cheapest plans, so, uh, that&rsquo;s not gonna happen. Maybe a custom shortener service that I run myself?</p>\n<p>After two hours of trying to find a URL shortener that I could easily deploy to Heroku&hellip; I gave up. There are thousands of URL shorteners, some of them even seem actually quite nice, and none of them can be deployed in less than 15 minutes.</p>\n<h3 id=\"tldr\">tl;dr</h3>\n<p>As long as you have cloud storage that you already pay for, like Dropbox, Box, OneDrive, or a cloud server (S3 and SSH both work), you can get pretty close to replacing CloudApp for Mac like this:</p>\n<ul>\n<li>Buy <a href=\"https://getcleanshot.com/\">CleanShot X</a> for $29</li>\n<li>Buy <a href=\"https://dropshare.app/\">Dropshare</a> for $25</li>\n<li>Connect Dropshare to your cloud storage</li>\n<li>Stop paying CloudApp $10/mo</li>\n<li>Profit?</li>\n</ul>\n<p>If you find a good URL shortening option, <a href=\"mailto:andre@arko.net\">let me know</a>.</p>\n",
				"content_text": "I've been using [CloudApp](https://getcloudapp.com/) since 2010. It was a pioneer in a service category that's incredibly busy today, full of companies like Droplr, Jumpshare, Dropshare, and honestly way more than I could possibly name. The concept is pretty simple: you get a keyboard shortcut and a menu bar icon that let you upload a screenshot or file, and your clipboard fills with a URL you can paste.\n\nThe screenshot taking and markup is more or less copied from an earlier app, Skitch. (RIP Skitch, you were amazing until Evernote bought you.) The appeal of \"you take your screenshots and there is always a permanent link directly to them in the clipboard\" made me feel like it was worth paying for.\n\n### the good\n\nWhen I started, it had a cleverly short url (`cl.ly`), [an open API](https://github.com/cloudapp/api), a native Mac app, multiple RubyGems. Later, I discovered that I was friends with someone who would listen to my complaints and feature requests (\\<3 @lmarburger), and that made it even better.\n\nEventually, the service changed hands, sold to a holding company that believed they could turn it into a dramatically more profitable business. That's completely understandable, especially given my understanding that the tiny team wasn't even making a living from it at the time.\n\n### the bad\n\nSadly, over the past few years, CloudApp has been incredibly pushy and aggressive about how much I should be using \"CloudApp Teams\". I don't need a team, and it absolutely sucks that you are showing me ads even though I pay $10/mo. 😠\n\nAfter a while, teams wasn't even enough. The Mac app started aggressively telling me that I was using an obsolete version of CloudApp and I \"need to upgrade soon\". Calling it an upgrade was absolutely a lie: it was a completely different app, and much, much worse.\n\n### the ugly\n\nThe new Mac app lost the ability to copy direct links to images to the clipboard, and didn't get it back for months. To add insult to injury, it also gained prominent buttons telling me to sign up for a team. Even today, every time I open the Mac app menu to see my uploaded files, the tiny modal window includes a prominent button that says \"Need a TEAM? Sign up today!\".\n\nThe new web app is... really, really bad. The old web app had search by file name, date, upload type, and even the colors present in the image. The new webapp has... nothing. It shows me 221 pages of uploads, in chronological order. I can't sort them, I can't search them, I can only click through all 221 pages hoping my eyes find the old upload I am looking for.\n\nThe old web app had an API, making me feel comfortable about keeping my files in someone else's service. If something went wrong, I could easily pull all my data out using a Ruby library maintained by the company itself, which is very reassuring. But this week, I found out that the new webapp doesn't have an API! They threw it away, and then said \"Curious what you used the API for?\" when I complained about it being gone on Twitter.\n\nEven though the new web app launched 6 months ago, and still doesn't have search or an API, it has new features that CloudApp has put enormous effort into promoting: call to action buttons directly on uploads! So if I want to... sell someone something... from my uploaded screenshot... I can do that. This is the exact opposite of what I want in a personal file sharing service.\n\nIn a final bit of horror to me, as a web developer, I just noticed that the entire contents of every page on the new website is rendered directly from a Javascript string full of HTML. Apparently, adding a toggle between grid and list view requires sending two JS strings full of an entire page worth of HTML, and using an if/else statement to decide which HTML string to dump into the page to be visible. I shivered with horror just writing that. 😬\n\nSo today I'm screen-scraping the new webapp to get a copy of my uploads, to take them somewhere else. Unfortunately, \"somewhere else\" turns out to be a huge problem, too.\n\n### the cloudapp bundle\n\nCloudApp provides, in a bundle, these four things:\n\n- Screenshot/video capture and annotation\n- File uploads from a Mac menu bar app\n- Permanent cloud storage\n- URL shortening on my own domain\n\nThere don't seem to be any competitors that hit all four, which just... ugh, of course not, why would there be.\n\nDroplr only allows custom domains on their enterprise plan. Jumpshare has been promising an API \"very soon\" for *eight years*. Dropshare has unusably bad annotation tools. CleanShot X doesn't offer a cloud service (yet).\n\n### build-a-bundle\n\nAfter giving up on the entire bundle, I started with Dropbox: I already pay for Dropbox, so can I get a Mac app that uploads to Dropbox and copies a share URL to the clipboard? Confusingly, the answer is Dropshare: the Mac app is also a standalone purchase, and works with any cloud storage, including Dropbox.\n\nCleanShot X is by far the best capture and annotation tool, surpassing even CloudApp in my estimation. It is also available as a standalone purchase, so I can drag directly from CleanShot to Dropshare and have something that seems pretty good, I guess?\n\n### oh no, url shortening\n\nWith that, I have annotation, menu bar uploads, and cloud storage, so I just need URL shortening for the share links. Amazingly, Dropshare also supports URL shortening, with built-in support for Rebrand, Bitly or custom services. As great as that is, Rebrand and Bitly charge $29/mo for their cheapest plans, so, uh, that's not gonna happen. Maybe a custom shortener service that I run myself?\n\nAfter two hours of trying to find a URL shortener that I could easily deploy to Heroku... I gave up. There are thousands of URL shorteners, some of them even seem actually quite nice, and none of them can be deployed in less than 15 minutes.\n\n### tl;dr\n\nAs long as you have cloud storage that you already pay for, like Dropbox, Box, OneDrive, or a cloud server (S3 and SSH both work), you can get pretty close to replacing CloudApp for Mac like this:\n\n- Buy [CleanShot X](https://getcleanshot.com/) for $29\n- Buy [Dropshare](https://dropshare.app/) for $25\n- Connect Dropshare to your cloud storage\n- Stop paying CloudApp $10/mo\n- Profit?\n\nIf you find a good URL shortening option, [let me know](mailto:andre@arko.net).\n",
				"date_published": "2020-05-19T00:00:00-08:00",
				"url": "https://andre.arko.net/2020/05/19/migrating-off-cloudapp-to-dropbox/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2020/05/18/custom-styles-for-notion/",
				"title": "Custom styles for Notion",
				"content_html": "<p>I recently helped <a href=\"https://instagram.com/sailorhg\">someone with strong aesthetic preferences</a> set up a custom Mac app for <a href=\"https://notion.so\">Notion</a>. It’s approximately the same thing you could get with a browser extension like <a href=\"https://add0n.com/stylus.html\">Stylus</a>, but it retains the convenient Electron app parts of getting its own app icon and its own separate cookie storage.</p>\n<p>Luckily enough, someone else had <a href=\"https://github.com/Peter-JanGootzen/notion-custom-css-builder\">already figured out how to do it on Linux</a>, which was close enough for me to <a href=\"https://github.com/indirect/notion-custom-css-builder\">reuse most of the work for Mac</a>.</p>\n<p>Custom CSS in Notion turned out to be more interesting than I expected, since almost all of the CSS in the app is inlined directly into <code>style</code> attributes. That makes it… awkward to apply site-wide styles using CSS files.</p>\n<p>Changing the font to Latin Modern Mono Italic turned out to be easy enough, but led to a somewhat unexpected result: slanted emoji. 😆 Adding an override for <code>span[role=image]</code> turned out to be enough.</p>\n<p>Changing colors, though, was a real challenge. There’s no class, there’s no element, there’s no attribute… wait. The <code>style</code> attribute is always going to have the color in it, right? You can select based on “attribute value contains”, right? Turns out yes, you can select elements based on the inline style that gives them the color that you want to overrule. 😂</p>\n<p>This is probably the most cursed CSS I have ever written.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-css\" data-lang=\"css\"><span style=\"display:flex;\"><span><span style=\"color:#f92672\">div</span><span style=\"color:#f92672\">[</span><span style=\"color:#f92672\">style</span><span style=\"color:#f92672\">*=</span><span style=\"color:#e6db74\">&#34;rgb(173, 26, 114)&#34;</span><span style=\"color:#f92672\">]</span> {\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">color</span>: <span style=\"color:#ae81ff\">#F9D2EE</span> <span style=\"color:#75715e\">!important</span>;\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><p>It works, though!</p>\n<img src=\"https://indirect.micro.blog/uploads/2025/af6d884992.jpg\">\n",
				"content_text": "\nI recently helped [someone with strong aesthetic preferences](https://instagram.com/sailorhg) set up a custom Mac app for [Notion](https://notion.so). It’s approximately the same thing you could get with a browser extension like [Stylus](https://add0n.com/stylus.html), but it retains the convenient Electron app parts of getting its own app icon and its own separate cookie storage. \n\nLuckily enough, someone else had [already figured out how to do it on Linux](https://github.com/Peter-JanGootzen/notion-custom-css-builder), which was close enough for me to [reuse most of the work for Mac](https://github.com/indirect/notion-custom-css-builder).\n\nCustom CSS in Notion turned out to be more interesting than I expected, since almost all of the CSS in the app is inlined directly into `style` attributes. That makes it… awkward to apply site-wide styles using CSS files.\n\nChanging the font to Latin Modern Mono Italic turned out to be easy enough, but led to a somewhat unexpected result: slanted emoji. 😆 Adding an override for `span[role=image]` turned out to be enough.\n\nChanging colors, though, was a real challenge. There’s no class, there’s no element, there’s no attribute… wait. The `style` attribute is always going to have the color in it, right? You can select based on “attribute value contains”, right? Turns out yes, you can select elements based on the inline style that gives them the color that you want to overrule. 😂\n\nThis is probably the most cursed CSS I have ever written.\n\n```css\ndiv[style*=\"rgb(173, 26, 114)\"] {\n    color: #F9D2EE !important;\n}\n```\n\nIt works, though!\n\n<img src=\"https://indirect.micro.blog/uploads/2025/af6d884992.jpg\">\n",
				"date_published": "2020-05-18T00:00:00-08:00",
				"url": "https://andre.arko.net/2020/05/18/custom-styles-for-notion/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2020/05/16/tab-completion-for-chruby-and/",
				"title": "Tab completion for chruby and ruby-install on zsh",
				"content_html": "<p>I switched to zsh as part of upgrading to macOS 10.15 Catalina. I&rsquo;m not using oh-my-zsh, but I was <em>incredibly</em> helped by the <a href=\"https://scriptingosx.com/2019/06/moving-to-zsh/\">Scripting OSX series</a>, and my new best friend is <a href=\"https://github.com/romkatv/powerlevel10k\">Powerlevel10k</a>.</p>\n<p>Anyway, now that you&rsquo;re caught up, my problem of the day is wanting tab-completion for my other best friends: <code>chruby</code> and <code>ruby-install</code>. There&rsquo;s a bunch of tab-completion options for <code>chruby</code> rattling around in GitHub issues and pull requests, but none of them were easy enough to find. I eventually wound up extracting one from <a href=\"https://github.com/ohmyzsh/ohmyzsh/blob/master/plugins/chruby/chruby.plugin.zsh\">oh-my-zsh&rsquo;s chruby plugin</a>, which does <em>way</em> more than I wanted. Here&rsquo;s the whole thing, which I keep in <code>~/.zsh/completion/_chruby</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-zsh\" data-lang=\"zsh\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\">#compdef chruby</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>compadd <span style=\"color:#66d9ef\">$(</span>chruby | tr -d <span style=\"color:#e6db74\">&#39;* &#39;</span><span style=\"color:#66d9ef\">)</span>\n</span></span><span style=\"display:flex;\"><span>local default_path<span style=\"color:#f92672\">=</span><span style=\"color:#e6db74\">&#39;/usr/local/bin:/usr/bin&#39;</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">if</span> PATH<span style=\"color:#f92672\">=</span><span style=\"color:#e6db74\">${</span>default_path<span style=\"color:#e6db74\">}</span> type ruby &amp;&gt; /dev/null; <span style=\"color:#66d9ef\">then</span>\n</span></span><span style=\"display:flex;\"><span>  compadd system\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">fi</span>\n</span></span></code></pre></div><p>Once I had <code>chruby</code> working, I wanted tab-completion for <code>ruby-install</code>. I figured I could also extract that from oh-my-zsh, but&hellip; it&rsquo;s listed as a TODO. :/ So I wrote my own! As far as I can tell from my scatterbrained googling, this is literally the first zsh completion for ruby-install to ever be posted on the internet. Yay me? I keep this in (predictably) <code>~/.zsh/completion/_ruby-install</code>.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-zsh\" data-lang=\"zsh\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\">#compdef ruby-install</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>compadd <span style=\"color:#66d9ef\">$(</span>ruby-install | tail -n+2 | ruby -e <span style=\"color:#e6db74\">&#39;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">ARGF.read.lines.each do |l|\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">  next @name = l.tr(&#34;:&#34;, &#34;&#34;).strip if l.include?(&#34;:&#34;)\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">  puts &#34;#{@name}-#{l.strip}&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">end\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">&#39;</span><span style=\"color:#66d9ef\">)</span>\n</span></span></code></pre></div><p>(I know, I know, macOS won&rsquo;t ship with a built-in Ruby starting with 10.16 or 10.17, and this will break then. I just don&rsquo;t have the patience to rewrite it as a zsh script today.)</p>\n<p>To include and activate these, you need something like this in your <code>~/.zshrc</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-zsh\" data-lang=\"zsh\"><span style=\"display:flex;\"><span>fpath<span style=\"color:#f92672\">+=</span>~/.zsh/completion\n</span></span><span style=\"display:flex;\"><span>autoload -Uz compinit <span style=\"color:#f92672\">&amp;&amp;</span> compinit\n</span></span></code></pre></div><p>And that&rsquo;s it! Tab complete your way to happiness:</p>\n<pre tabindex=\"0\"><code>$ chruby ⇤\nruby-2.6.6  ruby-2.7.1  system\n\n$ ruby-install ⇤\njruby-9.2.11.1  rbx-4.15  ruby-2.5.8  ruby-2.7.1  mruby-2.1.0  ruby-2.4.10  ruby-2.6.6  truffleruby-20.0.0  \n</code></pre>",
				"content_text": "I switched to zsh as part of upgrading to macOS 10.15 Catalina. I'm not using oh-my-zsh, but I was _incredibly_ helped by the [Scripting OSX series](https://scriptingosx.com/2019/06/moving-to-zsh/), and my new best friend is [Powerlevel10k](https://github.com/romkatv/powerlevel10k).\n\nAnyway, now that you're caught up, my problem of the day is wanting tab-completion for my other best friends: `chruby` and `ruby-install`. There's a bunch of tab-completion options for `chruby` rattling around in GitHub issues and pull requests, but none of them were easy enough to find. I eventually wound up extracting one from [oh-my-zsh's chruby plugin](https://github.com/ohmyzsh/ohmyzsh/blob/master/plugins/chruby/chruby.plugin.zsh), which does _way_ more than I wanted. Here's the whole thing, which I keep in `~/.zsh/completion/_chruby`:\n\n```zsh\n#compdef chruby\n\ncompadd $(chruby | tr -d '* ')\nlocal default_path='/usr/local/bin:/usr/bin'\nif PATH=${default_path} type ruby &> /dev/null; then\n  compadd system\nfi\n```\n\nOnce I had `chruby` working, I wanted tab-completion for `ruby-install`. I figured I could also extract that from oh-my-zsh, but... it's listed as a TODO. :/ So I wrote my own! As far as I can tell from my scatterbrained googling, this is literally the first zsh completion for ruby-install to ever be posted on the internet. Yay me? I keep this in (predictably) `~/.zsh/completion/_ruby-install`.\n\n```zsh\n#compdef ruby-install\n\ncompadd $(ruby-install | tail -n+2 | ruby -e '\nARGF.read.lines.each do |l|\n  next @name = l.tr(\":\", \"\").strip if l.include?(\":\")\n  puts \"#{@name}-#{l.strip}\"\nend\n')\n```\n\n(I know, I know, macOS won't ship with a built-in Ruby starting with 10.16 or 10.17, and this will break then. I just don't have the patience to rewrite it as a zsh script today.)\n\nTo include and activate these, you need something like this in your `~/.zshrc`:\n\n```zsh\nfpath+=~/.zsh/completion\nautoload -Uz compinit && compinit\n```\n\nAnd that's it! Tab complete your way to happiness:\n\n```\n$ chruby ⇤\nruby-2.6.6  ruby-2.7.1  system\n\n$ ruby-install ⇤\njruby-9.2.11.1  rbx-4.15  ruby-2.5.8  ruby-2.7.1  mruby-2.1.0  ruby-2.4.10  ruby-2.6.6  truffleruby-20.0.0  \n```\n",
				"date_published": "2020-05-16T00:00:00-08:00",
				"url": "https://andre.arko.net/2020/05/16/tab-completion-for-chruby-and/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2020/04/16/sustainable-work-from-home-while/",
				"title": "Sustainable work from home while the world is burning",
				"content_html": "<p><small>This post was originally given as a talk for <a href=\"https://www.cloudcity.io\">Cloud City</a>. The <a href=\"https://speakerdeck.com/indirect/engineering-teams-in-a-time-of-corona\">slides</a> and <a href=\"https://www.youtube.com/watch?v=L7UUNbySwkU\">video</a> are also available.</small></p>\n<script async class=\"speakerdeck-embed\" data-id=\"e3e0b75102d24d16b8410feb24915fcb\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n<h3 id=\"where-were-at\">Where we&rsquo;re at</h3>\n<p>It’s been a month since San Francisco and the surrounding counties announced a lockdown, and at least a couple of weeks in most of the US. The good news is that it’s possible to build software even when everyone is staying at home every day, unlike a lot of jobs!</p>\n<p>The bad news is this isn’t remote working, not the way that anyone has ever talked about it before. In the words of Juan Pablo Buritica, this is &ldquo;stuck at home work&rdquo;, and that’s much worse.</p>\n<h3 id=\"the-worse-news\">The worse news</h3>\n<p>Not only is it completely different to build software with everyone at home, everyone is having to do it without preparation or a transition period. To add a cherry on top, the global pandemic is destroying everyone’s ability to do new hard things, along with the rest of their mental health.</p>\n<p>The Bay Area lockdown has already been extended once, and there is no consensus yet about when it will be safe to allow workers to return to stores, let alone offices and schools. No one is getting out of here in less than weeks, and most experts seem to be saying it will take at least a few months. We need a new mindset: working from home isn’t a temporary workaround, it’s just how things are now.</p>\n<h3 id=\"reset-your-expectations\">Reset your expectations</h3>\n<p>Time for some more bad news: if you&rsquo;ve been working with this team for a while, you now have expectations that are wildly off-base. It&rsquo;s a global pandemic. There&rsquo;s no childcare. There are shortages of food and toilet paper. Your team is not firing on all cylinders. You are not firing on all cylinders.</p>\n<h3 id=\"what-is-this-productivity-you-speak-of\">What is this &ldquo;productivity&rdquo; you speak of</h3>\n<p>Create a new definition of success: if you are still here, you are succeeding.</p>\n<p>If the stress of the botched governmental response to a global pandemic hasn&rsquo;t taken you down, you are succeeding.</p>\n<p>If you are feeding yourself and checking in with the people who matter to you, you are succeeding.</p>\n<p>If your company and team still exist, you are incredibly lucky and have something to celebrate.</p>\n<p>If your company and team are able to accomplish literally anything, you are doing better than most.</p>\n<p>You should feel lucky, proud, and accomplished, purely based on having any capacity available at all, and you should tell your team you feel that way about every one of them.</p>\n<h3 id=\"how-to-work-now\">How to work, now</h3>\n<p>As a manager or team lead, your job is not just to get your own shit together, your job is to model working from home for your team, offer them resources, and suggest ways they can improve.</p>\n<p>The best starting point I know of for working from home is <a href=\"https://blog.alicegoldfuss.com/work-in-the-time-of-corona/\">Work in the Time of Corona</a> by Alice Goldfuss. She offers both extremely practical advice about how to improve video calls and insight into the mental strains of working from home and how those strains interact with a pandemic.</p>\n<h3 id=\"implicit-to-explicit\">Implicit to explicit</h3>\n<p>Once you have your own situation sorted out, examine how and where you interact with your team. The biggest change switching to home work is that communication needs to become more explicit.</p>\n<p>Are you explicitly checking in with your team? Are you explicitly supporting them? Are you providing them with a forum to check in with and support each other?</p>\n<h3 id=\"model-new-norms\">Model new norms</h3>\n<p>In offices, norms and expected behaviors are shared by literally sharing space. While working from home, norms and expected behaviors not only need to be explicitly spelled out, you need to make your own actions explicit and legible.</p>\n<p>You can&rsquo;t count on walking past someone in the hallway, or running into them in the kitchen. You can&rsquo;t expect people to see when you arrive, when you leave, what you&rsquo;re looking at, or who you&rsquo;re talking to.</p>\n<p>Do the actions you want to see. Let people know what you&rsquo;re doing, so they know those behaviors are normal and expected. If you say &ldquo;I&rsquo;m closing down for the day to be with my family, see you all in the morning.&rdquo;, it models work-life balance in a way that disappearing from Slack does not.</p>\n<h3 id=\"have-less-meetings-communicate-more\">Have less meetings; communicate more</h3>\n<p>Meetings are optimized for synchronous communication. They assume everyone is available at the same time, and can concentrate and contribute at the same time. Working from home makes this much harder; a pandemic makes this nearly impossible.</p>\n<p>Schedule less meetings, and spend that time explicitly communicating instead. Master the asynchronous communication tools available to you. Support your team by modeling the behavior you want to see, even when it feels like a slog.</p>\n<p>Deliberately make your proposals, feedback, and discussions all asynchronous. Write documents and pass the link around for feedback. Create room for others to chime in later. Explicitly ask for thoughts from those who aren&rsquo;t online during discussions.</p>\n<p>Make sure your team has a way to keep up with each other, even when they don&rsquo;t work at the same time. Maybe that means daily summaries via a tool like <a href=\"http://geekbot.io/\">GeekBot</a>, maybe that means everyone keeping a <a href=\"https://ma.tt/2009/05/how-p2-changed-automattic/\">public worklog</a>, or maybe that means team scribes that summarize the work of each team to an internal mailing list.</p>\n<h3 id=\"interviewing-from-home\">Interviewing from home</h3>\n<p>Pandemics bring chaos with them; all that chaos increases the chances that you will need to interview or be interviewed. Interviewing is extremely hard at the very best of times, and interviewing someone with stuttering audio and blurry video is nowhere close to the best of times.</p>\n<p>The best advice I&rsquo;ve seen about conducting interviews is in the post <a href=\"https://www.moishelettvin.com/2020/03/16/Remote-Interviewing/\">Remote Interviewing</a> by Moishe Lettvin. The post is chock full of extremely good advice, but the biggest call-out to me was to &ldquo;acknowledge the strangeness and awkwardness&rdquo;.</p>\n<p>It&rsquo;s a strange time! It&rsquo;s an awkward time! Pretending it&rsquo;s not strange and awkward makes everything even more stressful, which is the opposite of what you want.</p>\n<h3 id=\"its-a-marathon-not-a-sprint\">It&rsquo;s a marathon, not a sprint</h3>\n<p>Finally, but very importantly, work to internalize that all your new behaviors and expectations need to be sustainable. We aren&rsquo;t sprinting to the end of quarantine; we can&rsquo;t outrun a pandemic.</p>\n<p>Don&rsquo;t expect anyone to be working &ldquo;normally&rdquo;, least of all yourself. Make sure you&rsquo;re working the amount you can work. Check with your team and support them in working the amount that they can work.</p>\n<p>Be humane and supportive in an inhumane and traumatizing time. Even though we can&rsquo;t do it in person, the only way we&rsquo;re going to get through this is by helping each other.</p>\n<p><br><br></p>\n<h4 id=\"further-resources\">Further resources</h4>\n<p><a href=\"https://www.chronicle.com/article/Why-You-Should-Ignore-All-That/248366\">How to adapt to a long term crisis</a> by Aisha Ahmad<br>\n<a href=\"https://blog.alicegoldfuss.com/work-in-the-time-of-corona/\">Work in the Time of Corona</a> by Alice Goldfuss<br>\n<a href=\"https://www.moishelettvin.com/2020/03/16/Remote-Interviewing/\">Remote Interviewing</a> by Moishe Lettvin<br>\n<a href=\"/2018/04/26/pairing-a-guide-to-fruitful-collaboration/\">Pairing: A Guide to Fruitful Collaboration 🍓🍑🍐</a> by André Arko<br>\n<a href=\"https://increment.com/teams/a-guide-to-distributed-teams/\">A guide to distributed teams</a> by Juan Pablo Buriticá and Katie Womersley<br>\n<a href=\"https://wordpress.com/blog/2020/03/06/a-crash-course-in-remote-management/\">A Crash Course in Remote Management</a> by Cate Huston</p>\n<h4 id=\"useful-tools\">Useful tools</h4>\n<p>Screen sharing: <a href=\"https://screen.so/\">screen.so</a>, <a href=\"https://tuple.app/\">tuple.app</a>, or <a href=\"https://zoom.com/\">Zoom</a><br>\nAsynchronous updates: <a href=\"https://geekbot.com/\">GeekBot</a>, <a href=\"https://p2theme.com/\">P2</a>, or email lists<br>\nSocializing: <a href=\"https://www.donut.com/\">Donut</a>, or scheduled team chats during work hours</p>\n<h4 id=\"thanks\">Thanks</h4>\n<p>Feedback from <a href=\"http://durettihirpa.com/\">Duretti Hirpa</a> &amp; <a href=\"https://twitter.com/buritica\">Juan Pablo Buriticá</a><br>\nIllustrations from <a href=\"https://icons8.com/\">icons8</a><br>\nSlide design by Cloud City&rsquo;s <a href=\"https://www.brendanpgh.com/\">Brendan Miller</a><br>\nSupport from the <a href=\"https://cloudcity.io/\">Cloud City</a> team</p>\n",
				"content_text": "\n<small>This post was originally given as a talk for <a href=\"https://www.cloudcity.io\">Cloud City</a>. The [slides](https://speakerdeck.com/indirect/engineering-teams-in-a-time-of-corona) and [video](https://www.youtube.com/watch?v=L7UUNbySwkU) are also available.</small>\n\n<script async class=\"speakerdeck-embed\" data-id=\"e3e0b75102d24d16b8410feb24915fcb\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n\n### Where we're at\n\nIt’s been a month since San Francisco and the surrounding counties announced a lockdown, and at least a couple of weeks in most of the US. The good news is that it’s possible to build software even when everyone is staying at home every day, unlike a lot of jobs!\n\nThe bad news is this isn’t remote working, not the way that anyone has ever talked about it before. In the words of Juan Pablo Buritica, this is \"stuck at home work\", and that’s much worse.\n\n### The worse news\n\nNot only is it completely different to build software with everyone at home, everyone is having to do it without preparation or a transition period. To add a cherry on top, the global pandemic is destroying everyone’s ability to do new hard things, along with the rest of their mental health.\n\nThe Bay Area lockdown has already been extended once, and there is no consensus yet about when it will be safe to allow workers to return to stores, let alone offices and schools. No one is getting out of here in less than weeks, and most experts seem to be saying it will take at least a few months. We need a new mindset: working from home isn’t a temporary workaround, it’s just how things are now.\n\n### Reset your expectations\n\nTime for some more bad news: if you've been working with this team for a while, you now have expectations that are wildly off-base. It's a global pandemic. There's no childcare. There are shortages of food and toilet paper. Your team is not firing on all cylinders. You are not firing on all cylinders.\n\n### What is this \"productivity\" you speak of\n\nCreate a new definition of success: if you are still here, you are succeeding.\n\nIf the stress of the botched governmental response to a global pandemic hasn't taken you down, you are succeeding.\n\nIf you are feeding yourself and checking in with the people who matter to you, you are succeeding.\n\nIf your company and team still exist, you are incredibly lucky and have something to celebrate.\n\nIf your company and team are able to accomplish literally anything, you are doing better than most.\n\nYou should feel lucky, proud, and accomplished, purely based on having any capacity available at all, and you should tell your team you feel that way about every one of them.\n\n### How to work, now\n\nAs a manager or team lead, your job is not just to get your own shit together, your job is to model working from home for your team, offer them resources, and suggest ways they can improve.\n\nThe best starting point I know of for working from home is [Work in the Time of Corona](https://blog.alicegoldfuss.com/work-in-the-time-of-corona/) by Alice Goldfuss. She offers both extremely practical advice about how to improve video calls and insight into the mental strains of working from home and how those strains interact with a pandemic.\n\n### Implicit to explicit\n\nOnce you have your own situation sorted out, examine how and where you interact with your team. The biggest change switching to home work is that communication needs to become more explicit.\n\nAre you explicitly checking in with your team? Are you explicitly supporting them? Are you providing them with a forum to check in with and support each other?\n\n### Model new norms\n\nIn offices, norms and expected behaviors are shared by literally sharing space. While working from home, norms and expected behaviors not only need to be explicitly spelled out, you need to make your own actions explicit and legible.\n\nYou can't count on walking past someone in the hallway, or running into them in the kitchen. You can't expect people to see when you arrive, when you leave, what you're looking at, or who you're talking to.\n\nDo the actions you want to see. Let people know what you're doing, so they know those behaviors are normal and expected. If you say \"I'm closing down for the day to be with my family, see you all in the morning.\", it models work-life balance in a way that disappearing from Slack does not.\n\n### Have less meetings; communicate more\n\nMeetings are optimized for synchronous communication. They assume everyone is available at the same time, and can concentrate and contribute at the same time. Working from home makes this much harder; a pandemic makes this nearly impossible.\n\nSchedule less meetings, and spend that time explicitly communicating instead. Master the asynchronous communication tools available to you. Support your team by modeling the behavior you want to see, even when it feels like a slog.\n\nDeliberately make your proposals, feedback, and discussions all asynchronous. Write documents and pass the link around for feedback. Create room for others to chime in later. Explicitly ask for thoughts from those who aren't online during discussions.\n\nMake sure your team has a way to keep up with each other, even when they don't work at the same time. Maybe that means daily summaries via a tool like [GeekBot](http://geekbot.io/), maybe that means everyone keeping a [public worklog](https://ma.tt/2009/05/how-p2-changed-automattic/), or maybe that means team scribes that summarize the work of each team to an internal mailing list.\n\n### Interviewing from home\n\nPandemics bring chaos with them; all that chaos increases the chances that you will need to interview or be interviewed. Interviewing is extremely hard at the very best of times, and interviewing someone with stuttering audio and blurry video is nowhere close to the best of times.\n\nThe best advice I've seen about conducting interviews is in the post [Remote Interviewing](https://www.moishelettvin.com/2020/03/16/Remote-Interviewing/) by Moishe Lettvin. The post is chock full of extremely good advice, but the biggest call-out to me was to \"acknowledge the strangeness and awkwardness\".\n\nIt's a strange time! It's an awkward time! Pretending it's not strange and awkward makes everything even more stressful, which is the opposite of what you want.\n\n### It's a marathon, not a sprint\n\nFinally, but very importantly, work to internalize that all your new behaviors and expectations need to be sustainable. We aren't sprinting to the end of quarantine; we can't outrun a pandemic.\n\nDon't expect anyone to be working \"normally\", least of all yourself. Make sure you're working the amount you can work. Check with your team and support them in working the amount that they can work.\n\nBe humane and supportive in an inhumane and traumatizing time. Even though we can't do it in person, the only way we're going to get through this is by helping each other.\n\n\n<br><br>\n\n#### Further resources\n\n[How to adapt to a long term crisis](https://www.chronicle.com/article/Why-You-Should-Ignore-All-That/248366) by Aisha Ahmad  \n[Work in the Time of Corona](https://blog.alicegoldfuss.com/work-in-the-time-of-corona/) by Alice Goldfuss  \n[Remote Interviewing](https://www.moishelettvin.com/2020/03/16/Remote-Interviewing/) by Moishe Lettvin  \n[Pairing: A Guide to Fruitful Collaboration 🍓🍑🍐](/2018/04/26/pairing-a-guide-to-fruitful-collaboration/) by André Arko  \n[A guide to distributed teams](https://increment.com/teams/a-guide-to-distributed-teams/) by Juan Pablo Buriticá and Katie Womersley  \n[A Crash Course in Remote Management](https://wordpress.com/blog/2020/03/06/a-crash-course-in-remote-management/) by Cate Huston  \n\n#### Useful tools\n\nScreen sharing: [screen.so](https://screen.so/), [tuple.app](https://tuple.app/), or [Zoom](https://zoom.com/)  \nAsynchronous updates: [GeekBot](https://geekbot.com/), [P2](https://p2theme.com/), or email lists  \nSocializing: [Donut](https://www.donut.com/), or scheduled team chats during work hours  \n\n#### Thanks\n\nFeedback from [Duretti Hirpa](http://durettihirpa.com/) & [Juan Pablo Buriticá](https://twitter.com/buritica)  \nIllustrations from [icons8](https://icons8.com/)  \nSlide design by Cloud City's [Brendan Miller](https://www.brendanpgh.com/)  \nSupport from the [Cloud City](https://cloudcity.io/) team\n",
				"date_published": "2020-04-16T00:00:00-08:00",
				"url": "https://andre.arko.net/2020/04/16/sustainable-work-from-home-while/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2020/03/15/built-an-atreus/",
				"title": "Built an Atreus 2",
				"content_html": "<p><a href=\"/2020/03/14/keyboards-tell-me-more/\">As I mentioned yesterday</a>, I have a bit of a keyboard problem. Despite having owned dozens of mechanical, touchsurface, experimental, and otherwise weird keyboqards and other input devices, I had never built one myself. (I bought an ErgoDox Infinity kit at one point, but sold it after more than a year of never getting around to it.)</p>\n<p>In late 2019, I wound up with an <a href=\"https://atreus.technomancy.us/2\">Atreus 2</a> PCB, and resolved to finally build a keyboard myself. I am very boring and just want all blues all the time, so the switches were easy, but I was a lot less sure about the case. After fiddling with SVGs for two days, I finally managed to order lasercut parts from Ponoko for a maple case, to match my Keyboard.io Model 01.</p>\n<p>Some generous spray applications of polyurethane later (also to match the Keyboard.io finish), I managed to break the topcase while trying to jam in switches 😬. Happily, it turned out totally fine after some wood glue and a day of clamping!</p>\n<p>Soldering in the switches was a lot less difficult than I expected, based on my limited past experience soldering things a lot trickier than post-and-hole connections. I still had to go back and fix maybe 5-6 connections that didn&rsquo;t quite take the first time, but it felt pretty straightforward.</p>\n<p>The last tricky bit to assemble was finding screws the right size, length, and head shape. I tried to countersink the screws into the bottom case plate and it&hellip; sort of worked, although none of the screws wound up flush. The case is solid, and it doesn&rsquo;t matter that the screws stick out 1mm since I needed to add rubber feet anyway to keep the case from sliding around.</p>\n<p>Overall, I&rsquo;m really happy with it! After a few nights of fiddling and a couple of PRs, I even got <a href=\"https://github.com/indirect/Atreus2-Firmware\">my Kaleidoscope firmware</a> up and running on it. Now that I finished I&rsquo;m of course already starting to think about assembling something much harder, like a Dactyl Manuform. 😂</p>\n<p>If you&rsquo;re interested in an Atreus 2 of your own, check out <a href=\"https://www.kickstarter.com/projects/keyboardio/atreus\">the Keyboard.io Atreus 2 Kickstarter</a>, which starts in two days on March 17.</p>\n<img alt=\"atreus2-01.jpg\" src=\"https://indirect.micro.blog/uploads/2025/28d3416ebf.jpg\">\n<img alt=\"atreus2-02.jpg\" src=\"https://indirect.micro.blog/uploads/2025/c245237eaf.jpg\">\n<img alt=\"atreus2-03.jpg\" src=\"https://indirect.micro.blog/uploads/2025/d7980f96fc.jpg\">\n<img alt=\"atreus2-04.jpg\" src=\"https://indirect.micro.blog/uploads/2025/f09f04c1ee.jpg\">\n<img alt=\"atreus2-05.jpg\" src=\"https://indirect.micro.blog/uploads/2025/60cba1dbb8.jpg\">\n<img alt=\"atreus2-06.jpg\" src=\"https://indirect.micro.blog/uploads/2025/140a00e96c.jpg\">\n<img alt=\"atreus2-07.jpg\" src=\"https://indirect.micro.blog/uploads/2025/617c0ea189.jpg\">\n<img alt=\"atreus2-08.jpg\" src=\"https://indirect.micro.blog/uploads/2025/ef453c94ac.jpg\">\n<img alt=\"atreus2-09.jpg\" src=\"https://indirect.micro.blog/uploads/2025/f948e2dacc.jpg\">\n<img alt=\"atreus2-10.jpg\" src=\"https://indirect.micro.blog/uploads/2025/678a4b7f17.jpg\">\n<img alt=\"atreus2-11.jpg\" src=\"https://indirect.micro.blog/uploads/2025/88a3a07506.jpg\">\n",
				"content_text": "\n[As I mentioned yesterday](/2020/03/14/keyboards-tell-me-more/), I have a bit of a keyboard problem. Despite having owned dozens of mechanical, touchsurface, experimental, and otherwise weird keyboqards and other input devices, I had never built one myself. (I bought an ErgoDox Infinity kit at one point, but sold it after more than a year of never getting around to it.)\n\nIn late 2019, I wound up with an [Atreus 2](https://atreus.technomancy.us/2) PCB, and resolved to finally build a keyboard myself. I am very boring and just want all blues all the time, so the switches were easy, but I was a lot less sure about the case. After fiddling with SVGs for two days, I finally managed to order lasercut parts from Ponoko for a maple case, to match my Keyboard.io Model 01.\n\nSome generous spray applications of polyurethane later (also to match the Keyboard.io finish), I managed to break the topcase while trying to jam in switches 😬. Happily, it turned out totally fine after some wood glue and a day of clamping!\n\nSoldering in the switches was a lot less difficult than I expected, based on my limited past experience soldering things a lot trickier than post-and-hole connections. I still had to go back and fix maybe 5-6 connections that didn't quite take the first time, but it felt pretty straightforward.\n\nThe last tricky bit to assemble was finding screws the right size, length, and head shape. I tried to countersink the screws into the bottom case plate and it... sort of worked, although none of the screws wound up flush. The case is solid, and it doesn't matter that the screws stick out 1mm since I needed to add rubber feet anyway to keep the case from sliding around.\n\nOverall, I'm really happy with it! After a few nights of fiddling and a couple of PRs, I even got [my Kaleidoscope firmware](https://github.com/indirect/Atreus2-Firmware) up and running on it. Now that I finished I'm of course already starting to think about assembling something much harder, like a Dactyl Manuform. 😂\n\nIf you're interested in an Atreus 2 of your own, check out [the Keyboard.io Atreus 2 Kickstarter](https://www.kickstarter.com/projects/keyboardio/atreus), which starts in two days on March 17.\n\n<img alt=\"atreus2-01.jpg\" src=\"https://indirect.micro.blog/uploads/2025/28d3416ebf.jpg\">\n<img alt=\"atreus2-02.jpg\" src=\"https://indirect.micro.blog/uploads/2025/c245237eaf.jpg\">\n<img alt=\"atreus2-03.jpg\" src=\"https://indirect.micro.blog/uploads/2025/d7980f96fc.jpg\">\n<img alt=\"atreus2-04.jpg\" src=\"https://indirect.micro.blog/uploads/2025/f09f04c1ee.jpg\">\n<img alt=\"atreus2-05.jpg\" src=\"https://indirect.micro.blog/uploads/2025/60cba1dbb8.jpg\">\n<img alt=\"atreus2-06.jpg\" src=\"https://indirect.micro.blog/uploads/2025/140a00e96c.jpg\">\n<img alt=\"atreus2-07.jpg\" src=\"https://indirect.micro.blog/uploads/2025/617c0ea189.jpg\">\n<img alt=\"atreus2-08.jpg\" src=\"https://indirect.micro.blog/uploads/2025/ef453c94ac.jpg\">\n<img alt=\"atreus2-09.jpg\" src=\"https://indirect.micro.blog/uploads/2025/f948e2dacc.jpg\">\n<img alt=\"atreus2-10.jpg\" src=\"https://indirect.micro.blog/uploads/2025/678a4b7f17.jpg\">\n<img alt=\"atreus2-11.jpg\" src=\"https://indirect.micro.blog/uploads/2025/88a3a07506.jpg\">\n",
				"date_published": "2020-03-15T00:00:00-08:00",
				"url": "https://andre.arko.net/2020/03/15/built-an-atreus/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2020/03/14/keyboards-tell-me-more/",
				"title": "Keyboards? Tell Me More",
				"content_html": "<p>I have <strike>a lot of keyboards</strike> a keyboard problem.</p>\n<p>Now that everyone is <a href=\"https://aphyr.com/posts/350-stay-home\">staying home</a> (you&rsquo;re staying home too, right?), some friends who know about my keyboards have started asking me to give them keyboard advice. Since it&rsquo;s impossible to that without a ton more context, I wrote this guide to help people figure out more specific questions they can ask about keyboards.</p>\n<p>If you don&rsquo;t really know much about keyboards, this post is some context describing the functionality of some popular keyboards, in escalating order of expense and&hellip; particularness.</p>\n<p>On the other hand, if you want an aesthetics-focused list, I suggest jumping over to <a href=\"http://astrolokeys.com\">astrolokeys.com</a> where you can read <em>the signs as mechanical keyboards</em>, instead of the rest of this post. 😅</p>\n<h3 id=\"i-just-want-an-external-keyboard-for-my-laptop\">I just want an external keyboard for my laptop</h3>\n<p>Fortunately this one is pretty easy. If you have a Mac, get an <a href=\"https://www.apple.com/shop/product/MLA22LL/A/magic-keyboard-us-english\">Apple Magic Keyboard</a>. It&rsquo;s wireless, has a layout copied from your laptop so you already know it, and is a totally reasonable keyboard.</p>\n<p>It pairs instantly when you plug it in to your computer using a Lightning cable, and it has a built-in battery that lasts for weeks. If you leave it plugged in to your computer, it even switches over to being a wired keyboard and the delay between pushing a key and the computer showing the letter goes down.</p>\n<p>If you don&rsquo;t have a Mac, the Wirecutter recommends the <a href=\"https://www.amazon.com/dp/B0148NPH9I\">Logitech K380</a>. Seems good.</p>\n<div style=\"width: 48%; float: left;\">\n  <img alt=\"Apple Magic Keyboard\" src=\"https://indirect.micro.blog/uploads/2025/c6ad3c0a8a.jpg\">\n  Apple Magic Keyboard\n</div>\n<div style=\"width: 48%; float: right; text-align: right;\">\n  <img alt=\"Logitech K380\" src=\"https://indirect.micro.blog/uploads/2025/74caf4c490.jpg\">\n  Logitech K380\n</div>\n<div style=\"clear: both\"></div>\n<h3 id=\"i-heard-ergonomic-keyboards-are-better\">I heard ergonomic keyboards are better</h3>\n<p>If you want something that&rsquo;s pretty close to the keyboard you&rsquo;re already familiar with, but more ergonomic, I have heard that the <a href=\"https://www.microsoft.com/accessories/en-us/products/keyboards/sculpt-ergonomic-desktop/l5v-00001\">Microsoft Sculpt Ergonomic</a>  is good. I&rsquo;ve never used one, but maybe that&rsquo;s a good way to ease yourself in to weirder keyboards? 😅</p>\n<div style=\"width: 48%; float: right; text-align: right;\">\n  <img alt=\"Microsoft Sculpt Ergonomic\" src=\"https://indirect.micro.blog/uploads/2025/f21a45f855.jpg\">\n  Microsoft Sculpt Ergonomic\n</div>\n<div style=\"clear: both\"></div>\n<h3 id=\"i-heard-mechanical-keyboards-are-better\">I heard mechanical keyboards are better</h3>\n<p>Maybe! Mechanical switches are generally more pleasant to type on, and last much longer. I would suggest reading the Wirecutter&rsquo;s guide to &ldquo;regular-shaped&rdquo; mechanical keyboards called <a href=\"https://thewirecutter.com/blog/how-to-shop-for-a-mechanical-keyboard/\">How to Shop for a Mechanical Keyboard</a>.</p>\n<p>If you want to give it a shot, the Wirecutter recommends the <a href=\"https://mechanicalkeyboards.com/shop/index.php?l=product_list&amp;c=324\">Varmilo VA87M</a>, and I&rsquo;ve heard positive things about the <a href=\"https://mechanicalkeyboards.com/shop/index.php?l=product_list&amp;c=341\">Filco Majestouch 2</a>. In case you were worried about colors, there are pink versions of both! (As well as the usual black, white, and a bunch of others.)</p>\n<div style=\"width: 48%; float: left;\">\n  <img alt=\"Varmilo VA87M Sakura\" src=\"https://indirect.micro.blog/uploads/2025/132b550422.jpg\">\n  Varmilo VA87M Sakura\n</div>\n<div style=\"width: 48%; float: right; text-align: right;\">\n  <img alt=\"Filco Majestouch 2 Pink\" src=\"https://indirect.micro.blog/uploads/2025/59e9443ce9.jpg\">\n  Filco Majestouch 2 Pink\n</div>\n<div style=\"clear: both\"></div>\n<h3 id=\"i-have-rsi--typing-hurts-sometimes\">I have RSI / typing hurts sometimes</h3>\n<p>If you have RSI or you&rsquo;re looking to reduce hand or wrist pain, the most ergonomic keyboard on the market is the <a href=\"https://kinesis-ergo.com/shop/advantage2/\">Kinesis Advantage 2</a>. It&rsquo;s a weird shape, specifically because it&rsquo;s shaped to fill in the space under and around your hands if you hold your arms out neutrally. Most relaxed and least effort to type on of any keyboard I&rsquo;ve ever used in my life.</p>\n<div style=\"width: 48%; float: right; text-align: right;\">\n  <img alt=\"Kinesis Advantage 2\" src=\"https://indirect.micro.blog/uploads/2025/5985b97c27.jpg\">\n  Kinesis Advantage 2\n</div>\n<div style=\"clear: both\"></div>\n<h3 id=\"i-want-ultimate-control-and-customization\">I want ultimate control and customization</h3>\n<p>They aren&rsquo;t quite as ergonomic as the Kinesis, but there are two options that are still pretty ergonomic while being 100% customizable: the <a href=\"https://ergodox-ez.com/\">Ergodox EZ</a> and <a href=\"http://keyboard.io/\">Keyboard.io Model 01</a>. Split with thumb keys like a Kinesis, but flat rather than shaped to keep your hands and arms neutral. The upside is they have open source firmware, and you can program them to do literally anything you want. Notable anythings include: multiple fn keys for many layers, single keys that press multiple other keys, one key if you tap different key if you hold, etc.</p>\n<div style=\"width: 48%; float: left;\">\n  <img alt=\"Ergodox EZ\" src=\"https://indirect.micro.blog/uploads/2025/7493d6339d.jpg\">\n  Ergodox EZ\n</div>\n<div style=\"width: 48%; float: right; text-align: right;\">\n  <img alt=\"Keyboard.io Model 01\" src=\"https://indirect.micro.blog/uploads/2025/4d81c322fc.jpg\">\n  Keyboard.io Model 01\n</div>\n<div style=\"clear: both\"></div>\n<h3 id=\"i-want-mechanical-keys-but-laptop-sized\">I want mechanical keys but laptop sized</h3>\n<p>The smallest purchasable mechanical keyboard I know of is the <a href=\"https://atreus.technomancy.us\">Atreus 2</a>. It was designed by my college friend <a href=\"http://technomancy.us\">Phil</a> so he could use a mechanical keyboard when working on his laptop from coffee shops. It&rsquo;s conveniently about to be mass-manufactured by the <a href=\"http://keyboard.io/\">Keyboard.io</a> folks, with <a href=\"https://www.kickstarter.com/projects/keyboardio/atreus\">a Kickstarter that begins in a few days</a>. I built one last month, laser cutting the case from maple and soldering on Kalih blue switches, and posted <a href=\"/2020/03/15/built-an-atreus-2/\">some pictures of that build</a> if you&rsquo;re interested.</p>\n<p>Almost as small is the <a href=\"https://ergodox-ez.com/pages/planck\">Planck EZ</a>, from the ErgoDox EZ folks. The physical case is a perfect rectangle and the keys are in straight rows and columns (&ldquo;ortholinear&rdquo; in mechanical keyboard-speak).</p>\n<div style=\"width: 48%; float: left;\">\n  <img alt=\"Atreus 2\" src=\"https://indirect.micro.blog/uploads/2025/38a8318fe2.jpg\">\n  Atreus 2\n</div>\n<div style=\"width: 48%; float: right; text-align: right;\">\n  <img alt=\"Planck EZ\" src=\"https://indirect.micro.blog/uploads/2025/1b20b5cf33.jpg\">\n  Planck EZ\n</div>\n<div style=\"clear: both\"></div>\n<h3 id=\"i-am-obsessed-with-keyboards-and-know-all-this-already\">I am obsessed with keyboards and know all this already</h3>\n<p>Okay now we&rsquo;re getting pretty out there, up against the boundaries of keyboarding that&rsquo;s possible today. My personal wishlist/buildlist is a <a href=\"https://www.reddit.com/r/MechanicalKeyboards/comments/66588f/wireless_split_qmk_mitosis/\">Mitosis</a> (split, smaller than ergodox, but 100% wireless), and a <a href=\"https://github.com/20lives/Dactyl-Manuform/blob/devel/README.md\">Dactyl Manuform</a> (an open source attempt at shaping around your hand via 3d printing, requires completely manual wiring). Fun, huh?</p>\n<div style=\"width: 48%; float: left;\">\n  <img alt=\"mitosis\" src=\"https://indirect.micro.blog/uploads/2025/420c4bce04.jpg\">\n  mitosis\n</div>\n<div style=\"width: 48%; float: right; text-align: right;\">\n  <img alt=\"dactyl manuform\" src=\"https://indirect.micro.blog/uploads/2025/46e56fc2b8.jpg\">\n  dactyl manuform\n</div>\n<div style=\"clear: both\"></div>\n<p>I just realized we haven&rsquo;t even started to touch on firmware, switches, or keycaps yet, but that&rsquo;ll have to wait for another post. See you next time!</p>\n",
				"content_text": "I have <strike>a lot of keyboards</strike> a keyboard problem.\n\nNow that everyone is [staying home](https://aphyr.com/posts/350-stay-home) (you're staying home too, right?), some friends who know about my keyboards have started asking me to give them keyboard advice. Since it's impossible to that without a ton more context, I wrote this guide to help people figure out more specific questions they can ask about keyboards. \n\nIf you don't really know much about keyboards, this post is some context describing the functionality of some popular keyboards, in escalating order of expense and... particularness.\n\nOn the other hand, if you want an aesthetics-focused list, I suggest jumping over to [astrolokeys.com](http://astrolokeys.com) where you can read *the signs as mechanical keyboards*, instead of the rest of this post. 😅\n\n### I just want an external keyboard for my laptop\n\nFortunately this one is pretty easy. If you have a Mac, get an [Apple Magic Keyboard](https://www.apple.com/shop/product/MLA22LL/A/magic-keyboard-us-english). It's wireless, has a layout copied from your laptop so you already know it, and is a totally reasonable keyboard.\n\n It pairs instantly when you plug it in to your computer using a Lightning cable, and it has a built-in battery that lasts for weeks. If you leave it plugged in to your computer, it even switches over to being a wired keyboard and the delay between pushing a key and the computer showing the letter goes down.\n\nIf you don't have a Mac, the Wirecutter recommends the [Logitech K380](https://www.amazon.com/dp/B0148NPH9I). Seems good.\n\n<div style=\"width: 48%; float: left;\">\n  <img alt=\"Apple Magic Keyboard\" src=\"https://indirect.micro.blog/uploads/2025/c6ad3c0a8a.jpg\">\n  Apple Magic Keyboard\n</div>\n\n<div style=\"width: 48%; float: right; text-align: right;\">\n  <img alt=\"Logitech K380\" src=\"https://indirect.micro.blog/uploads/2025/74caf4c490.jpg\">\n  Logitech K380\n</div>\n<div style=\"clear: both\"></div>\n\n### I heard ergonomic keyboards are better\n\nIf you want something that's pretty close to the keyboard you're already familiar with, but more ergonomic, I have heard that the [Microsoft Sculpt Ergonomic](https://www.microsoft.com/accessories/en-us/products/keyboards/sculpt-ergonomic-desktop/l5v-00001)  is good. I've never used one, but maybe that's a good way to ease yourself in to weirder keyboards? 😅\n\n<div style=\"width: 48%; float: right; text-align: right;\">\n  <img alt=\"Microsoft Sculpt Ergonomic\" src=\"https://indirect.micro.blog/uploads/2025/f21a45f855.jpg\">\n  Microsoft Sculpt Ergonomic\n</div>\n<div style=\"clear: both\"></div>\n\n### I heard mechanical keyboards are better\n\nMaybe! Mechanical switches are generally more pleasant to type on, and last much longer. I would suggest reading the Wirecutter's guide to \"regular-shaped\" mechanical keyboards called [How to Shop for a Mechanical Keyboard](https://thewirecutter.com/blog/how-to-shop-for-a-mechanical-keyboard/).\n\nIf you want to give it a shot, the Wirecutter recommends the [Varmilo VA87M](https://mechanicalkeyboards.com/shop/index.php?l=product_list&c=324), and I've heard positive things about the [Filco Majestouch 2](https://mechanicalkeyboards.com/shop/index.php?l=product_list&c=341). In case you were worried about colors, there are pink versions of both! (As well as the usual black, white, and a bunch of others.)\n\n<div style=\"width: 48%; float: left;\">\n  <img alt=\"Varmilo VA87M Sakura\" src=\"https://indirect.micro.blog/uploads/2025/132b550422.jpg\">\n  Varmilo VA87M Sakura\n</div>\n\n<div style=\"width: 48%; float: right; text-align: right;\">\n  <img alt=\"Filco Majestouch 2 Pink\" src=\"https://indirect.micro.blog/uploads/2025/59e9443ce9.jpg\">\n  Filco Majestouch 2 Pink\n</div>\n<div style=\"clear: both\"></div>\n\n### I have RSI / typing hurts sometimes\n\nIf you have RSI or you're looking to reduce hand or wrist pain, the most ergonomic keyboard on the market is the [Kinesis Advantage 2](https://kinesis-ergo.com/shop/advantage2/). It's a weird shape, specifically because it's shaped to fill in the space under and around your hands if you hold your arms out neutrally. Most relaxed and least effort to type on of any keyboard I've ever used in my life.\n\n<div style=\"width: 48%; float: right; text-align: right;\">\n  <img alt=\"Kinesis Advantage 2\" src=\"https://indirect.micro.blog/uploads/2025/5985b97c27.jpg\">\n  Kinesis Advantage 2\n</div>\n<div style=\"clear: both\"></div>\n\n### I want ultimate control and customization\n\nThey aren't quite as ergonomic as the Kinesis, but there are two options that are still pretty ergonomic while being 100% customizable: the [Ergodox EZ](https://ergodox-ez.com/) and [Keyboard.io Model 01](http://keyboard.io/). Split with thumb keys like a Kinesis, but flat rather than shaped to keep your hands and arms neutral. The upside is they have open source firmware, and you can program them to do literally anything you want. Notable anythings include: multiple fn keys for many layers, single keys that press multiple other keys, one key if you tap different key if you hold, etc.\n\n<div style=\"width: 48%; float: left;\">\n  <img alt=\"Ergodox EZ\" src=\"https://indirect.micro.blog/uploads/2025/7493d6339d.jpg\">\n  Ergodox EZ\n</div>\n\n<div style=\"width: 48%; float: right; text-align: right;\">\n  <img alt=\"Keyboard.io Model 01\" src=\"https://indirect.micro.blog/uploads/2025/4d81c322fc.jpg\">\n  Keyboard.io Model 01\n</div>\n<div style=\"clear: both\"></div>\n\n### I want mechanical keys but laptop sized\n\nThe smallest purchasable mechanical keyboard I know of is the [Atreus 2](https://atreus.technomancy.us). It was designed by my college friend [Phil](http://technomancy.us) so he could use a mechanical keyboard when working on his laptop from coffee shops. It's conveniently about to be mass-manufactured by the [Keyboard.io](http://keyboard.io/) folks, with [a Kickstarter that begins in a few days](https://www.kickstarter.com/projects/keyboardio/atreus). I built one last month, laser cutting the case from maple and soldering on Kalih blue switches, and posted [some pictures of that build](/2020/03/15/built-an-atreus-2/) if you're interested.\n\nAlmost as small is the [Planck EZ](https://ergodox-ez.com/pages/planck), from the ErgoDox EZ folks. The physical case is a perfect rectangle and the keys are in straight rows and columns (\"ortholinear\" in mechanical keyboard-speak).\n\n<div style=\"width: 48%; float: left;\">\n  <img alt=\"Atreus 2\" src=\"https://indirect.micro.blog/uploads/2025/38a8318fe2.jpg\">\n  Atreus 2\n</div>\n\n<div style=\"width: 48%; float: right; text-align: right;\">\n  <img alt=\"Planck EZ\" src=\"https://indirect.micro.blog/uploads/2025/1b20b5cf33.jpg\">\n  Planck EZ\n</div>\n<div style=\"clear: both\"></div>\n\n### I am obsessed with keyboards and know all this already\n\nOkay now we're getting pretty out there, up against the boundaries of keyboarding that's possible today. My personal wishlist/buildlist is a [Mitosis](https://www.reddit.com/r/MechanicalKeyboards/comments/66588f/wireless_split_qmk_mitosis/) (split, smaller than ergodox, but 100% wireless), and a [Dactyl Manuform](https://github.com/20lives/Dactyl-Manuform/blob/devel/README.md) (an open source attempt at shaping around your hand via 3d printing, requires completely manual wiring). Fun, huh?\n\n<div style=\"width: 48%; float: left;\">\n  <img alt=\"mitosis\" src=\"https://indirect.micro.blog/uploads/2025/420c4bce04.jpg\">\n  mitosis\n</div>\n\n<div style=\"width: 48%; float: right; text-align: right;\">\n  <img alt=\"dactyl manuform\" src=\"https://indirect.micro.blog/uploads/2025/46e56fc2b8.jpg\">\n  dactyl manuform\n</div>\n<div style=\"clear: both\"></div>\n\nI just realized we haven't even started to touch on firmware, switches, or keycaps yet, but that'll have to wait for another post. See you next time!\n",
				"date_published": "2020-03-14T00:00:00-08:00",
				"url": "https://andre.arko.net/2020/03/14/keyboards-tell-me-more/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2020/03/11/announcing-rubygemsorg-stats/",
				"title": "Announcing RubyGems.org Stats",
				"content_html": "<p><small>This post was originally written for the <a href=\"https://blog.rubygems.org/2020/03/09/announcing-rubygems-stats.html\">the RubyGems.org blog</a>.</small></p>\n<p>Ever since it was first released, the Bundler team has wanted to know more about the developers out there using our code. What versions of Ruby are still being actively used? What versions of RubyGems is it safe to stop supporting? Which operating systems should we focus on testing?</p>\n<p>It’s been almost 10 years since that first release, but today the RubyGems and Bundler team is excited to announce that everyone can see the answers to these questions at <a href=\"https://stats.rubygems.org\">stats.rubygems.org</a>. It’s been a long journey from <a href=\"https://github.com/rubygems/bundler/commit/7a95b0cbbcacbd899bd108319fffb57c327ad6f7\">the first commit in 2013</a> to the working website today, so I’ll try to stick to the highlights.</p>\n<p>After reading reports like <a href=\"https://marco.org/2011/08/13/instapaper-ios-device-and-version-stats-update\">iOS device and OS version stats from Instapaper</a> and <a href=\"https://blog.planetargon.com/entries/the-2018-ruby-on-rails-survey\">Planet Argon&rsquo;s long-running Ruby on Rails community survey</a>, I was inspired to try to collect similar stats about Bundler users. Knowing that Bundler already had to to send some information to RubyGems.org during every <code>bundle install</code>, I started there. Sending Bundler, RubyGems, and Ruby version information in the User-Agent header meant that the versions we wanted to track would be available in the RubyGems.org server logs.</p>\n<p>Merely 2 or 3 years later, I was able to ensure that a redacted copy of the RubyGems.org server logs would be saved to storage sponsored by <a href=\"https://rubytogether.org\">Ruby Together</a>. Then, I just needed to figure out how to take the files on S3 and turn them into useful daily numbers.</p>\n<p>Unfortunately, RubyGems.org is an extremely popular website, and it produces a truly stupendous amount of logs: something on the order of 500GB, every single day. Downloading those logs, parsing them to get out the user agent, trying to remove duplicates, and then saving the results, becomes an extremely hard job when you need to be able to do it cheaply, every day, and faster than 20GB/hour.</p>\n<p>It took several more years, and <a href=\"/2018/10/25/parsing-logs-230x-faster-with-rust/\">lots of experimentation with Ruby, Python, Apache Spark, AWS Glue, Rust, and Amazon Lambda</a>, but I eventually managed to create a system that could reliably process the RubyGems.org logs firehose and provide daily numbers in the second half of 2018.</p>\n<p>At that point, André started work on a webapp that could display that data, but then completely ran out of spare cycles to work on the project for all of 2019. Fortunately, that’s when <a href=\"https://github.com/sidk\">@sidk</a> stepped in, heroically working to complete and expand the webapp into the site we have today. Here&rsquo;s a summary from Sid of how the final site is set up:</p>\n<hr>\n<p>The display webapp is comprised of the following components:</p>\n<ul>\n<li>A daily rake task, to download data from S3 into Postgres. Data is uploaded to S3 by kirby (the log parser) after it processes log data.</li>\n<li>A JSON API, with the following endpoints:\n<ul>\n<li>/versions/{thing}</li>\n<li>/comparison/{thing1}/{thing2}</li>\n</ul>\n</li>\n</ul>\n<p>In the API, each <code>thing</code> is one of: ruby, bundler, rubygems, platform, or ci.</p>\n<p>On the frontend, we currently use <a href=\"https://apexcharts.com\">ApexCharts</a>. Every graph on the page is a partial that makes a request to the server for version or comparison data and then instantiates an ApexChart.</p>\n<hr>\n<p>In the end, things worked out pretty well, and we’re very excited and proud to make such a useful resource available to the Ruby community. We’re excited to work with all of you to keep making both the <a href=\"https://github.com/rubytogether/kirby\">log parser</a> and <a href=\"https://github.com/rubytogether/ecosystem\">display webapp</a> even better.</p>\n<p>Check out <a href=\"https://stats.rubygems.org\">stats.rubygems.org</a> and tell us what you think!</p>\n",
				"content_text": "\n<small>This post was originally written for the [the RubyGems.org blog](https://blog.rubygems.org/2020/03/09/announcing-rubygems-stats.html).</small>\n\nEver since it was first released, the Bundler team has wanted to know more about the developers out there using our code. What versions of Ruby are still being actively used? What versions of RubyGems is it safe to stop supporting? Which operating systems should we focus on testing?\n\nIt’s been almost 10 years since that first release, but today the RubyGems and Bundler team is excited to announce that everyone can see the answers to these questions at [stats.rubygems.org](https://stats.rubygems.org). It’s been a long journey from [the first commit in 2013](https://github.com/rubygems/bundler/commit/7a95b0cbbcacbd899bd108319fffb57c327ad6f7) to the working website today, so I’ll try to stick to the highlights.\n\nAfter reading reports like [iOS device and OS version stats from Instapaper](https://marco.org/2011/08/13/instapaper-ios-device-and-version-stats-update) and [Planet Argon's long-running Ruby on Rails community survey](https://blog.planetargon.com/entries/the-2018-ruby-on-rails-survey), I was inspired to try to collect similar stats about Bundler users. Knowing that Bundler already had to to send some information to RubyGems.org during every `bundle install`, I started there. Sending Bundler, RubyGems, and Ruby version information in the User-Agent header meant that the versions we wanted to track would be available in the RubyGems.org server logs.\n\nMerely 2 or 3 years later, I was able to ensure that a redacted copy of the RubyGems.org server logs would be saved to storage sponsored by [Ruby Together](https://rubytogether.org). Then, I just needed to figure out how to take the files on S3 and turn them into useful daily numbers.\n\nUnfortunately, RubyGems.org is an extremely popular website, and it produces a truly stupendous amount of logs: something on the order of 500GB, every single day. Downloading those logs, parsing them to get out the user agent, trying to remove duplicates, and then saving the results, becomes an extremely hard job when you need to be able to do it cheaply, every day, and faster than 20GB/hour.\n\nIt took several more years, and [lots of experimentation with Ruby, Python, Apache Spark, AWS Glue, Rust, and Amazon Lambda](/2018/10/25/parsing-logs-230x-faster-with-rust/), but I eventually managed to create a system that could reliably process the RubyGems.org logs firehose and provide daily numbers in the second half of 2018.\n\nAt that point, André started work on a webapp that could display that data, but then completely ran out of spare cycles to work on the project for all of 2019. Fortunately, that’s when [@sidk](https://github.com/sidk) stepped in, heroically working to complete and expand the webapp into the site we have today. Here's a summary from Sid of how the final site is set up:\n\n<hr>\n\nThe display webapp is comprised of the following components:\n\n- A daily rake task, to download data from S3 into Postgres. Data is uploaded to S3 by kirby (the log parser) after it processes log data.\n- A JSON API, with the following endpoints:\n    * /versions/{thing}\n    * /comparison/{thing1}/{thing2}\n\nIn the API, each `thing` is one of: ruby, bundler, rubygems, platform, or ci.\n\nOn the frontend, we currently use [ApexCharts](https://apexcharts.com). Every graph on the page is a partial that makes a request to the server for version or comparison data and then instantiates an ApexChart.\n\n<hr>\n\nIn the end, things worked out pretty well, and we’re very excited and proud to make such a useful resource available to the Ruby community. We’re excited to work with all of you to keep making both the [log parser](https://github.com/rubytogether/kirby) and [display webapp](https://github.com/rubytogether/ecosystem) even better.\n\nCheck out [stats.rubygems.org](https://stats.rubygems.org) and tell us what you think!\n",
				"date_published": "2020-03-11T00:00:00-08:00",
				"url": "https://andre.arko.net/2020/03/11/announcing-rubygemsorg-stats/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2020/02/19/git-golf-continued/",
				"title": "git golf continued",
				"content_html": "<p>In the ongoing quest to type even less while using git on a day to day basis, I noticed that I sometimes need to switch back and forth between branches with similar names. To pick a completely hypothetical example, let’s call those branches <code>update-ruby</code> and <code>update-rails</code>.</p>\n<p>When all my branches have nice, unique names I can switch between them by typing <code>git co X&lt;tab&gt;</code>, where X is the first letter of the branch name. In this case, though, I have to type <code>git co u&lt;tab&gt;&lt;tab&gt;</code>, then read the list of autocomplete options to figure out what the next letter I need to type is, and then type <code>u&lt;tab&gt;</code> to complete the branch I actually want.</p>\n<p>Wouldn’t it be easier to check out the branch by any unique string contained within its name, without having to tab-complete? Yes, it would.</p>\n<p>Leveraging the magic of <a href=\"https://github.com/junegunn/fzf\">fzf</a>, I have updated my <a href=\"/2019/01/20/git-in-as-fw-chrs-as-psbl/\">previously mentioned <code>gb</code> alias</a> to be even more powerful:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">function</span> gb <span style=\"color:#f92672\">{</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">if</span> <span style=\"color:#f92672\">[[</span> -z <span style=\"color:#e6db74\">&#34;</span>$1<span style=\"color:#e6db74\">&#34;</span> <span style=\"color:#f92672\">]]</span>; <span style=\"color:#66d9ef\">then</span>\n</span></span><span style=\"display:flex;\"><span>    git branch -v\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">else</span>\n</span></span><span style=\"display:flex;\"><span>    git branch | grep -v <span style=\"color:#e6db74\">&#34;^*&#34;</span> | fzf -f <span style=\"color:#e6db74\">&#34;</span>$1<span style=\"color:#e6db74\">&#34;</span> | head -n1 | xargs git checkout\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">fi</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#f92672\">}</span>\n</span></span></code></pre></div><p>Now, see a list of local branches with <code>gb</code>, and then choose a branch with <code>gb foo</code>, where <code>foo</code> is any string that allows fzf to tell which branch you mean. That could be a unique string, but it could even be a unique set of characters that appear in the same order in the target branch name. fzf is great.</p>\n<p>Now my daily branch workflow is more like <code>gb</code>, <code>gb ruby</code>, do some work, <code>gi</code>, <code>gp</code>, <code>gb rails</code>, do some other work, <code>gi</code>. As long as we measure only in terms of buttons pushed to use git, life continues to improve!</p>\n",
				"content_text": "In the ongoing quest to type even less while using git on a day to day basis, I noticed that I sometimes need to switch back and forth between branches with similar names. To pick a completely hypothetical example, let’s call those branches `update-ruby` and `update-rails`.\n\nWhen all my branches have nice, unique names I can switch between them by typing `git co X<tab>`, where X is the first letter of the branch name. In this case, though, I have to type `git co u<tab><tab>`, then read the list of autocomplete options to figure out what the next letter I need to type is, and then type `u<tab>` to complete the branch I actually want.\n\nWouldn’t it be easier to check out the branch by any unique string contained within its name, without having to tab-complete? Yes, it would.\n\nLeveraging the magic of [fzf](https://github.com/junegunn/fzf), I have updated my [previously mentioned `gb` alias](/2019/01/20/git-in-as-fw-chrs-as-psbl/) to be even more powerful:\n\n```bash\nfunction gb {\n  if [[ -z \"$1\" ]]; then\n    git branch -v\n  else\n    git branch | grep -v \"^*\" | fzf -f \"$1\" | head -n1 | xargs git checkout\n  fi\n}\n```\n\nNow, see a list of local branches with `gb`, and then choose a branch with `gb foo`, where `foo` is any string that allows fzf to tell which branch you mean. That could be a unique string, but it could even be a unique set of characters that appear in the same order in the target branch name. fzf is great.\n\nNow my daily branch workflow is more like `gb`, `gb ruby`, do some work, `gi`, `gp`, `gb rails`, do some other work, `gi`. As long as we measure only in terms of buttons pushed to use git, life continues to improve!\n",
				"date_published": "2020-02-19T00:00:00-08:00",
				"url": "https://andre.arko.net/2020/02/19/git-golf-continued/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2020/01/24/how-to-do-code-review/",
				"title": "🩺 How to do Code Review",
				"content_html": "<div class=\"callout\">\nThese guidelines were provided by an anonymous benefactor. They are reposted here (with permission) in the hope that future code review discussions can start from this excellent basis. — André\n</div>\n<p>As a reviewer you’re responsible for <strong>improving safety</strong>, <strong>enforcing broadly agreed upon standards</strong> and should use the opportunity to <strong>teach and learn from your colleagues</strong>.</p>\n<h2 id=\"-the-goal-of-code-reviews\">🎯 The goal of code reviews</h2>\n<p>Code reviews have a few goals.</p>\n<ol>\n<li>\n<p><strong>⛑ Safety</strong></p>\n<p>The #1 priority in a code review is to provide a layer of safety to protect things from breaking for your users.</p>\n</li>\n<li>\n<p><strong>🚔 Enforcement of broadly agreed upon standards</strong></p>\n<p>In the cases where there are broadly agreed upon standards that aren’t enforced by your test infrastructure we may need to enforce standards in reviews. Reviewers should never enforce standards that aren’t broadly agreed upon.</p>\n</li>\n<li>\n<p><strong>🧠 Education and context sharing</strong></p>\n<p>Nobody comes to a company already knowing the stack. Code reviews provide a chance for us to learn from each other.</p>\n</li>\n</ol>\n<p>Authors are required to get an approval from any colleague in order to merge code, but that doesn’t give reviewers a license to block reviews for just any reason.</p>\n<ul>\n<li>\n<p><strong>❌ No stylistic preferences</strong></p>\n<p>Linters enforce style guidelines. Reviewers shouldn’t push their personal preferences in a blocking code review.</p>\n</li>\n<li>\n<p><strong>❌ No blocking tips</strong></p>\n<p>Offering tips for how things could be improved is great, but reviewers shouldn’t block PRs on nits.</p>\n</li>\n<li>\n<p><strong>✅ Block unsafe or non-standard code</strong></p>\n<p>Only safety issues and broadly agreed upon standards that aren’t caught by the linter should block PRs.</p>\n</li>\n</ul>\n<h3 id=\"--safety\">⛑  Safety</h3>\n<p>The most important goal of a code review is to provide a layer of safety. The top priority of a reviewer is to look for risks that may have been missed. There are many known types of risks that folks will be looking for and many unknown risks that you’ll need to use your discretion to navigate.</p>\n<p><strong>🔐 Security and privacy</strong></p>\n<p>Any PR that introduces (or appears to introduce) a security or privacy issue should be blocked until the issue is resolved.</p>\n<p><strong>🐛 Bugs</strong></p>\n<p>Reviewers should be on the lookout for bugs where the product might not behave as intended by the author or could otherwise cause issues for users, integrity of our data, etc..</p>\n<p><strong>🧨 Traps for other engineers</strong></p>\n<p>If a method is called <code>renameUser</code> but it actually deletes the user we could expect future engineers to get confused and will be at risk of introducing bugs.</p>\n<p><strong>🕸 System failures</strong></p>\n<p>It’s not always easy to spot changes that could trigger cascading failures or instability of our systems. Reviewers should be on the lookout for how unexpected circumstances might impact the broader network.</p>\n<p><strong>💸 Excessive costs</strong></p>\n<p>If a change might impact our costs in a material way needs to be prepared for. Reviewers should prevent changes that could unexpectedly increase costs.</p>\n<h3 id=\"-enforcement-of-broadly-agreed-upon-standards\">🚔 Enforcement of broadly agreed upon standards</h3>\n<p>Ideally all of our code base policies should be encoded as linters and tests, but sometimes the infrastructure doesn’t exist and we rely on engineers to enforce policy for broadly agreed upon standards.</p>\n<p>Reviewers shouldn’t enforce standards that aren’t broadly agreed upon. If something has been posted in a widely read channel or at an all-hands it probably should be enforced.</p>\n<h3 id=\"-education-and-shared-context\">🧠 Education and shared context</h3>\n<p>Nobody went to school for hacking on your company’s stack. Outside of software fundamentals all of us had to learn how to make things work while on the job. Code reviews are one of the best ways for us to share knowledge and context about different ways things are done or tricks we’ve figured out to get things done in better ways.</p>\n<p>Reviewers should freely share questions they have about why things are done the way they’re done in a review or offer insights into how things are done elsewhere.</p>\n<p>That being said, education and context sharing isn’t blocking. If as a reviewer you see something that’s safe and aligns to broadly agreed standards but could be done in a different way you should let the author know but approve the pull request unless there are other issues.</p>\n<h2 id=\"-appendix-a-related-docs\">📚 Appendix A: Related Docs</h2>\n<ul>\n<li><a href=\"https://kickstarter.engineering/a-guide-to-mindful-communication-in-code-reviews-48aab5282e5e\">A Guide to Mindful Communication in Code Reviews</a> by Amy Ciavolino</li>\n</ul>\n",
				"content_text": "<div class=\"callout\">\nThese guidelines were provided by an anonymous benefactor. They are reposted here (with permission) in the hope that future code review discussions can start from this excellent basis. — André\n</div>\n\nAs a reviewer you’re responsible for **improving safety**, **enforcing broadly agreed upon standards** and should use the opportunity to **teach and learn from your colleagues**.\n\n## 🎯 The goal of code reviews\n\nCode reviews have a few goals.\n\n1. **⛑ Safety**\n\n    The #1 priority in a code review is to provide a layer of safety to protect things from breaking for your users.\n\n2. **🚔 Enforcement of broadly agreed upon standards**\n\n    In the cases where there are broadly agreed upon standards that aren’t enforced by your test infrastructure we may need to enforce standards in reviews. Reviewers should never enforce standards that aren’t broadly agreed upon.\n\n3. **🧠 Education and context sharing**\n\n    Nobody comes to a company already knowing the stack. Code reviews provide a chance for us to learn from each other.\n\nAuthors are required to get an approval from any colleague in order to merge code, but that doesn’t give reviewers a license to block reviews for just any reason.\n\n- **❌ No stylistic preferences**\n\n    Linters enforce style guidelines. Reviewers shouldn’t push their personal preferences in a blocking code review.\n\n- **❌ No blocking tips**\n\n    Offering tips for how things could be improved is great, but reviewers shouldn’t block PRs on nits.\n\n- **✅ Block unsafe or non-standard code**\n\n    Only safety issues and broadly agreed upon standards that aren’t caught by the linter should block PRs.\n\n### ⛑  Safety\n\nThe most important goal of a code review is to provide a layer of safety. The top priority of a reviewer is to look for risks that may have been missed. There are many known types of risks that folks will be looking for and many unknown risks that you’ll need to use your discretion to navigate.\n\n**🔐 Security and privacy**\n\nAny PR that introduces (or appears to introduce) a security or privacy issue should be blocked until the issue is resolved.\n\n**🐛 Bugs**\n\nReviewers should be on the lookout for bugs where the product might not behave as intended by the author or could otherwise cause issues for users, integrity of our data, etc..\n\n**🧨 Traps for other engineers**\n\nIf a method is called `renameUser` but it actually deletes the user we could expect future engineers to get confused and will be at risk of introducing bugs.\n\n**🕸 System failures**\n\nIt’s not always easy to spot changes that could trigger cascading failures or instability of our systems. Reviewers should be on the lookout for how unexpected circumstances might impact the broader network.\n\n**💸 Excessive costs**\n\nIf a change might impact our costs in a material way needs to be prepared for. Reviewers should prevent changes that could unexpectedly increase costs.\n\n### 🚔 Enforcement of broadly agreed upon standards\n\nIdeally all of our code base policies should be encoded as linters and tests, but sometimes the infrastructure doesn’t exist and we rely on engineers to enforce policy for broadly agreed upon standards.\n\nReviewers shouldn’t enforce standards that aren’t broadly agreed upon. If something has been posted in a widely read channel or at an all-hands it probably should be enforced.\n\n### 🧠 Education and shared context\n\nNobody went to school for hacking on your company’s stack. Outside of software fundamentals all of us had to learn how to make things work while on the job. Code reviews are one of the best ways for us to share knowledge and context about different ways things are done or tricks we’ve figured out to get things done in better ways.\n\nReviewers should freely share questions they have about why things are done the way they’re done in a review or offer insights into how things are done elsewhere.\n\nThat being said, education and context sharing isn’t blocking. If as a reviewer you see something that’s safe and aligns to broadly agreed standards but could be done in a different way you should let the author know but approve the pull request unless there are other issues.\n\n## 📚 Appendix A: Related Docs\n\n- [A Guide to Mindful Communication in Code Reviews](https://kickstarter.engineering/a-guide-to-mindful-communication-in-code-reviews-48aab5282e5e) by Amy Ciavolino\n",
				"date_published": "2020-01-24T00:00:00-08:00",
				"url": "https://andre.arko.net/2020/01/24/how-to-do-code-review/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2019/01/20/git-in-as-fw-chrs/",
				"title": "git in as fw chrs as psbl",
				"content_html": "<h3 id=\"or-learn-from-my-dotfile-mistakes\">or: learn from my dotfile mistakes</h3>\n<p>Over the years, I have accumulated a <em>lot</em> of dotfiles around my coding workflow, and most of them are focused on git. Each time I notice that I&rsquo;m spending a lot of time doing something over and over, I looked for a way to wrap that work up into a shortcut to make it faster. Today, I have a vibrant ecosystem of git aliases, bash aliases, and scripts that interact with both <code>git</code> and <code>hub</code> to make things happen.</p>\n<p>None of it was designed, and none of it fundamentally makes sense, but I have a decade worth of muscle memory built up for this exact set of shortcuts and there&rsquo;s not a whole lot I can do about that now. So I&rsquo;m going to show off my nonsensical gibberish that makes git do exactly what I want, and you can decide which pieces you want to steal but give names or shortcuts that make more sense.</p>\n<p>Let&rsquo;s start with the workflow that I have personally optimized the most: cloning an open source project, forking it, making changes, pushing those changes, and opening a pull request. This will only touch a few of my shortcuts, but it will include the ones that I&rsquo;ve put the most time and work into.</p>\n<p>In this hypothetical example, I&rsquo;m going to use <a href=\"https://github.com/bundler/bundler\">Bundler</a> (as if I were not a maintainer).</p>\n<h3 id=\"oss-pull-requests\">OSS pull requests</h3>\n<pre><code>\t$ gc bundler/bundler\n\t$ hub fork\n\t$ git co -b indirect/bugfix\n\t# do some work here\n\t$ gs\n\t$ gd\n\t$ gi -am &quot;Fixed the bug&quot;\n\t$ # get distracted by something else for two days\n\t$ git rup\n\t$ git rebase origin master\n\t$ gp indirect\n\t$ hub prl\n\thttps://github.com/bundler/bundler/pulls/12345\n</code></pre>\n<p>As you probably noticed, every line there (except <code>hub fork</code>) was some sort of shortcut. Let&rsquo;s look through them one at a time.</p>\n<p>The <code>gc</code> command checks out a git repo into a specific directory structure: <code>~/src/username/reponame</code>, and then <code>cd</code>s into the directory. The <code>hub fork</code> command creates a fork of the repo under my own GitHub account. The <code>gs</code> command runs <code>git status</code>, the <code>gd</code> command shows a <code>git diff</code>, and the <code>gi</code> command runs <code>git commit</code>.</p>\n<p>After two days of progress on the upstream, I use <code>rup</code> as an alias for <code>remote update</code>, which fetches the latest commits from all remotes, including both my fork and the upstream. Then I rebase against the upstream, use <code>gp indirect</code> to <code>git push</code> my HEAD commit to the <code>indirect</code> remote with the same remote branch name as I am using locally (which is <code>indirect/bugfix</code> in this example). After pushing to my fork, I use the <code>hub prl</code> shortcut to create a new pull request from my fork against the upstream, using the title of my last commit as the PR title, and using the body of my last commit message as the PR body.</p>\n<p>It’s probably over-optimized, but when your workday includes anywhere from a few to dozens of PRs against repos that you don’t own, it really adds up.</p>\n<p>I also have some other aliases that I use as a maintainer of Bundler. Here’s an example workflow.</p>\n<h3 id=\"oss-code-review\">OSS code review</h3>\n<pre><code>\t$ j bundler\n\t$ git rup\n\t$ git ff\n\t$ git cleanup\n\t$ gb\n\t$ git pr 6754\n\t$ gd master\n\t# review the diff, run the code, etc\n</code></pre>\n<p>This set of commands fetches updates both from my fork and the upstream repo, fast forwards the main branch to the latest commit, deletes any local branches that have been merged into the main branch, and then checks out and reviews a PR.</p>\n<pre><code>\t$ hub remote add username\n\t# make changes\n\t$ gp username pr-source-branch\n\t$ git wipe\n</code></pre>\n<p>In this OSS bonus round, I’m making some changes to an open PR against a repo where I am a maintainer. Since PRs grant edit permissions to maintainers, I am able to add the fork of the PR author, make changes, push to their PR branch, and then remove all remotes other than my own and the upstream with <code>wipe</code>.</p>\n<h3 id=\"daily-work\">Daily work</h3>\n<pre><code>\t$ j codebase\n\t$ git rup\n\t$ git co latest\n\t$ git ff\n\t$ git cleanup\n\t$ git co -b indirect/something\n\t# make some commits here\n\t$ gp\n\t# notice a typo, fix it\n\t$ git add .\n\t$ git fixup\n\t$ gp -f\n\t# come back the next day\n\t$ git rup\n\t$ git co master\n\t$ git ff\n\t$ git co -\n\t$ git rebase master\n\t$ gp\n\t$ hub browse\n\t# use a browser to make a pull request\n</code></pre>\n<h3 id=\"other-git-aliases\">Other git aliases</h3>\n<p>There are also a lot of shortcuts for more&hellip; esoteric&hellip; usecases. Let&rsquo;s look at some of those, too.</p>\n<p><code>git sha</code> prints the SHA of the HEAD commit.</p>\n<p><code>git cpsha</code> copies the SHA of the HEAD commit to the clipboard, so I can paste it somewhere else.</p>\n<p><code>git burn</code> deletes the most recent commit. It’s an alias to <code>git reset --hard HEAD^</code>.</p>\n<p><code>git nuke</code> removes every file that isn’t tracked by git. It’s an alias for <code>git reset --hard HEAD</code>.</p>\n<p><code>git nuke-all</code> removes every file and every directory that isn’t tracked by git. It’s an alias for <code>git reset --hard HEAD &amp;&amp; git clean -fd</code>.</p>\n<p><code>git ls</code> combines a one-line git log format with verification of git commit signatures. The output shows sha, relative time, author, and commit subject, as well as a color-coded letter indicating whether a signature is valid, invalid, unknown, or missing.</p>\n<p><img src=\"https://indirect.micro.blog/uploads/2025/66848525e0.jpg\" alt=\"example of git ls\"></p>\n<h3 id=\"other-bash-aliases\">Other bash aliases</h3>\n<p>I also have a lot of Bash aliases, although most of them are shortcuts to common git commands.</p>\n<pre><code>alias gb=&quot;git branch -v&quot;\nalias gba=&quot;git branch -va --color | grep -v 'remotes/origin/pr'&quot;\nalias gbl=&quot;git branch -vv --color | grep -v '\\[.*\\/.*\\] '&quot;\nalias gbr=&quot;git branch -vr --color | grep -v 'origin/HEAD'&quot;\nalias gcp=&quot;git cherry-pick&quot;\nalias gd=&quot;git diff&quot;\nalias gds=&quot;git diff --cached&quot;\nalias gdw=&quot;git diff --word-diff&quot;\nalias gf=&quot;git fetch --all&quot;\nalias gi=&quot;git commit -v&quot;\nalias gia=&quot;git commit --amend -v&quot;\nalias gl=&quot;git lg&quot;\nalias gs=&quot;git status -sb&quot;\n\nfunction gc {\n  local repo=${1#*github.com/}\n  repo=${repo%.git}\n  hub clone --recursive &quot;$repo&quot; &quot;$HOME/src/$repo&quot;\n  cd &quot;$HOME/src/$repo&quot;\n}\n\nfunction current_branch {\n  status=$(git status 2&gt; /dev/null)\n\n  # Bail if status failed, not a git repo\n  if [[ $? -ne 0 ]]; then return 1; fi\n\n  # Try to get the branch from the status we already have\n  if [[ $status =~ &quot;# On branch (.*?) &quot; ]]; then\n    name=&quot;${BASH_REMATCH[1]}&quot;\n  fi\n\n  # Check the output of `branch` next\n  if [[ -z &quot;$name&quot; ]]; then\n    name=$(git branch | grep '^*' | cut -b3- | grep -v '^(')\n  fi\n\n  # Fall back on name-rev\n  if [[ -z &quot;$name&quot; ]]; then\n    name=$(git name-rev --name-only --no-undefined --always HEAD)\n    name=&quot;${name#tags/}&quot;\n    name=&quot;${name#remotes/}&quot;\n  fi\n\n  echo &quot;$name&quot;\n}\n\nfunction gp {\n  local current_branch=$(current_branch)\n  local upstream=$(git config branch.$current_branch.remote)\n\n  if [[ -z &quot;$upstream&quot; ]]; then\n    if [[ &quot;$1&quot; == &quot;-f&quot; ]]; then\n      local options=&quot;-uf&quot;\n      local remote=&quot;${2-origin}&quot;\n    else\n      local remote=&quot;${1-origin}&quot;\n    fi\n\n    git push &quot;${options--u}&quot; &quot;$remote&quot; &quot;$current_branch&quot;\n  else\n    git push &quot;$@&quot;\n  fi\n}\n\nfunction mcd {\n  mkdir -p &quot;$@&quot;\n  cd &quot;$@&quot;\n}\n</code></pre>\n<p>I use <code>gc</code> and <code>gp</code> a lot in my day to day work.</p>\n<p>The <code>current_branch</code> function does something that I’ve never seen anywhere else. Not only does it show the branch name, if you are in one, if you check out a commit that doesn’t have its own branch it will show you where you are relative to the nearest named branch or tag. For example, if you check out <code>branchname~6</code>, my git status line will show <code>branchname~6</code>. Every other status line that I’ve seen falls back to a generic 8-character SHA when you check out a commit that isn’t at the tip of a branch, which is (IMO) wayyyy less useful.</p>\n<h3 id=\"other-git-config\">Other git config</h3>\n<p>I don&rsquo;t actually know how to use git without these things turned on.</p>\n<pre><code>\t[rerere]\n\t\tenabled = true\n\t[merge]\n\t\tconflictstyle = diff3\n\t[rebase]\n\t\tautoStash = true\n\t[diff]\n\t\talgorithm = patience\n</code></pre>\n<p>The <code>rerere</code> option means that you can resolve a rebase conflict one time, and have that resolution applied anytime you hit the same rebase conflict in the future.</p>\n<p>The <code>diff3</code> conflict style means that when there’s a merge conflict, not only do you see the other commit, and your commit, you also see <em>the original content before either commit</em>. Those lines are often invaluable to me when I’m trying to figure out what the other person changed, what I changed, and how to combine them.</p>\n<p>The <code>autoStash</code> option just means that you can rebase without committing everything first—the rebase command stashes before running, and pops after running. It’s great.</p>\n<p>Finally, the <code>patience</code> algorithm optimizes diffs to reduce the (so, so common!) issue where adding a function creates a diff partly in the previous function and partly in the next function. With <code>patience</code> turned on, the diff will show just the new function, where you added it.</p>\n<h3 id=\"conclusion\">Conclusion</h3>\n<p>That about wraps things up! These functions and aliases definitely aren’t something that I would directly recommend to anyone else, but hopefully seeing the kinds of things that I find useful has given you some inspiration for your own shortcuts. Hpy hkng!</p>\n",
				"content_text": "### or: learn from my dotfile mistakes\n\nOver the years, I have accumulated a _lot_ of dotfiles around my coding workflow, and most of them are focused on git. Each time I notice that I'm spending a lot of time doing something over and over, I looked for a way to wrap that work up into a shortcut to make it faster. Today, I have a vibrant ecosystem of git aliases, bash aliases, and scripts that interact with both `git` and `hub` to make things happen.\n\nNone of it was designed, and none of it fundamentally makes sense, but I have a decade worth of muscle memory built up for this exact set of shortcuts and there's not a whole lot I can do about that now. So I'm going to show off my nonsensical gibberish that makes git do exactly what I want, and you can decide which pieces you want to steal but give names or shortcuts that make more sense.\n\nLet's start with the workflow that I have personally optimized the most: cloning an open source project, forking it, making changes, pushing those changes, and opening a pull request. This will only touch a few of my shortcuts, but it will include the ones that I've put the most time and work into.\n\nIn this hypothetical example, I'm going to use [Bundler](https://github.com/bundler/bundler) (as if I were not a maintainer).\n\n### OSS pull requests\n\n\t\t$ gc bundler/bundler\n\t\t$ hub fork\n\t\t$ git co -b indirect/bugfix\n\t\t# do some work here\n\t\t$ gs\n\t\t$ gd\n\t\t$ gi -am \"Fixed the bug\"\n\t\t$ # get distracted by something else for two days\n\t\t$ git rup\n\t\t$ git rebase origin master\n\t\t$ gp indirect\n\t\t$ hub prl\n\t\thttps://github.com/bundler/bundler/pulls/12345\n\nAs you probably noticed, every line there (except `hub fork`) was some sort of shortcut. Let's look through them one at a time.\n\nThe `gc` command checks out a git repo into a specific directory structure: `~/src/username/reponame`, and then `cd`s into the directory. The `hub fork` command creates a fork of the repo under my own GitHub account. The `gs` command runs `git status`, the `gd` command shows a `git diff`, and the `gi` command runs `git commit`.\n\nAfter two days of progress on the upstream, I use `rup` as an alias for `remote update`, which fetches the latest commits from all remotes, including both my fork and the upstream. Then I rebase against the upstream, use `gp indirect` to `git push` my HEAD commit to the `indirect` remote with the same remote branch name as I am using locally (which is `indirect/bugfix` in this example). After pushing to my fork, I use the `hub prl` shortcut to create a new pull request from my fork against the upstream, using the title of my last commit as the PR title, and using the body of my last commit message as the PR body.\n\nIt’s probably over-optimized, but when your workday includes anywhere from a few to dozens of PRs against repos that you don’t own, it really adds up.\n\nI also have some other aliases that I use as a maintainer of Bundler. Here’s an example workflow.\n\n### OSS code review\n\n\t\t$ j bundler\n\t\t$ git rup\n\t\t$ git ff\n\t\t$ git cleanup\n\t\t$ gb\n\t\t$ git pr 6754\n\t\t$ gd master\n\t\t# review the diff, run the code, etc\n\nThis set of commands fetches updates both from my fork and the upstream repo, fast forwards the main branch to the latest commit, deletes any local branches that have been merged into the main branch, and then checks out and reviews a PR.\n\n\t\t$ hub remote add username\n\t\t# make changes\n\t\t$ gp username pr-source-branch\n\t\t$ git wipe\n\nIn this OSS bonus round, I’m making some changes to an open PR against a repo where I am a maintainer. Since PRs grant edit permissions to maintainers, I am able to add the fork of the PR author, make changes, push to their PR branch, and then remove all remotes other than my own and the upstream with `wipe`.\n\n### Daily work\n\n\t\t$ j codebase\n\t\t$ git rup\n\t\t$ git co latest\n\t\t$ git ff\n\t\t$ git cleanup\n\t\t$ git co -b indirect/something\n\t\t# make some commits here\n\t\t$ gp\n\t\t# notice a typo, fix it\n\t\t$ git add .\n\t\t$ git fixup\n\t\t$ gp -f\n\t\t# come back the next day\n\t\t$ git rup\n\t\t$ git co master\n\t\t$ git ff\n\t\t$ git co -\n\t\t$ git rebase master\n\t\t$ gp\n\t\t$ hub browse\n\t\t# use a browser to make a pull request\n\n### Other git aliases\n\nThere are also a lot of shortcuts for more... esoteric... usecases. Let's look at some of those, too.\n\n`git sha` prints the SHA of the HEAD commit.\n\n`git cpsha` copies the SHA of the HEAD commit to the clipboard, so I can paste it somewhere else.\n\n`git burn` deletes the most recent commit. It’s an alias to `git reset --hard HEAD^`.\n\n`git nuke` removes every file that isn’t tracked by git. It’s an alias for `git reset --hard HEAD`.\n\n`git nuke-all` removes every file and every directory that isn’t tracked by git. It’s an alias for `git reset --hard HEAD && git clean -fd`.\n\n`git ls` combines a one-line git log format with verification of git commit signatures. The output shows sha, relative time, author, and commit subject, as well as a color-coded letter indicating whether a signature is valid, invalid, unknown, or missing.\n\n![example of git ls](https://indirect.micro.blog/uploads/2025/66848525e0.jpg)\n\n### Other bash aliases\n\nI also have a lot of Bash aliases, although most of them are shortcuts to common git commands.\n\n    alias gb=\"git branch -v\"\n    alias gba=\"git branch -va --color | grep -v 'remotes/origin/pr'\"\n    alias gbl=\"git branch -vv --color | grep -v '\\[.*\\/.*\\] '\"\n    alias gbr=\"git branch -vr --color | grep -v 'origin/HEAD'\"\n    alias gcp=\"git cherry-pick\"\n    alias gd=\"git diff\"\n    alias gds=\"git diff --cached\"\n    alias gdw=\"git diff --word-diff\"\n    alias gf=\"git fetch --all\"\n    alias gi=\"git commit -v\"\n    alias gia=\"git commit --amend -v\"\n    alias gl=\"git lg\"\n    alias gs=\"git status -sb\"\n\n    function gc {\n      local repo=${1#*github.com/}\n      repo=${repo%.git}\n      hub clone --recursive \"$repo\" \"$HOME/src/$repo\"\n      cd \"$HOME/src/$repo\"\n    }\n\n    function current_branch {\n      status=$(git status 2> /dev/null)\n\n      # Bail if status failed, not a git repo\n      if [[ $? -ne 0 ]]; then return 1; fi\n\n      # Try to get the branch from the status we already have\n      if [[ $status =~ \"# On branch (.*?) \" ]]; then\n        name=\"${BASH_REMATCH[1]}\"\n      fi\n\n      # Check the output of `branch` next\n      if [[ -z \"$name\" ]]; then\n        name=$(git branch | grep '^*' | cut -b3- | grep -v '^(')\n      fi\n\n      # Fall back on name-rev\n      if [[ -z \"$name\" ]]; then\n        name=$(git name-rev --name-only --no-undefined --always HEAD)\n        name=\"${name#tags/}\"\n        name=\"${name#remotes/}\"\n      fi\n\n      echo \"$name\"\n    }\n\n    function gp {\n      local current_branch=$(current_branch)\n      local upstream=$(git config branch.$current_branch.remote)\n\n      if [[ -z \"$upstream\" ]]; then\n        if [[ \"$1\" == \"-f\" ]]; then\n          local options=\"-uf\"\n          local remote=\"${2-origin}\"\n        else\n          local remote=\"${1-origin}\"\n        fi\n\n        git push \"${options--u}\" \"$remote\" \"$current_branch\"\n      else\n        git push \"$@\"\n      fi\n    }\n\n    function mcd {\n      mkdir -p \"$@\"\n      cd \"$@\"\n    }\n\nI use `gc` and `gp` a lot in my day to day work.\n\nThe `current_branch` function does something that I’ve never seen anywhere else. Not only does it show the branch name, if you are in one, if you check out a commit that doesn’t have its own branch it will show you where you are relative to the nearest named branch or tag. For example, if you check out `branchname~6`, my git status line will show `branchname~6`. Every other status line that I’ve seen falls back to a generic 8-character SHA when you check out a commit that isn’t at the tip of a branch, which is (IMO) wayyyy less useful.\n\n### Other git config\n\nI don't actually know how to use git without these things turned on.\n\n\t\t[rerere]\n\t\t\tenabled = true\n\t\t[merge]\n\t\t\tconflictstyle = diff3\n\t\t[rebase]\n\t\t\tautoStash = true\n\t\t[diff]\n\t\t\talgorithm = patience\n\nThe `rerere` option means that you can resolve a rebase conflict one time, and have that resolution applied anytime you hit the same rebase conflict in the future.\n\nThe `diff3` conflict style means that when there’s a merge conflict, not only do you see the other commit, and your commit, you also see _the original content before either commit_. Those lines are often invaluable to me when I’m trying to figure out what the other person changed, what I changed, and how to combine them.\n\nThe `autoStash` option just means that you can rebase without committing everything first—the rebase command stashes before running, and pops after running. It’s great.\n\nFinally, the `patience` algorithm optimizes diffs to reduce the (so, so common!) issue where adding a function creates a diff partly in the previous function and partly in the next function. With `patience` turned on, the diff will show just the new function, where you added it.\n\n### Conclusion\n\nThat about wraps things up! These functions and aliases definitely aren’t something that I would directly recommend to anyone else, but hopefully seeing the kinds of things that I find useful has given you some inspiration for your own shortcuts. Hpy hkng!\n",
				"date_published": "2019-01-20T00:00:00-08:00",
				"url": "https://andre.arko.net/2019/01/20/git-in-as-fw-chrs/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2019/01/16/secure-passwords-without-punishing-rules/",
				"title": "Secure Passwords Without Punishing Rules",
				"content_html": "<p>Building secure web applications is really, really hard. One of the biggest attack vectors in modern webapps is passwords. Even if we set aside the dangers of phishing or other more sophisticated attacks, passwords themselves are a source of danger, between simple passwords, guessable passwords, shared passwords among family members or teammates, and reused passwords across accounts.</p>\n<p>Security teams have traditionally responded to the dangers inherent in passwords by imposing onerous rules and requirements: perhaps your password has to be between 6 and 15 characters, must contain at least one number, at least one uppercase letter, at least one lowercase letter, and at least one symbol. (Even worse, there&rsquo;s usually a secret list of forbidden symbols they won&rsquo;t show you until you try to use one, which happens to me all the time.)</p>\n<p>In higher security environments like workplaces or banking, it’s very common to make things even worse by expiring passwords every 90 (or even every 30!) days, as well as tracking previous passwords to ensure that previous passwords are never used again.</p>\n<p>Unfortunately, all the research we have about password policies indicates that they don’t help with security. At all.</p>\n<p>Punishing users with harsh requirements most commonly results in a sticky note underneath the keyboard or even stuck to the side of the monitor with the latest password written on it for anyone to see—which completely defeats the point of passwords in the first place.</p>\n<p>Instead of brutal password requirements that defeat their own purpose, follow the evidence-based <a href=\"https://pages.nist.gov/800-63-3/sp800-63b.html#sec5\">guidelines issued by the National Institute of Standards and Technology</a>. The full document isn&rsquo;t hard to understand, and their recommendations are clear:</p>\n<ul>\n<li>Never require special characters, including upper, lower, digit, or symbol</li>\n<li>Never prohibit certain characters</li>\n<li>Never automatically expire passwords</li>\n<li>Never allow passwords that have previously been exposed in a data breach</li>\n</ul>\n<p>In the digital wastelands of 2018, there have been so many data breaches that it is that last requirement that truly keeps your users&rsquo; accounts safe and secure. Using a password that has been leaked in a previous breach means that it will be easy to guess or brute force, because it&rsquo;s already out there in lists of passwords to try.</p>\n<p>Simply blocking passwords that are known to have been leaked means that your users will have the highest possible protection a password can offer: only a keylogger or a brute force attack from scratch can break into their account.</p>\n<p>With inspiration from <a href=\"https://github.com/philnash/pwned\">the <code>pwned</code> gem</a>, <a href=\"https://github.com/codahale/passpol\">the Java library <code>passpol</code></a>, and <a href=\"https://github.com/cry/nbp\">the Javascript library <code>nbp</code></a>, we&rsquo;ve created a library you can use to follow the NIST password guidelines (and keep your accounts safe) without punishing your users with impossible password guidelines: <a href=\"https://github.com/indirect/unpwn\"><code>unpwn</code></a>.</p>\n<p>The <code>unpwn</code> gem takes a hybrid approach to validating passwords. First, it checks the proposed password against the top one million most common passwords extremely quickly, and with no network requests, by using a bloom filter.</p>\n<p><a href=\"https://llimllib.github.io/bloomfilter-tutorial/\">Bloom filters</a> are both very cool and an extremely good fit for this particular problem. We want to know if the proposed password is included in the top one million leaked passwords, but that list is almost 100mb and checking passwords against it would take a long time. The bloom filter included with this gem is only 1.7mb, but allows us to check passwords as if we had the entire top one million list available locally.</p>\n<p>If the proposed password passes the bloom filter check, the gem then uses the <code>pwned</code> gem to make a call to the <code>haveibeenpwned</code> API.</p>\n<p>The <code>haveibeenpwned</code> API offers the most comprehensive public database of leaked passwords in existence, and the API is very clever. Your application doesn&rsquo;t send the possible password to the API. Instead, it hashes the password and sends just the first few characters of the hash. The API returns all of the known password hashes that start with those characters, and your application can then check to see if the proposed password is one that has already been hacked in the past.</p>\n<p><a href=\"https://github.com/indirect/unpwn/\">Give <code>unpwn</code> a try</a> today, and keep your users safe without punishing password rules. We plan to add direct integration with <a href=\"https://github.com/plataformatec/devise\">Devise</a> in the future. If you try it out, or you have ideas for how to improve <code>unpwn</code>, we&rsquo;d love to <a href=\"https://github.com/indirect/unpwn/issues/new\">hear from you</a>!</p>\n<small>\nThis was cross-posted from [the Cloud City blog](https://www.cloudcity.io/blog/2019/01/08/secure-passwords-without-punishing-users/), where we offer this kind of advice and expertise as software consultants. [Contact us](https://www.cloudcity.io/ruby-development/) to learn more about what we can do for your team.\n</small>\n",
				"content_text": "\nBuilding secure web applications is really, really hard. One of the biggest attack vectors in modern webapps is passwords. Even if we set aside the dangers of phishing or other more sophisticated attacks, passwords themselves are a source of danger, between simple passwords, guessable passwords, shared passwords among family members or teammates, and reused passwords across accounts.\n\nSecurity teams have traditionally responded to the dangers inherent in passwords by imposing onerous rules and requirements: perhaps your password has to be between 6 and 15 characters, must contain at least one number, at least one uppercase letter, at least one lowercase letter, and at least one symbol. (Even worse, there's usually a secret list of forbidden symbols they won't show you until you try to use one, which happens to me all the time.)\n\nIn higher security environments like workplaces or banking, it’s very common to make things even worse by expiring passwords every 90 (or even every 30!) days, as well as tracking previous passwords to ensure that previous passwords are never used again.\n\nUnfortunately, all the research we have about password policies indicates that they don’t help with security. At all.\n\nPunishing users with harsh requirements most commonly results in a sticky note underneath the keyboard or even stuck to the side of the monitor with the latest password written on it for anyone to see—which completely defeats the point of passwords in the first place.\n\nInstead of brutal password requirements that defeat their own purpose, follow the evidence-based [guidelines issued by the National Institute of Standards and Technology](https://pages.nist.gov/800-63-3/sp800-63b.html#sec5). The full document isn't hard to understand, and their recommendations are clear:\n\n- Never require special characters, including upper, lower, digit, or symbol\n- Never prohibit certain characters\n- Never automatically expire passwords\n- Never allow passwords that have previously been exposed in a data breach\n\nIn the digital wastelands of 2018, there have been so many data breaches that it is that last requirement that truly keeps your users' accounts safe and secure. Using a password that has been leaked in a previous breach means that it will be easy to guess or brute force, because it's already out there in lists of passwords to try.\n\nSimply blocking passwords that are known to have been leaked means that your users will have the highest possible protection a password can offer: only a keylogger or a brute force attack from scratch can break into their account.\n\nWith inspiration from [the `pwned` gem](https://github.com/philnash/pwned), [the Java library `passpol`](https://github.com/codahale/passpol), and [the Javascript library `nbp`](https://github.com/cry/nbp), we've created a library you can use to follow the NIST password guidelines (and keep your accounts safe) without punishing your users with impossible password guidelines: [`unpwn`](https://github.com/indirect/unpwn). \n\nThe `unpwn` gem takes a hybrid approach to validating passwords. First, it checks the proposed password against the top one million most common passwords extremely quickly, and with no network requests, by using a bloom filter.\n\n[Bloom filters](https://llimllib.github.io/bloomfilter-tutorial/) are both very cool and an extremely good fit for this particular problem. We want to know if the proposed password is included in the top one million leaked passwords, but that list is almost 100mb and checking passwords against it would take a long time. The bloom filter included with this gem is only 1.7mb, but allows us to check passwords as if we had the entire top one million list available locally.\n\nIf the proposed password passes the bloom filter check, the gem then uses the `pwned` gem to make a call to the `haveibeenpwned` API.\n\nThe `haveibeenpwned` API offers the most comprehensive public database of leaked passwords in existence, and the API is very clever. Your application doesn't send the possible password to the API. Instead, it hashes the password and sends just the first few characters of the hash. The API returns all of the known password hashes that start with those characters, and your application can then check to see if the proposed password is one that has already been hacked in the past.\n\n[Give `unpwn` a try](https://github.com/indirect/unpwn/) today, and keep your users safe without punishing password rules. We plan to add direct integration with [Devise](https://github.com/plataformatec/devise) in the future. If you try it out, or you have ideas for how to improve `unpwn`, we'd love to [hear from you](https://github.com/indirect/unpwn/issues/new)!\n\n<small>\nThis was cross-posted from [the Cloud City blog](https://www.cloudcity.io/blog/2019/01/08/secure-passwords-without-punishing-users/), where we offer this kind of advice and expertise as software consultants. [Contact us](https://www.cloudcity.io/ruby-development/) to learn more about what we can do for your team.\n</small>\n",
				"date_published": "2019-01-16T00:00:00-08:00",
				"url": "https://andre.arko.net/2019/01/16/secure-passwords-without-punishing-rules/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2019/01/11/parsing-logs-faster-with-rust/",
				"title": "Parsing logs faster with Rust, continued",
				"content_html": "<p>Previously, I <a href=\"/2018/10/25/parsing-logs-230x-faster-with-rust/\">wrote about parsing logs 230x faster using Rust</a>. Since publishing that post, I&rsquo;ve discovered some new information! Here&rsquo;s what I&rsquo;ve learned.</p>\n<p>First, several people argued that that it is unfair to use Python inside Apache Spark as the base case. Either Python or Ruby by itself is capable of processing logs much faster than Python scripts that have to go through the Spark APIs while running in a Hadoop cluster. That&rsquo;s&hellip; kind of true. Python or Ruby by itself can manage much more than 525 records/second/cpu. For me, the problem was that I had too many records to process on a single CPU, and needed an automatic way to parallelize the work. AWS Glue was the first thing I stumbled across, so I tried using it.</p>\n<p>The next thing I discovered is that I was benchmarking the Rust code completely wrong. Running <code>time cargo run --release</code> always invokes the Cargo compilation cycle, which takes a lot of time. Instead, I should have been doing something like <code>cargo build --release &amp;&amp; time ./kirby</code>. I&rsquo;ve resolved that issue by writing <a href=\"https://github.com/rubytogether/kirby/blob/main/bin/bench\">a script to benchmark kirby commits</a> using the fantastic benchmarking tool (also written in Rust) called <a href=\"https://github.com/sharkdp/hyperfine\">Hyperfine</a>. After removing the overhead from Cargo, I discovered I was wildly underselling how fast the Rust log parser was.</p>\n<p>Finally, random people on the internet made a bunch of suggestions after my original post. Most of the suggestions were not super clear wins based on benchmarking, but when all of them were combined the overall result was definitely faster. The two biggest changes were <a href=\"https://github.com/rubytogether/kirby/pull/6\">using CoW strings with Serde</a> and <a href=\"https://github.com/rubytogether/kirby/pull/4\">reducing backtracking inside the regex</a>.</p>\n<p>After combining all of the suggestions, the final result was a pretty dramatic improvement. The <a href=\"https://github.com/rubytogether/kirby/commit/2cabdd4cad0038d1bdbb029bf4ded689cfa4e8c2\">previous code</a> processed the example log file in 4.001 seconds, but the <a href=\"https://github.com/rubytogether/kirby/commit/1571ff116c4920bea596186b3f1cbbb397af548e\">latest commit</a> takes just 2.875 seconds. That&rsquo;s a 28% improvement!</p>\n<p>After removing the call to Cargo, and applying all of the suggested optimizations, the result is a bit more stark than it was last time:</p>\n<pre tabindex=\"0\"><code>   ~525 records/second/cpu in Python on Apache Spark in AWS Glue\n 13,760 records/second/cpu in Ruby\n353,374 records/second/cpu in Rust\n</code></pre><p>That&rsquo;s a total of 673x faster than the AWS Glue script, and still 25.7x faster than a single-threaded Ruby script. (And the Ruby script is using a JSON library and a Regex library both written in C!).</p>\n<p>I&rsquo;ve been really impressed with the Rust community, providing not just helpful suggestions but even sending PRs with benchmarks showing the exact improvements for their changes. If you think you can improve things even more, I&rsquo;d <a href=\"https://github.com/rubytogether/kirby/issues/new\">love to hear from you</a>.</p>\n",
				"content_text": "\nPreviously, I [wrote about parsing logs 230x faster using Rust](/2018/10/25/parsing-logs-230x-faster-with-rust/). Since publishing that post, I've discovered some new information! Here's what I've learned.\n\nFirst, several people argued that that it is unfair to use Python inside Apache Spark as the base case. Either Python or Ruby by itself is capable of processing logs much faster than Python scripts that have to go through the Spark APIs while running in a Hadoop cluster. That's... kind of true. Python or Ruby by itself can manage much more than 525 records/second/cpu. For me, the problem was that I had too many records to process on a single CPU, and needed an automatic way to parallelize the work. AWS Glue was the first thing I stumbled across, so I tried using it.\n\nThe next thing I discovered is that I was benchmarking the Rust code completely wrong. Running `time cargo run --release` always invokes the Cargo compilation cycle, which takes a lot of time. Instead, I should have been doing something like `cargo build --release && time ./kirby`. I've resolved that issue by writing [a script to benchmark kirby commits](https://github.com/rubytogether/kirby/blob/main/bin/bench) using the fantastic benchmarking tool (also written in Rust) called [Hyperfine](https://github.com/sharkdp/hyperfine). After removing the overhead from Cargo, I discovered I was wildly underselling how fast the Rust log parser was.\n\nFinally, random people on the internet made a bunch of suggestions after my original post. Most of the suggestions were not super clear wins based on benchmarking, but when all of them were combined the overall result was definitely faster. The two biggest changes were [using CoW strings with Serde](https://github.com/rubytogether/kirby/pull/6) and [reducing backtracking inside the regex](https://github.com/rubytogether/kirby/pull/4).\n\nAfter combining all of the suggestions, the final result was a pretty dramatic improvement. The [previous code](https://github.com/rubytogether/kirby/commit/2cabdd4cad0038d1bdbb029bf4ded689cfa4e8c2) processed the example log file in 4.001 seconds, but the [latest commit](https://github.com/rubytogether/kirby/commit/1571ff116c4920bea596186b3f1cbbb397af548e) takes just 2.875 seconds. That's a 28% improvement!\n\nAfter removing the call to Cargo, and applying all of the suggested optimizations, the result is a bit more stark than it was last time:\n\n```\n   ~525 records/second/cpu in Python on Apache Spark in AWS Glue\n 13,760 records/second/cpu in Ruby\n353,374 records/second/cpu in Rust\n```\n\nThat's a total of 673x faster than the AWS Glue script, and still 25.7x faster than a single-threaded Ruby script. (And the Ruby script is using a JSON library and a Regex library both written in C!).\n\nI've been really impressed with the Rust community, providing not just helpful suggestions but even sending PRs with benchmarks showing the exact improvements for their changes. If you think you can improve things even more, I'd [love to hear from you](https://github.com/rubytogether/kirby/issues/new).\n",
				"date_published": "2019-01-11T00:00:00-08:00",
				"url": "https://andre.arko.net/2019/01/11/parsing-logs-faster-with-rust/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2018/10/25/parsing-logs-x-faster-with/",
				"title": "Parsing logs 230x faster with Rust",
				"content_html": "<p>Perhaps surprisingly, one of the most challenging things about operating <a href=\"https://rubygems.org\">RubyGems.org</a> is the logs. Unlike most Rails applications, RubyGems sees between 4,000 and 25,000 requests per second, all day long, every single day. As you can probably imagine, this creates&hellip; a lot of logs. A single day of request logs is usually around 500 gigabytes on disk. We&rsquo;ve tried some hosted logging products, but at our volume they can typically only offer us a retention measured in hours.</p>\n<p>About a year ago, the only thing I could think of to do with the full log firehose was to run it through <code>gzip -9</code> and then drop it in S3. With gzip, the files shrink by about 92%, and with S3&rsquo;s &ldquo;infrequent access&rdquo; and &ldquo;less duplication&rdquo; tiers, it&rsquo;s actually affordable to keep those logs in a bucket: each month worth of logs costs about $3.50 per month to store.</p>\n<p>Buried in those logs, there are a bunch of stats that I&rsquo;m super interested in: what versions of Ruby and Bundler are actively using RubyGems.org, or per-version and per-day gem download counts. Unfortunately, gzipped JSON streams in S3 are super hard to query for data.</p>\n<h3 id=\"is-this-big-data\">is this&hellip; big data?</h3>\n<p>So every day, we generate about 500 files that are 85MB on disk, and contain about a million streaming JSON objects that take up 1GB when uncompressed. What we want out of those files is incredibly tiny—a few thousand integers, labelled with names and version numbers. For example, &ldquo;2018-10-25 Bundler 1.16.2 123456&rdquo;, or &ldquo;2018-10-25 platform x86_64-darwin17 9876&rdquo;.</p>\n<p>With a full set of those counts, we would be able provide seriously useful information about the state of the whole Ruby ecosystem. It would help gem authors to know what versions of Ruby are important to support, and help everyone using Ruby to know whether or not they are upgrading in pace with the majority of other Ruby devs.</p>\n<h3 id=\"the-slow-way\">the slow way</h3>\n<p>Without any real idea of how to get those counts out of S3, I started by writing a proof of concept Ruby script that could parse one of the 500 log files and print out stats from it. It proved that the logs did contain the data I wanted, but it also took a <em>really long time</em>. Even on my super-fast laptop, my prototype script would take more than 16 hours to parse 24 hours worth of logs.</p>\n<p>If I was going to make this work, I would need to figure out some way to massively parallelize the work. After setting it aside for a while, I noticed that AWS had just announced <a href=\"https://aws.amazon.com/glue/\">Glue</a>, their managed Hadoop cluster that runs Apache Spark scripts.</p>\n<h3 id=\"python-and-glue\">python and glue</h3>\n<p>Starting from zero experience with Glue, Hadoop, or Spark, I was able to rewrite my Ruby prototype and extend it to collect more complete statistics in Python for Spark, running directly against the S3 bucket of logs. With 100 parallel workers, it took 3 wall-clock hours to parse a full day worth of logs and consolidate the results.</p>\n<p>While 3 realtime hours is pretty great, my script must have been very bad, because it was using 300 cpu-hours per day of logs, an average of 36 minutes per log file. That worked out to almost $1,000 per month, which was too much for Glue to work as a permanent solution.</p>\n<h3 id=\"maybe-rust\">maybe rust?</h3>\n<p>After shelving the problem again, I thought of it while idly wondering if there was anything that I&rsquo;d like to use <a href=\"https://www.rust-lang.org/en-US/\">Rust</a> for. I&rsquo;d heard good things about <a href=\"https://github.com/serde-rs/json-benchmark#-cargo-run---release---bin-json-benchmark\">fast JSON</a> and <a href=\"https://blog.burntsushi.net/ripgrep/\">fast text search</a> in Rust, so it seemed like it might be a good fit.</p>\n<p>It turns out <a href=\"https://serde.rs\"><code>serde</code></a>, the Rust JSON library, is  super fast. It tries very hard to not allocate, and it can deserialize the (uncompressed) 1GB of JSON into Rust structs in 2 seconds flat.</p>\n<p>Impressed by how fast Rust was at JSON, I searched for &ldquo;rust parsing&rdquo; and found <a href=\"https://github.com/Geal/nom\"><code>nom</code></a>, a parser combinator library. After a few nights of work, I had a working parser combinator that did what I wanted, and I used it to parse the same log files. Excitingly, it could parse a 1GB logfile in just 3 minutes, which felt like a huge win coming from ~30 minutes in Python on Glue.</p>\n<p>While wondering if there was a way to make it faster, I started re-reading the <code>nom</code> docs carefully, and that&rsquo;s when I noticed that &ldquo;sometimes, <code>nom</code> can be almost as fast as <code>regex</code>&rdquo;. 🤦🏻‍♂️ Feeling pretty silly, I went and rewrote my rust program to use the <a href=\"https://github.com/rust-lang/regex#regex\"><code>regex</code></a> crate, and sure enough it got 3x faster. Down to 60 seconds per file, or 30x as fast as Python in Spark in Glue. Even 2x faster than the Ruby prototype! (Though that comparison isn&rsquo;t very fair because the Python and Rust versions collect more data.)</p>\n<p>At that point, I excitedly shared how fast my Rust version was with <a href=\"https://twitter.com/reinh\">@reinh</a>&hellip; and his response was &ldquo;WHY IS IT SO SLOW YOU MUST PROFILE IT&rdquo;. I&rsquo;m still not sure how much of that was a joke, since it was already 30x faster than my last version. But I was curious, so I started looking into how to profile programs in Rust.</p>\n<h3 id=\"release-mode\">release mode</h3>\n<p>The first thing I learned about profiling programs in Rust is that you have to do it with compiler optimizations turned on. Which I was not doing. 🤯 Rerunning the exact same Rust program while passing the <code>--release</code> flag to <code>cargo</code> turned on compiler optimizations, and suddenly I could parse a 1GB log file in&hellip; <strong>8 seconds</strong>.</p>\n<p>So, to recap, here&rsquo;s a table of processing speeds:</p>\n<pre tabindex=\"0\"><code>   ~525 records/second/cpu in Python on AWS Glue\n 50,534 records/second/cpu in Rust with nom\n121,153 records/second/cpu in Rust with regex\n</code></pre><h3 id=\"thanks-rayon-thayon\">thanks, rayon. thayon.</h3>\n<p>At that point, I remembered that Rust also has a <a href=\"https://github.com/rayon-rs/rayon\">parallel iteration library, Rayon</a>. With a 5 character change to my program, Rayon ran the program against multiple log files at the same time. I was able to use all 8 cores on my laptop, and go even faster:</p>\n<pre tabindex=\"0\"><code> ~4,200 records/second in Python with 8 worker instances on AWS Glue\n399,300 records/second in Rust with 8 cores and rayon on a MacBook Pro\n</code></pre><p>While workers on Glue seem to scale linearly, that definitely wasn&rsquo;t the case on my laptop. Even with 8x the cores, I only got a 3.3x speedup. It&rsquo;s not a super fair comparison since the code is running on different machines, but it&rsquo;s 100x faster with 8 cores, and 230x faster on one core.</p>\n<p>I didn&rsquo;t include it in the table above, beacuse it&rsquo;s sort of cheating, but I was able to go even faster than that. By leaving some JSON fields out of the Rust struct, and skipping some JSON objects if I could tell they had duplicate information, I was able to get the runtime for a 1GB log file down to 6.4 seconds. That&rsquo;s 151,441 records/second/cpu, or 288x faster.</p>\n<h3 id=\"back-to-aws\">back to aws</h3>\n<p>After rewriting my parser in Rust, I had a new problem: how do I deploy this thing? My first idea (which probably would have worked?) was to cross-compile Rust binaries for Heroku and make <a href=\"https://sidekiq.org\">Sidekiq</a> run the binary once for each new log file in S3.</p>\n<p>Fortunately, before I tried to actually do that, I discovered <a href=\"https://github.com/srijs/rust-aws-lambda\"><code>rust-aws-lambda</code></a>, a crate that lets your Rust program run on AWS Lambda by pretending to be a Go binary. As a nice bonus for my usecase, it&rsquo;s only a few clicks to have AWS run a Lambda as a callback every time a new file is added to an S3 bucket.</p>\n<p>Between <code>rust-aws-lambda</code> and  <code>docker-lambda</code>, I was able to port my parser to accept an AWS S3 Event, and output a few lines of JSON with counters in them. From there, I can read those tiny files out of S3 and import the counts into a database. With Rust in Lambda, each 1GB file takes about 23 seconds to download and parse. That&rsquo;s about a 78x speedup compared to each Python Glue worker.</p>\n<h3 id=\"wait-_how_-much\">wait, <em>how</em> much?</h3>\n<p>As fantastic gravy on top of this whole situation, after a few days I realized that I needed to know exactly how much it would cost. With each log file taking about 23 seconds, and there being about 500 log files per day, it seemed like I would need about 350,000 seconds of Lambda execution time per month.</p>\n<p>Then, when I went to look up Lambda pricing, I noticed that it has a free tier: 400,000 seconds per month. So in the end, it seems like I&rsquo;m parsing 500GB of logs per day&hellip; for free. 😆</p>\n<p>If you want to read the code, or better yet send me pull requests making it even faster, it lives on GitHub at <a href=\"https://github.com/rubytogether/kirby\">rubytogether/kirby</a>.</p>\n<p><small>Thanks to Steve Klabnik, Ashley Williams, without boats, Rein Henrichs, Coda Hale, Nelson Minar, Chris Dary, Sunah Suh, Tim Kordas, and Larry Marburger for feedback and encouragement to turn our conversations into a post.</small></p>\n",
				"content_text": "\nPerhaps surprisingly, one of the most challenging things about operating [RubyGems.org](https://rubygems.org) is the logs. Unlike most Rails applications, RubyGems sees between 4,000 and 25,000 requests per second, all day long, every single day. As you can probably imagine, this creates... a lot of logs. A single day of request logs is usually around 500 gigabytes on disk. We've tried some hosted logging products, but at our volume they can typically only offer us a retention measured in hours.\n\nAbout a year ago, the only thing I could think of to do with the full log firehose was to run it through `gzip -9` and then drop it in S3. With gzip, the files shrink by about 92%, and with S3's \"infrequent access\" and \"less duplication\" tiers, it's actually affordable to keep those logs in a bucket: each month worth of logs costs about $3.50 per month to store.\n\nBuried in those logs, there are a bunch of stats that I'm super interested in: what versions of Ruby and Bundler are actively using RubyGems.org, or per-version and per-day gem download counts. Unfortunately, gzipped JSON streams in S3 are super hard to query for data.\n\n### is this... big data?\n\nSo every day, we generate about 500 files that are 85MB on disk, and contain about a million streaming JSON objects that take up 1GB when uncompressed. What we want out of those files is incredibly tiny—a few thousand integers, labelled with names and version numbers. For example, \"2018-10-25 Bundler 1.16.2 123456\", or \"2018-10-25 platform x86\\_64-darwin17 9876\".\n\nWith a full set of those counts, we would be able provide seriously useful information about the state of the whole Ruby ecosystem. It would help gem authors to know what versions of Ruby are important to support, and help everyone using Ruby to know whether or not they are upgrading in pace with the majority of other Ruby devs.\n\n### the slow way\n\nWithout any real idea of how to get those counts out of S3, I started by writing a proof of concept Ruby script that could parse one of the 500 log files and print out stats from it. It proved that the logs did contain the data I wanted, but it also took a _really long time_. Even on my super-fast laptop, my prototype script would take more than 16 hours to parse 24 hours worth of logs.\n\nIf I was going to make this work, I would need to figure out some way to massively parallelize the work. After setting it aside for a while, I noticed that AWS had just announced [Glue](https://aws.amazon.com/glue/), their managed Hadoop cluster that runs Apache Spark scripts.\n\n### python and glue\n\nStarting from zero experience with Glue, Hadoop, or Spark, I was able to rewrite my Ruby prototype and extend it to collect more complete statistics in Python for Spark, running directly against the S3 bucket of logs. With 100 parallel workers, it took 3 wall-clock hours to parse a full day worth of logs and consolidate the results.\n\nWhile 3 realtime hours is pretty great, my script must have been very bad, because it was using 300 cpu-hours per day of logs, an average of 36 minutes per log file. That worked out to almost $1,000 per month, which was too much for Glue to work as a permanent solution.\n\n### maybe rust?\n\nAfter shelving the problem again, I thought of it while idly wondering if there was anything that I'd like to use [Rust](https://www.rust-lang.org/en-US/) for. I'd heard good things about [fast JSON](https://github.com/serde-rs/json-benchmark#-cargo-run---release---bin-json-benchmark) and [fast text search](https://blog.burntsushi.net/ripgrep/) in Rust, so it seemed like it might be a good fit.\n\nIt turns out [`serde`](https://serde.rs), the Rust JSON library, is  super fast. It tries very hard to not allocate, and it can deserialize the (uncompressed) 1GB of JSON into Rust structs in 2 seconds flat.\n\nImpressed by how fast Rust was at JSON, I searched for \"rust parsing\" and found [`nom`](https://github.com/Geal/nom), a parser combinator library. After a few nights of work, I had a working parser combinator that did what I wanted, and I used it to parse the same log files. Excitingly, it could parse a 1GB logfile in just 3 minutes, which felt like a huge win coming from ~30 minutes in Python on Glue.\n\nWhile wondering if there was a way to make it faster, I started re-reading the `nom` docs carefully, and that's when I noticed that \"sometimes, `nom` can be almost as fast as `regex`\". 🤦🏻‍♂️ Feeling pretty silly, I went and rewrote my rust program to use the [`regex`](https://github.com/rust-lang/regex#regex) crate, and sure enough it got 3x faster. Down to 60 seconds per file, or 30x as fast as Python in Spark in Glue. Even 2x faster than the Ruby prototype! (Though that comparison isn't very fair because the Python and Rust versions collect more data.)\n\nAt that point, I excitedly shared how fast my Rust version was with [@reinh](https://twitter.com/reinh)... and his response was \"WHY IS IT SO SLOW YOU MUST PROFILE IT\". I'm still not sure how much of that was a joke, since it was already 30x faster than my last version. But I was curious, so I started looking into how to profile programs in Rust.\n\n### release mode\n\nThe first thing I learned about profiling programs in Rust is that you have to do it with compiler optimizations turned on. Which I was not doing. 🤯 Rerunning the exact same Rust program while passing the `--release` flag to `cargo` turned on compiler optimizations, and suddenly I could parse a 1GB log file in... **8 seconds**.\n\n\nSo, to recap, here's a table of processing speeds:\n\n```\n   ~525 records/second/cpu in Python on AWS Glue\n 50,534 records/second/cpu in Rust with nom\n121,153 records/second/cpu in Rust with regex\n```\n\n### thanks, rayon. thayon.\n\nAt that point, I remembered that Rust also has a [parallel iteration library, Rayon](https://github.com/rayon-rs/rayon). With a 5 character change to my program, Rayon ran the program against multiple log files at the same time. I was able to use all 8 cores on my laptop, and go even faster:\n\n```\n ~4,200 records/second in Python with 8 worker instances on AWS Glue\n399,300 records/second in Rust with 8 cores and rayon on a MacBook Pro\n```\n\nWhile workers on Glue seem to scale linearly, that definitely wasn't the case on my laptop. Even with 8x the cores, I only got a 3.3x speedup. It's not a super fair comparison since the code is running on different machines, but it's 100x faster with 8 cores, and 230x faster on one core.\n\nI didn't include it in the table above, beacuse it's sort of cheating, but I was able to go even faster than that. By leaving some JSON fields out of the Rust struct, and skipping some JSON objects if I could tell they had duplicate information, I was able to get the runtime for a 1GB log file down to 6.4 seconds. That's 151,441 records/second/cpu, or 288x faster.\n\n### back to aws\n\nAfter rewriting my parser in Rust, I had a new problem: how do I deploy this thing? My first idea (which probably would have worked?) was to cross-compile Rust binaries for Heroku and make [Sidekiq](https://sidekiq.org) run the binary once for each new log file in S3.\n\nFortunately, before I tried to actually do that, I discovered [`rust-aws-lambda`](https://github.com/srijs/rust-aws-lambda), a crate that lets your Rust program run on AWS Lambda by pretending to be a Go binary. As a nice bonus for my usecase, it's only a few clicks to have AWS run a Lambda as a callback every time a new file is added to an S3 bucket.\n\nBetween `rust-aws-lambda` and  `docker-lambda`, I was able to port my parser to accept an AWS S3 Event, and output a few lines of JSON with counters in them. From there, I can read those tiny files out of S3 and import the counts into a database. With Rust in Lambda, each 1GB file takes about 23 seconds to download and parse. That's about a 78x speedup compared to each Python Glue worker.\n\n### wait, _how_ much?\n\nAs fantastic gravy on top of this whole situation, after a few days I realized that I needed to know exactly how much it would cost. With each log file taking about 23 seconds, and there being about 500 log files per day, it seemed like I would need about 350,000 seconds of Lambda execution time per month.\n\nThen, when I went to look up Lambda pricing, I noticed that it has a free tier: 400,000 seconds per month. So in the end, it seems like I'm parsing 500GB of logs per day... for free. 😆\n\nIf you want to read the code, or better yet send me pull requests making it even faster, it lives on GitHub at [rubytogether/kirby](https://github.com/rubytogether/kirby).\n\n<small>Thanks to Steve Klabnik, Ashley Williams, without boats, Rein Henrichs, Coda Hale, Nelson Minar, Chris Dary, Sunah Suh, Tim Kordas, and Larry Marburger for feedback and encouragement to turn our conversations into a post.</small>\n",
				"date_published": "2018-10-25T00:00:00-08:00",
				"url": "https://andre.arko.net/2018/10/25/parsing-logs-x-faster-with/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2018/06/03/bundler-tips-and-tricks/",
				"title": "Bundler Tips and Tricks",
				"content_html": "<p><small>This post was originally <a href=\"https://www.rubytapas.com/2018/03/27/bundler-tips-and-tricks-andre-arko/\">a guest episode of Ruby Tapas</a>, a series of regular, short screencasts about Ruby topics. If that sounds good to you, <a href=\"https://www.rubytapas.com\">try out Ruby Tapas</a>.</small></p>\n<p>As a Ruby developer, chances are really good that you already know and use <a href=\"https://bundler.io\">Bundler</a> on a daily basis, and you can <code>git pull &amp;&amp; bundle install</code> with the best of them. What you might not know is that Bundler has changed and grown over the last 8 years. The newer, lesser-known features can provide a lot of help with other gem-related workflows and problems. What other problems, you ask? Let’s take a look.</p>\n<h3 id=\"creating-and-releasing-your-own-gems\">Creating and releasing your own gems</h3>\n<p>The first thing Bundler can help with is making your own gems. It’s as easy as <code>bundle gem foobar</code>, and you end up with a new gem named <code>foobar</code> ready for you to add code.</p>\n<p>There’s a one-time setup to tell Bundler if you want <a href=\"http://rspec.info\">rspec</a> or minitest, an <a href=\"https://github.com/bundler/bundler/blob/master/lib/bundler/templates/newgem/LICENSE.txt.tt\">MIT license</a>, or a <a href=\"https://github.com/bundler/bundler/blob/master/lib/bundler/templates/newgem/CODE_OF_CONDUCT.md.tt\">code of conduct</a>. After that, you can create gem after gem in just a few seconds each.</p>\n<pre><code>$ bundle gem foobar\nCreating gem 'foobar'...\nDo you want to generate tests with your gem?\nType 'rspec' or 'minitest' to generate those test files now and in the future. rspec/minitest/(none): rspec\nDo you want to license your code permissively under the MIT license?\nThis means that any other developer or company will be legally allowed to use your code for free as long as they admit you created it. You can read more about the MIT license at http://choosealicense.com/licenses/mit. y/(n): y\nMIT License enabled in config\nDo you want to include a code of conduct in gems you generate?\nCodes of conduct can increase contributions to your project by contributors who prefer collaborative, safe spaces. You can read more about the code of conduct at contributor-covenant.org. Having a code of conduct means agreeing to the responsibility of enforcing it, so be sure that you are prepared to do that. Be sure that your email address is specified as a contact in the generated code of conduct so that people know who to contact in case of a violation. For suggestions about how to enforce codes of conduct, see http://bit.ly/coc-enforcement. y/(n): y\nCode of conduct enabled in config\n      create  foobar/Gemfile\n      create  foobar/lib/foobar.rb\n      create  foobar/lib/foobar/version.rb\n      create  foobar/foobar.gemspec\n      create  foobar/Rakefile\n      create  foobar/README.md\n      create  foobar/bin/console\n      create  foobar/bin/setup\n      create  foobar/.gitignore\n      create  foobar/.travis.yml\n      create  foobar/.rspec\n      create  foobar/spec/spec_helper.rb\n      create  foobar/spec/foobar_spec.rb\n      create  foobar/LICENSE.txt\n      create  foobar/CODE_OF_CONDUCT.md\nInitializing git repo in /Users/andre/Downloads/foobar\n</code></pre>\n<p>Any gem created by Bundler comes with a couple of nice touches: first, a <code>bin/setup</code> file that acts as a centralized, well-known location to install dependencies and do any other specific setup needed to develop on your library. By default, it creates a bash script that echoes commands, and runs <code>bundle install</code>, but it’s very easy to add your own commands.</p>\n<pre><code>$ cd foobar\n$ cat bin/setup\n#!/usr/bin/env bash\nset -euo pipefail\nIFS=$'\\n\\t'\nset -vx\n\nbundle install\n\n# Do any other automated setup that you need to do here\n</code></pre>\n<p>Every gem also includes a <code>bin/console</code>, to load your gems and then launch <a href=\"https://ruby-doc.org/stdlib-2.5.1/libdoc/irb/rdoc/IRB.html\">IRB</a>, <a href=\"http://pryrepl.org/\">Pry</a>, <a href=\"https://github.com/dnasseri/fir\">Fir</a>, or whatever interactive prompt you prefer. It’s the fastest way to experiment with the code from your gem.</p>\n<pre><code>$ cat bin/console\n#!/usr/bin/env ruby\n\nrequire &quot;bundler/setup&quot;\nrequire &quot;foobar&quot;\n\n# You can add fixtures and/or initialization code here to make experimenting\n# with your gem easier. You can also use a different console, if you like.\n\n# (If you use this, don't forget to add pry to your Gemfile!)\n# require &quot;pry&quot;\n# Pry.start\n\nrequire &quot;irb&quot;\nIRB.start(__FILE__)\n</code></pre>\n<p>Finally, every gem includes two extremely helpful <a href=\"https://ruby.github.io/rake/\">rake</a> tasks. The <code>rake install</code> will build your gem into a literal <code>.gem</code> file, and then run <code>gem install</code> to install it onto your local machine.</p>\n<pre><code>$ rake install\nfoobar 0.1.0 built to pkg/foobar-0.1.0.gem.\nfoobar (0.1.0) installed.\n$ gem list foobar\n\n*** LOCAL GEMS ***\n\nfoobar (0.1.0)\n</code></pre>\n<p>You can easily test that building, installing, and using your gem all work the way that you expect them to.</p>\n<pre><code>$ ruby -rfoobar -e 'puts Foobar::VERSION'\n0.1.0\n</code></pre>\n<p>The other extremely useful task is <code>rake release</code>, which creates and pushes a git tag for your version, builds a <code>.gem</code> file, and releases the gem on <a href=\"https://rubygems.org/\">RubyGems.org</a>! What used to be an error-prone process that could take minutes is now just a single command and a few seconds. It’s marvelous.</p>\n<pre><code>$ rake release\nfoobar 0.1.0 built to pkg/foobar-0.1.0.gem.\nTagged v0.1.0.\nPushed git commits and tags.\nPushed foobar 0.1.0 to rubygems.org\n</code></pre>\n<h3 id=\"developing-multiple-repos-at-once\">Developing multiple repos at once</h3>\n<p>Now that you have a gem, what if you need to make changes to the gem and your app that depends on it at the same time? Bundler already has a feature to make this work as smoothly as possible: <strong>Local Git Repos</strong>.</p>\n<p>To start, you tell Bundler where your local checkout of a git repo is. For example, we could continue to work on our <code>foobar</code> gem locally while using it in an application by running this configuration command in the application. Now that we’ve done that, running the application locally will use the code from our checkout. We can make changes, reload the application, and see them live.</p>\n<pre><code>$ cd app\n$ bundle config local.foobar ~/src/indirect/foobar\n$ bundle exec ruby -rfoobar -e 'puts Foobar::VERSION'\n0.1.0\n</code></pre>\n<h3 id=\"adding-gems\">Adding gems</h3>\n<p>Now that we&rsquo;re up to speed on creating and using our own gems, the next tip is about speeding up using gems that already exist. Starting with Bundler 1.15, there is an <code>add</code> command that will automatically add a gem to your Gemfile and install it.</p>\n<p>Given a gem name, Bundler will look up the gem by name, add it to your Gemfile, and then resolve and install your entire bundle.</p>\n<p>Adding new gems to your application got easier starting with Bundler 1.15–now, you can simply run <code>bundle add GEM</code> and watch as Bundler adds the gem.</p>\n<pre><code>$ bundle add rack\nFetching gem metadata from https://rubygems.org/..............\nResolving dependencies...\nFetching gem metadata from https://rubygems.org/..............\nResolving dependencies...\nUsing bundler 1.16.1\nFetching rack 2.0.4\nInstalling rack 2.0.4\n</code></pre>\n<p>Now that the command has run, we can take a look inside the Gemfile using <code>cat</code> to see the changes that Bundler made.</p>\n<pre><code>$ cat Gemfile\nsource &quot;https://rubygems.org&quot;\n\ngem &quot;rack&quot;, &quot;~&gt; 2.0&quot;\n</code></pre>\n<p>Adding gems is pretty basic so far, but we’re continuing to improve gem management from the command-line. Watch for this to keep getting better.</p>\n<h3 id=\"editing-installed-gems\">Editing installed gems</h3>\n<p>After we&rsquo;ve installed all of our gems, it&rsquo;s a common wish to want to see the code for a gem directly. Bundler makes it easy to open any installed gem directly in your editor so you can see (or even edit) that gem&rsquo;s code.</p>\n<p>When you run <code>bundle open GEM</code>, Bundler will look up the location of that gem on your machine, and then open it in your editor.</p>\n<pre><code>$ bundle open rack\n</code></pre>\n<p>The default editor is Vim, but Bundler will respect the <code>EDITOR</code> variable to open any editor you want.</p>\n<p>Once you&rsquo;ve got the gem open in your editor, you can browse the source for the gem, search for the definition of a method, and even edit that gem to change behavior or add debugging code. In this example, we&rsquo;re editing the the rack gem&rsquo;s main file, <code>rack.rb</code>. To show how this works, we&rsquo;ll change the <code>VERSION</code> constant.</p>\n<p>Any changes that you make will be picked up by the next Ruby process you run. We can see the effect of our changes by printing the VERSION constant that we just edited.</p>\n<pre><code>$ bundle exec ruby -rrack -e 'p Rack::VERSION'\n[5,0]\n</code></pre>\n<p>As you can probably imagine, being able to change your gems locally is an incredibly valuable tool for the times when it seems like the bug might be in a gem rather than in your own code.</p>\n<h3 id=\"searching-gems\">Searching gems</h3>\n<p>If you’re not yet sure which gem to open, you can do a search across exactly the gems in this particular application by using the slightly-obscure command <code>bundle show --paths</code>. Combine that command with grep, <a href=\"https://beyondgrep.com\">ack</a>, <a href=\"https://github.com/BurntSushi/ripgrep\">ripgrep</a>, or your favorite search tool to get extremely precise results.</p>\n<p>In this example, we&rsquo;re using the <a href=\"https://github.com/rubygems/rubygems.org\">Rails app that powers RubyGems.org</a>. Running <code>bundle show --paths</code> will print out the list of directories, one for each gem used by the application.</p>\n<pre><code>$ bundle show --paths\n/Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/actioncable-5.0.3\n/Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/actionmailer-5.0.3\n/Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/actionpack-5.0.3\n/Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/actionview-5.0.3\n/Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/activejob-5.0.3\n/Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/activemodel-5.0.3\n/Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/activerecord-5.0.3\n/Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/activesupport-5.0.3\n[...]\n</code></pre>\n<p>Once we have that list of paths, we can combine it with a search tool. In this example, we&rsquo;re using <code>rg</code>, which is the <code>ripgrep</code> tool. Ripgrep is a search tool similar to <code>grep</code>, but optimized for source code. Finding places in our gems where the method <code>create_or_update</code> is defined is suddenly a breeze once we have Bundler and Ripgrep working together.</p>\n<pre><code>$ rg 'def create_or_update' $(bundle show --paths)\n/Users/andre/.gem/ruby/2.3.3/gems/bundler-1.14.6/lib/gems/bundler-1.14.6: No such file or directory (os error 2)\n/Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/activerecord-5.0.3/lib/active_record/callbacks.rb\n297:    def create_or_update(*) #:nodoc:\n\n/Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/activerecord-5.0.3/lib/active_record/persistence.rb\n546:    def create_or_update(*args, &amp;block)\n</code></pre>\n<h3 id=\"bundlerinline-for-single-file-scripts\">bundler/inline for single-file scripts</h3>\n<p>We&rsquo;re starting to run out of time, but before we wrap up I want to highlight one more feature that offers developers a very powerful tool. Every tip so far has been about managing gems for an application. What about the times when an application is overkill, and you just want to write a few lines of code into a single file?</p>\n<p>Ruby was originally created for that kind of small, helpful script, and makes it very easy… until your small script starts depending on gems. Then you have to think about installing them, making sure the right version is available, and all the other thing that Bundler was invented to help with. If you have a small script that could use some gems, Bundler can help with that as well. This feature is called ‘inline Gemfiles&rsquo;, and it gives your single-file scripts superpowers.</p>\n<p>At the top of your script, require <code>bundler/inline</code>. Then, use the <code>gemfile</code> method to declare your dependencies just like you would in a standalone file. When you run the script, Bundler will jump in and make sure the gems you need are installed and loaded, and your script will always be able to run successfully.</p>\n<pre><code>$ vim script.rb\n\nrequire &quot;bundler/inline&quot;\n\ngemfile do\n  source &quot;https://rubygems.org&quot;\n  gem &quot;rack-obama&quot;\nend\n\nputs &quot;rack-obama's version is: #{Rack::Obama::VERSION}&quot;\n</code></pre>\n<p>Once we&rsquo;ve created a script that uses inline Gemfiles, just running it means Bundler will take care of everything else. Any missing gems are automatically installed, all installed gems are automatically used, and you never have to think about it. As you can see, we do not currently have the <code>rack-obama</code> gem installed on this machine.</p>\n<pre><code>$ gem list rack-obama\n\n*** LOCAL GEMS ***\n</code></pre>\n<p>Under normal circumstances, our script would fail with an error about a missing constant. Bundler is going to silently install our missing gem as part of running our script. Take a look:</p>\n<pre><code>$ ruby script.rb\nrack-obama's version is: 0.1.1\n</code></pre>\n<p>Bundler made the script&rsquo;s dependencies work, completely automatically! If we check on installed gems again, we can see that Bundler installed the gems we needed exactly as if we had run <code>gem install</code> ourselves:</p>\n<pre><code>$ gem list rack-obama\n\n*** LOCAL GEMS ***\n\nrack-obama (0.1.1)\n</code></pre>\n<p>With that, it’s time to wrap things up! If you’re interested in the latest developments (ha) in Bundler, check out the Bundler blog at <a href=\"http://bundler.io/blog/\">bundler.io/blog</a>, or follow us on <a href=\"https://twitter.com/bundlerio\">twitter</a> at <a href=\"https://twitter.com/bundlerio\">@bundlerio</a>. We post and tweet about what changed anytime there’s a new release.</p>\n<p>If you want to support development work and maintenance on <a href=\"https://bundler.io\">Bundler</a>, <a href=\"https://github.com/rubygems/rubygems\">RubyGems</a>, and <a href=\"https://rubygems.org/\">RubyGems.org</a>, check out <a href=\"https://rubytogether.org/\">Ruby Together</a> and follow us at <a href=\"https://twitter.com/rubytogether\">@rubytogether</a>.</p>\n",
				"content_text": "<small>This post was originally [a guest episode of Ruby Tapas](https://www.rubytapas.com/2018/03/27/bundler-tips-and-tricks-andre-arko/), a series of regular, short screencasts about Ruby topics. If that sounds good to you, [try out Ruby Tapas](https://www.rubytapas.com).</small>\n\nAs a Ruby developer, chances are really good that you already know and use [Bundler](https://bundler.io) on a daily basis, and you can `git pull && bundle install` with the best of them. What you might not know is that Bundler has changed and grown over the last 8 years. The newer, lesser-known features can provide a lot of help with other gem-related workflows and problems. What other problems, you ask? Let’s take a look.\n\n### Creating and releasing your own gems\n\nThe first thing Bundler can help with is making your own gems. It’s as easy as `bundle gem foobar`, and you end up with a new gem named `foobar` ready for you to add code.\n\nThere’s a one-time setup to tell Bundler if you want [rspec](http://rspec.info) or minitest, an [MIT license](https://github.com/bundler/bundler/blob/master/lib/bundler/templates/newgem/LICENSE.txt.tt), or a [code of conduct](https://github.com/bundler/bundler/blob/master/lib/bundler/templates/newgem/CODE_OF_CONDUCT.md.tt). After that, you can create gem after gem in just a few seconds each.\n\n    $ bundle gem foobar\n    Creating gem 'foobar'...\n    Do you want to generate tests with your gem?\n    Type 'rspec' or 'minitest' to generate those test files now and in the future. rspec/minitest/(none): rspec\n    Do you want to license your code permissively under the MIT license?\n    This means that any other developer or company will be legally allowed to use your code for free as long as they admit you created it. You can read more about the MIT license at http://choosealicense.com/licenses/mit. y/(n): y\n    MIT License enabled in config\n    Do you want to include a code of conduct in gems you generate?\n    Codes of conduct can increase contributions to your project by contributors who prefer collaborative, safe spaces. You can read more about the code of conduct at contributor-covenant.org. Having a code of conduct means agreeing to the responsibility of enforcing it, so be sure that you are prepared to do that. Be sure that your email address is specified as a contact in the generated code of conduct so that people know who to contact in case of a violation. For suggestions about how to enforce codes of conduct, see http://bit.ly/coc-enforcement. y/(n): y\n    Code of conduct enabled in config\n          create  foobar/Gemfile\n          create  foobar/lib/foobar.rb\n          create  foobar/lib/foobar/version.rb\n          create  foobar/foobar.gemspec\n          create  foobar/Rakefile\n          create  foobar/README.md\n          create  foobar/bin/console\n          create  foobar/bin/setup\n          create  foobar/.gitignore\n          create  foobar/.travis.yml\n          create  foobar/.rspec\n          create  foobar/spec/spec_helper.rb\n          create  foobar/spec/foobar_spec.rb\n          create  foobar/LICENSE.txt\n          create  foobar/CODE_OF_CONDUCT.md\n    Initializing git repo in /Users/andre/Downloads/foobar\n\nAny gem created by Bundler comes with a couple of nice touches: first, a `bin/setup` file that acts as a centralized, well-known location to install dependencies and do any other specific setup needed to develop on your library. By default, it creates a bash script that echoes commands, and runs `bundle install`, but it’s very easy to add your own commands.\n\n    $ cd foobar\n    $ cat bin/setup\n    #!/usr/bin/env bash\n    set -euo pipefail\n    IFS=$'\\n\\t'\n    set -vx\n\n    bundle install\n\n    # Do any other automated setup that you need to do here\n\nEvery gem also includes a `bin/console`, to load your gems and then launch [IRB](https://ruby-doc.org/stdlib-2.5.1/libdoc/irb/rdoc/IRB.html), [Pry](http://pryrepl.org/), [Fir](https://github.com/dnasseri/fir), or whatever interactive prompt you prefer. It’s the fastest way to experiment with the code from your gem.\n\n    $ cat bin/console\n    #!/usr/bin/env ruby\n\n    require \"bundler/setup\"\n    require \"foobar\"\n\n    # You can add fixtures and/or initialization code here to make experimenting\n    # with your gem easier. You can also use a different console, if you like.\n\n    # (If you use this, don't forget to add pry to your Gemfile!)\n    # require \"pry\"\n    # Pry.start\n\n    require \"irb\"\n    IRB.start(__FILE__)\n\nFinally, every gem includes two extremely helpful [rake](https://ruby.github.io/rake/) tasks. The `rake install` will build your gem into a literal `.gem` file, and then run `gem install` to install it onto your local machine.\n\n    $ rake install\n    foobar 0.1.0 built to pkg/foobar-0.1.0.gem.\n    foobar (0.1.0) installed.\n    $ gem list foobar\n\n    *** LOCAL GEMS ***\n\n    foobar (0.1.0)\n\nYou can easily test that building, installing, and using your gem all work the way that you expect them to.\n\n    $ ruby -rfoobar -e 'puts Foobar::VERSION'\n    0.1.0\n\nThe other extremely useful task is `rake release`, which creates and pushes a git tag for your version, builds a `.gem` file, and releases the gem on [RubyGems.org](https://rubygems.org/)! What used to be an error-prone process that could take minutes is now just a single command and a few seconds. It’s marvelous.\n\n    $ rake release\n    foobar 0.1.0 built to pkg/foobar-0.1.0.gem.\n    Tagged v0.1.0.\n    Pushed git commits and tags.\n    Pushed foobar 0.1.0 to rubygems.org\n\n### Developing multiple repos at once\n\nNow that you have a gem, what if you need to make changes to the gem and your app that depends on it at the same time? Bundler already has a feature to make this work as smoothly as possible: **Local Git Repos**.\n\nTo start, you tell Bundler where your local checkout of a git repo is. For example, we could continue to work on our `foobar` gem locally while using it in an application by running this configuration command in the application. Now that we’ve done that, running the application locally will use the code from our checkout. We can make changes, reload the application, and see them live.\n\n    $ cd app\n    $ bundle config local.foobar ~/src/indirect/foobar\n    $ bundle exec ruby -rfoobar -e 'puts Foobar::VERSION'\n    0.1.0\n\n### Adding gems\n\nNow that we're up to speed on creating and using our own gems, the next tip is about speeding up using gems that already exist. Starting with Bundler 1.15, there is an `add` command that will automatically add a gem to your Gemfile and install it.\n\nGiven a gem name, Bundler will look up the gem by name, add it to your Gemfile, and then resolve and install your entire bundle.\n\nAdding new gems to your application got easier starting with Bundler 1.15–now, you can simply run `bundle add GEM` and watch as Bundler adds the gem.\n\n    $ bundle add rack\n    Fetching gem metadata from https://rubygems.org/..............\n    Resolving dependencies...\n    Fetching gem metadata from https://rubygems.org/..............\n    Resolving dependencies...\n    Using bundler 1.16.1\n    Fetching rack 2.0.4\n    Installing rack 2.0.4\n\nNow that the command has run, we can take a look inside the Gemfile using `cat` to see the changes that Bundler made.\n\n    $ cat Gemfile\n    source \"https://rubygems.org\"\n\n    gem \"rack\", \"~> 2.0\"\n\nAdding gems is pretty basic so far, but we’re continuing to improve gem management from the command-line. Watch for this to keep getting better.\n\n### Editing installed gems\n\nAfter we've installed all of our gems, it's a common wish to want to see the code for a gem directly. Bundler makes it easy to open any installed gem directly in your editor so you can see (or even edit) that gem's code.\n\nWhen you run `bundle open GEM`, Bundler will look up the location of that gem on your machine, and then open it in your editor.\n\n    $ bundle open rack\n\nThe default editor is Vim, but Bundler will respect the `EDITOR` variable to open any editor you want.\n\nOnce you've got the gem open in your editor, you can browse the source for the gem, search for the definition of a method, and even edit that gem to change behavior or add debugging code. In this example, we're editing the the rack gem's main file, `rack.rb`. To show how this works, we'll change the `VERSION` constant.\n\nAny changes that you make will be picked up by the next Ruby process you run. We can see the effect of our changes by printing the VERSION constant that we just edited.\n\n    $ bundle exec ruby -rrack -e 'p Rack::VERSION'\n    [5,0]\n\nAs you can probably imagine, being able to change your gems locally is an incredibly valuable tool for the times when it seems like the bug might be in a gem rather than in your own code.\n\n### Searching gems\n\nIf you’re not yet sure which gem to open, you can do a search across exactly the gems in this particular application by using the slightly-obscure command `bundle show --paths`. Combine that command with grep, [ack](https://beyondgrep.com), [ripgrep](https://github.com/BurntSushi/ripgrep), or your favorite search tool to get extremely precise results.\n\nIn this example, we're using the [Rails app that powers RubyGems.org](https://github.com/rubygems/rubygems.org). Running `bundle show --paths` will print out the list of directories, one for each gem used by the application.\n\n    $ bundle show --paths\n    /Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/actioncable-5.0.3\n    /Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/actionmailer-5.0.3\n    /Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/actionpack-5.0.3\n    /Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/actionview-5.0.3\n    /Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/activejob-5.0.3\n    /Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/activemodel-5.0.3\n    /Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/activerecord-5.0.3\n    /Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/activesupport-5.0.3\n    [...]\n\nOnce we have that list of paths, we can combine it with a search tool. In this example, we're using `rg`, which is the `ripgrep` tool. Ripgrep is a search tool similar to `grep`, but optimized for source code. Finding places in our gems where the method `create_or_update` is defined is suddenly a breeze once we have Bundler and Ripgrep working together.\n\n    $ rg 'def create_or_update' $(bundle show --paths)\n    /Users/andre/.gem/ruby/2.3.3/gems/bundler-1.14.6/lib/gems/bundler-1.14.6: No such file or directory (os error 2)\n    /Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/activerecord-5.0.3/lib/active_record/callbacks.rb\n    297:    def create_or_update(*) #:nodoc:\n\n    /Users/andre/src/rubygems/rubygems.org/.bundle/ruby/2.3.0/gems/activerecord-5.0.3/lib/active_record/persistence.rb\n    546:    def create_or_update(*args, &block)\n\n### bundler/inline for single-file scripts\n\nWe're starting to run out of time, but before we wrap up I want to highlight one more feature that offers developers a very powerful tool. Every tip so far has been about managing gems for an application. What about the times when an application is overkill, and you just want to write a few lines of code into a single file?\n\nRuby was originally created for that kind of small, helpful script, and makes it very easy… until your small script starts depending on gems. Then you have to think about installing them, making sure the right version is available, and all the other thing that Bundler was invented to help with. If you have a small script that could use some gems, Bundler can help with that as well. This feature is called ‘inline Gemfiles', and it gives your single-file scripts superpowers.\n\nAt the top of your script, require `bundler/inline`. Then, use the `gemfile` method to declare your dependencies just like you would in a standalone file. When you run the script, Bundler will jump in and make sure the gems you need are installed and loaded, and your script will always be able to run successfully.\n\n    $ vim script.rb\n\n    require \"bundler/inline\"\n\n    gemfile do\n      source \"https://rubygems.org\"\n      gem \"rack-obama\"\n    end\n\n    puts \"rack-obama's version is: #{Rack::Obama::VERSION}\"\n\nOnce we've created a script that uses inline Gemfiles, just running it means Bundler will take care of everything else. Any missing gems are automatically installed, all installed gems are automatically used, and you never have to think about it. As you can see, we do not currently have the `rack-obama` gem installed on this machine.\n\n    $ gem list rack-obama\n\n    *** LOCAL GEMS ***\n\nUnder normal circumstances, our script would fail with an error about a missing constant. Bundler is going to silently install our missing gem as part of running our script. Take a look:\n\n    $ ruby script.rb\n    rack-obama's version is: 0.1.1\n\nBundler made the script's dependencies work, completely automatically! If we check on installed gems again, we can see that Bundler installed the gems we needed exactly as if we had run `gem install` ourselves:\n\n    $ gem list rack-obama\n\n    *** LOCAL GEMS ***\n\n    rack-obama (0.1.1)\n\nWith that, it’s time to wrap things up! If you’re interested in the latest developments (ha) in Bundler, check out the Bundler blog at [bundler.io/blog](http://bundler.io/blog/), or follow us on [twitter](https://twitter.com/bundlerio) at [@bundlerio](https://twitter.com/bundlerio). We post and tweet about what changed anytime there’s a new release.\n\nIf you want to support development work and maintenance on [Bundler](https://bundler.io), [RubyGems](https://github.com/rubygems/rubygems), and [RubyGems.org](https://rubygems.org/), check out [Ruby Together](https://rubytogether.org/) and follow us at [@rubytogether](https://twitter.com/rubytogether).\n",
				"date_published": "2018-06-03T00:00:00-08:00",
				"url": "https://andre.arko.net/2018/06/03/bundler-tips-and-tricks/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2018/05/24/everyone-knows-a-lunar-cycle/",
				"title": "Everyone Knows a Lunar Cycle is 28 Days",
				"content_html": "<h3 class=\"subtitle\">or, falsehoods werewolves believe about time</h3>\n<p><small>This blog post was eventually given as a talk at <a href=\"https://bangbangcon.com/west\">@bangbangconwest</a>. The <a href=\"https://speakerdeck.com/indirect/how-to-calculate-the-phase-of-the-moon-very-very-badly\">slides</a> and <a href=\"https://www.youtube.com/watch?v=syx4pWxu_sk\">video</a> are also available.</small></p>\n<p>A few years ago, I collaborated with <a href=\"https://twitter.com/sailorhg\">@sailorhg</a> to make an iPhone app: a lunar calendar (for witches and werewolves), <a href=\"https://itunes.apple.com/us/app/luna-lunar-calendar-for-witches/id1052484934\">Luna</a>. She also made a swanky website for it over at <a href=\"http://witchy.co\">witchy.co</a>.</p>\n<p>She created the concept and visual design, as well as managing the freelancers who provided the music and lunar horoscopes. I wrote the code, excited to have the chance to ship a mobile app and learn Swift.</p>\n<p>Fast forward a few weeks, and I just couldn’t find a library in Swift or Objective-C to calculate the current phase of the moon. Thinking I could crib from one of the many public C or JavaScript implementations, I tried to port one. And then another. And then another. After trying to port three separate algorithms, and never once getting results that were anywhere close to the actual phase of the moon, I gave up.</p>\n<p>At that point, I read on Wikipedia that lunar cycles are exactly 27.321661 days long and had an epiphany: I could use the precise date and time of the most recent full moon, adding 27.321661 days over and over to predict the phase of the moon in the future.</p>\n<p>Amazingly, this worked! For the very first time, my predicted moon phases lined up with the many free lunar calendars on the internet. I was extremely relieved, and the app went live on the app store.</p>\n<p>Fast forward two years: the app has sold three copies per week, and has 150 active users. @sailorhg gets the nicest, sweetest message on Twitter from someone who says, approximately, “I am a practicing wiccan and I LOVE your app but also it is wrong about when the full moon is, is there any chance you could fix it?”.</p>\n<p>Hearing this, I open the app myself (for the first time in about 18 months) and discover that the app is <em>three days off</em>. I knew that there would eventually be compounding error from multiplication with floats, but that was way, way more error than seemed reasonable.</p>\n<p>Hours of investigation later, I discovered that my entire premise was hilariously wrong.</p>\n<p>First of all, 27.321661 is the average length of the <em>siderial</em> month. I didn&rsquo;t know exactly what that meant when I used the number, but it sounded like it was related to astronomy! It turns out the word sidereal comes from the Latin word <em>sidera</em>, which means &ldquo;star&rdquo;.</p>\n<p>Knowing that, it will now make sense to hear that a sidereal month is the time it takes for the moon to return to the same position among the stars in the sky. Unfortunately, the moon&rsquo;s illumination is determined by position relative to the sun and earth, so the length of a siderial month is totally useless for calculating moon phases.</p>\n<p>A full cycle of lunar phases that we see on earth is called a <em>synodic</em> month, the amount of time it takes for the moon to return to the same position relative to the earth and sun. Synodic and siderial months are different lengths because 1) the earth moves around the sun while the moon moves around the earth, and 2) the earth and moon&rsquo;s orbits are both ellipses!</p>\n<p>(You can learn more about how the positions of the sun and earth determine the moon&rsquo;s illumination at the website made by <a href=\"https://twitter.com/sailorhg\">@sailorhg</a> for her talk at HawaiiJS: <a href=\"https://witchy.co/trig\">witchy.co/trig</a>.)</p>\n<p>So now that we’re using the average length of a synodic month, 29.530587981 days, can we calculate accurate moon phases? Still no. We know the average length of a synodic month, but it&rsquo;s an average. Almost no individual synodic months actually last 29.530587981 days.</p>\n<p>At this point, I had painfully rediscovered that <a href=\"https://99percentinvisible.org/episode/on-average/\">averages don’t actually exist</a>. In another talk I&rsquo;ve given, <a href=\"https://speakerdeck.com/indirect/lies-damn-lies-and-metrics-1\">Lies, Damn Lies, and Metrics</a>, I call this out by saying &ldquo;averages are lie-candy for your brain&rdquo;. Even though I already knew that, I still fell for the idea that the average would resemble reality!</p>\n<p>If we could wait decades (or centuries!) individual differences in lunar cycle lengths will eventually average out&hellip; but that doesn’t help Wiccans in the year 2018. At the moment that message arrived, the full moon in the real world arrived three days after the app’s perfectly average spherical moon in a vacuum.</p>\n<p>Fortunately, Swift matured pretty significantly while my app was getting more and more wrong. A few seconds of searching provided several open source astronomical libraries implemented entirely in Swift.</p>\n<p>Within a few hours, I had a working function call that told me the exact phase of the moon with 64-bit precision. <code>0.0</code> for a full moon, <code>0.5</code> for a new moon, up to <code>0.9999</code> right before the next full moon, and then wrapping back around to <code>0.0</code>.</p>\n<p>That’s when I discovered my other fatal flaw. When putting together the visuals for the calendar view, I used the (beautiful!) <a href=\"https://erikflowers.github.io/weather-icons/\">Weather Icons</a>. As you might expect, there are exactly 28 icons. Everyone knows a lunar month is 28 days long, right?</p>\n<p>Since I knew that lunar months were actually 27.321661 days long, I converted lunar months into icons with a relatively straightforward formula: <code>percent_of_lunar_month * 27.321661 / 28</code>. Then I took that number, and used it to index into an array of the 28 icons.</p>\n<p>As you might have guessed by now, this was a total disaster. For one thing, I had now learned that individual lunar months might be as short as 25 days, or as long as 30. Which day should you show the new moon on when two (or even three!) entire days round to an integer icon index of <code>0</code>?</p>\n<p>As a calendar to help you plan your life around significant lunar events, Luna was still an absolute failure. You might see a new moon on two or even three consecutive days, but you also might hit a month where you never see a full moon at all!</p>\n<p>I spent <em>days</em> trying to handle the edge cases in my approach. It turns out, when every month has differently-sized days, there is no single list of numbers you can use to catch all the lunar events.</p>\n<p>Eventually, I gave up, and started preparing to ship the app, figuring that only one or two errors per month was a lot better than everything being wrong by three days.</p>\n<p>Right before shipping a still-broken app, I realized that I could completely solve the problem by inverting my calculations. Instead of calculating the percentage phase of the moon at noon local time, I needed to calculate the phase of the moon at the midnight that started the day and the midnight that ended the day.</p>\n<p>Armed with the starting and ending points of each day, I could check to see if the points I cared about would happen on that day&hellip; and only on that day. For example, knowing that the day started at moon phase <code>0.46</code> and ended at <code>0.51</code> meant that I could be <em>sure</em> that day was the new moon. Even better, no other day would be counted as the new moon, no matter how long the lunar month was.</p>\n<p>Considering how little code went into this application overall, I feel like I learned an unusually high amount about programming, astronomy, and bad assumptions that seem reasonable at the time.</p>\n<p>Today, Luna’s astronomical calculations are accurate to within a few minutes, each quarter of the moon falls on the same day as other moon calendars, and it even stays accurate when the date is manually jumped years into the past or future. It only took three years. 😅</p>\n<small>\n_Thanks to Chris Dary for the subtitle, Kyle Kingsbury for inadvertently reminding me that this happened, and to all of Kyle, Chris, Coda Hale, Marc Hedlund, Nelson Minar, Sunah Suh, and Daniel Espeset for enjoying this story so much that I was motivated to publish it._\n</small>\n<small>\n_Updated 2019-02-25 to add an explanation of siderial versus synodic months, link to [witchy.co/trig](https://witchy.co/trig), and link to the slides from !!ConWest._\n</small>\n",
				"content_text": "<h3 class=\"subtitle\">or, falsehoods werewolves believe about time</h3>\n\n<small>This blog post was eventually given as a talk at [@bangbangconwest](https://bangbangcon.com/west). The [slides](https://speakerdeck.com/indirect/how-to-calculate-the-phase-of-the-moon-very-very-badly) and [video](https://www.youtube.com/watch?v=syx4pWxu_sk) are also available.</small>\n\nA few years ago, I collaborated with [@sailorhg](https://twitter.com/sailorhg) to make an iPhone app: a lunar calendar (for witches and werewolves), [Luna](https://itunes.apple.com/us/app/luna-lunar-calendar-for-witches/id1052484934). She also made a swanky website for it over at [witchy.co](http://witchy.co).\n\nShe created the concept and visual design, as well as managing the freelancers who provided the music and lunar horoscopes. I wrote the code, excited to have the chance to ship a mobile app and learn Swift.\n\nFast forward a few weeks, and I just couldn’t find a library in Swift or Objective-C to calculate the current phase of the moon. Thinking I could crib from one of the many public C or JavaScript implementations, I tried to port one. And then another. And then another. After trying to port three separate algorithms, and never once getting results that were anywhere close to the actual phase of the moon, I gave up.\n\nAt that point, I read on Wikipedia that lunar cycles are exactly 27.321661 days long and had an epiphany: I could use the precise date and time of the most recent full moon, adding 27.321661 days over and over to predict the phase of the moon in the future.\n\nAmazingly, this worked! For the very first time, my predicted moon phases lined up with the many free lunar calendars on the internet. I was extremely relieved, and the app went live on the app store.\n\nFast forward two years: the app has sold three copies per week, and has 150 active users. @sailorhg gets the nicest, sweetest message on Twitter from someone who says, approximately, “I am a practicing wiccan and I LOVE your app but also it is wrong about when the full moon is, is there any chance you could fix it?”.\n\nHearing this, I open the app myself (for the first time in about 18 months) and discover that the app is _three days off_. I knew that there would eventually be compounding error from multiplication with floats, but that was way, way more error than seemed reasonable.\n\nHours of investigation later, I discovered that my entire premise was hilariously wrong.\n\nFirst of all, 27.321661 is the average length of the _siderial_ month. I didn't know exactly what that meant when I used the number, but it sounded like it was related to astronomy! It turns out the word sidereal comes from the Latin word _sidera_, which means \"star\".\n\nKnowing that, it will now make sense to hear that a sidereal month is the time it takes for the moon to return to the same position among the stars in the sky. Unfortunately, the moon's illumination is determined by position relative to the sun and earth, so the length of a siderial month is totally useless for calculating moon phases.\n\nA full cycle of lunar phases that we see on earth is called a _synodic_ month, the amount of time it takes for the moon to return to the same position relative to the earth and sun. Synodic and siderial months are different lengths because 1) the earth moves around the sun while the moon moves around the earth, and 2) the earth and moon's orbits are both ellipses!\n\n(You can learn more about how the positions of the sun and earth determine the moon's illumination at the website made by [@sailorhg](https://twitter.com/sailorhg) for her talk at HawaiiJS: [witchy.co/trig](https://witchy.co/trig).)\n\nSo now that we’re using the average length of a synodic month, 29.530587981 days, can we calculate accurate moon phases? Still no. We know the average length of a synodic month, but it's an average. Almost no individual synodic months actually last 29.530587981 days.\n\nAt this point, I had painfully rediscovered that [averages don’t actually exist](https://99percentinvisible.org/episode/on-average/). In another talk I've given, [Lies, Damn Lies, and Metrics](https://speakerdeck.com/indirect/lies-damn-lies-and-metrics-1), I call this out by saying \"averages are lie-candy for your brain\". Even though I already knew that, I still fell for the idea that the average would resemble reality!\n\nIf we could wait decades (or centuries!) individual differences in lunar cycle lengths will eventually average out... but that doesn’t help Wiccans in the year 2018. At the moment that message arrived, the full moon in the real world arrived three days after the app’s perfectly average spherical moon in a vacuum.\n\nFortunately, Swift matured pretty significantly while my app was getting more and more wrong. A few seconds of searching provided several open source astronomical libraries implemented entirely in Swift.\n\nWithin a few hours, I had a working function call that told me the exact phase of the moon with 64-bit precision. `0.0` for a full moon, `0.5` for a new moon, up to `0.9999` right before the next full moon, and then wrapping back around to `0.0`.\n\nThat’s when I discovered my other fatal flaw. When putting together the visuals for the calendar view, I used the (beautiful!) [Weather Icons](https://erikflowers.github.io/weather-icons/). As you might expect, there are exactly 28 icons. Everyone knows a lunar month is 28 days long, right?\n\nSince I knew that lunar months were actually 27.321661 days long, I converted lunar months into icons with a relatively straightforward formula: `percent_of_lunar_month * 27.321661 / 28`. Then I took that number, and used it to index into an array of the 28 icons.\n\nAs you might have guessed by now, this was a total disaster. For one thing, I had now learned that individual lunar months might be as short as 25 days, or as long as 30. Which day should you show the new moon on when two (or even three!) entire days round to an integer icon index of `0`?\n\nAs a calendar to help you plan your life around significant lunar events, Luna was still an absolute failure. You might see a new moon on two or even three consecutive days, but you also might hit a month where you never see a full moon at all!\n\nI spent _days_ trying to handle the edge cases in my approach. It turns out, when every month has differently-sized days, there is no single list of numbers you can use to catch all the lunar events.\n\nEventually, I gave up, and started preparing to ship the app, figuring that only one or two errors per month was a lot better than everything being wrong by three days.\n\nRight before shipping a still-broken app, I realized that I could completely solve the problem by inverting my calculations. Instead of calculating the percentage phase of the moon at noon local time, I needed to calculate the phase of the moon at the midnight that started the day and the midnight that ended the day.\n\nArmed with the starting and ending points of each day, I could check to see if the points I cared about would happen on that day... and only on that day. For example, knowing that the day started at moon phase `0.46` and ended at `0.51` meant that I could be _sure_ that day was the new moon. Even better, no other day would be counted as the new moon, no matter how long the lunar month was.\n\nConsidering how little code went into this application overall, I feel like I learned an unusually high amount about programming, astronomy, and bad assumptions that seem reasonable at the time.\n\nToday, Luna’s astronomical calculations are accurate to within a few minutes, each quarter of the moon falls on the same day as other moon calendars, and it even stays accurate when the date is manually jumped years into the past or future. It only took three years. 😅\n\n<small>\n_Thanks to Chris Dary for the subtitle, Kyle Kingsbury for inadvertently reminding me that this happened, and to all of Kyle, Chris, Coda Hale, Marc Hedlund, Nelson Minar, Sunah Suh, and Daniel Espeset for enjoying this story so much that I was motivated to publish it._\n</small>\n\n<small>\n_Updated 2019-02-25 to add an explanation of siderial versus synodic months, link to [witchy.co/trig](https://witchy.co/trig), and link to the slides from !!ConWest._\n</small>\n",
				"date_published": "2018-05-24T00:00:00-08:00",
				"url": "https://andre.arko.net/2018/05/24/everyone-knows-a-lunar-cycle/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2018/05/09/food-identification/",
				"title": "Food Identification",
				"content_html": "<p>I&rsquo;m very excited to announce my latest project, <a href=\"http://cuberule.com\">cuberule.com</a>.</p>\n<p>Are you tired of arguing with your friends about whether some food is or is not a sandwich? Are you confused about how to tell what type of food you are about to eat? Never worry about such questions again! Use <a href=\"http://cuberule.com\">The Cube Rule of Food Identification</a> and all such arguments will fade away into the past.</p>\n<p>🍞🥪🌮🌯🥙🥟🥗</p>\n",
				"content_text": "I'm very excited to announce my latest project, [cuberule.com](http://cuberule.com).\n\nAre you tired of arguing with your friends about whether some food is or is not a sandwich? Are you confused about how to tell what type of food you are about to eat? Never worry about such questions again! Use [The Cube Rule of Food Identification](http://cuberule.com) and all such arguments will fade away into the past.\n\n🍞🥪🌮🌯🥙🥟🥗\n",
				"date_published": "2018-05-09T00:00:00-08:00",
				"url": "https://andre.arko.net/2018/05/09/food-identification/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2018/04/26/pairing-a-guide-to-fruitful/",
				"title": "Pairing: A Guide to Fruitful Collaboration 🍓🍑🍐",
				"content_html": "<p><small>This post was originally given as a presentation at <a href=\"http://rubyconf.org\">RailsConf 2018</a>. The <a href=\"https://speakerdeck.com/indirect/a-history-of-bundles-2010-to-2017\">slides</a> and <a href=\"https://www.youtube.com/watch?v=km7uGUEd4fk\">video</a> are also available.</small></p>\n<script async class=\"speakerdeck-embed\" data-id=\"8678a82b77a54d488c3cb7945b81a7b5\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n<p>When interacting with Ruby devs, I’ve heard a lot of feedback along the lines of “I‘ve heard that pairing is supposed to be good, but every time I try to do it I get more and more discouraged”. Other devs I’ve talked to have lots of great experience pairing with their peers, but aren’t sure how to work with someone more or less experienced than they are. The goal of this talk is to prepare you so that pairing is not only something that you <em>can</em> do with any other dev, but something that you <em>want</em> to do with any other dev. By the end of this talk, I want you to be ready have awesome pairing sessions where you are energized and excited by working together with other devs to conquer your shared problems. Pairing is a fantastic tool for your professional toolbox: let’s learn how to design, discuss, refine, and refactor… together.</p>\n<p>So, before we dive in, why would you want to listen to me talk about this anyway? I’ve collaborated with other Ruby developers for 14 years, and for the last 5 years my day job has been almost entirely pairing. All day long, every day. In that time, I’ve paired with devs ranging from twice my age and experience all the way down to brand new devs writing code for the very first time. This talk is an attempt to gather everything that I’ve learned into one place, so that you (and, honestly, also me) can easily refer to it later.</p>\n<h3 id=\"what-is-pairing-we-just-dont-know\">What is pairing? We just don’t know.</h3>\n<p>Even if you’ve already been convinced that pairing is good, the desire to pair doesn’t come with instructions. What is pairing? It’s not incredibly clear, especially since  asking 10 developers will usually get you 10 different answers. For this talk, we’re going to define pairing as working with another developer on the same machine.</p>\n<p>If you haven’t had a good pairing experience yet, this is probably the point where you’re thinking “why would you have two developers use one machine? only one keyboard and mouse work at a time, so one of those developers is just going to be sitting around doing nothing!”. In a good pairing session, nothing could be further from the truth.</p>\n<p>Programming isn’t pushing buttons, programming is solving problems. It’s normal for more than one person to work together on a solution to a problem! Just think about anytime you’ve had a productive conversation, exchanging ideas, learning, and finding new insights you couldn’t have alone. Pairing is about bringing that experience to the work of solving problems with code.</p>\n<p>Rather than one developer sitting around, both developers can be engaged with their work, communicating, planning, theorizing, end experimenting. Pairing provides many of the benefits of code review, with the fastest possible turnaround time and the easiest communication between author and reviewer. Another person can notice that you’ve fallen down a rabbit hole and you’re far, far away from the problem you set out to solve. Rotating control back and forth can reduce fatigue from repeated actions, since it gives each of you a chance to sit back, read, think, and discuss.</p>\n<p>Communicating your ideas to your pair means you are continuously “rubber ducking”, explaining your understanding of the situation to someone else and having more ideas about what to do as a result. When your have a positive relationship with your pair, pairing also provides motivation to be the best version of yourself—with someone else watching, it’s harder to justify bad habits to yourself. Since future you is often the person who pays the price for your own bad habits programming, that’s a win for everyone.</p>\n<h3 id=\"pairing-pitfalls\">Pairing pitfalls</h3>\n<p>It’s not all roses, though. Pair programming brings with it a new set of problems, and can magnify existing problematic habits. It’s extremely common for a pair with uneven experience levels to degrade into one person working and one person watching—most likely without understanding what they’re seeing, and without learning anything. Pairing with someone who is ideologically rigid can mean you end up in an argument about every single decision, and compromise is impossible.</p>\n<p>When pairing with someone more experienced, it’s not uncommon to feel like you’re trapped in the interview or performance review from hell—it lasts all day long, every day. That pressure creates knock-on problems, like panic and blanking in the face of uncertainty, or shaky nerves leading to more mistakes. Pairing with condescending developers acting in bad faith can be a nightmare, and in those kinds of situations pairing provides social pressure towards bad habits instead of good ones.</p>\n<p>The rest of this talk is to help you get to a point where you can maximize those benefits while minimizing those pitfalls. It will take thought, and effort. You won’t get it right overnight, and you’ll need to keep thinking about it and working at it even after years of practice. All that work is worth it, though, because pairing can be one of the most positive and satisfying ways to experience programming.</p>\n<h3 id=\"two-people-working-together\">Two people, working together</h3>\n<p>This is a great time to talk about the first and most basic requirement of pairing: you need two people who trust each other enough to cooperate and work together in good faith. You can only be one of those people! If your pair is condescending, discouraging, insulting, or casting doubt on your skills or abilities&hellip; that’s not okay.   If you’ve ever been made to feel stupid, or like a burden, or like a lesser partner while you were pairing, that wasn’t your fault—that was the other person failing to be a good pair.</p>\n<p>In addition, your pair has to want to work <em>with</em> you, and not want to work <em>at</em> you. A good pair has to be willing to share control, share ideas, and share credit. If your pair is not engaging in good faith, you have two options. First, you can call them out (if you feel safe doing that), and hope they listen and change. Beyond that, all you can do is find someone else who does want to pair collaboratively.</p>\n<p>So! Let’s say you’ve found someone who isn’t going to condescend to you, and who wants to work with you instead of simply working while you watch. Awesome. The rest of my suggestions fall into three categories: pairing with similarly experienced devs, with devs more experienced than you, and with devs less experienced than you. The work you do is the same in all three of those situations, but behaviors that are helpful in one of them can be harmful in others.</p>\n<p>Let’s start with the most straightforward situation: pairing with your peers, where you both have about the same amount of experience. Pairing with devs at your own level gives you the chance to swap tips, support one another in learning and growing, commiserate through the tough parts, and generally produce results that combine the best of what both of you have to offer.</p>\n<p>Based on years of doing this kind of work, my experience is that good pairing sessions come down to consent and communication. Staying on the same page the whole time you’re working together will take some work, but it will help you produce better results.</p>\n<p>To kick things off at the very beginning, you and your pair are going to need to communicate and consent about your programming environment. What machine, OS, editor, terminal, and shell are you going to use? Whose configuration files are going to be active?</p>\n<p>If you’re going to pair a lot, the ideal is one dedicated pairing station per pair of developers, all running the same OS, ruby versions, editor, terminal app, and shell, all configured exactly the same way. This not only makes it easy to rotate who you’re pairing with, it makes sure everyone is able to sit down and start working at any time.</p>\n<p>It’s not ideal, but in a pinch you can also pair on someone‘s personal machine. I strongly suggest creating a separate pairing user account, where you and your pair can work out together what to install and how to configure things. Dropping someone directly into your own personalized environment (and expecting them to just deal with it) starts things of on the wrong foot. If you want to collaborate as equals, act like it! Level the playing field to include only things you have <em>both</em> agreed on.</p>\n<h3 id=\"its-not-about-writing-code\">It’s not about writing code</h3>\n<p>Once you have a pairing environment ready, you’ll probably be tempted to dive right in and start writing some code. Resist! This is one of the moments pairing can be massively better than working alone. Before writing any code, establish shared understanding about the situation. Articulate the problem you want to solve with your pair, and ask them for their feedback and ideas. Keep taking turns talking until you both agree on a shared understanding of the problem. That may just take a minute, but it might also require reading documentation, researching existing code, or even seeking out designers, PMs, or other stakeholders to ask questions and clarify requirements.</p>\n<p>Right off the bat, pairing helps you avoid assumptions you won’t notice are wrong until you’ve already built the wrong thing. Another perspective is one of the best ways to cover your blind spots. Reaching shared understanding with another person dramatically reduces the chances that you missed something without realizing it.</p>\n<p>Once you’ve come to an agreement about the problem you’re going to solve, work out guidelines for who will be doing the typing and who will be thinking about what you’re doing and narrating the actions and choices. Those roles are sometimes called “driver” and “navigator”, analogizing to a common division of tasks when taking a trip in a car. It’s not a perfect analogy, but it can be pretty helpful to think about dividing the labor up that way. It’s much easier to notice things, good or bad, when one person is typing and one person is observing and contemplating the broader context.</p>\n<p>One common approach is to combine pairing with test-driven development—in that scheme, one pair types while coding until the current red test is green and a new red test has been added. Then the driver and navigator alternate, continuing until the problem is solved or it’s time for a break.</p>\n<p>Another approach, often called “ping pong programming”, involves one person writing a test, and the other person writing as little code as possible to get that test green, back and forth. It doesn’t work for everyone, but it can be very eye-opening to discover that the tests you thought were very thorough actually all pass when the code returns a single hardcoded value every time.</p>\n<p>Now that you’re getting the idea, this is a good time to mention what is probably the most common problem while pairing: when you say “oh, let me just do this myself really quick”, grab the controls, and do something yourself. No! Don’t do that! Pairing is based on consent—you can’t unilaterally decide to take over the controls. As soon as you do that, pairing is over and the other person is just watching you show off.</p>\n<p>Even if the person agrees to let you take over, though, it’s a terrible idea. The payoff of pairing is that you both understand not just <em>what</em> is happening, but <em>why</em> it is happening. If you can’t or don’t want to explain how and why to do something, you shouldn’t be doing it. You might be feeling frustrated that something “simple” is taking so long, but instead you should recognize the opportunity! This is a chance for the other person to gain new skills and new understanding. After this, either one of you will be able to do it.</p>\n<p>Finally, both of your names are going to go on that code. It is the height of rudeness to ask someone else to put their name and reputation on the line for something they don’t understand or can’t do themselves.</p>\n<p>Whichever option you pick, it needs to be clear to both people who is driving and who is navigating at any time. Trust and collaboration is hard to sustain when your keyboard and mouse are unexpectedly flying around underneath you and doing things you don’t expect or want. At its most rigid, that could be a timer that tells you when to switch. At its most fluid, that could mean requesting and yielding control as part of the conversation, with control going back and forth anytime it’s convenient. When you’re doing this right, another dev could walk up to your pair anytime and ask “who’s driving right now?”, and both of you will not only give the same answer, but feel confident about it.</p>\n<p>Now that we’ve covered the mechanics of pairing and made some suggestions about how to pair with your peers, let’s talk about how things change when you’re pairing with someone more experienced than you are.</p>\n<h3 id=\"pairing-with-someone-more-experienced\">Pairing with someone more experienced</h3>\n<p>Pairing with someone more experienced is a great opportunity to talk through ideas, hear about new options for solving problems, learn about tradeoffs built in to different options, and see how devs who have spent more time at this job have chosen to adapt to it.</p>\n<p>There most common pitfall when pairing with someone more experienced is the assumption that you have nothing to contribute. It will take work, but you can fight that feeling. The person you’re paring with should be helping you fight that feeling, too. They’re on the hook to make sure that pairing is a cooperative exercise, and not just a chance for them to try to show off.</p>\n<p>While pairing with someone experienced, your main job is to provide feedback. Hear a word you’re not familiar with? Ask what it is. See something go by too fast to tell what it was? Ask to see it again. Notice your pair jumping in and taking over the controls? Point out they took over, and ask if they can help you do the same thing instead.</p>\n<p>Just because someone has more experience than you doesn’t mean they’re automatically right. In fact, having more experience in some areas doesn’t mean they have more experience than you in every area. When pairing with devs who seemed to know more than me about everything, I have and discovered  strengths and specializations I didn’t know I had. When pairing with devs just a few weeks into coding, I have regularly discovered that they have many kinds of experience and knowledge that I don’t.</p>\n<p>The bigger the experience difference is, the more important it is for both of you to make sure that your time working together is informative and productive for <em>both</em> of you, not just the supposed bigshot. Ask questions, give feedback, and work together to calibrate the speed and depth of your work so that it’s beneficial. You might feel like a burden, but trust me… the best part of pairing with someone less experienced is being able to introduce them to new things!</p>\n<p>Another thing to keep in mind when pairing with someone more experienced is that they will know the answers to many of your questions. Rather than going straight to the API docs, or straight to Google, ask the questions you have. Humans are by far the best interface to information that you don’t understand yet. Google can answer your question, but you have to know how to phrase your question to find the right results. People can help you figure out how to phrase your question, usually with much less flailing around.</p>\n<p>If you run across something you don’t understand, ask about it. Maybe you’ll find out the answer. Maybe you’ll figure out the answer together. Maybe you’ll find out that even experienced people don’t know a lot of stuff. Probably, you’ll do all three. Take advantage of the experience that you have access to, and work together to reach a shared understanding of concepts and code. That’s what pairing is all about.</p>\n<h3 id=\"pairing-with-someone-less-experienced\">Pairing with someone less experienced</h3>\n<p>Pairing with a dev less experienced than yourself offers a chance to let them drive, helps you catch gaps in your planning, and makes you better at understanding and communicating your own ideas. Even though you’re more experienced, this is still a chance for you to learn, grow, and practice your skills. These are some tactics I’ve picked up that can make pairing with someone less experienced more positive, cooperative, and productive.</p>\n<p>Before you start, establish cooperative, non-judgmental ground rules. You&rsquo;re going to be working together on two goals: one is getting the work done, but the other goal is giving the less experienced developer more experience.</p>\n<p>Make it clear that computers are hard, you often don’t know what to do, and if they don’t know what to do while you’re pairing that is totally okay. If they pause, wait for them! If they pause for more than (say) 10 seconds, let them know that you&rsquo;re happy to wait for as long as they need, but you&rsquo;re also happy to start talking about what you could do next if they are feeling stuck.</p>\n<p>Make it as hard as possible for yourself to jump in or take over. I have personally gotten a lot of use out of fidget toys or grabbing a giant stuffed animal to hug. In cases where my self-control is bad, I have sometimes even unplugged my keyboard and mouse. It&rsquo;s easy to not type when your keyboard doesn&rsquo;t work at all!</p>\n<p>Finally, keep in mind that all of this work is ultimately a chance for you to level up yourself. High level independent contributor work depends on clear, persuasive communication. Pairing is a prime opportunity to practice and hone those skills. Coordinating development work across teams, gaining support for your policy proposal, encouraging developers to adopt good practices, and many other tasks—all require excellent communication skills.</p>\n<h3 id=\"programming-as-relationships\">Programming as relationships</h3>\n<p>Ultimately, software is made <em>by</em> people, <em>for</em> people. Every line of code is the result of some relationship between human beings. Working with a team means planning, communicating, discussing, compromising, debugging, and more. Even though it can be hard to tell when your up to your eyeballs in code, all programming is a relationship, with stakeholders, customers, managers, and coworkers. Pair programming makes more of that relationship direct and explicit, including both the benefits and the hazards that come from closer relationships.</p>\n",
				"content_text": "<small>This post was originally given as a presentation at [RailsConf 2018](http://rubyconf.org). The [slides](https://speakerdeck.com/indirect/a-history-of-bundles-2010-to-2017) and [video](https://www.youtube.com/watch?v=km7uGUEd4fk) are also available.</small>\n\n<script async class=\"speakerdeck-embed\" data-id=\"8678a82b77a54d488c3cb7945b81a7b5\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n\nWhen interacting with Ruby devs, I’ve heard a lot of feedback along the lines of “I‘ve heard that pairing is supposed to be good, but every time I try to do it I get more and more discouraged”. Other devs I’ve talked to have lots of great experience pairing with their peers, but aren’t sure how to work with someone more or less experienced than they are. The goal of this talk is to prepare you so that pairing is not only something that you _can_ do with any other dev, but something that you _want_ to do with any other dev. By the end of this talk, I want you to be ready have awesome pairing sessions where you are energized and excited by working together with other devs to conquer your shared problems. Pairing is a fantastic tool for your professional toolbox: let’s learn how to design, discuss, refine, and refactor… together.\n\nSo, before we dive in, why would you want to listen to me talk about this anyway? I’ve collaborated with other Ruby developers for 14 years, and for the last 5 years my day job has been almost entirely pairing. All day long, every day. In that time, I’ve paired with devs ranging from twice my age and experience all the way down to brand new devs writing code for the very first time. This talk is an attempt to gather everything that I’ve learned into one place, so that you (and, honestly, also me) can easily refer to it later.\n\n### What is pairing? We just don’t know.\n\nEven if you’ve already been convinced that pairing is good, the desire to pair doesn’t come with instructions. What is pairing? It’s not incredibly clear, especially since  asking 10 developers will usually get you 10 different answers. For this talk, we’re going to define pairing as working with another developer on the same machine.\n\nIf you haven’t had a good pairing experience yet, this is probably the point where you’re thinking “why would you have two developers use one machine? only one keyboard and mouse work at a time, so one of those developers is just going to be sitting around doing nothing!”. In a good pairing session, nothing could be further from the truth.\n\nProgramming isn’t pushing buttons, programming is solving problems. It’s normal for more than one person to work together on a solution to a problem! Just think about anytime you’ve had a productive conversation, exchanging ideas, learning, and finding new insights you couldn’t have alone. Pairing is about bringing that experience to the work of solving problems with code.\n\nRather than one developer sitting around, both developers can be engaged with their work, communicating, planning, theorizing, end experimenting. Pairing provides many of the benefits of code review, with the fastest possible turnaround time and the easiest communication between author and reviewer. Another person can notice that you’ve fallen down a rabbit hole and you’re far, far away from the problem you set out to solve. Rotating control back and forth can reduce fatigue from repeated actions, since it gives each of you a chance to sit back, read, think, and discuss. \n\nCommunicating your ideas to your pair means you are continuously “rubber ducking”, explaining your understanding of the situation to someone else and having more ideas about what to do as a result. When your have a positive relationship with your pair, pairing also provides motivation to be the best version of yourself—with someone else watching, it’s harder to justify bad habits to yourself. Since future you is often the person who pays the price for your own bad habits programming, that’s a win for everyone.\n\n### Pairing pitfalls\n\nIt’s not all roses, though. Pair programming brings with it a new set of problems, and can magnify existing problematic habits. It’s extremely common for a pair with uneven experience levels to degrade into one person working and one person watching—most likely without understanding what they’re seeing, and without learning anything. Pairing with someone who is ideologically rigid can mean you end up in an argument about every single decision, and compromise is impossible.\n\nWhen pairing with someone more experienced, it’s not uncommon to feel like you’re trapped in the interview or performance review from hell—it lasts all day long, every day. That pressure creates knock-on problems, like panic and blanking in the face of uncertainty, or shaky nerves leading to more mistakes. Pairing with condescending developers acting in bad faith can be a nightmare, and in those kinds of situations pairing provides social pressure towards bad habits instead of good ones.\n\nThe rest of this talk is to help you get to a point where you can maximize those benefits while minimizing those pitfalls. It will take thought, and effort. You won’t get it right overnight, and you’ll need to keep thinking about it and working at it even after years of practice. All that work is worth it, though, because pairing can be one of the most positive and satisfying ways to experience programming.\n\n### Two people, working together\n\nThis is a great time to talk about the first and most basic requirement of pairing: you need two people who trust each other enough to cooperate and work together in good faith. You can only be one of those people! If your pair is condescending, discouraging, insulting, or casting doubt on your skills or abilities... that’s not okay.   If you’ve ever been made to feel stupid, or like a burden, or like a lesser partner while you were pairing, that wasn’t your fault—that was the other person failing to be a good pair.\n\nIn addition, your pair has to want to work _with_ you, and not want to work _at_ you. A good pair has to be willing to share control, share ideas, and share credit. If your pair is not engaging in good faith, you have two options. First, you can call them out (if you feel safe doing that), and hope they listen and change. Beyond that, all you can do is find someone else who does want to pair collaboratively.\n\nSo! Let’s say you’ve found someone who isn’t going to condescend to you, and who wants to work with you instead of simply working while you watch. Awesome. The rest of my suggestions fall into three categories: pairing with similarly experienced devs, with devs more experienced than you, and with devs less experienced than you. The work you do is the same in all three of those situations, but behaviors that are helpful in one of them can be harmful in others.\n\nLet’s start with the most straightforward situation: pairing with your peers, where you both have about the same amount of experience. Pairing with devs at your own level gives you the chance to swap tips, support one another in learning and growing, commiserate through the tough parts, and generally produce results that combine the best of what both of you have to offer.\n\nBased on years of doing this kind of work, my experience is that good pairing sessions come down to consent and communication. Staying on the same page the whole time you’re working together will take some work, but it will help you produce better results.\n\nTo kick things off at the very beginning, you and your pair are going to need to communicate and consent about your programming environment. What machine, OS, editor, terminal, and shell are you going to use? Whose configuration files are going to be active?\n\nIf you’re going to pair a lot, the ideal is one dedicated pairing station per pair of developers, all running the same OS, ruby versions, editor, terminal app, and shell, all configured exactly the same way. This not only makes it easy to rotate who you’re pairing with, it makes sure everyone is able to sit down and start working at any time.\n\nIt’s not ideal, but in a pinch you can also pair on someone‘s personal machine. I strongly suggest creating a separate pairing user account, where you and your pair can work out together what to install and how to configure things. Dropping someone directly into your own personalized environment (and expecting them to just deal with it) starts things of on the wrong foot. If you want to collaborate as equals, act like it! Level the playing field to include only things you have _both_ agreed on.\n\n### It’s not about writing code\n\nOnce you have a pairing environment ready, you’ll probably be tempted to dive right in and start writing some code. Resist! This is one of the moments pairing can be massively better than working alone. Before writing any code, establish shared understanding about the situation. Articulate the problem you want to solve with your pair, and ask them for their feedback and ideas. Keep taking turns talking until you both agree on a shared understanding of the problem. That may just take a minute, but it might also require reading documentation, researching existing code, or even seeking out designers, PMs, or other stakeholders to ask questions and clarify requirements.\n\nRight off the bat, pairing helps you avoid assumptions you won’t notice are wrong until you’ve already built the wrong thing. Another perspective is one of the best ways to cover your blind spots. Reaching shared understanding with another person dramatically reduces the chances that you missed something without realizing it. \n\nOnce you’ve come to an agreement about the problem you’re going to solve, work out guidelines for who will be doing the typing and who will be thinking about what you’re doing and narrating the actions and choices. Those roles are sometimes called “driver” and “navigator”, analogizing to a common division of tasks when taking a trip in a car. It’s not a perfect analogy, but it can be pretty helpful to think about dividing the labor up that way. It’s much easier to notice things, good or bad, when one person is typing and one person is observing and contemplating the broader context. \n\nOne common approach is to combine pairing with test-driven development—in that scheme, one pair types while coding until the current red test is green and a new red test has been added. Then the driver and navigator alternate, continuing until the problem is solved or it’s time for a break.\n\nAnother approach, often called “ping pong programming”, involves one person writing a test, and the other person writing as little code as possible to get that test green, back and forth. It doesn’t work for everyone, but it can be very eye-opening to discover that the tests you thought were very thorough actually all pass when the code returns a single hardcoded value every time.\n\nNow that you’re getting the idea, this is a good time to mention what is probably the most common problem while pairing: when you say “oh, let me just do this myself really quick”, grab the controls, and do something yourself. No! Don’t do that! Pairing is based on consent—you can’t unilaterally decide to take over the controls. As soon as you do that, pairing is over and the other person is just watching you show off.\n\nEven if the person agrees to let you take over, though, it’s a terrible idea. The payoff of pairing is that you both understand not just _what_ is happening, but _why_ it is happening. If you can’t or don’t want to explain how and why to do something, you shouldn’t be doing it. You might be feeling frustrated that something “simple” is taking so long, but instead you should recognize the opportunity! This is a chance for the other person to gain new skills and new understanding. After this, either one of you will be able to do it.\n\nFinally, both of your names are going to go on that code. It is the height of rudeness to ask someone else to put their name and reputation on the line for something they don’t understand or can’t do themselves.\n\nWhichever option you pick, it needs to be clear to both people who is driving and who is navigating at any time. Trust and collaboration is hard to sustain when your keyboard and mouse are unexpectedly flying around underneath you and doing things you don’t expect or want. At its most rigid, that could be a timer that tells you when to switch. At its most fluid, that could mean requesting and yielding control as part of the conversation, with control going back and forth anytime it’s convenient. When you’re doing this right, another dev could walk up to your pair anytime and ask “who’s driving right now?”, and both of you will not only give the same answer, but feel confident about it.\n\nNow that we’ve covered the mechanics of pairing and made some suggestions about how to pair with your peers, let’s talk about how things change when you’re pairing with someone more experienced than you are.\n\n### Pairing with someone more experienced\n\nPairing with someone more experienced is a great opportunity to talk through ideas, hear about new options for solving problems, learn about tradeoffs built in to different options, and see how devs who have spent more time at this job have chosen to adapt to it.\n\nThere most common pitfall when pairing with someone more experienced is the assumption that you have nothing to contribute. It will take work, but you can fight that feeling. The person you’re paring with should be helping you fight that feeling, too. They’re on the hook to make sure that pairing is a cooperative exercise, and not just a chance for them to try to show off.\n\nWhile pairing with someone experienced, your main job is to provide feedback. Hear a word you’re not familiar with? Ask what it is. See something go by too fast to tell what it was? Ask to see it again. Notice your pair jumping in and taking over the controls? Point out they took over, and ask if they can help you do the same thing instead.\n\nJust because someone has more experience than you doesn’t mean they’re automatically right. In fact, having more experience in some areas doesn’t mean they have more experience than you in every area. When pairing with devs who seemed to know more than me about everything, I have and discovered  strengths and specializations I didn’t know I had. When pairing with devs just a few weeks into coding, I have regularly discovered that they have many kinds of experience and knowledge that I don’t.\n\nThe bigger the experience difference is, the more important it is for both of you to make sure that your time working together is informative and productive for _both_ of you, not just the supposed bigshot. Ask questions, give feedback, and work together to calibrate the speed and depth of your work so that it’s beneficial. You might feel like a burden, but trust me… the best part of pairing with someone less experienced is being able to introduce them to new things!\n\nAnother thing to keep in mind when pairing with someone more experienced is that they will know the answers to many of your questions. Rather than going straight to the API docs, or straight to Google, ask the questions you have. Humans are by far the best interface to information that you don’t understand yet. Google can answer your question, but you have to know how to phrase your question to find the right results. People can help you figure out how to phrase your question, usually with much less flailing around.\n\nIf you run across something you don’t understand, ask about it. Maybe you’ll find out the answer. Maybe you’ll figure out the answer together. Maybe you’ll find out that even experienced people don’t know a lot of stuff. Probably, you’ll do all three. Take advantage of the experience that you have access to, and work together to reach a shared understanding of concepts and code. That’s what pairing is all about.\n\n### Pairing with someone less experienced\n\nPairing with a dev less experienced than yourself offers a chance to let them drive, helps you catch gaps in your planning, and makes you better at understanding and communicating your own ideas. Even though you’re more experienced, this is still a chance for you to learn, grow, and practice your skills. These are some tactics I’ve picked up that can make pairing with someone less experienced more positive, cooperative, and productive.\n\nBefore you start, establish cooperative, non-judgmental ground rules. You're going to be working together on two goals: one is getting the work done, but the other goal is giving the less experienced developer more experience.\n\nMake it clear that computers are hard, you often don’t know what to do, and if they don’t know what to do while you’re pairing that is totally okay. If they pause, wait for them! If they pause for more than (say) 10 seconds, let them know that you're happy to wait for as long as they need, but you're also happy to start talking about what you could do next if they are feeling stuck.\n\nMake it as hard as possible for yourself to jump in or take over. I have personally gotten a lot of use out of fidget toys or grabbing a giant stuffed animal to hug. In cases where my self-control is bad, I have sometimes even unplugged my keyboard and mouse. It's easy to not type when your keyboard doesn't work at all!\n\nFinally, keep in mind that all of this work is ultimately a chance for you to level up yourself. High level independent contributor work depends on clear, persuasive communication. Pairing is a prime opportunity to practice and hone those skills. Coordinating development work across teams, gaining support for your policy proposal, encouraging developers to adopt good practices, and many other tasks—all require excellent communication skills.\n\n### Programming as relationships\n\nUltimately, software is made _by_ people, _for_ people. Every line of code is the result of some relationship between human beings. Working with a team means planning, communicating, discussing, compromising, debugging, and more. Even though it can be hard to tell when your up to your eyeballs in code, all programming is a relationship, with stakeholders, customers, managers, and coworkers. Pair programming makes more of that relationship direct and explicit, including both the benefits and the hazards that come from closer relationships.\n",
				"date_published": "2018-04-26T00:00:00-08:00",
				"url": "https://andre.arko.net/2018/04/26/pairing-a-guide-to-fruitful/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2017/11/16/a-history-of-bundles-to/",
				"title": "A History of Bundles: 2010 to 2017",
				"content_html": "<h3 class=\"subtitle\">a one-person oral history of Ruby’s dependency manager</h3>\n<p><small>This post was originally given as a presentation at <a href=\"http://rubyconf.org\">RubyConf 2017</a>. The <a href=\"https://speakerdeck.com/indirect/a-history-of-bundles-2010-to-2017\">slides</a> and <a href=\"https://www.youtube.com/watch?v=BXFYjO8qDxk\">video</a> are also available.</small></p>\n<script async class=\"speakerdeck-embed\" data-id=\"18db05d85be142e0a3c125547eeb3098\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n<p>When Bundler 1.0 came out in 2010, it did something really great: installed all of your gems and let you use them in your app. Today, Bundler does something really great: it installs all your gems and lets you use them. So, given that, why has Bundler needed thousands upon thousands of hours of development work? What exactly has changed since then? Prepare to find out.</p>\n<p>This post combines a historical retrospective with a guide through Bundler’s advanced features, which were almost all added in releases after 1.0. Since Bundler has been actively developed for almost the entirety of the seven years that it’s been around, there are a lot of things to learn about.</p>\n<p>By the time you’ve finished reading this, you’ll have a better understanding of why Bundler has needed ongoing development, what that ongoing development has accomplished, and how to use Bundler’s advanced features to help you with their own work and projects. So! Let’s get started.</p>\n<h3 id=\"the-road-to-10-2008-2010\">The Road to 1.0 (2008-2010)</h3>\n<p>When Ruby was first released, sharing code with other developers meant copying files by hand and then using  <code>require</code> and the <code>$LOAD_PATH</code>. It was a lot of work, and &ldquo;versions&rdquo; meant comments inside the files you had downloaded from the author&rsquo;s personal website.</p>\n<p>Later, Jim Weirich, Chad Fowler, and Rich Hickey worked together, combining their forces for the power of good, and created RubyGems. With RubyGems, any library could be installed with a single command, and required and used immediately. It was immensely easier than manually downloading tarballs full of Ruby files and requiring them. It was so much better that we were happy with it for a long time.</p>\n<p>After a few years, we noticed that while installing gems was easy, using them in an application was unpleasantly hard. When gems released new versions, the next developer or server to install that gem would get the new version—even if that broke the app. This caused a lot of pain, and for a while the most popular approach was to commit all of your gems into your git repo, since at least then you knew they would be the same on another machine.</p>\n<p>That world gave birth to Bundler, the application dependency manager. Today, managing app dependencies with Bundler is taken for granted, and everyone does it. However! In the long ago times (like 2008) there was no such thing as dependency management. There was simply installing some gems, running your development server, and crossing your fingers. To hear more details about how sharing Ruby code evolved over time, check out my talk from last year, titled <a href=\"/2015/04/28/how-does-bundler-work-anyway/\">How Does Bundler Work, Anyway?</a>.</p>\n<p>Despite being used in nearly every Ruby application and script today, Bundler was developed in response to a specific developer need: web applications with many complicated gem dependencies, especially frameworks composed of many gems. When Bundler was first prototyped, that framework was Merb.</p>\n<p>As time went on, the Merb and Rails teams agreed to merge, and the framework Bundler was being designed for switched to Rails 3. At the time Bundler launched, a default Rails app needed something like 18 gems. Today that number is closer to 30—which that is only possible because of Bundler.</p>\n<p>There were a couple of specific insights driving the development of Bundler as a tool: first, that an <em>install-time dependency resolver</em> was needed. Second, the resolution process had to produce a <em>lockfile</em> that could then be used to repeatably install the exact same gems on another machine at a later time.</p>\n<p>So what is a <em>dependency resolver</em>? Put simply, it takes the list of gems that you have asked for, asks those gems what gems they need, asks <em>those</em> gems which gems they need, and so on. Eventually, it has a complete list of every gem that could possibly be needed. At that point, it checks every <em>version requirement</em> to make sure that they are all compatible. For example, one gem might depend on <code>rack &gt; 1.0</code>. Another gem might depend on <code>rack &lt;= 2.2</code>. Those requirements are compatible, since versions like <code>1.1.1</code> or <code>2.0.4</code> will meet both of them.</p>\n<p>What about <em>install-time</em>? If you do your dependency resolution while you are installing gems, before you are running your application, it is possible to flag problems in advance. If you are resolving dependencies after the application is already running, it might be too late. For example, if you run a certain version of the Thin web server, and then try to load ActiveSupport, your app will always crash. It crashes because Thin can only use one version of Rack, and ActiveSupport can only use a different, incompatible version. As you can imagine, finding out about these sorts of problems before you deploy your application to your production servers can be extremely useful.</p>\n<p>Finally, when we talk about the <em>lockfile</em>, we mean that the resolved dependencies need to be written down somewhere, so that those exact gems and versions can be installed again later. Those written down gem names and versions make up the “bundle” that gives its name to the Bundler gem. Installing and running Ruby software in a deterministic and repeatable way is the goal behind the entire Bundler project.</p>\n<p>The tooling built on top of those concepts is almost entirely recognizable today, almost ten years later: devs put gems into a <code>Gemfile</code>, they run <code>bundle install</code>, and then they use <code>bundle exec foo</code> to run the <code>foo</code> command inside their bundle.</p>\n<p>One especially nice feature, for the time it came out, was the ability to use gems directly from git repos. Before Bundler, using a gem before it was released was a huge hassle. Once you had the changes in git, you still had to build a .gem file from those changes and then run your own gem server just for that version, or release a new gem with a different name and switch to depending on that new gem instead.</p>\n<p>GitHub tried to help with this problem by automatically creating .gems from any repo that contained a gem. The new problem became GitHub’s explosion of gems: every time someone forked a gem, GutHub had to add a new gem named <code>username-gemname</code> to their server. Even worse, public gems that depended on these per-user forks needed both RubyGems and GitHub to be up at the same time to install their gems. Thanks partly to Bundler’s support for git gems, GitHub decided to shut down their gem server, and removed it entirely a few years ago.</p>\n<p>To encourage developers to create their own gems, and feel comfortable forking and editing other gems, Bundler 1.0 included tools for creating, building, and releasing gems. The <code>bundle gem</code> command generates a new empty gem, and the Bundler gem helpers provide <code>rake build</code> and <code>rake release</code> tasks. Today, not only are most gems installed with Bundler, but most gems are created with Bundler as well.</p>\n<p>While using Bundler to create and manage gems might feel obvious and natural nowadays, using early versions of Bundler felt unnatural or unnecessary to many Ruby devs. The entire concept of Bundler was met with a lot of resistance, andt he Bundler team spent a lot of time discussing, arguing, debating, and cajoling developers on the internet. It was so non-obvious, in fact, that I gave <a href=\"http://andre.arko.net/2010/06/09/railsconf-2010-bundler-talk-slides/\">an entire talk at RubyConf 2010 arguing that Bundler was actually worth using</a>.</p>\n<h3 id=\"now-its-too-slow-2010-2012\">Now It’s Too Slow (2010-2012)</h3>\n<p>Fortunately, within a couple of years the community resistance had largely died down. Bundler had proven itself to be a hugely useful tool in the day-to-day workflow of many developers and companies. Once Bundler started to see widespread adoption, there was a new problem to address: many users means many edge-cases. For several months, the entire Bundler team focused on fixing bugs, handling tricky edge cases, and trying to keep things working as more and more users appeared. Finally, once things had settled into a relatively steady state, the Bundler team started hearing about a new problem: installing gems is really, really slow.</p>\n<p>Part of that was because while making and shipping Bundler 1.0, we had focused almost exclusively on making it work. We did complete ground-up rewrites of how Bundler worked internally moving from 0.7 to 0.8, and then again from 0.8 to 0.9, and finally another time moving from 0.9 to 1.0. By the time we were trying to ship 1.0, it was almost entirely about being excited that we had something that actually worked to resolve, install, and isolate gems for an application.</p>\n<p>On top of not focusing on performance, when Bundler was still new it wasn’t yet being used by huge, old applications. After we shipped 1.0 and promised stability and backwards compatibility, many more applications started using Bundler. The size of a “typical” Gemfile went up very quickly during the first few months and years of Bundler’s existence. Today, it’s not uncommon to see businesses built around Ruby webapps that have not just 200 or 300 gems, but sometimes 500 or 600 gems! We could never have imagined a single application with that many dependencies while we were trying to ship Bundler 1.0.</p>\n<p>Since Bundler was slow while installing big applications, you might think that would mean it was at least fast installing small applications. Unfortunately, that wasn’t true either. Even if your Gemfile only had one gem in it, and that gem had no dependencies, we still had to download the list of every gem in existence from RubyGems.org. So we were in a place where small installs were slow because of downloading more data than we needed, and big installs were slow because we had never optimized for installing hundreds of gems.</p>\n<p>At this point, while the Bundler team was mostly sitting around discussing possible ways to try and solve the problem, someone else decided to simply do it. Nick Quaranto (the original creator of the RubyGems.org app) pragmatically wrote a new API for RubyGems.org, shipped it, and let us know that we could use it. Instead of returning information about every gem in existence, it only returned information about the gems it was asked about. If you’re interested in the details of the new API, I gave a talk at Ruby on Ales 2012 with Terence Lee about <a href=\"http://confreaks.tv/videos/roa2012-bundle-install-y-u-so-slow\">the process of reworking Bundler to use the new API</a>.</p>\n<p>If you had a fast connection, or a small Gemfile, this was <em>way</em> faster. The catch to this speed-up was that Bundler now needs to make many individual requests to the server. If your Gemfile said Rails, Bundler would ask the server about Rails, but then it would learn it needed to ask about ActionPack, and then need to ask about Rack, etc. As long as you could reach the RubyGems servers quickly, making more requests was much faster than downloading lots of unneeded data.</p>\n<p>If the RubyGems servers were far away, however, which they were for anyone outside the US, it was either the same speed or slower. For apps with huge numbers of dependencies, it was <em>much</em> slower. If you lived in Europe, it was generally no faster, and if you lived in Japan, Australia, or had a wireless connection in the US, it was usually a little slower. If you lived in Africa, you could forget about it. I heard from more than one South African dev that they could run <code>bundle install</code>, go make a cup of coffee, and drink most of it before Bundler could finish.</p>\n<p>That slowness was a problem, since we wanted installing gems to be faster for everyone. In response to these issues, the Bundler team started working on a new index format—some way to install gems without needing to either make many requests or download metadata about every gem that exists. It took almost three years to finish that new format, so we’ll come back to it later.</p>\n<p>During this period, we also continued to develop Bundler, fixing bugs, adding features, and trying to make it better for everyone who writes Ruby. Some especially notable features from this era include:</p>\n<p>The <code>clean</code> command, which removes installed but unused gems after the Gemfile changes. Before <code>clean</code> was available, CI systems and platforms like Heroku had a problem: installing all gems for every new commit is slow, but installing new gems on top of old ones meant unused old gems would stick around, even if the app didn’t need them anymore. Adding <code>bundle clean</code> meant reusing installed gems without unused gems piling up over time.</p>\n<p>The <code>outdated</code>command allowed users to see which gems in their Bundle has new versions, without having to update to those versions. It also surfaced updates that were not allowed by the version requirements in the Gemfile, alerting devs even when an <code>update</code> would have ignored the new version.</p>\n<p>We expanded the <code>cache</code> (aka <code>package</code>) command to include git gems when asked, allowing users to create a single directory containing everything they need to install their app on another machine even without access to RubyGems or GitHub.</p>\n<p>As part of improving support for Git gems, we also added support for developing dependencies locally. By running <code>bundle config local.rack ~/src/rack/rack</code>, you can tell your application to use your local git checkout of rack instead of installing the rack gem. Even better, Bundler will update your lockfile with the latest commit in that repository, ensuring that when you deploy later, you’ll get the exact same code. And if you forgot to push to the dependency repo, like I usually do, Bundler will let you know it is missing, which is awesome.</p>\n<p>Finally, we added support for Ruby versions inside the Gemfile. If you want to make sure that all of your devs and your production servers are all running the exact same version of Ruby, you can do that as simply as adding <code>ruby</code> to your Gemfile. This feature wound up causing some problems, but we’ll get to that later. For now, let’s move on to the next era in Bundler history, where we’ll discover many ways that fixes from this era caused their own, new problems.</p>\n<h3 id=\"victims-of-our-own-success-2012-2014\">Victims of Our Own Success (2012-2014)</h3>\n<p>The biggest thing that happened during this era is that Bundler adoption really took off. Bundler 1.0 came out in August of 2010, and averaged 8,700 downloads per day. Bundler 1.1 was finally released in March 2012, and it averaged 20,000 downloads per day. By the time Bundler 1.2 came out in August 2012, it was averaging 30,000 downloads per day.</p>\n<p>The growing number of Bundler users slowly built up until October 2012, when we discovered that Bundler was effectively running a DDoS attack against RubyGems.org when the servers went down, hard. There was no way for the existing architecture to handle the huge number of requests coming in at all times. We had to completely disable the dependency API, and Bundler went back to being slow.</p>\n<p>At this point, a team including myself, Terence Lee, Larry Marburger, and others, took the time to design, implement, deploy, and scale a separate Bundler API web application to serve the dependency API for Bundler users. With the cooperation of the RubyGems.org team, including Evan Phoenix and David Radcliffe, we were even able to make the original API urls continue to work.</p>\n<blockquote>\n<p>Some people, when confronted with a problem, think “I know, I&rsquo;ll write a webapp and throw it up on Heroku.”  Now they have two problems.</p>\n</blockquote>\n<p>(Apologies to Jamie Zawinski for mangling his aphorism about regular expressions.)</p>\n<p>As you may have guessed, this did provide an API for Bundler users, but it came with a completely new set of problems! One of the problems was that our separate web application had a completely separate database from RubyGems.org itself. We tried subscribing to RubyGems.org webhooks to be notified every time there was a new gem, but sometimes the webhooks failed. We tried scraping the API for every gem every night, and we hit the API rate limits. We tried to import a database dump, and wound up with data that didn’t quite match up with the live data in RubyGems.org.</p>\n<p>In addition to the challenges of syncing to a continuously-updating data set, no matter what we did there was always a propagation delay between pushing a gem and being able to install that gem using Bundler. You might not think that is something that people do too frequently, but anytime replication fell behind we would see many new tickets complaining about not being able to install newly pushed gems within a few seconds. And then there was the CDN propagation delay. Some days, if you lived in Canada, it took 3 hours to see new gems after they were pushed, and there was nothing we could do about it.</p>\n<p>On top of that, the standalone API was written on top of Sinatra and Sequel. The API application was extremely small, and I think it was a completely reasonable decision to make it a tiny app in a tiny framework. The downside that we weren’t expecting was existing contributors to RubyGems.org (or even developers who wrote Rails apps for their dayjob) weren’t easily able to contribute.</p>\n<p>The story of creating the Bundler API, deploying it, and then scaling it up to handle the traffic from every Bundler user in the world is a lot longer than I have time to fit into this talk. If you’re interested, you can find out a lot from my talk <a href=\"http://andre.arko.net/2013/05/12/deathmatch-bundler-vs-rubygemsorg/\">Deathmatch: Bundler vs RubyGems</a> or Terence Lee’s talk <a href=\"http://rubykaigi.org/2013/talk/S54\">Bundler Y U So Slow: Server Edition</a>.</p>\n<p>While we had a lot of additional work to do, the growing popularity of Bundler meant that it had many more users, and some of those new users turned into new contributors. With the help of new core team members, we were able to ship several significant improvements to Bundler in addition to the new API service.</p>\n<p>The biggest new feature was the addition of threaded downloading and installation of gems. Using every core of a multi-core CPU meant dramatically faster installs. Bundler was finally IO-bound, by the network and disks, rather than downloading, decompressing, and installing just one gem at a time.</p>\n<p>We also rewrote the dependency resolver at this point, refactoring it to stop using recursion thanks to a contribution from Smit Shah. On Ruby 1.8 and 1.9, the recursion was usually not a problem, but on JRuby, where stack frames take up more memory, the thousands of recursive calls could easily overflow all available memory and cause Bundler to crash.</p>\n<p>This time period is also when Git and GitHub added support for using git over HTTP instead of only over SSH. Hoping to take advantage of the ways that HTTP git operations can be faster than the same operations over SSH, Bundler added support for HTTP authentication during git operations.</p>\n<p>Last, and possibly saddest, Bundler had its very first CVE. If you’re interested, I’ve given another talk <a href=\"http://andre.arko.net/2013/08/22/security-is-hard-but-we-cant-go-shopping/\">on security and the background behind CVEs</a>. The short version is that a CVE means that your software has a critical security issue. 😰 In our case, the critical security issue was that we allowed multiple <code>source</code> declarations inside a Gemfile, and simply looked inside every source for every gem that we needed. Unfortunately, since anyone can claim a gem on RubyGems.org, the possible name conflicts create a security risk.</p>\n<p>If you run a private gemserver at your company, and use a private gem that you have named <code>my-cool-thing</code>, someone else could push a gem named <code>my-cool-thing</code> to RubyGems.org, and you might (suddenly, and without warning) start downloading and installing and running the code from that gem, which might be malicious. We <a href=\"http://bundler.io/blog/2014/08/14/bundler-may-install-gems-from-a-different-source-than-expected-cve-2013-0334.html\">blogged about the problem, and tried to fix it</a>, but in the end the only way to be sure that the problem can’t ever happen is to stop allowing more than one <code>source</code> for any gems inside your Gemfile.</p>\n<p>You can still use other sources, but you have to tie any additional sources to a particular gem. Then, Bundler will only get that gem from that source, and not use that source for any other gems. Well, that’s actually something of a simplification because of the complications added by gems in one source that depend on gems in another source, but I think it’s close enough for this discussion.</p>\n<h3 id=\"a-new-hope-2015-2017\">A New Hope (2015-2017)</h3>\n<p>While we had finally accomplished our goal of a separate web service to make installing fast for Bundler users, having a separate API sucked. The API was an optimization, and so gems could still be installed if it was down, but any downtime meant a lot of upset and complaining users. Trying to keep the API up meant that the Bundler team was suddenly on call <em>all the time</em>. It was exciting to learn about devops, but keeping everything running was a huge source of stress for years.</p>\n<p>No one wants to deal with a lot of stress for years at a time, especially not , and so this period also saw several RubyGems.org and Bundler contributors slowly burn out and drift away. Fortunately, as that was happening, the Ruby community came together and started paying developers to work on the gem infrastructure that everyone uses.</p>\n<p>First, Ruby Central provided grants for work on RubyGems.org, Bundler, and RubyGems. Thanks to time paid for by Ruby Central, myself and others were able to finish new releases, continue development work on the compact index format, and much more.</p>\n<p>In addition to grants from Ruby Central, Stripe also started an open source grants program. One of their grants went to a college student named Samuel Giddins. As an iOS developer, he had started contributing to CocoaPods, the application dependency manager for Objective-C. Since CocoaPods was written in Ruby, his Stripe grant was able to fund development work on a completely new dependency resolver, written from the ground up to be more easily maintained. It was also written to be used by multiple projects. Today, Sam’s resolver library Molinillo is used not just by CocoaPods, but also by Bundler, by RubyGems itself, and by Berkshelf, the Chef dependency manager.</p>\n<p>Around the same time, Stripe and Engine Yard started funding the Bundler project, allowing us to <a href=\"https://rubytogether.org/news/2015-03-17-announcing-ruby-together\">incorporate the first Ruby trade association, Ruby Together</a>. Ruby Together is a non-profit dedicated to funding open source Ruby development using funds raised from developers and companies in the community. It has slowly grown over the years, and today Ruby Together pays for regular developer time spent on Bundler, RubyGems, the RubyGems.org Rails app, the Gemstash gem server and mirror, ops work on the RubyGems.org servers, and even the new Ruby Toolbox 2.0 open source project.</p>\n<p>While Ruby Central has given grants for specific projects, and continues to pay the server bills for RubyGems.org, they do not fund developers to do ongoing maintenance on the tools we use every day. As Ruby Together grows, it will be able to fund even more developer time, so please <a href=\"https://rubytogether.org/developers\">join as a developer</a> or <a href=\"https://rubytogether.org/companies\">join as a company</a> today. We want to be able to start supporting even more of the Ruby projects that the entire community depends on.</p>\n<p>With support from Ruby Central, Stripe, and Ruby devs and companies around the world via Ruby Together, the Bundler and RubyGems projects started to see work done by paid devs. Probably not too surprisingly, this resulted in much more regular, consistent development work. That, in turn, meant we were able to ship projects that had been in progress for years.</p>\n<p>The first project that we were able to finish thanks to paid dev work was migrating the entirety of RubyGems.org to run behind the Fastly CDN. This means that whenever you or your computer makes a request to <a href=\"https://rubygems.org\">https://rubygems.org</a>, you are actually talking to the closest Fastly server. Since Fastly runs servers in hundreds of data centers around the world, users all around the world see dramatically faster responses. Installing gems is not longer bottlenecked by reaching around the world to the servers in AWS on the West Coast of the US.</p>\n<p>Before moving everything to be served by Fastly, the situation was pretty crappy: your computer would have to make a request all the way to the West Coast, and then that server would send back a redirect request that sent you to the closest CDN server, and then that CDN server might have the file cached, or it might have to go and get the file from our servers on the West Coast itself, and then give that file to you. As you can probably tell just listening to that description, that system was slow, and complicated, and hard to understand, and often had problems.</p>\n<p>Once that was done, we started to move the Bundler API back into the RubyGems.org Rails app. In the years since we had moved it out, the RubyGems ops team had done a great job of building a new and scalable architecture on AWS that could easily handle all of the traffic from every Bundler user. Amazingly, by the time we moved it back into the Rails app, there was already 10x more traffic than there had been when it took RubyGems.org down the first time. This time, with a paid devops team behind it, RubyGems.org was able to handle the API traffic without any issues. Today, the separate Bundler API has been shut down, and everything is served from the RubyGems.org servers.</p>\n<p>Parallel to getting RubyGems completely moved over to Fastly, the RubyGems and Bundler teams were working to complete the long-awaited compact index format. In short, it is a plain-text format, with one file listing every gem name and version number, and one file per gem listing the full dependency information for each version of that gem. The text files are append-only, so that they can be cached on each machine and updated by requesting only the part of the file that comes after the part that is already cached.</p>\n<p>Even though the new format had been proved to work by a prototype I wrote, it took more than a year for myself, Sam Giddins, our Google Summer of Code student Felipe Tanus, and the rest of the Bundler and RubyGems.org teams to work together to finalize the format, write server and client libraries, and release. For more information about the compact index and related changes, check out the talk <a href=\"http://andre.arko.net/2013/12/09/extreme-makeover-rubygems-edition/\">Extreme Makeover: RubyGems Edition</a> from RubyConf 2013.</p>\n<p>By combining the power of Fastly’s CDN and the caching strategy of the compact index, installing gems became faster again, no matter where you lived in the world. Today, most of the time in a <code>bundle install</code> run is actually installing gems, rather than resolving complicated gemfiles or downloading information about gems.</p>\n<p>Combining all sources of community funding, we have been able to average something like 10 or 15 hours per week of paid development time consistently spent on Bundler, RubyGems, and RubyGems.org. We’re still pretty far away from being able to employ developers to work full time on Ruby infrastructure, but even those few hours have enabled us to get a lot more done.</p>\n<p>In addition to the Fastly migration, the completely new dependency resolver, and the completely new compact index format, we shipped a lot of features in Bundler itself. Here are some of the highlights:</p>\n<p>After many years of discussion around <a href=\"http://blog.hasmanythrough.com/2011/12/1/i-heard-you-liked-files\">Filefiles</a> and <a href=\"https://github.com/bundler/bundler/issues/694\">the misleading name of Gemfile.lock</a>, we added support for different filenames: <code>gems.rb</code> and <code>gems.locked</code>. With those filenames, it’s unambiguous that those files list gems, that one is written in Ruby, and that one contains the locked gems. No more confusing new developers about what the <code>.lock</code> extension means! The new filenames are supported today, if you want to change your existing projects. In Bundler 2.0, we will switch the default file created by <code>bundle init</code> to be <code>gems.rb</code>, but both filenames will continue to be support through at least the entire Bundler 2.0 lifecycle.</p>\n<p>As I alluded to earlier, we discovered some problems with the <code>ruby</code> directive in Gemfiles. Namely, it was too specific, and didn’t allow setting a range of allowed ruby versions. We extended the <code>ruby</code> directive to support version requirements just like gems, and now the exact ruby version is recorded in the lockfile. This makes it possible to manage ruby version upgrades just like you manage gem versions, which is pretty nice.</p>\n<p>Now that we record the expected Ruby version in the lockfile, it’s also possible to support the Ruby versions required by individual gems while resolving gemfiles. If you’ve ever seen an error while installing a gem that your version of Ruby is not supported, that is completely fixed in the latest versions of Bundler.</p>\n<p>We also added new commands, including <code>lock</code> to resolve your gem versions and write a lockfile without installing those gems. We also extended the lock to support individual platforms, making it possible to lock a single application on both Unix systems and Windows systems at the same time.</p>\n<p>Another new command, <code>doctor</code>, created by Misty DeMeo, tries to help users figure out what could have gone wrong, including gems not installed, gems with native extensions that haven’t been built, and other possible problems.</p>\n<p>The new <code>bundle pristine</code> command works just like <code>gem pristine</code> but for the gems in your application bundle, including git gems. If you’ve ever edited an installed gem as part of debugging an issue, the <code>pristine</code> command is extremely handy for undoing those changes and going back to the factory-fresh gem files.</p>\n<p>The <code>add</code> command works like <code>npm install --save</code>, putting a new line in your Gemfile, doing a full resolution run, and then installing any new gems. It dramatically speeds up the early stages of a project when you’re adding many gems quickly in a short period of time.</p>\n<p>The <code>update</code> command, while not new, got a significant overhaul.  It now supports options that let you limit what kind of version upgrades you want to see. You can pass <code>--major</code>, <code>--minor</code>, or <code>--patch</code> in order to get only upgrades at that level.</p>\n<p>For users who run gem mirrors or proxies, like Squid, Varnish or the Bundler team’s Gemstash server, it is now possible to configure Bundler to use mirrors automatically, without editing your Gemfile. After configuring a mirror, Bundler will automatically try to use the configured mirror instead of the URL listed in your Gemfile. This makes it possible to run a Gemstash or other mirror locally in an office or datacenter, greatly speeding up install operations.</p>\n<p>We shipped a beta version of the plugin system, allowing other developers to provide new Bundler commands, hooks that run when gems are installed or updated, and even new gem sources. If you’ve always wanted to be able to install gems from mercurial repositories, you can write a plugin to make that happen.</p>\n<p>Finally, in a nice quality of life and security improvement, Bundler now has checksums for each .gem file as part of the compact index. At install time, Bundler uses those checksums to make sure that it is installing the correct gem, and the file wasn’t corrupted in transit.</p>\n<h3 id=\"the-future-2017-\">The Future (2017-????)</h3>\n<p>Today, we’ve just shipped Bundler 1.16 with all of the features I mentioned above. We’re actively working on Bundler 2.0, with a target release date (which admittedly might slip) of Christmas 2017. I don’t have room in this talk to include details about 2.0, but I can say that we value compatibility extremely highly.</p>\n<p>While we plan to make breaking changes in 2.0, we want to make it easy to continue to use applications that use both 1.x and 2.x on the same machine. You’ll be able to upgrade each application individually, and at your own pace. For more information about planned changes in Bundler 2, check out Colby Swandale’s talk <a href=\"http://rubykaigi.org/2017/presentations/0xColby.html\">Bundler 2</a>, from Ruby Kaigi 2017.</p>\n<p>In the meantime, here are some Bundler best practices that you can use to get the benefits of Bundler 2.0 today! First, you probably want to set the config option <code>only_update_to_newer_versions</code> to true. That setting changes the <code>update</code> command to ensure that you will never run <code>update</code> and end up with an older version of a gem than you already have. That option will be turned on by default in Bundler 2.0.</p>\n<p>You also probably want to turn on the <code>disable_multisource</code> setting. As I mentioned earlier in this talk, it’s fundamentally unsafe to have multiple sources in a single gemfile. We can’t raise an error by default because of existing users, but you can opt in to that option for yourself, and the option will be turned on by default when we release 2.0.</p>\n<p>If you develop or deploy on more than one platform, especially if some of your developers or servers run Windows, you also likely want to enable the <code>specific_platform</code> option. That turns on our next-generation platform support, allowing Bundler to resolve each platform separately and install precompiled gems for the platform it is installing onto, if precompiled gems exist.</p>\n<p>We’ve also implemented much more extensive shared caches. It’s now possible to share .gem files and compiled extensions between applications that have the same gems, by turning on <code>global_gem_cache</code>. That change pairs extremely will with another change that will be on by default in 2.0: <code>default_install_uses_path</code>. When you turn that on, Bundler will install gems separately for each application, ensuring that RubyGems never has to deal with loading unused gems only to ignore them. Combining this with the global gem cache gives every app its own set of gems without having to download or compile gems multiple times on a single machine. It’s pretty great.</p>\n<p>Finally, while we can’t force existing users to connect to github via HTTPS, because that would break backwards compatibility, you can make that change yourself today. Include this snippet at the top of your Gemfile, and all of your <code>github</code> gems will use HTTPS to connect to GitHub:</p>\n<pre><code>git_source(:github) {|repo_name| &quot;https://github.com/#{repo_name}&quot; }\n</code></pre>\n<h3 id=\"fancy-workflows-and-tools\">Fancy workflows and tools</h3>\n<p>Phew! Now that we’ve caught up completely on the history of Bundler and everything that we’ve done to it over the last decade or so, let me give you a chaser of a few more handy tips and workflows that you can use to improve your Bundler experience.</p>\n<p>While you can always run a gem command using <code>bundle exec</code>, that depends on you being in the right directory or manually setting the location of the Gemfile. Instead, you can use <code>bundle binstubs GEM</code> to create an executable in <code>bin/gem</code>. You can run that file directly to load Bundler, find your application Gemfile, and load the correct version of that gem, no matter where you are on your system. That can be especially helpful for cronjobs, but is honestly just nicer and easier than using <code>bundle exec</code> all the time.</p>\n<p>If you’re interested in seeing a visual layout of your application’s gem dependencies, you can install Graphviz and then run the <code>bundle viz</code> command. If your application is small enough and simple enough, you’ll end up with a graph that looks something like this:</p>\n<a class=\"image\" href=\"https://indirect.micro.blog/uploads/2025/1eae908958.jpg\">\n  ![Default Rails app dependency visualization](https://indirect.micro.blog/uploads/2025/1eae908958.jpg)\n</a>\n<p>If you want to start running your application on a new platform, like JRuby, or Windows, you can now add that platform in advance, on any machine, by running <code>bundle lock --add PLATFORM</code>. Once you’ve done that, running your application on that platform won’t cause changes to your lockfile. While Bundler can’t guarantee identical code runs on different platforms, it can guarantee that every machine on a particular platform will run exactly the same code as every other machine on that platform.</p>\n<p>I mentioned the local git gems feature during the history section, but it’s so useful that I think it’s worth reiterating here. If you want to be able to make changes to a gem and immediately try out those changes in your application that depends on that gem, you can! Change the gem in your Gemfile to a git gem, and then run <code>bundle config local.GEM ~/path/to/checkout</code>. On that machine, Bundler will use that checkout instead of downloading and installing that gem when your application runs. As you make changes to the local checkout, Bundler will update your application lockfile to include the SHA of your latest commit to that gem, ensuring that other developers and production servers will get your changes immediately.</p>\n<p>Ever wanted to write a simple one-file script, but the script depends on some gems? You can use <code>bundler/inline</code> to write scripts that have bundled gems. Here’s what the code looks like.</p>\n<pre tabindex=\"0\"><code>$ cat http.rb\nrequire &#39;bundler/inline&#39;\ngemfile do\n  source &#39;https://rubygems.org&#39;\n  gem &#39;http&#39;\nend\nputs HTTP.get(&#39;http://example.com&#39;)\n</code></pre><p>Here’s what running that code looks like, including installing the <code>http</code> gem as part of running the script.</p>\n<pre tabindex=\"0\"><code>$ gem uninstall http\nSuccessfully uninstalled http-3.0.0\n\n$ ruby http.rb\n&lt;!doctype html&gt;\n&lt;html&gt;[…]\n&lt;body&gt;\n&lt;div&gt;\n    &lt;h1&gt;Example Domain&lt;/h1&gt;\n    &lt;p&gt;This domain is established to be used for illustrative examples in documents. You may use this\n    domain in examples without prior coordination or asking for permission.&lt;/p&gt;\n    &lt;p&gt;&lt;a href=&#34;http://www.iana.org/domains/example&#34;&gt;More information...&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre><p>In a pair of related hints, you can easily search through the code of all of the gems in your bundle by using <code>bundle show --paths</code>. For example, if your searching tool of choice is <code>ripgrep</code>, you can run <code>rg STRING $(bundle show --paths)</code> to find <code>STRING</code> anywhere in your bundled gems. Once you’ve found the gem you care about, you can open it directly in your editor by running <code>bundle open GEM</code>. Bundler will respect the <code>$EDITOR</code> environment variable, if you have set it. After you’ve edited the installed gem as much as needed for debugging, you can remove the changes you’ve made by running <code>bundle pristine GEM</code>.</p>\n<p>Finally, in my personal favorite quality of life improvement, it is now possible to disable gem post-install messages by running <code>bundle config --system ignore_messages true</code>. Now, you can never be told to HTTParty hard, ever again.</p>\n<h3 id=\"the-end\">The End</h3>\n<p>And with that, we’ve finished our journey through a decade of Bundler history and features! If there’s anything you’re confused about, or if I left out your favorite Bundler feature or trick, let me know on Twitter, where I am <a href=\"https://twitter.com/indirect\">@indirect</a>. If you have questions about Bundler, I encourage you to <a href=\"http://slack.bundler.io\">join the Bundler Slack</a>, where the Bundler, RubyGems, and RubyGems.org teams, contributors, and users all hang out. We’d love to hear from you!</p>\n",
				"content_text": "<h3 class=\"subtitle\">a one-person oral history of Ruby’s dependency manager</h3>\n\n<small>This post was originally given as a presentation at [RubyConf 2017](http://rubyconf.org). The [slides](https://speakerdeck.com/indirect/a-history-of-bundles-2010-to-2017) and [video](https://www.youtube.com/watch?v=BXFYjO8qDxk) are also available.</small>\n\n<script async class=\"speakerdeck-embed\" data-id=\"18db05d85be142e0a3c125547eeb3098\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n\nWhen Bundler 1.0 came out in 2010, it did something really great: installed all of your gems and let you use them in your app. Today, Bundler does something really great: it installs all your gems and lets you use them. So, given that, why has Bundler needed thousands upon thousands of hours of development work? What exactly has changed since then? Prepare to find out.\n\nThis post combines a historical retrospective with a guide through Bundler’s advanced features, which were almost all added in releases after 1.0. Since Bundler has been actively developed for almost the entirety of the seven years that it’s been around, there are a lot of things to learn about.\n\nBy the time you’ve finished reading this, you’ll have a better understanding of why Bundler has needed ongoing development, what that ongoing development has accomplished, and how to use Bundler’s advanced features to help you with their own work and projects. So! Let’s get started.\n\n### The Road to 1.0 (2008-2010)\nWhen Ruby was first released, sharing code with other developers meant copying files by hand and then using  `require` and the `$LOAD_PATH`. It was a lot of work, and \"versions\" meant comments inside the files you had downloaded from the author's personal website.\n\nLater, Jim Weirich, Chad Fowler, and Rich Hickey worked together, combining their forces for the power of good, and created RubyGems. With RubyGems, any library could be installed with a single command, and required and used immediately. It was immensely easier than manually downloading tarballs full of Ruby files and requiring them. It was so much better that we were happy with it for a long time.\n\nAfter a few years, we noticed that while installing gems was easy, using them in an application was unpleasantly hard. When gems released new versions, the next developer or server to install that gem would get the new version—even if that broke the app. This caused a lot of pain, and for a while the most popular approach was to commit all of your gems into your git repo, since at least then you knew they would be the same on another machine.\n\nThat world gave birth to Bundler, the application dependency manager. Today, managing app dependencies with Bundler is taken for granted, and everyone does it. However! In the long ago times (like 2008) there was no such thing as dependency management. There was simply installing some gems, running your development server, and crossing your fingers. To hear more details about how sharing Ruby code evolved over time, check out my talk from last year, titled [How Does Bundler Work, Anyway?](/2015/04/28/how-does-bundler-work-anyway/).\n  \nDespite being used in nearly every Ruby application and script today, Bundler was developed in response to a specific developer need: web applications with many complicated gem dependencies, especially frameworks composed of many gems. When Bundler was first prototyped, that framework was Merb. \n\nAs time went on, the Merb and Rails teams agreed to merge, and the framework Bundler was being designed for switched to Rails 3. At the time Bundler launched, a default Rails app needed something like 18 gems. Today that number is closer to 30—which that is only possible because of Bundler.\n\nThere were a couple of specific insights driving the development of Bundler as a tool: first, that an _install-time dependency resolver_ was needed. Second, the resolution process had to produce a _lockfile_ that could then be used to repeatably install the exact same gems on another machine at a later time. \n\nSo what is a _dependency resolver_? Put simply, it takes the list of gems that you have asked for, asks those gems what gems they need, asks _those_ gems which gems they need, and so on. Eventually, it has a complete list of every gem that could possibly be needed. At that point, it checks every _version requirement_ to make sure that they are all compatible. For example, one gem might depend on `rack > 1.0`. Another gem might depend on `rack <= 2.2`. Those requirements are compatible, since versions like `1.1.1` or `2.0.4` will meet both of them.\n\nWhat about _install-time_? If you do your dependency resolution while you are installing gems, before you are running your application, it is possible to flag problems in advance. If you are resolving dependencies after the application is already running, it might be too late. For example, if you run a certain version of the Thin web server, and then try to load ActiveSupport, your app will always crash. It crashes because Thin can only use one version of Rack, and ActiveSupport can only use a different, incompatible version. As you can imagine, finding out about these sorts of problems before you deploy your application to your production servers can be extremely useful.\n\nFinally, when we talk about the _lockfile_, we mean that the resolved dependencies need to be written down somewhere, so that those exact gems and versions can be installed again later. Those written down gem names and versions make up the “bundle” that gives its name to the Bundler gem. Installing and running Ruby software in a deterministic and repeatable way is the goal behind the entire Bundler project.\n\nThe tooling built on top of those concepts is almost entirely recognizable today, almost ten years later: devs put gems into a `Gemfile`, they run `bundle install`, and then they use `bundle exec foo` to run the `foo` command inside their bundle.\n\nOne especially nice feature, for the time it came out, was the ability to use gems directly from git repos. Before Bundler, using a gem before it was released was a huge hassle. Once you had the changes in git, you still had to build a .gem file from those changes and then run your own gem server just for that version, or release a new gem with a different name and switch to depending on that new gem instead.\n\nGitHub tried to help with this problem by automatically creating .gems from any repo that contained a gem. The new problem became GitHub’s explosion of gems: every time someone forked a gem, GutHub had to add a new gem named `username-gemname` to their server. Even worse, public gems that depended on these per-user forks needed both RubyGems and GitHub to be up at the same time to install their gems. Thanks partly to Bundler’s support for git gems, GitHub decided to shut down their gem server, and removed it entirely a few years ago.\n\nTo encourage developers to create their own gems, and feel comfortable forking and editing other gems, Bundler 1.0 included tools for creating, building, and releasing gems. The `bundle gem` command generates a new empty gem, and the Bundler gem helpers provide `rake build` and `rake release` tasks. Today, not only are most gems installed with Bundler, but most gems are created with Bundler as well.\n\nWhile using Bundler to create and manage gems might feel obvious and natural nowadays, using early versions of Bundler felt unnatural or unnecessary to many Ruby devs. The entire concept of Bundler was met with a lot of resistance, andt he Bundler team spent a lot of time discussing, arguing, debating, and cajoling developers on the internet. It was so non-obvious, in fact, that I gave [an entire talk at RubyConf 2010 arguing that Bundler was actually worth using](http://andre.arko.net/2010/06/09/railsconf-2010-bundler-talk-slides/).\n\n### Now It’s Too Slow (2010-2012)\n\nFortunately, within a couple of years the community resistance had largely died down. Bundler had proven itself to be a hugely useful tool in the day-to-day workflow of many developers and companies. Once Bundler started to see widespread adoption, there was a new problem to address: many users means many edge-cases. For several months, the entire Bundler team focused on fixing bugs, handling tricky edge cases, and trying to keep things working as more and more users appeared. Finally, once things had settled into a relatively steady state, the Bundler team started hearing about a new problem: installing gems is really, really slow.\n\nPart of that was because while making and shipping Bundler 1.0, we had focused almost exclusively on making it work. We did complete ground-up rewrites of how Bundler worked internally moving from 0.7 to 0.8, and then again from 0.8 to 0.9, and finally another time moving from 0.9 to 1.0. By the time we were trying to ship 1.0, it was almost entirely about being excited that we had something that actually worked to resolve, install, and isolate gems for an application.\n\nOn top of not focusing on performance, when Bundler was still new it wasn’t yet being used by huge, old applications. After we shipped 1.0 and promised stability and backwards compatibility, many more applications started using Bundler. The size of a “typical” Gemfile went up very quickly during the first few months and years of Bundler’s existence. Today, it’s not uncommon to see businesses built around Ruby webapps that have not just 200 or 300 gems, but sometimes 500 or 600 gems! We could never have imagined a single application with that many dependencies while we were trying to ship Bundler 1.0.\n\nSince Bundler was slow while installing big applications, you might think that would mean it was at least fast installing small applications. Unfortunately, that wasn’t true either. Even if your Gemfile only had one gem in it, and that gem had no dependencies, we still had to download the list of every gem in existence from RubyGems.org. So we were in a place where small installs were slow because of downloading more data than we needed, and big installs were slow because we had never optimized for installing hundreds of gems.\n\nAt this point, while the Bundler team was mostly sitting around discussing possible ways to try and solve the problem, someone else decided to simply do it. Nick Quaranto (the original creator of the RubyGems.org app) pragmatically wrote a new API for RubyGems.org, shipped it, and let us know that we could use it. Instead of returning information about every gem in existence, it only returned information about the gems it was asked about. If you’re interested in the details of the new API, I gave a talk at Ruby on Ales 2012 with Terence Lee about [the process of reworking Bundler to use the new API](http://confreaks.tv/videos/roa2012-bundle-install-y-u-so-slow).\n\nIf you had a fast connection, or a small Gemfile, this was _way_ faster. The catch to this speed-up was that Bundler now needs to make many individual requests to the server. If your Gemfile said Rails, Bundler would ask the server about Rails, but then it would learn it needed to ask about ActionPack, and then need to ask about Rack, etc. As long as you could reach the RubyGems servers quickly, making more requests was much faster than downloading lots of unneeded data.\n\nIf the RubyGems servers were far away, however, which they were for anyone outside the US, it was either the same speed or slower. For apps with huge numbers of dependencies, it was _much_ slower. If you lived in Europe, it was generally no faster, and if you lived in Japan, Australia, or had a wireless connection in the US, it was usually a little slower. If you lived in Africa, you could forget about it. I heard from more than one South African dev that they could run `bundle install`, go make a cup of coffee, and drink most of it before Bundler could finish.\n\nThat slowness was a problem, since we wanted installing gems to be faster for everyone. In response to these issues, the Bundler team started working on a new index format—some way to install gems without needing to either make many requests or download metadata about every gem that exists. It took almost three years to finish that new format, so we’ll come back to it later.\n\nDuring this period, we also continued to develop Bundler, fixing bugs, adding features, and trying to make it better for everyone who writes Ruby. Some especially notable features from this era include:\n\nThe `clean` command, which removes installed but unused gems after the Gemfile changes. Before `clean` was available, CI systems and platforms like Heroku had a problem: installing all gems for every new commit is slow, but installing new gems on top of old ones meant unused old gems would stick around, even if the app didn’t need them anymore. Adding `bundle clean` meant reusing installed gems without unused gems piling up over time.\n\nThe `outdated`command allowed users to see which gems in their Bundle has new versions, without having to update to those versions. It also surfaced updates that were not allowed by the version requirements in the Gemfile, alerting devs even when an `update` would have ignored the new version.\n\nWe expanded the `cache` (aka `package`) command to include git gems when asked, allowing users to create a single directory containing everything they need to install their app on another machine even without access to RubyGems or GitHub.\n\nAs part of improving support for Git gems, we also added support for developing dependencies locally. By running `bundle config local.rack ~/src/rack/rack`, you can tell your application to use your local git checkout of rack instead of installing the rack gem. Even better, Bundler will update your lockfile with the latest commit in that repository, ensuring that when you deploy later, you’ll get the exact same code. And if you forgot to push to the dependency repo, like I usually do, Bundler will let you know it is missing, which is awesome.\n\nFinally, we added support for Ruby versions inside the Gemfile. If you want to make sure that all of your devs and your production servers are all running the exact same version of Ruby, you can do that as simply as adding `ruby` to your Gemfile. This feature wound up causing some problems, but we’ll get to that later. For now, let’s move on to the next era in Bundler history, where we’ll discover many ways that fixes from this era caused their own, new problems.\n\n### Victims of Our Own Success (2012-2014)\n\nThe biggest thing that happened during this era is that Bundler adoption really took off. Bundler 1.0 came out in August of 2010, and averaged 8,700 downloads per day. Bundler 1.1 was finally released in March 2012, and it averaged 20,000 downloads per day. By the time Bundler 1.2 came out in August 2012, it was averaging 30,000 downloads per day.\n\nThe growing number of Bundler users slowly built up until October 2012, when we discovered that Bundler was effectively running a DDoS attack against RubyGems.org when the servers went down, hard. There was no way for the existing architecture to handle the huge number of requests coming in at all times. We had to completely disable the dependency API, and Bundler went back to being slow.\n\nAt this point, a team including myself, Terence Lee, Larry Marburger, and others, took the time to design, implement, deploy, and scale a separate Bundler API web application to serve the dependency API for Bundler users. With the cooperation of the RubyGems.org team, including Evan Phoenix and David Radcliffe, we were even able to make the original API urls continue to work.\n\n> Some people, when confronted with a problem, think “I know, I'll write a webapp and throw it up on Heroku.”  Now they have two problems.\n\n(Apologies to Jamie Zawinski for mangling his aphorism about regular expressions.)\n\nAs you may have guessed, this did provide an API for Bundler users, but it came with a completely new set of problems! One of the problems was that our separate web application had a completely separate database from RubyGems.org itself. We tried subscribing to RubyGems.org webhooks to be notified every time there was a new gem, but sometimes the webhooks failed. We tried scraping the API for every gem every night, and we hit the API rate limits. We tried to import a database dump, and wound up with data that didn’t quite match up with the live data in RubyGems.org.\n\n In addition to the challenges of syncing to a continuously-updating data set, no matter what we did there was always a propagation delay between pushing a gem and being able to install that gem using Bundler. You might not think that is something that people do too frequently, but anytime replication fell behind we would see many new tickets complaining about not being able to install newly pushed gems within a few seconds. And then there was the CDN propagation delay. Some days, if you lived in Canada, it took 3 hours to see new gems after they were pushed, and there was nothing we could do about it.\n\nOn top of that, the standalone API was written on top of Sinatra and Sequel. The API application was extremely small, and I think it was a completely reasonable decision to make it a tiny app in a tiny framework. The downside that we weren’t expecting was existing contributors to RubyGems.org (or even developers who wrote Rails apps for their dayjob) weren’t easily able to contribute.\n\nThe story of creating the Bundler API, deploying it, and then scaling it up to handle the traffic from every Bundler user in the world is a lot longer than I have time to fit into this talk. If you’re interested, you can find out a lot from my talk [Deathmatch: Bundler vs RubyGems](http://andre.arko.net/2013/05/12/deathmatch-bundler-vs-rubygemsorg/) or Terence Lee’s talk [Bundler Y U So Slow: Server Edition](http://rubykaigi.org/2013/talk/S54).\n\nWhile we had a lot of additional work to do, the growing popularity of Bundler meant that it had many more users, and some of those new users turned into new contributors. With the help of new core team members, we were able to ship several significant improvements to Bundler in addition to the new API service.\n\nThe biggest new feature was the addition of threaded downloading and installation of gems. Using every core of a multi-core CPU meant dramatically faster installs. Bundler was finally IO-bound, by the network and disks, rather than downloading, decompressing, and installing just one gem at a time.\n\nWe also rewrote the dependency resolver at this point, refactoring it to stop using recursion thanks to a contribution from Smit Shah. On Ruby 1.8 and 1.9, the recursion was usually not a problem, but on JRuby, where stack frames take up more memory, the thousands of recursive calls could easily overflow all available memory and cause Bundler to crash.\n\nThis time period is also when Git and GitHub added support for using git over HTTP instead of only over SSH. Hoping to take advantage of the ways that HTTP git operations can be faster than the same operations over SSH, Bundler added support for HTTP authentication during git operations.\n\nLast, and possibly saddest, Bundler had its very first CVE. If you’re interested, I’ve given another talk [on security and the background behind CVEs](http://andre.arko.net/2013/08/22/security-is-hard-but-we-cant-go-shopping/). The short version is that a CVE means that your software has a critical security issue. 😰 In our case, the critical security issue was that we allowed multiple `source` declarations inside a Gemfile, and simply looked inside every source for every gem that we needed. Unfortunately, since anyone can claim a gem on RubyGems.org, the possible name conflicts create a security risk.\n\nIf you run a private gemserver at your company, and use a private gem that you have named `my-cool-thing`, someone else could push a gem named `my-cool-thing` to RubyGems.org, and you might (suddenly, and without warning) start downloading and installing and running the code from that gem, which might be malicious. We [blogged about the problem, and tried to fix it](http://bundler.io/blog/2014/08/14/bundler-may-install-gems-from-a-different-source-than-expected-cve-2013-0334.html), but in the end the only way to be sure that the problem can’t ever happen is to stop allowing more than one `source` for any gems inside your Gemfile.\n\nYou can still use other sources, but you have to tie any additional sources to a particular gem. Then, Bundler will only get that gem from that source, and not use that source for any other gems. Well, that’s actually something of a simplification because of the complications added by gems in one source that depend on gems in another source, but I think it’s close enough for this discussion.\n\n### A New Hope (2015-2017)\n\nWhile we had finally accomplished our goal of a separate web service to make installing fast for Bundler users, having a separate API sucked. The API was an optimization, and so gems could still be installed if it was down, but any downtime meant a lot of upset and complaining users. Trying to keep the API up meant that the Bundler team was suddenly on call _all the time_. It was exciting to learn about devops, but keeping everything running was a huge source of stress for years.\n\nNo one wants to deal with a lot of stress for years at a time, especially not , and so this period also saw several RubyGems.org and Bundler contributors slowly burn out and drift away. Fortunately, as that was happening, the Ruby community came together and started paying developers to work on the gem infrastructure that everyone uses.\n\nFirst, Ruby Central provided grants for work on RubyGems.org, Bundler, and RubyGems. Thanks to time paid for by Ruby Central, myself and others were able to finish new releases, continue development work on the compact index format, and much more.\n\nIn addition to grants from Ruby Central, Stripe also started an open source grants program. One of their grants went to a college student named Samuel Giddins. As an iOS developer, he had started contributing to CocoaPods, the application dependency manager for Objective-C. Since CocoaPods was written in Ruby, his Stripe grant was able to fund development work on a completely new dependency resolver, written from the ground up to be more easily maintained. It was also written to be used by multiple projects. Today, Sam’s resolver library Molinillo is used not just by CocoaPods, but also by Bundler, by RubyGems itself, and by Berkshelf, the Chef dependency manager.\n\nAround the same time, Stripe and Engine Yard started funding the Bundler project, allowing us to [incorporate the first Ruby trade association, Ruby Together](https://rubytogether.org/news/2015-03-17-announcing-ruby-together). Ruby Together is a non-profit dedicated to funding open source Ruby development using funds raised from developers and companies in the community. It has slowly grown over the years, and today Ruby Together pays for regular developer time spent on Bundler, RubyGems, the RubyGems.org Rails app, the Gemstash gem server and mirror, ops work on the RubyGems.org servers, and even the new Ruby Toolbox 2.0 open source project.\n\nWhile Ruby Central has given grants for specific projects, and continues to pay the server bills for RubyGems.org, they do not fund developers to do ongoing maintenance on the tools we use every day. As Ruby Together grows, it will be able to fund even more developer time, so please [join as a developer](https://rubytogether.org/developers) or [join as a company](https://rubytogether.org/companies) today. We want to be able to start supporting even more of the Ruby projects that the entire community depends on.\n\nWith support from Ruby Central, Stripe, and Ruby devs and companies around the world via Ruby Together, the Bundler and RubyGems projects started to see work done by paid devs. Probably not too surprisingly, this resulted in much more regular, consistent development work. That, in turn, meant we were able to ship projects that had been in progress for years.\n\nThe first project that we were able to finish thanks to paid dev work was migrating the entirety of RubyGems.org to run behind the Fastly CDN. This means that whenever you or your computer makes a request to https://rubygems.org, you are actually talking to the closest Fastly server. Since Fastly runs servers in hundreds of data centers around the world, users all around the world see dramatically faster responses. Installing gems is not longer bottlenecked by reaching around the world to the servers in AWS on the West Coast of the US.\n\nBefore moving everything to be served by Fastly, the situation was pretty crappy: your computer would have to make a request all the way to the West Coast, and then that server would send back a redirect request that sent you to the closest CDN server, and then that CDN server might have the file cached, or it might have to go and get the file from our servers on the West Coast itself, and then give that file to you. As you can probably tell just listening to that description, that system was slow, and complicated, and hard to understand, and often had problems.\n\nOnce that was done, we started to move the Bundler API back into the RubyGems.org Rails app. In the years since we had moved it out, the RubyGems ops team had done a great job of building a new and scalable architecture on AWS that could easily handle all of the traffic from every Bundler user. Amazingly, by the time we moved it back into the Rails app, there was already 10x more traffic than there had been when it took RubyGems.org down the first time. This time, with a paid devops team behind it, RubyGems.org was able to handle the API traffic without any issues. Today, the separate Bundler API has been shut down, and everything is served from the RubyGems.org servers.\n\nParallel to getting RubyGems completely moved over to Fastly, the RubyGems and Bundler teams were working to complete the long-awaited compact index format. In short, it is a plain-text format, with one file listing every gem name and version number, and one file per gem listing the full dependency information for each version of that gem. The text files are append-only, so that they can be cached on each machine and updated by requesting only the part of the file that comes after the part that is already cached.\n\nEven though the new format had been proved to work by a prototype I wrote, it took more than a year for myself, Sam Giddins, our Google Summer of Code student Felipe Tanus, and the rest of the Bundler and RubyGems.org teams to work together to finalize the format, write server and client libraries, and release. For more information about the compact index and related changes, check out the talk [Extreme Makeover: RubyGems Edition](http://andre.arko.net/2013/12/09/extreme-makeover-rubygems-edition/) from RubyConf 2013.\n\nBy combining the power of Fastly’s CDN and the caching strategy of the compact index, installing gems became faster again, no matter where you lived in the world. Today, most of the time in a `bundle install` run is actually installing gems, rather than resolving complicated gemfiles or downloading information about gems.\n\nCombining all sources of community funding, we have been able to average something like 10 or 15 hours per week of paid development time consistently spent on Bundler, RubyGems, and RubyGems.org. We’re still pretty far away from being able to employ developers to work full time on Ruby infrastructure, but even those few hours have enabled us to get a lot more done.\n\nIn addition to the Fastly migration, the completely new dependency resolver, and the completely new compact index format, we shipped a lot of features in Bundler itself. Here are some of the highlights:\n\nAfter many years of discussion around [Filefiles](http://blog.hasmanythrough.com/2011/12/1/i-heard-you-liked-files) and [the misleading name of Gemfile.lock](https://github.com/bundler/bundler/issues/694), we added support for different filenames: `gems.rb` and `gems.locked`. With those filenames, it’s unambiguous that those files list gems, that one is written in Ruby, and that one contains the locked gems. No more confusing new developers about what the `.lock` extension means! The new filenames are supported today, if you want to change your existing projects. In Bundler 2.0, we will switch the default file created by `bundle init` to be `gems.rb`, but both filenames will continue to be support through at least the entire Bundler 2.0 lifecycle.\n\nAs I alluded to earlier, we discovered some problems with the `ruby` directive in Gemfiles. Namely, it was too specific, and didn’t allow setting a range of allowed ruby versions. We extended the `ruby` directive to support version requirements just like gems, and now the exact ruby version is recorded in the lockfile. This makes it possible to manage ruby version upgrades just like you manage gem versions, which is pretty nice.\n\nNow that we record the expected Ruby version in the lockfile, it’s also possible to support the Ruby versions required by individual gems while resolving gemfiles. If you’ve ever seen an error while installing a gem that your version of Ruby is not supported, that is completely fixed in the latest versions of Bundler.\n\nWe also added new commands, including `lock` to resolve your gem versions and write a lockfile without installing those gems. We also extended the lock to support individual platforms, making it possible to lock a single application on both Unix systems and Windows systems at the same time.\n\nAnother new command, `doctor`, created by Misty DeMeo, tries to help users figure out what could have gone wrong, including gems not installed, gems with native extensions that haven’t been built, and other possible problems.\n\nThe new `bundle pristine` command works just like `gem pristine` but for the gems in your application bundle, including git gems. If you’ve ever edited an installed gem as part of debugging an issue, the `pristine` command is extremely handy for undoing those changes and going back to the factory-fresh gem files.\n\nThe `add` command works like `npm install --save`, putting a new line in your Gemfile, doing a full resolution run, and then installing any new gems. It dramatically speeds up the early stages of a project when you’re adding many gems quickly in a short period of time.\n\nThe `update` command, while not new, got a significant overhaul.  It now supports options that let you limit what kind of version upgrades you want to see. You can pass `--major`, `--minor`, or `--patch` in order to get only upgrades at that level.\n\nFor users who run gem mirrors or proxies, like Squid, Varnish or the Bundler team’s Gemstash server, it is now possible to configure Bundler to use mirrors automatically, without editing your Gemfile. After configuring a mirror, Bundler will automatically try to use the configured mirror instead of the URL listed in your Gemfile. This makes it possible to run a Gemstash or other mirror locally in an office or datacenter, greatly speeding up install operations.\n\nWe shipped a beta version of the plugin system, allowing other developers to provide new Bundler commands, hooks that run when gems are installed or updated, and even new gem sources. If you’ve always wanted to be able to install gems from mercurial repositories, you can write a plugin to make that happen.\n\nFinally, in a nice quality of life and security improvement, Bundler now has checksums for each .gem file as part of the compact index. At install time, Bundler uses those checksums to make sure that it is installing the correct gem, and the file wasn’t corrupted in transit.\n\n### The Future (2017-????)\n\nToday, we’ve just shipped Bundler 1.16 with all of the features I mentioned above. We’re actively working on Bundler 2.0, with a target release date (which admittedly might slip) of Christmas 2017. I don’t have room in this talk to include details about 2.0, but I can say that we value compatibility extremely highly. \n\nWhile we plan to make breaking changes in 2.0, we want to make it easy to continue to use applications that use both 1.x and 2.x on the same machine. You’ll be able to upgrade each application individually, and at your own pace. For more information about planned changes in Bundler 2, check out Colby Swandale’s talk [Bundler 2](http://rubykaigi.org/2017/presentations/0xColby.html), from Ruby Kaigi 2017.\n\nIn the meantime, here are some Bundler best practices that you can use to get the benefits of Bundler 2.0 today! First, you probably want to set the config option `only_update_to_newer_versions` to true. That setting changes the `update` command to ensure that you will never run `update` and end up with an older version of a gem than you already have. That option will be turned on by default in Bundler 2.0.\n\nYou also probably want to turn on the `disable_multisource` setting. As I mentioned earlier in this talk, it’s fundamentally unsafe to have multiple sources in a single gemfile. We can’t raise an error by default because of existing users, but you can opt in to that option for yourself, and the option will be turned on by default when we release 2.0.\n\nIf you develop or deploy on more than one platform, especially if some of your developers or servers run Windows, you also likely want to enable the `specific_platform` option. That turns on our next-generation platform support, allowing Bundler to resolve each platform separately and install precompiled gems for the platform it is installing onto, if precompiled gems exist.\n\nWe’ve also implemented much more extensive shared caches. It’s now possible to share .gem files and compiled extensions between applications that have the same gems, by turning on `global_gem_cache`. That change pairs extremely will with another change that will be on by default in 2.0: `default_install_uses_path`. When you turn that on, Bundler will install gems separately for each application, ensuring that RubyGems never has to deal with loading unused gems only to ignore them. Combining this with the global gem cache gives every app its own set of gems without having to download or compile gems multiple times on a single machine. It’s pretty great.\n\nFinally, while we can’t force existing users to connect to github via HTTPS, because that would break backwards compatibility, you can make that change yourself today. Include this snippet at the top of your Gemfile, and all of your `github` gems will use HTTPS to connect to GitHub:\n\n\tgit_source(:github) {|repo_name| \"https://github.com/#{repo_name}\" }\n\n### Fancy workflows and tools\n\nPhew! Now that we’ve caught up completely on the history of Bundler and everything that we’ve done to it over the last decade or so, let me give you a chaser of a few more handy tips and workflows that you can use to improve your Bundler experience.\n\nWhile you can always run a gem command using `bundle exec`, that depends on you being in the right directory or manually setting the location of the Gemfile. Instead, you can use `bundle binstubs GEM` to create an executable in `bin/gem`. You can run that file directly to load Bundler, find your application Gemfile, and load the correct version of that gem, no matter where you are on your system. That can be especially helpful for cronjobs, but is honestly just nicer and easier than using `bundle exec` all the time.\n\nIf you’re interested in seeing a visual layout of your application’s gem dependencies, you can install Graphviz and then run the `bundle viz` command. If your application is small enough and simple enough, you’ll end up with a graph that looks something like this:\n\n<a class=\"image\" href=\"https://indirect.micro.blog/uploads/2025/1eae908958.jpg\">\n  ![Default Rails app dependency visualization](https://indirect.micro.blog/uploads/2025/1eae908958.jpg)\n</a>\n\nIf you want to start running your application on a new platform, like JRuby, or Windows, you can now add that platform in advance, on any machine, by running `bundle lock --add PLATFORM`. Once you’ve done that, running your application on that platform won’t cause changes to your lockfile. While Bundler can’t guarantee identical code runs on different platforms, it can guarantee that every machine on a particular platform will run exactly the same code as every other machine on that platform.\n\nI mentioned the local git gems feature during the history section, but it’s so useful that I think it’s worth reiterating here. If you want to be able to make changes to a gem and immediately try out those changes in your application that depends on that gem, you can! Change the gem in your Gemfile to a git gem, and then run `bundle config local.GEM ~/path/to/checkout`. On that machine, Bundler will use that checkout instead of downloading and installing that gem when your application runs. As you make changes to the local checkout, Bundler will update your application lockfile to include the SHA of your latest commit to that gem, ensuring that other developers and production servers will get your changes immediately.\n\nEver wanted to write a simple one-file script, but the script depends on some gems? You can use `bundler/inline` to write scripts that have bundled gems. Here’s what the code looks like.\n\n```\n$ cat http.rb\nrequire 'bundler/inline'\ngemfile do\n  source 'https://rubygems.org'\n  gem 'http'\nend\nputs HTTP.get('http://example.com')\n```\n\nHere’s what running that code looks like, including installing the `http` gem as part of running the script.\n\n```\n$ gem uninstall http\nSuccessfully uninstalled http-3.0.0\n\n$ ruby http.rb\n<!doctype html>\n<html>[…]\n<body>\n<div>\n    <h1>Example Domain</h1>\n    <p>This domain is established to be used for illustrative examples in documents. You may use this\n    domain in examples without prior coordination or asking for permission.</p>\n    <p><a href=\"http://www.iana.org/domains/example\">More information...</a></p>\n</div>\n</body>\n</html>\n```\n\nIn a pair of related hints, you can easily search through the code of all of the gems in your bundle by using `bundle show --paths`. For example, if your searching tool of choice is `ripgrep`, you can run `rg STRING $(bundle show --paths)` to find `STRING` anywhere in your bundled gems. Once you’ve found the gem you care about, you can open it directly in your editor by running `bundle open GEM`. Bundler will respect the `$EDITOR` environment variable, if you have set it. After you’ve edited the installed gem as much as needed for debugging, you can remove the changes you’ve made by running `bundle pristine GEM`.\n\nFinally, in my personal favorite quality of life improvement, it is now possible to disable gem post-install messages by running `bundle config --system ignore_messages true`. Now, you can never be told to HTTParty hard, ever again.\n\n### The End\n\nAnd with that, we’ve finished our journey through a decade of Bundler history and features! If there’s anything you’re confused about, or if I left out your favorite Bundler feature or trick, let me know on Twitter, where I am [@indirect](https://twitter.com/indirect). If you have questions about Bundler, I encourage you to [join the Bundler Slack](http://slack.bundler.io), where the Bundler, RubyGems, and RubyGems.org teams, contributors, and users all hang out. We’d love to hear from you!\n",
				"date_published": "2017-11-16T00:00:00-08:00",
				"url": "https://andre.arko.net/2017/11/16/a-history-of-bundles-to/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2017/08/24/robin-hood-hashing/",
				"title": "Robin Hood Hashing",
				"content_html": "<p><small>This post was originally given as a presentation at <a href=\"https://www.meetup.com/papers-we-love-too/\">Papers We Love Too</a>, and the <a href=\"https://speakerdeck.com/indirect/robin-hood-hashing-papers-we-love-sf-august-2017\">slides</a> are also available.</small></p>\n<script async class=\"speakerdeck-embed\" data-id=\"ba7a175e5cb543d7b09db0b2d067b64d\" data-ratio=\"1.33333333333333\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n<p>Depending on where you learned about them from, you might call them associative arrays, dictionaries, hash tables, hash maps, or just hashes. Regardless of what you call them, hashes are one of the most commonly used data structures in all of computer science, and for good reason! They are incredibly handy, because they let you use one thing to keep track of another thing.</p>\n<p>Before we can get to the magic of robin hood hashing, and why I’m so excited about it, I’m going to explain how hash data structures can be implemented. That way we can all be on the same page for the exciting part. So: how are hash tables implemented? It may surprise you, especially if you’ve never really thought about it before, but hash tables are almost always implemented using… arrays.</p>\n<h3 id=\"hash-tables-secretly-arrays\">Hash tables: secretly arrays</h3>\n<p>Down at the memory level, computers read and write data based on numerical indexes. Hash tables let you use strings or even entire objects as indexes by adding two layers of indirection on top of arrays: first, applying a <em>hashing algorithm</em> to convert the given key into a number, and second by <em>resolving collisions</em> when two keys hash down to the same address in the array that backs the hash table. Here’s a conceptual diagram illustrating how a hash table stores data in a backing array.</p>\n<blockquote>\n<p><img src=\"01-backing-array.png\" alt=\"\">\nIn this illustration, the keys “Leia”, “Han”, and “Rey” are converted into numerical indexes by the hashing function—2, 4, and 0 respectively. Then, the key and value is stored in the backing array at each index.</p>\n</blockquote>\n<p>The name “hash table” comes from the way that all hash tables use a hashing algorithm to calculate array indexes from arbitrary keys. Hashing is an entire fascinating field of computer science all by itself, but for our purposes today we can define the kind of hashing used by a hash table and then take it as a given while we look at the Robin Hood technique.</p>\n<h3 id=\"hash-collisions\">Hash collisions</h3>\n<p>Hashing algorithms for hash tables are generally evaluated based a single criteria: do they distribute items evenly and randomly, even when the inputs are not random? The more evenly distributed the outputs are, the less there will be collisions. A collision is when two keys hash to the same index. That’s a problem, because each index number can only hold one item.</p>\n<p>Even though the paper is named “Robin Hood Hashing”, the technique it describes only applies to this second aspect of hash tables, resolving collisions. As the paper notes, there are two general approaches to handling collisions: chaining, and open addressing.</p>\n<p><em>Chaining</em> means that every value in the hash table is the head of a linked list, and additional memory must be allocated elsewhere to store the contents of each linked list. In addition, reading and writing can become quite slow because each read and write not only has to go to a completely different location in memory, it also has to traverse the entire linked list, no matter how long it is.</p>\n<p><em>Open addressing</em>, on the other hand, overflows into other slots as needed. There are many techniques available for calculating the second-choice slot, the third-choice slot, and so on. For our purposes today, I’m going to use the simplest algorithm imaginable: try the next slot. This is horribly inefficient, but will make it much easier to illustrate the Robin Hood technique. Let’s look at a table that uses open addressing to store several items that hash to the same values.</p>\n<blockquote>\n<p><img src=\"02-open-addressing.png\" alt=\"\">\nIn this illustration, several keys hash to the same index. As a result, several subsequent indexes have been filled by data that “overflowed” from previous indexes.</p>\n</blockquote>\n<h3 id=\"heres-where-it-gets-tricky\">Here’s where it gets tricky</h3>\n<p>As you can imagine, the more collisions there are, the worse everything gets—reading slows down, writing slows down, and the closer to full the backing array is, the more extra steps need to be taken for every action. Here’s an illustration of a simplified worst-case type scenario.</p>\n<blockquote>\n<p><img src=\"03-open-addressing-full.png\" alt=\"\">\nIn this illustration, you can see how just two or three collisions can create a situation where data has to be stored extremely far away from the index calculated by the hash function.</p>\n</blockquote>\n<p>The frequency of collisions can be somewhat mitigated by having an extremely good hashing function. Unfortunately, thanks to <a href=\"https://en.wikipedia.org/wiki/Birthday_problem\">the Birthday Paradox</a>, collisions are still more frequent than you would expect, even with a small amount of data. As the backing array gets closer and closer to full, the number of extra steps, or <em>probes</em>, required to find any piece of data grows very fast.</p>\n<h3 id=\"robin-hood-to-the-rescue\">Robin Hood to the rescue</h3>\n<p>This is the exact point where Robin Hood can save us from the sheriff of Big O complexity. Without requiring calculations in advance or additional arrays to store extra data, Robin Hood Hashing provides a system that results in a maximum of O(ln <em>n</em>) probes per operation, where <em>n</em> is the number of items stored in the hash table.</p>\n<p>How does it do this? By stealing from the rich and giving to the poor, of course. 😆 In the context of a hash table, the rich are those items that are located very close to their hash index, and the poor items are located far away. The core technique of Robin Hood Hashing is this: when adding new items, replace any item that is closer to its index (“richer”) than the item you are adding. Then, continue adding but with the item that was just replaced. Here’s an illustration of a table filled with data using the Robin Hood Hashing technique.</p>\n<blockquote>\n<p><img src=\"04-robin-hood.png\" alt=\"\">\nIn this illustration, each collision was resolved by moving the later item to the next index. The number on the right of each item indicates how far away from its originally calculated index it is.</p>\n</blockquote>\n<p>With this technique, the same worst-case collision that we observed previously produces an extremely different outcome. Here’s what that looks like instead.</p>\n<blockquote>\n<p><img src=\"05-robin-hood-full.png\" alt=\"\">\nIn this diagram, the same wost-case data from the diagram before last has been inserted into the backing array. Using the Robin Hood technique, every item is displaced by only 2 slots or less.</p>\n</blockquote>\n<p>Stealing from the rich and giving to the poor? That’s Robin Hood all over.</p>\n<p><img src=\"06-robin-hood-disney.jpg\" alt=\"\"></p>\n<h3 id=\"further-reading\">Further reading</h3>\n<p>The original Robin Hood paper covers several other aspects of hash table implementation techniques, including probing algorithms, handling deletions in an efficient way, and others. Over the years, both academic and hobbyist computer scientists have implemented it, experimented with it, benchmarked it, and refined it.</p>\n<p>Here is selection of interesting pieces discussing various aspects of Robin Hood hashing and techniques for implementing it efficiently, if you’d like to learn more.</p>\n<ul>\n<li><a href=\"https://cs.uwaterloo.ca/research/tr/1986/CS-86-14.pdf\">Robin Hood Hashing (1986 original paper)</a></li>\n<li><a href=\"https://www.dmtcs.org/pdfpapers/dmAD0127.pdf\">Robin Hood Hashing with Linear Probing paper (2005)</a></li>\n<li><a href=\"https://www.pvk.ca/Blog/numerical_experiments_in_hashing.html\">Paul Khuong experimenting with hashing options (2009)</a></li>\n<li><a href=\"https://www.pvk.ca/Blog/more_numerical_experiments_in_hashing.html\">Paul’s follow-up and conclusions (2011)</a></li>\n<li><a href=\"https://www.sebastiansylvan.com/post/robin-hood-hashing-should-be-your-default-hash-table-implementation/\">Sebastian Sylvan saying robin hood should be the default (2013)</a></li>\n<li><a href=\"https://www.sebastiansylvan.com/post/more-on-robin-hood-hashing-2/\">Sebastian following up on slowness after deletions (2013)</a></li>\n<li><a href=\"http://codecapsule.com/2013/11/11/robin-hood-hashing/\">Emmanuel Goossaert benchmarking in C++ (2013)</a></li>\n<li><a href=\"https://www.pvk.ca/Blog/2013/11/26/the-other-robin-hood-hashing/\">Paul Kuhong again, on linear probing for performance (2013)</a></li>\n<li><a href=\"http://codecapsule.com/2013/11/17/robin-hood-hashing-backward-shift-deletion/\">Emmanuel benchmarking again after tweaking deletions (2013)</a></li>\n</ul>\n",
				"content_text": "<small>This post was originally given as a presentation at [Papers We Love Too](https://www.meetup.com/papers-we-love-too/), and the [slides](https://speakerdeck.com/indirect/robin-hood-hashing-papers-we-love-sf-august-2017) are also available.</small>\n\n<script async class=\"speakerdeck-embed\" data-id=\"ba7a175e5cb543d7b09db0b2d067b64d\" data-ratio=\"1.33333333333333\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n\nDepending on where you learned about them from, you might call them associative arrays, dictionaries, hash tables, hash maps, or just hashes. Regardless of what you call them, hashes are one of the most commonly used data structures in all of computer science, and for good reason! They are incredibly handy, because they let you use one thing to keep track of another thing.\n\nBefore we can get to the magic of robin hood hashing, and why I’m so excited about it, I’m going to explain how hash data structures can be implemented. That way we can all be on the same page for the exciting part. So: how are hash tables implemented? It may surprise you, especially if you’ve never really thought about it before, but hash tables are almost always implemented using… arrays.\n\n### Hash tables: secretly arrays\n\nDown at the memory level, computers read and write data based on numerical indexes. Hash tables let you use strings or even entire objects as indexes by adding two layers of indirection on top of arrays: first, applying a _hashing algorithm_ to convert the given key into a number, and second by _resolving collisions_ when two keys hash down to the same address in the array that backs the hash table. Here’s a conceptual diagram illustrating how a hash table stores data in a backing array.\n\n> ![](01-backing-array.png)\n> In this illustration, the keys “Leia”, “Han”, and “Rey” are converted into numerical indexes by the hashing function—2, 4, and 0 respectively. Then, the key and value is stored in the backing array at each index.\n\nThe name “hash table” comes from the way that all hash tables use a hashing algorithm to calculate array indexes from arbitrary keys. Hashing is an entire fascinating field of computer science all by itself, but for our purposes today we can define the kind of hashing used by a hash table and then take it as a given while we look at the Robin Hood technique.\n\n### Hash collisions\n\nHashing algorithms for hash tables are generally evaluated based a single criteria: do they distribute items evenly and randomly, even when the inputs are not random? The more evenly distributed the outputs are, the less there will be collisions. A collision is when two keys hash to the same index. That’s a problem, because each index number can only hold one item.\n\nEven though the paper is named “Robin Hood Hashing”, the technique it describes only applies to this second aspect of hash tables, resolving collisions. As the paper notes, there are two general approaches to handling collisions: chaining, and open addressing.\n\n_Chaining_ means that every value in the hash table is the head of a linked list, and additional memory must be allocated elsewhere to store the contents of each linked list. In addition, reading and writing can become quite slow because each read and write not only has to go to a completely different location in memory, it also has to traverse the entire linked list, no matter how long it is.\n\n_Open addressing_, on the other hand, overflows into other slots as needed. There are many techniques available for calculating the second-choice slot, the third-choice slot, and so on. For our purposes today, I’m going to use the simplest algorithm imaginable: try the next slot. This is horribly inefficient, but will make it much easier to illustrate the Robin Hood technique. Let’s look at a table that uses open addressing to store several items that hash to the same values.\n\n> ![](02-open-addressing.png)\n> In this illustration, several keys hash to the same index. As a result, several subsequent indexes have been filled by data that “overflowed” from previous indexes.\n\n### Here’s where it gets tricky\n\nAs you can imagine, the more collisions there are, the worse everything gets—reading slows down, writing slows down, and the closer to full the backing array is, the more extra steps need to be taken for every action. Here’s an illustration of a simplified worst-case type scenario.\n\n> ![](03-open-addressing-full.png)\n> In this illustration, you can see how just two or three collisions can create a situation where data has to be stored extremely far away from the index calculated by the hash function.\n\nThe frequency of collisions can be somewhat mitigated by having an extremely good hashing function. Unfortunately, thanks to [the Birthday Paradox](https://en.wikipedia.org/wiki/Birthday_problem), collisions are still more frequent than you would expect, even with a small amount of data. As the backing array gets closer and closer to full, the number of extra steps, or _probes_, required to find any piece of data grows very fast.\n\n### Robin Hood to the rescue\n\nThis is the exact point where Robin Hood can save us from the sheriff of Big O complexity. Without requiring calculations in advance or additional arrays to store extra data, Robin Hood Hashing provides a system that results in a maximum of O(ln _n_) probes per operation, where _n_ is the number of items stored in the hash table.\n\nHow does it do this? By stealing from the rich and giving to the poor, of course. 😆 In the context of a hash table, the rich are those items that are located very close to their hash index, and the poor items are located far away. The core technique of Robin Hood Hashing is this: when adding new items, replace any item that is closer to its index (“richer”) than the item you are adding. Then, continue adding but with the item that was just replaced. Here’s an illustration of a table filled with data using the Robin Hood Hashing technique.\n\n> ![](04-robin-hood.png)\n> In this illustration, each collision was resolved by moving the later item to the next index. The number on the right of each item indicates how far away from its originally calculated index it is.\n\nWith this technique, the same worst-case collision that we observed previously produces an extremely different outcome. Here’s what that looks like instead.\n\n> ![](05-robin-hood-full.png)\n> In this diagram, the same wost-case data from the diagram before last has been inserted into the backing array. Using the Robin Hood technique, every item is displaced by only 2 slots or less.\n\nStealing from the rich and giving to the poor? That’s Robin Hood all over.\n\n![](06-robin-hood-disney.jpg)\n\n### Further reading\n\nThe original Robin Hood paper covers several other aspects of hash table implementation techniques, including probing algorithms, handling deletions in an efficient way, and others. Over the years, both academic and hobbyist computer scientists have implemented it, experimented with it, benchmarked it, and refined it.\n\nHere is selection of interesting pieces discussing various aspects of Robin Hood hashing and techniques for implementing it efficiently, if you’d like to learn more.\n\n- [Robin Hood Hashing (1986 original paper)](https://cs.uwaterloo.ca/research/tr/1986/CS-86-14.pdf)\n- [Robin Hood Hashing with Linear Probing paper (2005)](https://www.dmtcs.org/pdfpapers/dmAD0127.pdf)\n- [Paul Khuong experimenting with hashing options (2009)](https://www.pvk.ca/Blog/numerical_experiments_in_hashing.html)\n- [Paul’s follow-up and conclusions (2011)](https://www.pvk.ca/Blog/more_numerical_experiments_in_hashing.html)\n- [Sebastian Sylvan saying robin hood should be the default (2013)](https://www.sebastiansylvan.com/post/robin-hood-hashing-should-be-your-default-hash-table-implementation/)\n- [Sebastian following up on slowness after deletions (2013)](https://www.sebastiansylvan.com/post/more-on-robin-hood-hashing-2/)\n- [Emmanuel Goossaert benchmarking in C++ (2013)](http://codecapsule.com/2013/11/11/robin-hood-hashing/)\n- [Paul Kuhong again, on linear probing for performance (2013)](https://www.pvk.ca/Blog/2013/11/26/the-other-robin-hood-hashing/)\n- [Emmanuel benchmarking again after tweaking deletions (2013)](http://codecapsule.com/2013/11/17/robin-hood-hashing-backward-shift-deletion/)\n",
				"date_published": "2017-08-24T00:00:00-08:00",
				"url": "https://andre.arko.net/2017/08/24/robin-hood-hashing/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2017/01/15/a-history-of-talks/",
				"title": "A History of Talks",
				"content_html": "<p>I&rsquo;ve added a new section to this blog, listing all of the talks I have given at meetups, events, and conferences.</p>\n<p>Rather than organize by event, the way that websites like Lanyrd or Speaker Deck do, I&rsquo;ve combined every time I&rsquo;ve given a particular talk into a single entry. Where possible, each talk has not just a description, but also links to a written version of the talk, the slides I used while giving the talk, and a video of the talk from that particular event.</p>\n<p>Without further ado, please feel free to check out <a href=\"/talks/\">talks by André Arko</a>.</p>\n",
				"content_text": "I've added a new section to this blog, listing all of the talks I have given at meetups, events, and conferences.\n\nRather than organize by event, the way that websites like Lanyrd or Speaker Deck do, I've combined every time I've given a particular talk into a single entry. Where possible, each talk has not just a description, but also links to a written version of the talk, the slides I used while giving the talk, and a video of the talk from that particular event.\n\nWithout further ado, please feel free to check out [talks by André Arko](/talks/).\n",
				"date_published": "2017-01-15T00:00:00-08:00",
				"url": "https://andre.arko.net/2017/01/15/a-history-of-talks/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2016/11/12/how-to-contribute-to-open/",
				"title": "How to Contribute to Open Source",
				"content_html": "<h3 class=\"subtitle\">or: From No Experience to the Core Team<br> in 15 Minutes Per Day</h3>\n<p><small>This post was originally given as a presentation at <a href=\"http://rubyconf.org\">RubyConf 2016</a>, and the <a href=\"https://www.youtube.com/watch?v=6jUe-9Y__KM\">video</a> and <a href=\"https://speakerdeck.com/indirect/from-no-oss-experience-to-the-core-team-in-15-minutes-a-day\">slides</a> are also available.</small></p>\n<script async class=\"speakerdeck-embed\" data-id=\"f675fcfcca484f05b133ce71c22ab5ba\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n<p>At conferences and on the internet, the most common question I get isn’t about one of my open source projects. The most common question I get is “how can I start contributing to open source?”. After years of trying to answer that question, I have a list of suggestions that I think are pretty good. Before I get to that, though, we need to talk about what contributing to open source really means.</p>\n<p>The most common misconception about working on open source is that you need to have a lot of experience to be able to do it. I totally understand where that idea comes from, but it’s wrong. You don’t need to understand something before you can work on it. In fact, working on an open source project is a powerful way to learn more about the project, the language, and the open source ecosystem of developers who help one another write code.</p>\n<h3 id=\"you-are-experienced-enough-3\">you are experienced enough &lt;3</h3>\n<p>It’s not said often enough, but there are no “real” programmers. Everyone can tell a computer what to do. The most senior of programmers spend a huge amount of their time confused about why things are happening—in fact, I would argue that the core skill of programming is investigating unknown things until the computer does what you want.</p>\n<h3 id=\"what-do-you-want-to-get-from-this\">what do you want to get from this?</h3>\n<p>Despise some lazy companies’ claims, <a href=\"https://blog.jcoglan.com/2013/11/15/why-github-is-not-your-cv/\">Github is not your resumé</a>. Making free software does not prove that you are a good programmer, and some of the best programmers produce no open source. Before doing any work, sit down and think about what your goals are.</p>\n<p>Do you want to hone your skills? Gain acclaim from your peers? Shoulder a pile of crushing guilt? Open source can be a good way to accomplish those goals, but it’s not the only one. You could write software for yourself to hone your skills. You could volunteer as a teacher or mentor at <a href=\"https://railsbridge.org/\">RailsBridge</a>, <a href=\"https://www.code2040.org/\">Code 2040</a>, or a host of other programs. You could find an extended family full of entitled relatives who all demand that you fix their computers. You have lots of options!</p>\n<p>Your time is your own, and you don’t owe it to anyone else. Keep that in mind as we talk about all of the things that are available to you—think about your goals, and choose to spend your time on accomplishing those goals.</p>\n<p>If you want to write software for yourself that solves a problem that you have, you can do that! If you want to work on a project that you already use to make it better, you can do that too. If you want to work on a project that is hugely popular, with enough persistence you can even do that. But if you want to work on your career by writing software inside your company instead, that’s a legitimate option.</p>\n<h3 id=\"what-even-is-contributing\">what even is contributing</h3>\n<p>When I ask people what they think of as “contributing to open source”, they always paint the same picture: genius programmer fixing bugs, implementing huge features, and sending PRs that don’t even need code review before being merged. This is almost the exact opposite of reality.</p>\n<p>Contributing to open source, more than anything else, means trying to understand other people’s problems and then help them. Sometimes that will mean changing code, but more often that will mean explaining something. Or adding to the documentation, or improving an error message, or providing design, a website, or styling. You can absolutely contribute to open source without being ready to code. Writing code is a tiny fraction of the work needed to solve people’s problems.</p>\n<h3 id=\"the-benefits-are-real\">the benefits are real</h3>\n<p>Now let’s talk about the benefits available to open source contributors. You can practice building projects from scratch, by yourself. On other projects, you can practice working with a team of developers, learn about allocating tasks, estimating work, blowing through deadlines, and disagreeing about how things should be done.</p>\n<p>Working on an open source project provides a lot of the benefits of an internship. Unlike an internship, you won’t get paid. But the upside to not getting paid is that you don’t have to get permission or approval from anyone! You can practice and learn in your own time, at your own pace, on literally whatever seems interesting to you. Open source projects mean no obligation to work on a particular task, for a particular time, or even by a particular deadline.</p>\n<h3 id=\"should-_you_-work-on-open-source\">should <em>you</em> work on open source?</h3>\n<p>Now that you know about some of the benefits, though, I have to give you a pretty big caveat: Only work for free if you can and if you want to. This is maybe my most important point, so I’m going to say it again: only work for free if you <em>can</em> and if you <em>want to</em>.</p>\n<p>First, let’s talk about if you <em>can</em> work for free. Open source work is almost never paid work. Open source developers do a lot of work that companies use to make money. Open source designers do a lot of work that companies use to make money. Those developers and designers almost never see any of that money, in any form. Companies taking advantage of individuals to produce bigger profits for themselves is a very real problem with the open source community as a whole. I strongly encourage you to read Ashe Dryden’s <a href=\"https://www.ashedryden.com/blog/the-ethics-of-unpaid-labor-and-the-oss-community\">The Ethics of Unpaid Labor and the OSS Community</a> for a more detailed discussion of that particular problem.</p>\n<p>Creating open source tools and expanding your reputation is cool, but a side-project that earns you money is cool, too. Paying your bills and being able to afford things you want are important life goals. Even well-known open source developers get caught up in demands from their users and forget to take care of themselves. Every time you’re tempted to work on open source, think about the paid work (or free time!) that you’re giving up to do that. If you’re still interested, I have one more warning, and then I’ll give you the master plan.</p>\n<h3 id=\"open-source-is-out-of-your-control\">open source is out of your control</h3>\n<p>My last warning about doing open source work is this: choosing to release your work as open source means that you have agreed to give up control over it.</p>\n<p>In a recent real-life scenario, the author of many open source node packages was so upset by NPM, Inc. that he deleted all of his work from the npm servers. His work happened to include a very commonly used package called <code>left-pad</code>, and the removal of left-pad broke tests and deploys for many if not most software projects written in Node.js.</p>\n<p>Because <code>left-pad</code> was open source, though, someone was able to take their own copy of the package and upload it to npm’s servers under the same name. Even though the author wanted to remove his packages from NPM forever, his open source license meant that anyone else could (legally!) put them back, and everyone else could continue to use them.</p>\n<h3 id=\"some-licenses-let-you-keep-some-control\">some licenses let you keep some control</h3>\n<p>As you think about these issues, keep in mind that putting code on GitHub does not mean it is open source. Research the options for a license, and choose a license that you are comfortable with. GitHub created the website <a href=\"http://choosealicense.com\">http://choosealicense.com</a> to help developers pick a software license for open source code. The <a href=\"https://creativecommons.org/choose/\">Creative Commons license chooser</a> offers another option, allowing you to choose a pre-made license for your work based on what you want to allow or disallow.</p>\n<p>Research your license options, and pick one you’re comfortable with! Once you’ve made your work as open source, anyone can use it for anything your license allows. In one of the most dramatic and horrifying scenarios possible, that means you can’t stop someone from using your code to build a drone system that drops bombs and kills civilians. Be very sure that you are okay with the idea of your (unpaid!) work being used for something that you would never, ever do yourself.</p>\n<h3 id=\"still-here-time-for-the-plan\">still here? time for the plan</h3>\n<p>If you’ve still here after all of the warnings and lowering of expectations, congratulations! Now we’re ready to dive into the master plan for going from no experience to joining the core team of any project you care to target.</p>\n<p>Plan to spend at least 15 minutes every day on this. Depending on your personality, you may end up sucked in and spending an hour or two every day. There is no end date on this commitment—popular open source projects never run out of people that need help. You’ll be able to do this until you decide that the work isn’t worth it for you anymore.</p>\n<p>To start with, you’ll want to pick out one to three projects that you’re interested in. The more projects you pick, the more time you’ll need every day. Spend some time thinking about projects you use, projects you’ve heard of, and projects that you’re interested in. Once you have a project or two or three picked out, get ready to dig in.</p>\n<h3 id=\"stage-one-wtf-is-happening-here\">stage one: wtf is happening here</h3>\n<p>Read the project readme. Read the project manual. Read every single page of the project website. Read the developer documentation. Read the contributing guide. Read the changelog. Feeling warmed up? Now comes the fun stuff. Open the project on GitHub and read <em>every single open issue</em>. Read every single open pull request. Read every comment on every issue and every pull request. Follow the repo, so that you get notified about every new issue and every new comment. Read every single new issue and every single new comment.</p>\n<p>Keep in mind that you should only be doing this for 15 minutes a day! It will take a while. For big projects, like Rails, it might take you weeks to get through all of the issues. That’s okay, you’re in this for the long haul. Now that you’ve read all the issues and pull requests, start to watch for questions that you can answer. It won’t take too long before you notice that someone is asking a question that’s been answered before, or that’s answered in the docs that you just read. Answer the questions you know how to answer.</p>\n<p>Now you’re a contributor! Answering questions is just the beginning. Once you’re able to start answering questions, you can help with new tickets. A lot of new tickets are repeats of old tickets in some way. Once you’re familiar with the scenarios that repeat, you can not only handle those tickets, you now know material that should be added to the docs!</p>\n<p>Start improving the documentation based on what you’ve learned. Add warnings about common problems. Improve anything that’s confusing or misleading. Start thinking about how to improve error messages. Keep reading new issues for at least 15 minutes a day.</p>\n<h3 id=\"stage-two-youre-helping\">stage two: you’re helping!</h3>\n<p>At this point, you’re probably starting to get a feeling for how this project works. You’re familiar with the documentation, and you’ve seen a lot of the ways that things can go wrong by reading issues. Now is the ideal time to re-read all of the documentation and fix anything that you can find that could be improved. Rewrite for clarity, correct statements that are wrong, add guides for anything people keep asking how to do.</p>\n<p>This is also a great time to start helping with issue reports. When someone reports a bug, but can’t provide steps to reproduce the bug, figure out the reproduction steps and add them to the ticket. If you can’t figure out reproduction steps, explain what you tried on the ticket so the reporter can add detail about their problem.</p>\n<p>A bug that can’t be reproduced is a bug that can’t be fixed. I truly cannot overstate the value and importance of contributors who are willing to figure out what exactly does and does not work. That is what makes it possible to write a test, and that is what makes it possible to write a fix for the problem.</p>\n<p>The next level of helping after a reproduction case is writing a test. Even if you don’t know how to fix a bug, opening a PR with a failing test for a known bug is SO HELPFUL. I cannot even tell you how helpful it is. It is maybe the best present you can give to any maintainer.</p>\n<h3 id=\"stage-three-pretty-much-on-the-team-now\">stage three: pretty much on the team now</h3>\n<p>Once you’ve started writing failing tests, it’s not much farther to start fixing those failing tests. There’s no pressure to start doing this, but at some point you’ll probably find yourself reading a ticket and thinking something like “oh, that variable was somehow <code>nil</code>. I can probably fix that.” You can! Read the code, find the bug, and send a PR with the fix.</p>\n<p>Writing code that fixes bugs or adds features requires some level of understanding—understanding the code, understanding your users, and understanding the goals of the project. It can be hard! It’s hard for the person who wrote that code in the first place. Even if they wrote that code, probably years ago, it’s unlikely they remember the details.</p>\n<p>This is also a good time to start using the project yourself while looking for anything that could be improved. Could that info message be clearer? Can you use the project in a way that causes an error? Is there a set of options that are confusing? Open issues or PRs for everything that you find.</p>\n<p>After you’ve been helping with issues, sending PRs, and generally contributing for a while, starting talking to the maintainers about their plans. Many projects have successfully identified work that would be super helpful to be done, as soon as “someone” has time to do it. You can be someone! Talk with the other people working on the project, work out a consensus on priorities, and then start doing the work.</p>\n<h3 id=\"stage-four-you-are-the-brute-squad\">stage four: you are the brute squad</h3>\n<p>Guess what? You’re an open source contributor. Keep it up! Repeat this process for a few months, or maybe years. If you stick with it, you are more or less guaranteed to end up on the core team.</p>\n<p>Speaking from personal experience, this is also the point where it’s entirely possible that you will suddenly discover that you are the only person who is still working on the project. Now you’re in charge of your own open source project. Congratulations! 🎉 Also, my condolences. 😅</p>\n<h3 id=\"what-have-we-learned-from-this\">what have we learned from this</h3>\n<p>Open source can be rewarding, but it isn’t worthwhile for everyone. Consider what you’re giving up, and be sure that you are happy with the tradeoff.</p>\n<p>It is possible to figure out what is happening, and why, and fix it. As hard as computers seem to be, they are ultimately understandable.</p>\n<p>Finally, understanding and helping the humans who are using software is the core skill of open source work. Building tools that improve the lives of their users can be immensely satisfying, and that is what keeps me doing it.</p>\n<p><small>Thanks to <a href=\"https://twitter.com/ashedryden\">@ashedryden</a> and <a href=\"https://twitter.com/mountain_ghosts\">@mountain_ghosts</a> for writing about these topics previously, and <a href=\"https://twitter.com/sailorhg\">@sailorhg</a> for many of the ideas in this post.</small></p>\n",
				"content_text": "<h3 class=\"subtitle\">or: From No Experience to the Core Team<br> in 15 Minutes Per Day</h3>\n\n<small>This post was originally given as a presentation at [RubyConf 2016](http://rubyconf.org), and the [video](https://www.youtube.com/watch?v=6jUe-9Y__KM) and [slides](https://speakerdeck.com/indirect/from-no-oss-experience-to-the-core-team-in-15-minutes-a-day) are also available.</small>\n\n<script async class=\"speakerdeck-embed\" data-id=\"f675fcfcca484f05b133ce71c22ab5ba\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n\nAt conferences and on the internet, the most common question I get isn’t about one of my open source projects. The most common question I get is “how can I start contributing to open source?”. After years of trying to answer that question, I have a list of suggestions that I think are pretty good. Before I get to that, though, we need to talk about what contributing to open source really means.\n\nThe most common misconception about working on open source is that you need to have a lot of experience to be able to do it. I totally understand where that idea comes from, but it’s wrong. You don’t need to understand something before you can work on it. In fact, working on an open source project is a powerful way to learn more about the project, the language, and the open source ecosystem of developers who help one another write code.\n\n### you are experienced enough \\<3\n\nIt’s not said often enough, but there are no “real” programmers. Everyone can tell a computer what to do. The most senior of programmers spend a huge amount of their time confused about why things are happening—in fact, I would argue that the core skill of programming is investigating unknown things until the computer does what you want.\n\n### what do you want to get from this?\n\nDespise some lazy companies’ claims, [Github is not your resumé](https://blog.jcoglan.com/2013/11/15/why-github-is-not-your-cv/). Making free software does not prove that you are a good programmer, and some of the best programmers produce no open source. Before doing any work, sit down and think about what your goals are.\n\nDo you want to hone your skills? Gain acclaim from your peers? Shoulder a pile of crushing guilt? Open source can be a good way to accomplish those goals, but it’s not the only one. You could write software for yourself to hone your skills. You could volunteer as a teacher or mentor at [RailsBridge](https://railsbridge.org/), [Code 2040](https://www.code2040.org/), or a host of other programs. You could find an extended family full of entitled relatives who all demand that you fix their computers. You have lots of options!\n\nYour time is your own, and you don’t owe it to anyone else. Keep that in mind as we talk about all of the things that are available to you—think about your goals, and choose to spend your time on accomplishing those goals.\n\nIf you want to write software for yourself that solves a problem that you have, you can do that! If you want to work on a project that you already use to make it better, you can do that too. If you want to work on a project that is hugely popular, with enough persistence you can even do that. But if you want to work on your career by writing software inside your company instead, that’s a legitimate option.\n\n### what even is contributing\n\nWhen I ask people what they think of as “contributing to open source”, they always paint the same picture: genius programmer fixing bugs, implementing huge features, and sending PRs that don’t even need code review before being merged. This is almost the exact opposite of reality.\n\nContributing to open source, more than anything else, means trying to understand other people’s problems and then help them. Sometimes that will mean changing code, but more often that will mean explaining something. Or adding to the documentation, or improving an error message, or providing design, a website, or styling. You can absolutely contribute to open source without being ready to code. Writing code is a tiny fraction of the work needed to solve people’s problems. \n\n### the benefits are real\n\nNow let’s talk about the benefits available to open source contributors. You can practice building projects from scratch, by yourself. On other projects, you can practice working with a team of developers, learn about allocating tasks, estimating work, blowing through deadlines, and disagreeing about how things should be done.\n\nWorking on an open source project provides a lot of the benefits of an internship. Unlike an internship, you won’t get paid. But the upside to not getting paid is that you don’t have to get permission or approval from anyone! You can practice and learn in your own time, at your own pace, on literally whatever seems interesting to you. Open source projects mean no obligation to work on a particular task, for a particular time, or even by a particular deadline.\n\n### should _you_ work on open source?\n\nNow that you know about some of the benefits, though, I have to give you a pretty big caveat: Only work for free if you can and if you want to. This is maybe my most important point, so I’m going to say it again: only work for free if you _can_ and if you _want to_.\n\nFirst, let’s talk about if you _can_ work for free. Open source work is almost never paid work. Open source developers do a lot of work that companies use to make money. Open source designers do a lot of work that companies use to make money. Those developers and designers almost never see any of that money, in any form. Companies taking advantage of individuals to produce bigger profits for themselves is a very real problem with the open source community as a whole. I strongly encourage you to read Ashe Dryden’s [The Ethics of Unpaid Labor and the OSS Community](https://www.ashedryden.com/blog/the-ethics-of-unpaid-labor-and-the-oss-community) for a more detailed discussion of that particular problem.\n\nCreating open source tools and expanding your reputation is cool, but a side-project that earns you money is cool, too. Paying your bills and being able to afford things you want are important life goals. Even well-known open source developers get caught up in demands from their users and forget to take care of themselves. Every time you’re tempted to work on open source, think about the paid work (or free time!) that you’re giving up to do that. If you’re still interested, I have one more warning, and then I’ll give you the master plan.\n\n### open source is out of your control\n\nMy last warning about doing open source work is this: choosing to release your work as open source means that you have agreed to give up control over it.\n\nIn a recent real-life scenario, the author of many open source node packages was so upset by NPM, Inc. that he deleted all of his work from the npm servers. His work happened to include a very commonly used package called `left-pad`, and the removal of left-pad broke tests and deploys for many if not most software projects written in Node.js.\n\nBecause `left-pad` was open source, though, someone was able to take their own copy of the package and upload it to npm’s servers under the same name. Even though the author wanted to remove his packages from NPM forever, his open source license meant that anyone else could (legally!) put them back, and everyone else could continue to use them.\n\n### some licenses let you keep some control\n\nAs you think about these issues, keep in mind that putting code on GitHub does not mean it is open source. Research the options for a license, and choose a license that you are comfortable with. GitHub created the website [http://choosealicense.com](http://choosealicense.com) to help developers pick a software license for open source code. The [Creative Commons license chooser](https://creativecommons.org/choose/) offers another option, allowing you to choose a pre-made license for your work based on what you want to allow or disallow.\n\nResearch your license options, and pick one you’re comfortable with! Once you’ve made your work as open source, anyone can use it for anything your license allows. In one of the most dramatic and horrifying scenarios possible, that means you can’t stop someone from using your code to build a drone system that drops bombs and kills civilians. Be very sure that you are okay with the idea of your (unpaid!) work being used for something that you would never, ever do yourself.\n\n### still here? time for the plan\n\nIf you’ve still here after all of the warnings and lowering of expectations, congratulations! Now we’re ready to dive into the master plan for going from no experience to joining the core team of any project you care to target.\n\nPlan to spend at least 15 minutes every day on this. Depending on your personality, you may end up sucked in and spending an hour or two every day. There is no end date on this commitment—popular open source projects never run out of people that need help. You’ll be able to do this until you decide that the work isn’t worth it for you anymore.\n\nTo start with, you’ll want to pick out one to three projects that you’re interested in. The more projects you pick, the more time you’ll need every day. Spend some time thinking about projects you use, projects you’ve heard of, and projects that you’re interested in. Once you have a project or two or three picked out, get ready to dig in.\n\n### stage one: wtf is happening here\n\nRead the project readme. Read the project manual. Read every single page of the project website. Read the developer documentation. Read the contributing guide. Read the changelog. Feeling warmed up? Now comes the fun stuff. Open the project on GitHub and read _every single open issue_. Read every single open pull request. Read every comment on every issue and every pull request. Follow the repo, so that you get notified about every new issue and every new comment. Read every single new issue and every single new comment.\n\nKeep in mind that you should only be doing this for 15 minutes a day! It will take a while. For big projects, like Rails, it might take you weeks to get through all of the issues. That’s okay, you’re in this for the long haul. Now that you’ve read all the issues and pull requests, start to watch for questions that you can answer. It won’t take too long before you notice that someone is asking a question that’s been answered before, or that’s answered in the docs that you just read. Answer the questions you know how to answer.\n\nNow you’re a contributor! Answering questions is just the beginning. Once you’re able to start answering questions, you can help with new tickets. A lot of new tickets are repeats of old tickets in some way. Once you’re familiar with the scenarios that repeat, you can not only handle those tickets, you now know material that should be added to the docs!\n\nStart improving the documentation based on what you’ve learned. Add warnings about common problems. Improve anything that’s confusing or misleading. Start thinking about how to improve error messages. Keep reading new issues for at least 15 minutes a day.\n\n### stage two: you’re helping!\n\nAt this point, you’re probably starting to get a feeling for how this project works. You’re familiar with the documentation, and you’ve seen a lot of the ways that things can go wrong by reading issues. Now is the ideal time to re-read all of the documentation and fix anything that you can find that could be improved. Rewrite for clarity, correct statements that are wrong, add guides for anything people keep asking how to do.\n\nThis is also a great time to start helping with issue reports. When someone reports a bug, but can’t provide steps to reproduce the bug, figure out the reproduction steps and add them to the ticket. If you can’t figure out reproduction steps, explain what you tried on the ticket so the reporter can add detail about their problem.\n\nA bug that can’t be reproduced is a bug that can’t be fixed. I truly cannot overstate the value and importance of contributors who are willing to figure out what exactly does and does not work. That is what makes it possible to write a test, and that is what makes it possible to write a fix for the problem.\n\nThe next level of helping after a reproduction case is writing a test. Even if you don’t know how to fix a bug, opening a PR with a failing test for a known bug is SO HELPFUL. I cannot even tell you how helpful it is. It is maybe the best present you can give to any maintainer.\n\n### stage three: pretty much on the team now\n\nOnce you’ve started writing failing tests, it’s not much farther to start fixing those failing tests. There’s no pressure to start doing this, but at some point you’ll probably find yourself reading a ticket and thinking something like “oh, that variable was somehow `nil`. I can probably fix that.” You can! Read the code, find the bug, and send a PR with the fix.\n\nWriting code that fixes bugs or adds features requires some level of understanding—understanding the code, understanding your users, and understanding the goals of the project. It can be hard! It’s hard for the person who wrote that code in the first place. Even if they wrote that code, probably years ago, it’s unlikely they remember the details.\n\nThis is also a good time to start using the project yourself while looking for anything that could be improved. Could that info message be clearer? Can you use the project in a way that causes an error? Is there a set of options that are confusing? Open issues or PRs for everything that you find.\n\nAfter you’ve been helping with issues, sending PRs, and generally contributing for a while, starting talking to the maintainers about their plans. Many projects have successfully identified work that would be super helpful to be done, as soon as “someone” has time to do it. You can be someone! Talk with the other people working on the project, work out a consensus on priorities, and then start doing the work.\n\n### stage four: you are the brute squad\n\nGuess what? You’re an open source contributor. Keep it up! Repeat this process for a few months, or maybe years. If you stick with it, you are more or less guaranteed to end up on the core team.\n\nSpeaking from personal experience, this is also the point where it’s entirely possible that you will suddenly discover that you are the only person who is still working on the project. Now you’re in charge of your own open source project. Congratulations! 🎉 Also, my condolences. 😅\n\n### what have we learned from this\n\nOpen source can be rewarding, but it isn’t worthwhile for everyone. Consider what you’re giving up, and be sure that you are happy with the tradeoff.\n\nIt is possible to figure out what is happening, and why, and fix it. As hard as computers seem to be, they are ultimately understandable.\n\nFinally, understanding and helping the humans who are using software is the core skill of open source work. Building tools that improve the lives of their users can be immensely satisfying, and that is what keeps me doing it.\n\n<small>Thanks to [@ashedryden](https://twitter.com/ashedryden) and [@mountain\\_ghosts](https://twitter.com/mountain_ghosts) for writing about these topics previously, and [@sailorhg](https://twitter.com/sailorhg) for many of the ideas in this post.</small>\n",
				"date_published": "2016-11-12T00:00:00-08:00",
				"url": "https://andre.arko.net/2016/11/12/how-to-contribute-to-open/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2016/09/26/a-year-of-ruby-together/",
				"title": "A Year of Ruby, Together",
				"content_html": "<p><small>This post was originally given as a presentation at <a href=\"http://2016.euruko.org\">EuRuKo 2016</a>. (<a href=\"https://speakerdeck.com/indirect/a-year-of-ruby-together\">slides</a>)</small></p>\n<script async class=\"speakerdeck-embed\" data-id=\"94b37277d3d6423d9ebeb94de2add27b\" data-ratio=\"1.33333333333333\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n<p><a href=\"https://rubytogether.org\">Ruby Together</a> has been around for over a year! It’s actually been about 18 months, but that didn’t sound as snappy. To celebrate, I’ve put together this summary of our history: why and how we exist, what we’ve done, and what the future holds.</p>\n<p>So, what even <em>is</em> Ruby Together? Ruby Together is a non-profit that pays developers to work on RubyGems, Bundler, and more, using funds from members. Members can be any person or company who uses Ruby, and the work we pay for is free for anyone to use.</p>\n<p>But what, exactly, does that mean? And why do we need a non-profit to do this in the first place? The explanation lies the current state of the Ruby community.</p>\n<h2 id=\"there-are-some-problems\">there are some problems</h2>\n<p>Put simply, the problem is that Ruby has grown up. The Ruby community has become so large that our existing community infrastructure isn’t sustainable anymore.</p>\n<h3 id=\"the-gems-theyre-multiplying\">the gems! they’re multiplying!</h3>\n<p>As you’ve probably noticed, Bundler makes it easier to use gems and makes it easier to create gems. Over the 6 years that I’ve spent working on Bundler, the number of gems in existence has multiplied, and the number of gem downloads has multiplied. There are now over 100,000 gems, and those gems have over a million versions.</p>\n<p>With those million versions comes a completely unprecedented number of downloads. In the years from 2003 to the end of 2014, we tracked roughly 2 billion gem downloads. That’s 2 billion downloads over more than ten years. Then, in 2015 alone (just one year!) we served 4 billion gem downloads.</p>\n<p>That is an <em>extremely</em> steep increase. If we were a startup, we’d have great graphs to show VCs and convince them to give us money. But we’re not a startup. Before Ruby Together, all of RubyGems.org was maintained for free by volunteers. That’s a problem, because at the same time that usage has been increasing exponentially, volunteer help hasn’t been increasing.</p>\n<h3 id=\"the-volunteer-crisis\">the volunteer crisis</h3>\n<p>Most people don’t realize this, but the RubyGems.org team is incredibly small. It’s never had more than four people on it, and most often has only had two. The Bundler team is even smaller—it’s never had more than two people working on it consistently, and often only one.</p>\n<p>None of those team members were ever paid for their work. To pay their bills, every single one of them has to have a full-time job doing something else. A few volunteers giving up their nights and weekends to help the community used to be enough, but it’s not anymore.</p>\n<p>Using open source ethically means supporting the tools and infrastructure you use, with money or  time. Instead of supporting our community infrastructure, companies are taking community resources and then not giving back. These companies are making millions (and sometimes billions) of dollars. They could easily afford to support us, but choose not to. Let me give an example that really highlights this point.</p>\n<h3 id=\"remember-when-rubygemsorg-got-hacked\">remember when rubygems.org got hacked?</h3>\n<p>About 3 years ago, RubyGems.org went down completely, for over a week.  There was a security issue, and although the team knew about the security issue and planned to fix it, everyone on the team had a day job. The best we could do was plan to fix the problem that weekend. Unfortunately, a motivated hacker figured out how to break to during the week.</p>\n<p>We had to take the servers down and create completely new servers from scratch. We also had to download and verify every single one of the hundreds of thousands of .gem files, to make sure the hacker hadn’t replaced any of them while he had access.</p>\n<p>During this time, hundreds of Ruby developers volunteered to help. This isn’t super surprising, since almost every Ruby developer and company depends on RubyGems.org in order to be able to do their work. What is terrible is that none of those volunteers could do anything useful. None of them were already familiar with how the servers worked, and helping them get up to speed would have slowed down the recovery efforts.</p>\n<p>The worst part, though, is that once RubyGems.org was back up… all of those volunteers disappeared. None of them were wiling to get up to speed and help out when there wasn’t an emergency. Even though the RubyGems infrastructure is vitally important to all of these companies, they can’t or won’t allow their employees to help make sure it keeps working.</p>\n<h3 id=\"the-non-solution-of-open-source-jobs\">the non-solution of open source jobs</h3>\n<p>At this point, you might say “Aha! But companies sometimes hire Ruby developers specifically to work on open source. Let’s just keep doing that.” While that is a great thing, and I am very glad that companies sometimes choose to do that, it is not a solution to the crisis we are facing.</p>\n<p>Very few companies have ever hired Ruby developers to work on open source full time. Even including all of the last 10 years, I can only think of Engine Yard, Sun, AT&amp;T, RedHat, Heroku, Shopify, GitHub. I’m sure there are others, but there can’t be that many.</p>\n<p>Even if there were more companies doing this, though, it would still be a problem. Engine Yard is the perfect example of a very helpful company turning out to be extremely disruptive to the community. At one point in time, Engine Yard (by itself!) employed not just the Rubinius and JRuby teams, but also everyone working full-time on Rails.</p>\n<p>Eventually, though, they decided to stop. After Engine Yard wound down their open source positions, the JRuby team had moved to another company, but the Rubinius team and Rails team no longer had any full-time developers. If a single executive at SalesForce changes their mind, most of the full-time developers on Ruby core will be out of a job. We can do better than a community dependent on one or two companies.</p>\n<p>Ruby Together spreads the costs across many companies and people. Everyone shares the costs of paying developers, and everyone shares the benefits. Even better, that sharing means we’re not dependent on any single company to be able to do our work. A single company changing their mind or cutting costs is no longer enough to disrupt the community.</p>\n<h2 id=\"how-it-came-about\">how it came about</h2>\n<p>Working on Bundler and RubyGems over the last six years had made these problems clear to me, but most of the people I talked to didn’t think they were important problems. Bundler and RubyGems had always gotten along with work from volunteers! Companies were able to build huge and wildly profitable companies on top of Ruby open source, so everything seemed fine.</p>\n<p>I spent years experimenting with different ideas for how Bundler and RubyGems could make enough money to support paying developers. I tested out support contracts, feature bounties, enterprise-only features, and more. None of them were enough of an improvement for companies to want to pay for them. Even worse, every option was a huge time-sink away from the actual goal of working on the open source projects.</p>\n<p>One company was willing to support Bundler directly, without any conditions or requirements: <a href=\"https://stripe.com\">Stripe</a>. Using funds from Stripe, I was able to do research, hire a lawyer, and found Ruby Together.</p>\n<p>Ruby Together incorporated as a particular kind of non-profit company—what the U.S. government calls a “trade association”. Other trade associations you may have heard of include the Linux Foundation, the Jquery Foundation, and the Sqlite Foundation, but the best-known example is the American Dairy Farmers’ Association.</p>\n<p>The American Dairy Farmers’ Association pays for the “Got Milk?” ad campaigns. Those ad campaigns cost more than a single dairy farmer could afford, but they benefit all dairy farmers by promoting milk. Each trade association uses membership dues to fund community projects that benefit all members of that trade, whether they belong to the association or not.</p>\n<h2 id=\"what-weve-done\">what we’ve done</h2>\n<p>Speaking of benefiting everyone, let’s talk about some of the things that Ruby Together has done in it’s first year and a half. The big, underlying thing that we did is pay for 1,100 hours of developer time. Using that developer time, we’ve gotten a lot of stuff done. I can’t possibly cover everything in detail, but even an overview will give you a taste of what goes into keeping <code>bundle install</code> working month after month, and year after year.</p>\n<h3 id=\"bundler-accomplishments\">bundler accomplishments</h3>\n<p>Working on Bundler, have completed and 5 feature releases. In <a href=\"http://bundler.io/blog/2015/03/21/hello-bundler-19.html\">Bundler 1.9</a>, we started using Molinillo, a new dependency manager. It was funded by a Stripe grant and shared between Bundler, RubyGems, and CocoaPods. In <a href=\"http://bundler.io/blog/2015/06/24/version-1-10-released.html\">Bundler 1.10</a>, we added the <code>lock</code> command, optional groups, conditional groups, and the ability to mute post-install messages per gem. (Finally, it’s possible to never be told “You must HTTParty hard” again!) In <a href=\"http://bundler.io/blog/2015/12/12/version-1-11-released.html\">Bundler 1.11</a>, we dramatically improved error messages both in general and when the Gemfile cannot be resolved. In <a href=\"http://bundler.io/blog/2016/04/28/the-new-index-format-fastly-and-bundler-1-12.html\">Bundler 1.12</a>, we finished a 3-year-long project and started using a new gem metadata file format that allows us to finally stop sending every user information about every gem every time they install. We also increased the speed of <code>bundle exec</code> and added support for locking and updating the Ruby version. In <a href=\"http://bundler.io/blog/2016/09/08/bundler-1-13.html\">Bundler 1.13</a>, we added support for gems with <code>required_ruby_version</code> for Gemfiles that declare their <code>ruby</code> version, added the <code>doctor</code> command to fix broken compiled gems, and added the ability to add and remove platforms you want Bundler to resolve for. And that was just a summary of the big features!</p>\n<p>We also released <a href=\"https://rubygems.org/gems/bundler/versions\">38 bugfix releases</a>, completely <a href=\"http://bundler.io/blog/2016/07/10/bundler-1-13-and-redesigned-bundler-io.html\">redesigned the Bundler website</a>, and mentored nine <a href=\"http://summerofcode.withgoogle.com\">Google Summer of Code</a> students and four <a href=\"http://railsgirlssummerofcode.org\">RailsGirls Summer of Code</a> students across two summers.</p>\n<h3 id=\"now-rubygems-too\">now rubygems, too</h3>\n<p>As part of keeping Bundler working, Ruby Together has taken over maintenance of the <a href=\"https://github.com/rubygems/rubygems\">RubyGems project</a> as of 2016. Bundler already uses parts of RubyGems to install gems, and so it was a good fit. Even worse, ever since AT&amp;T Interactive shut down their Ruby open source department, RubyGems hasn’t had any dedicated development. For almost two years, the only significant change to RubyGems was a critical security fix.</p>\n<p>It was an awful situation. For that entire two years, installing compiled gems was broken on Windows for any Ruby version older than the latest. I’m very pleased to reveal that since the start of this year, we have fixed many bugs, including that one. Developers on Windows can install gems again. In total, we’ve released one minor and eight bugfix versions of RubyGems so far. We also have some great plans for RubyGems that I’ll cover in the section about the future, so keep reading!</p>\n<h3 id=\"oh-and-the-servers\">oh and the servers</h3>\n<p>On the server side, we’ve done a huge amount of work on RubyGems.org and on the <a href=\"https://github.com/bundler/bundler-api\">Bundler dependency API</a>. I’ve previously given <a href=\"/2013/12/09/extreme-makeover-rubygems-edition/\">a talk about setting up the Bundler API </a> as a separate thing from RubyGems.org. It’s not in my talk, but the biggest reason we had to do that was that at that time RubyGems.org didn’t have enough volunteers to stay functional if it ran the Bundler API as well.</p>\n<p>Because of that shortage, the Bundler dependency API is a completely separate application from RubyGems.org. Even though we now have one team cooperating to run everything, we have had to keep paying the cost of two separate systems: one for the gem metadata, and one for the gems themselves.</p>\n<p>For the last year and a half, we’ve applied countless security patches and  implemented the server-side part of the <a href=\"/2014/03/28/the-new-rubygems-index-format/\">new index format</a> used by Bundler 1.12. We’ve even ported the entire Bundler API Sinatra app into the RubyGems.org Rails app. Someday very soon, we’ll only have one platform to keep operational. That’s a big win, giving us more reliability while requiring us to do less work. We also switched to using the <a href=\"http://fastly.com\">Fastly CDN</a> for gem downloads, and we have ever so slowly reworked the entire architecture of RubyGems.org so that every page can be served from Fastly’s closest data center instead of from Amazon’s US-West region.</p>\n<p>On top of all of that proactive maintenance, we have paid developers to take on incident response. It’s much easier to prioritize keeping all of these systems operational when it’s paid work. Our team has handled around a dozen outages over the last 18 months, and every one of them has been shorter than it would have been without Ruby Together.</p>\n<h3 id=\"gemstash-also-pretty-neat\">gemstash, also pretty neat</h3>\n<p>One more thing that we did: we built a new tool to help everyone manage the gems that they need. It’s called <a href=\"https://github.com/bundler/gemstash\">Gemstash</a>, and it’s a server you can run that will cache every gem you download from RubyGems.org. You can run one on your local laptop to avoid downloading gems multiple times. You can run one in your office to speed up installing the gems your application needs. Or you can run one in your datacenter, and install gems across all your server from a local source. It can even act as a server for private gems, so you can keep your company’s internal gems on it, too.</p>\n<h3 id=\"bundler-20-is-coming\">bundler 2.0 is coming</h3>\n<p>Later this fall, we’re going to release <a href=\"https://github.com/bundler/bundler/issues/4853\">Bundler 1.14</a>, and then <a href=\"https://github.com/bundler/bundler/issues/4856\">Bundler 2.0</a> after that. When we were designing Bundler 1.0, we made tradeoffs based on a world that didn’t use Bundler. Today, not only Ruby developers but developers in most languages have a dependency manager—the old tradeoffs are a problem now, instead of a benefit.</p>\n<p>Plus, it’s a huge drain on our time to keep supporting Ruby 1.8.7 and all the other ancient versions of Ruby and RubyGems that were new at the time Bundler was originally released. The functionality provided by Bundler won’t change, and everyone will be able to use Bundler 1 in some projects and Bundler 2 in others on the same machine. I’m really excited about the improvements that are possible with backwards-breaking changes, and I’m looking forward to sharing those with you later this year or early next year.</p>\n<h3 id=\"plus-were-going-to-merge-bundler-and-rubygems\">plus we’re going to merge bundler and rubygems</h3>\n<p>Oh, and one more thing. For the last six years of working on Bundler, one of the most frustrating things about working on it has been the separation between Bundler and RubyGems. Sometimes making a change in one would break something in the other. The rest of the time it has just been a huge pain to make sure that they work to install gems the same way even when they aren’t working together.</p>\n<p>By talking to developers, companies, and the teams working on Bundler, RubyGems, and RubyGems.org, we’ve come up with a plan to combine the Bundler and RubyGems projects. They’ll still have separate commands, <code>gem</code> and <code>bundle</code>, but they’ll have a single codebase behind them. This summer, we mapped out everything that needed to be done for that merger, and started working on it.</p>\n<p>After Bundler 2 is released, the main focus of the Bundler and RubyGems teams will be merging the codebases together into <a href=\"https://github.com/rubygems/rubygems/issues/1681\">RubyGems+Bundler 3.0</a>.</p>\n<h2 id=\"somehow-its-working\">somehow, it’s working!</h2>\n<p>In many ways, the last year and a half has been more successful than I could have dreamed. Ruby Together is paying developers for work every week. We’ve fixed more bugs and made more progress on Bundler and RubyGems in the last year than we managed in the three years before that. We started out especially strong, and we saw new companies and developers signing up every single month for an entire year.</p>\n<h3 id=\"but-its-not-all-good\">but it’s not all good</h3>\n<p>But for the last six months, even as we’ve set new records for paid hours, while we managed the <a href=\"https://summerofcode.withgoogle.com\">Google Summer of Code</a> project for all of Ruby, and while we made these exciting plans to finally bring RubyGems and Bundler together… membership has been flat. We’ve seen a few new members, but a few members have left. Overall, we’re in the same place after 18 months as we were after 12 months: able to pay consistently for several hours a week, but not yet able to pay even two people part-time.</p>\n<h3 id=\"companies-are-short-sighted\">companies are short-sighted</h3>\n<p>Businesses are ultimately answerable to shareholders who want profit, and right now businesses think that they can get the benefits of Ruby community infrastructure and tools without needing to contribute back themselves. It’s always worked before, right?</p>\n<p>Just because it’s worked in the past doesn’t mean it will keep working in the future. As I showed earlier, the demands of the community keep growing. The situation we have now is unsustainable. RubyGems was broken on Windows for two years! Everyone in the Ruby community needs to know about the crisis that we’re heading towards. Every company in the community needs to understand that we can’t keep giving them a free ride even if we wanted to.</p>\n<p>Some companies (like <a href=\"https://stripe.com\">Stripe</a>, <a href=\"http://codeminer42.com\">CodeMiner42</a>, <a href=\"http://basecamp.com\">Basecamp</a>, <a href=\"http://travis-ci.com\">Travis</a>, and others) have stepped up to support us with actual money. Other companies tell us it’s a great initiative, and they’re really glad we exist, but they’re not willing to pitch in. We haven’t seen any new companies join for almost six months. If that keeps up, we won’t even be able to keep up paying for a few hours of work per week. If companies keep taking community benefits without giving back, it takes us straight back to unsustainable volunteers burning themselves out.</p>\n<h3 id=\"cooperating-is-good-for-everyone\">cooperating is good for everyone</h3>\n<p>We don’t want that! There’s a better way. It’s not even hard, or expensive. At Ruby Together, we’re already prepared to keep everything working—we just need support from Ruby developers and companies to make that possible.</p>\n<p>We offer the entire community a great deal: free tools and free hosting for Ruby code they want to share with the world. We offer a fantastic deal for companies, too: the benefits of full-time infrastructure developers at a tiny fraction of the cost.</p>\n<p>So for now, we’re asking: help us work on <a href=\"https://bundler.io\">Bundler</a>, <a href=\"https://rubygems.org\">RubyGems</a>, and everything else by <a href=\"https://rubytogether.org/join\">signing up at rubytogether.org</a>. Everything we build with money from the community is shared back to the community, and that’s what Ruby Together is all about.</p>\n<p><img src=\"https://indirect.micro.blog/uploads/2025/58da223eea.jpg\" alt=\"Let&rsquo;s Ruby Together\"></p>\n",
				"content_text": "\n<small>This post was originally given as a presentation at [EuRuKo 2016](http://2016.euruko.org). ([slides](https://speakerdeck.com/indirect/a-year-of-ruby-together))</small>\n\n<script async class=\"speakerdeck-embed\" data-id=\"94b37277d3d6423d9ebeb94de2add27b\" data-ratio=\"1.33333333333333\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n\n[Ruby Together](https://rubytogether.org) has been around for over a year! It’s actually been about 18 months, but that didn’t sound as snappy. To celebrate, I’ve put together this summary of our history: why and how we exist, what we’ve done, and what the future holds.\n\nSo, what even _is_ Ruby Together? Ruby Together is a non-profit that pays developers to work on RubyGems, Bundler, and more, using funds from members. Members can be any person or company who uses Ruby, and the work we pay for is free for anyone to use.\n\nBut what, exactly, does that mean? And why do we need a non-profit to do this in the first place? The explanation lies the current state of the Ruby community.\n\n## there are some problems\nPut simply, the problem is that Ruby has grown up. The Ruby community has become so large that our existing community infrastructure isn’t sustainable anymore.\n\n### the gems! they’re multiplying!\nAs you’ve probably noticed, Bundler makes it easier to use gems and makes it easier to create gems. Over the 6 years that I’ve spent working on Bundler, the number of gems in existence has multiplied, and the number of gem downloads has multiplied. There are now over 100,000 gems, and those gems have over a million versions.\n\nWith those million versions comes a completely unprecedented number of downloads. In the years from 2003 to the end of 2014, we tracked roughly 2 billion gem downloads. That’s 2 billion downloads over more than ten years. Then, in 2015 alone (just one year!) we served 4 billion gem downloads.\n\nThat is an _extremely_ steep increase. If we were a startup, we’d have great graphs to show VCs and convince them to give us money. But we’re not a startup. Before Ruby Together, all of RubyGems.org was maintained for free by volunteers. That’s a problem, because at the same time that usage has been increasing exponentially, volunteer help hasn’t been increasing.\n\n### the volunteer crisis\nMost people don’t realize this, but the RubyGems.org team is incredibly small. It’s never had more than four people on it, and most often has only had two. The Bundler team is even smaller—it’s never had more than two people working on it consistently, and often only one.\n\nNone of those team members were ever paid for their work. To pay their bills, every single one of them has to have a full-time job doing something else. A few volunteers giving up their nights and weekends to help the community used to be enough, but it’s not anymore. \n\nUsing open source ethically means supporting the tools and infrastructure you use, with money or  time. Instead of supporting our community infrastructure, companies are taking community resources and then not giving back. These companies are making millions (and sometimes billions) of dollars. They could easily afford to support us, but choose not to. Let me give an example that really highlights this point.\n\n### remember when rubygems.org got hacked?\nAbout 3 years ago, RubyGems.org went down completely, for over a week.  There was a security issue, and although the team knew about the security issue and planned to fix it, everyone on the team had a day job. The best we could do was plan to fix the problem that weekend. Unfortunately, a motivated hacker figured out how to break to during the week.\n\nWe had to take the servers down and create completely new servers from scratch. We also had to download and verify every single one of the hundreds of thousands of .gem files, to make sure the hacker hadn’t replaced any of them while he had access.\n\nDuring this time, hundreds of Ruby developers volunteered to help. This isn’t super surprising, since almost every Ruby developer and company depends on RubyGems.org in order to be able to do their work. What is terrible is that none of those volunteers could do anything useful. None of them were already familiar with how the servers worked, and helping them get up to speed would have slowed down the recovery efforts.\n\nThe worst part, though, is that once RubyGems.org was back up… all of those volunteers disappeared. None of them were wiling to get up to speed and help out when there wasn’t an emergency. Even though the RubyGems infrastructure is vitally important to all of these companies, they can’t or won’t allow their employees to help make sure it keeps working.\n\n### the non-solution of open source jobs\nAt this point, you might say “Aha! But companies sometimes hire Ruby developers specifically to work on open source. Let’s just keep doing that.” While that is a great thing, and I am very glad that companies sometimes choose to do that, it is not a solution to the crisis we are facing.\n\nVery few companies have ever hired Ruby developers to work on open source full time. Even including all of the last 10 years, I can only think of Engine Yard, Sun, AT&T, RedHat, Heroku, Shopify, GitHub. I’m sure there are others, but there can’t be that many.\n\nEven if there were more companies doing this, though, it would still be a problem. Engine Yard is the perfect example of a very helpful company turning out to be extremely disruptive to the community. At one point in time, Engine Yard (by itself!) employed not just the Rubinius and JRuby teams, but also everyone working full-time on Rails.\n\nEventually, though, they decided to stop. After Engine Yard wound down their open source positions, the JRuby team had moved to another company, but the Rubinius team and Rails team no longer had any full-time developers. If a single executive at SalesForce changes their mind, most of the full-time developers on Ruby core will be out of a job. We can do better than a community dependent on one or two companies.\n\nRuby Together spreads the costs across many companies and people. Everyone shares the costs of paying developers, and everyone shares the benefits. Even better, that sharing means we’re not dependent on any single company to be able to do our work. A single company changing their mind or cutting costs is no longer enough to disrupt the community.\n\n## how it came about\nWorking on Bundler and RubyGems over the last six years had made these problems clear to me, but most of the people I talked to didn’t think they were important problems. Bundler and RubyGems had always gotten along with work from volunteers! Companies were able to build huge and wildly profitable companies on top of Ruby open source, so everything seemed fine.\n\nI spent years experimenting with different ideas for how Bundler and RubyGems could make enough money to support paying developers. I tested out support contracts, feature bounties, enterprise-only features, and more. None of them were enough of an improvement for companies to want to pay for them. Even worse, every option was a huge time-sink away from the actual goal of working on the open source projects.\n\nOne company was willing to support Bundler directly, without any conditions or requirements: [Stripe](https://stripe.com). Using funds from Stripe, I was able to do research, hire a lawyer, and found Ruby Together.\n\nRuby Together incorporated as a particular kind of non-profit company—what the U.S. government calls a “trade association”. Other trade associations you may have heard of include the Linux Foundation, the Jquery Foundation, and the Sqlite Foundation, but the best-known example is the American Dairy Farmers’ Association.\n\nThe American Dairy Farmers’ Association pays for the “Got Milk?” ad campaigns. Those ad campaigns cost more than a single dairy farmer could afford, but they benefit all dairy farmers by promoting milk. Each trade association uses membership dues to fund community projects that benefit all members of that trade, whether they belong to the association or not.\n\n## what we’ve done\nSpeaking of benefiting everyone, let’s talk about some of the things that Ruby Together has done in it’s first year and a half. The big, underlying thing that we did is pay for 1,100 hours of developer time. Using that developer time, we’ve gotten a lot of stuff done. I can’t possibly cover everything in detail, but even an overview will give you a taste of what goes into keeping `bundle install` working month after month, and year after year.\n\n### bundler accomplishments\nWorking on Bundler, have completed and 5 feature releases. In [Bundler 1.9](http://bundler.io/blog/2015/03/21/hello-bundler-19.html), we started using Molinillo, a new dependency manager. It was funded by a Stripe grant and shared between Bundler, RubyGems, and CocoaPods. In [Bundler 1.10](http://bundler.io/blog/2015/06/24/version-1-10-released.html), we added the `lock` command, optional groups, conditional groups, and the ability to mute post-install messages per gem. (Finally, it’s possible to never be told “You must HTTParty hard” again!) In [Bundler 1.11](http://bundler.io/blog/2015/12/12/version-1-11-released.html), we dramatically improved error messages both in general and when the Gemfile cannot be resolved. In [Bundler 1.12](http://bundler.io/blog/2016/04/28/the-new-index-format-fastly-and-bundler-1-12.html), we finished a 3-year-long project and started using a new gem metadata file format that allows us to finally stop sending every user information about every gem every time they install. We also increased the speed of `bundle exec` and added support for locking and updating the Ruby version. In [Bundler 1.13](http://bundler.io/blog/2016/09/08/bundler-1-13.html), we added support for gems with `required_ruby_version` for Gemfiles that declare their `ruby` version, added the `doctor` command to fix broken compiled gems, and added the ability to add and remove platforms you want Bundler to resolve for. And that was just a summary of the big features!\n\nWe also released [38 bugfix releases](https://rubygems.org/gems/bundler/versions), completely [redesigned the Bundler website](http://bundler.io/blog/2016/07/10/bundler-1-13-and-redesigned-bundler-io.html), and mentored nine [Google Summer of Code](http://summerofcode.withgoogle.com) students and four [RailsGirls Summer of Code](http://railsgirlssummerofcode.org) students across two summers.\n\n### now rubygems, too\nAs part of keeping Bundler working, Ruby Together has taken over maintenance of the [RubyGems project](https://github.com/rubygems/rubygems) as of 2016. Bundler already uses parts of RubyGems to install gems, and so it was a good fit. Even worse, ever since AT&T Interactive shut down their Ruby open source department, RubyGems hasn’t had any dedicated development. For almost two years, the only significant change to RubyGems was a critical security fix.\n\nIt was an awful situation. For that entire two years, installing compiled gems was broken on Windows for any Ruby version older than the latest. I’m very pleased to reveal that since the start of this year, we have fixed many bugs, including that one. Developers on Windows can install gems again. In total, we’ve released one minor and eight bugfix versions of RubyGems so far. We also have some great plans for RubyGems that I’ll cover in the section about the future, so keep reading!\n\n### oh and the servers\nOn the server side, we’ve done a huge amount of work on RubyGems.org and on the [Bundler dependency API](https://github.com/bundler/bundler-api). I’ve previously given [a talk about setting up the Bundler API ](/2013/12/09/extreme-makeover-rubygems-edition/) as a separate thing from RubyGems.org. It’s not in my talk, but the biggest reason we had to do that was that at that time RubyGems.org didn’t have enough volunteers to stay functional if it ran the Bundler API as well.\n\nBecause of that shortage, the Bundler dependency API is a completely separate application from RubyGems.org. Even though we now have one team cooperating to run everything, we have had to keep paying the cost of two separate systems: one for the gem metadata, and one for the gems themselves. \n\nFor the last year and a half, we’ve applied countless security patches and  implemented the server-side part of the [new index format](/2014/03/28/the-new-rubygems-index-format/) used by Bundler 1.12. We’ve even ported the entire Bundler API Sinatra app into the RubyGems.org Rails app. Someday very soon, we’ll only have one platform to keep operational. That’s a big win, giving us more reliability while requiring us to do less work. We also switched to using the [Fastly CDN](http://fastly.com) for gem downloads, and we have ever so slowly reworked the entire architecture of RubyGems.org so that every page can be served from Fastly’s closest data center instead of from Amazon’s US-West region.\n\nOn top of all of that proactive maintenance, we have paid developers to take on incident response. It’s much easier to prioritize keeping all of these systems operational when it’s paid work. Our team has handled around a dozen outages over the last 18 months, and every one of them has been shorter than it would have been without Ruby Together.\n\n### gemstash, also pretty neat\nOne more thing that we did: we built a new tool to help everyone manage the gems that they need. It’s called [Gemstash](https://github.com/bundler/gemstash), and it’s a server you can run that will cache every gem you download from RubyGems.org. You can run one on your local laptop to avoid downloading gems multiple times. You can run one in your office to speed up installing the gems your application needs. Or you can run one in your datacenter, and install gems across all your server from a local source. It can even act as a server for private gems, so you can keep your company’s internal gems on it, too.\n\n### bundler 2.0 is coming\nLater this fall, we’re going to release [Bundler 1.14](https://github.com/bundler/bundler/issues/4853), and then [Bundler 2.0](https://github.com/bundler/bundler/issues/4856) after that. When we were designing Bundler 1.0, we made tradeoffs based on a world that didn’t use Bundler. Today, not only Ruby developers but developers in most languages have a dependency manager—the old tradeoffs are a problem now, instead of a benefit.\n\nPlus, it’s a huge drain on our time to keep supporting Ruby 1.8.7 and all the other ancient versions of Ruby and RubyGems that were new at the time Bundler was originally released. The functionality provided by Bundler won’t change, and everyone will be able to use Bundler 1 in some projects and Bundler 2 in others on the same machine. I’m really excited about the improvements that are possible with backwards-breaking changes, and I’m looking forward to sharing those with you later this year or early next year.\n\n### plus we’re going to merge bundler and rubygems\nOh, and one more thing. For the last six years of working on Bundler, one of the most frustrating things about working on it has been the separation between Bundler and RubyGems. Sometimes making a change in one would break something in the other. The rest of the time it has just been a huge pain to make sure that they work to install gems the same way even when they aren’t working together.\n\nBy talking to developers, companies, and the teams working on Bundler, RubyGems, and RubyGems.org, we’ve come up with a plan to combine the Bundler and RubyGems projects. They’ll still have separate commands, `gem` and `bundle`, but they’ll have a single codebase behind them. This summer, we mapped out everything that needed to be done for that merger, and started working on it.\n\nAfter Bundler 2 is released, the main focus of the Bundler and RubyGems teams will be merging the codebases together into [RubyGems+Bundler 3.0](https://github.com/rubygems/rubygems/issues/1681).\n\n## somehow, it’s working!\nIn many ways, the last year and a half has been more successful than I could have dreamed. Ruby Together is paying developers for work every week. We’ve fixed more bugs and made more progress on Bundler and RubyGems in the last year than we managed in the three years before that. We started out especially strong, and we saw new companies and developers signing up every single month for an entire year.\n\n### but it’s not all good\nBut for the last six months, even as we’ve set new records for paid hours, while we managed the [Google Summer of Code](https://summerofcode.withgoogle.com) project for all of Ruby, and while we made these exciting plans to finally bring RubyGems and Bundler together… membership has been flat. We’ve seen a few new members, but a few members have left. Overall, we’re in the same place after 18 months as we were after 12 months: able to pay consistently for several hours a week, but not yet able to pay even two people part-time.\n\n### companies are short-sighted\nBusinesses are ultimately answerable to shareholders who want profit, and right now businesses think that they can get the benefits of Ruby community infrastructure and tools without needing to contribute back themselves. It’s always worked before, right?\n\nJust because it’s worked in the past doesn’t mean it will keep working in the future. As I showed earlier, the demands of the community keep growing. The situation we have now is unsustainable. RubyGems was broken on Windows for two years! Everyone in the Ruby community needs to know about the crisis that we’re heading towards. Every company in the community needs to understand that we can’t keep giving them a free ride even if we wanted to.\n\nSome companies (like [Stripe](https://stripe.com), [CodeMiner42](http://codeminer42.com), [Basecamp](http://basecamp.com), [Travis](http://travis-ci.com), and others) have stepped up to support us with actual money. Other companies tell us it’s a great initiative, and they’re really glad we exist, but they’re not willing to pitch in. We haven’t seen any new companies join for almost six months. If that keeps up, we won’t even be able to keep up paying for a few hours of work per week. If companies keep taking community benefits without giving back, it takes us straight back to unsustainable volunteers burning themselves out.\n\n### cooperating is good for everyone\nWe don’t want that! There’s a better way. It’s not even hard, or expensive. At Ruby Together, we’re already prepared to keep everything working—we just need support from Ruby developers and companies to make that possible.\n\nWe offer the entire community a great deal: free tools and free hosting for Ruby code they want to share with the world. We offer a fantastic deal for companies, too: the benefits of full-time infrastructure developers at a tiny fraction of the cost.\n\nSo for now, we’re asking: help us work on [Bundler](https://bundler.io), [RubyGems](https://rubygems.org), and everything else by [signing up at rubytogether.org](https://rubytogether.org/join). Everything we build with money from the community is shared back to the community, and that’s what Ruby Together is all about.\n\n![Let's Ruby Together](https://indirect.micro.blog/uploads/2025/58da223eea.jpg)\n",
				"date_published": "2016-09-26T00:00:00-08:00",
				"url": "https://andre.arko.net/2016/09/26/a-year-of-ruby-together/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2016/08/31/including-people/",
				"title": "Including People",
				"content_html": "<p><small>This post was also given as a conference talk at <a href=\"http://ruby.onales.com\">Ruby on Ales 2016</a> and <a href=\"https://allremoteconfs.com/ruby-2016\">Ruby Remote Conf 2016</a>. If you prefer, you can <a href=\"https://www.youtube.com/watch?v=MrPtHogES6k\">watch the video</a> or <a href=\"https://speakerdeck.com/indirect/including-people-1\">see the slides</a>.</small></p>\n<script async class=\"speakerdeck-embed\" data-id=\"42eecdbf2fcd4f188426726cf7e4e013\" data-ratio=\"1.33333333333333\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n<p>This is about including people in projects, teams, and communities, but it would be more accurate to say “not excluding people”. Tech is a highly exclusive field, and we need to stop driving people away before we can make them feel welcome and included.</p>\n<p>To be clear: I’m not an expert at this, but I have (sometimes painfully) learned many ways to exclude people. I’ll tell you about them, and then suggest ways to avoid them! That’s not enough to guarantee inclusiveness, but it’s at least a start. That said, I can&rsquo;t be sure my suggestions will work for everyone. Keep your eyes open, form hypotheses, experiment, and evaluate the results.</p>\n<h3 id=\"wait-why-is-a-white-dude-talking-about-inclusion\">wait, why is a white dude talking about inclusion</h3>\n<p>Right off the bat, I’d like to point out that it’s kind of weird for me to be talking about anything related to diversity or inclusion—I’m cis, straight, white, and male. People assume that I know what I’m talking about (whether I do or not), and my experience with the programming community has been largely positive. As a result, these suggestions are the result of research, reading, and listening to excluded people about their experiences.</p>\n<p>But the main reason why I’m writing, as a white male, is that anyone else would suffer backlash for it. As reported in the Harvard Business Review, underrepresented people who bring up theses issues are <a href=\"https://hbr.org/2016/03/women-and-minorities-are-penalized-for-promoting-diversity\">penalized at work</a>. When women and minorities try to surface these issues, they lose social status and prestige, and their coworkers and managers question their competence.</p>\n<p>Now that you know why I’m the one bringing this up, let’s talk about it. It’s really important, we’ve been ignoring it for a long time, and everyone needs to act thoughtfully and considerately for us to have any chance of improving things.</p>\n<h3 id=\"lets-talk-about-diversity\">let’s talk about “diversity”</h3>\n<p>The topic of &ldquo;diversity&rdquo; has gotten a lot of press lately, both in tech in general and in the Ruby community in particular. The history of pervasive sexism and harassment in the Ruby community has even gone mainstream—it was the biggest fact about Ruby in a Bloomberg magazine article by Paul Ford called “<a href=\"http://www.bloomberg.com/graphics/2015-paul-ford-what-is-code/\">What is Code?</a>”, aimed at non-technical readers.</p>\n<p>Over the last few years, I&rsquo;ve put a lot of time and energy into research. I&rsquo;ve also written about <a href=\"/2013/12/04/how-to-be-an-ally/\">how to be an ally</a>, and about <a href=\"http://www.ashedryden.com/la-conf-tbd\">why diversity matters</a>. First, I’ll briefly summarize the current situation.</p>\n<blockquote>\n<p>How to start increasing inclusion in tech: s/diversity/inclusion/g — <a href=\"https://twitter.com/rockbot/status/622109542533300224\">@rockbot</a></p>\n</blockquote>\n<p>To start with, diversity is the wrong word to use while talking about these problems. The reason that underrepresented people are underrepresented is not because they don’t exist—graduating computer science majors include many women and many people of color, to name just two underrepresented groups.</p>\n<blockquote>\n<p>“Diversity is being asked to sit at the table while inclusivity is being asked your opinion at the table you are sitting at.” — <a href=\"https://twitter.com/taylor_atx/status/623598547837939712\">@taylor_atx</a></p>\n</blockquote>\n<p>Even though these people absolutely exist, they are not included in tech. Their opinions are not listened to, their feedback is not acted on, and their technical skills are suspect based purely on their background.</p>\n<p>The lack of diversity in tech is unsolvable as long as we remain exclusive. We need to become inclusive people, creating an inclusive community. In an inclusive community, there is no “diversity problem”, because everyone feels equally welcome.</p>\n<p>I talk about this in conversations with a lot of people, and this is usually the point where someone tells me that they feel very welcome and included, so this can’t possibly be the reason for the lack of diversity in tech. That position is demonstrably wrong.</p>\n<h3 id=\"we-have-a-bias--exclusion-problem\">we have a bias &amp; exclusion problem</h3>\n<p>One underlying problem is people being denied education, jobs, and promotions solely because of bias. Coworkers, managers, executives, and VCs have all been shown to unreasonably overvalue work by young, white, straight men, while unreasonably undervaluing work by anyone else.</p>\n<p>There is a huge amount of repeated research showing these biases, because of gender, <a href=\"http://news.rutgers.edu/research-news/rutgers-study-employers-discriminate-against-qualified-workers-disabilities/20151104#.VorAgZMrKRs\">disabilities</a>, <a href=\"http://williamsinstitute.law.ucla.edu/wp-content/uploads/Sears-Mallory-Discrimination-July-20111.pdf\">sexual orientation or status</a>, <a href=\"http://www.latimes.com/business/technology/la-fi-tn-asian-american-tech-20150506-story.html\">race</a>, age, weight, or any of a thousand other things. Just changing the name, gender, or race on a resume is enough to change a “good” resume to an “unacceptable” one.</p>\n<p>Even though this is a thoroughly researched phenomenon, people disagree with me every time I talk about this. They say things like &ldquo;<a href=\"butitsameritocracy.com\">but tech is a meritocracy</a> and everyone is valued based on their contributions to the whole&rdquo;. Well, about that…</p>\n<h3 id=\"meritocracy-is-a-lie-told-by-rich-white-men\">meritocracy is a lie told by rich white men</h3>\n<p>Successful people often <a href=\"https://en.wikipedia.org/wiki/Actor%E2%80%93observer_asymmetry\">assume they caused their success</a>, and then <a href=\"https://en.wikipedia.org/wiki/Survivorship_bias\">wrongly conclude</a> that anyone less successful <a href=\"https://en.wikipedia.org/wiki/Just-world_hypothesis\">somehow deserved to fail</a>. This is not only the <a href=\"https://en.wikipedia.org/wiki/Fundamental_attribution_error\">fundamental attribution error</a>, but holding this idea actively causes discrimination! At companies that claim to be a meritocracy, performance evaluations and hiring decisions <a href=\"http://www.tarahunt.com/blog/2013/10/28/meritocracy-is-almost-as-real-as-this-unicorn\">are <em>even more</em> biased</a>.</p>\n<p>Once that’s out of the way, everyone tells me that I should “obviously” focus on <a href=\"isitapipelineproblem.com\">the pipeline problem</a>. If new developers are diverse, then all of tech will become diverse eventually, right? Again: nope.</p>\n<h3 id=\"the-pipeline-leads-straight-to-a-sewage-plant\">the pipeline leads straight to a sewage plant</h3>\n<p>Let’s start with the most obvious counterpoint: when the first electronic computers were built, <a href=\"http://labweb.education.wisc.edu/elpa940/readings/Light.pdf\">all programmers were women</a>. The title “Computer” referred to a woman who performed calculations. The job of “Programmer” was created by those women as they wrote programs to automate their computational work. Despite starting at 100% women, programmers today are less than 25% women. If the trend continues, within a few decades no women at all will be programmers.</p>\n<p>Young women already like computers and are interested in working with them. <a href=\"https://mfbt.ca/your-diversity-problem-isnt-the-pipeline-s-fault-820e8e7e2b77#.pv80ijfnj\">There isn&rsquo;t anything wrong with the pipeline</a>, but tech is so hostile that it drives even interested women away. This is why we need inclusion—we had diversity already, but we fucked it up.</p>\n<p>At this point, objectors usually pivot to another tactic. I can&rsquo;t even count the number of times I&rsquo;ve heard the comeback “including everyone is a good idea, but it would lower the bar”.</p>\n<h3 id=\"the-bar-is-already-on-the-floor\">the bar is already on the floor</h3>\n<p>First, the very phrase &ldquo;lowering the bar&rdquo; is <a href=\"https://byrslf.co/what-you-re-really-saying-when-you-talk-about-lowering-the-bar-in-hiring-a30d9b12430f#.gfv372a53\">full of racist assumptions</a>. Second, research into bias during job applications shows that <a href=\"http://whatwouldkingleonidasdo.tumblr.com/post/54989171152/how-i-discovered-gender-discrimination\">white men have a huge advantage</a> while experience and competence are dismissed in  <a href=\"http://gender.stanford.edu/news/2014/why-does-john-get-stem-job-rather-jennifer\">women</a> or <a href=\"http://www.nber.org/digest/sep03/w9873.html\">people of color</a>.</p>\n<p>Despite the research showing otherwise, big tech companies claim they value everyone equally. Their dismal diversity reports are explained away as part of the pipeline problem (which, as we just covered, doesn&rsquo;t exist!) On top of that, employees confirm these companies <a href=\"https://42hire.com/the-big-lie-tech-companies-and-diversity-hiring-f52fb82abfbf#.i4qwkryed\">continue to treat white men as inherently better</a>, no matter what they say.</p>\n<blockquote>\n<p>Hiring Manager: &ldquo;We &lt;3 diversity, but can&rsquo;t lower the bar for our devs&rdquo;<br>\nMe: &ldquo;No, I was suggesting raising it. But, for your hiring managers&rdquo;<br>\n— <a href=\"https://twitter.com/josh_cheek/status/770624979436199936\">@josh_cheek</a></p>\n</blockquote>\n<p>In America in general and in tech specifically, the bar for white dudes is already unbelievably low. The reality is the exact opposite of the claim: inclusion requires <em>raising</em> the bar—up above incompetent white men. At that point, more jobs can go to underrepresented competent people.</p>\n<p>At this point, arguments for exclusion pivot to one more extremely popular claim: companies would love to be inclusive, but they can&rsquo;t because it would be costly and hurt profits.</p>\n<h3 id=\"the-economic-argument-is-diversionary-bullshit\">the economic argument is diversionary bullshit</h3>\n<p>I&rsquo;ve written about <a href=\"/2014/05/24/the-economic-argument-for-diversity/\">the economic argument for diversity</a> before, but the short response is “no, wrong again”. Research shows that <a href=\"http://www.scientificamerican.com/article/how-diversity-makes-us-smarter/\">diverse teams produce better results in less time</a>.  Diversity is measurably beneficial whether it is <a href=\"https://hbr.org/2014/03/the-case-for-team-diversity-gets-even-better/\">different expertise</a> or merely <a href=\"https://hbr.org/2013/12/how-diversity-can-drive-innovation\">different backgrounds or perspectives</a>. Companies choose to uphold the exclusive status quo even when it means harming themselves.</p>\n<p>But the underlying problem is even worse: there is no excuse for treating a group of people as inferior. Ever. Mercenary attempts to include underrepresented groups for the purpose of &ldquo;improving productivity&rdquo; are doomed to fail from the start. Companies motivated solely by profit will never be inclusive, because that requires valuing people as human beings rather than interchangeable cogs in a machine that creates value for investors.</p>\n<h3 id=\"inclusion-is-an-attitude-and-a-philosophy-of-interaction\">inclusion is an attitude, and a philosophy of interaction</h3>\n<p>Since you&rsquo;re still here, I&rsquo;m guessing that you&rsquo;re interested in ways to be genuinely inclusive (especially in open source projects, with no profits to worry about). Being inclusive isn&rsquo;t just a checklist of things for you to do: it&rsquo;s an attitude to adopt in every social interaction. That attitude needs to not just be present, but supported and defended against the condescending, hostile attitudes that are all too common in software.</p>\n<h3 id=\"people-are-why-you-are-here\">people are why you are here</h3>\n<p>Software is made by people for use by people. Keep that in mind while you work on it! Projects usually involve three groups of people: end-users, potential contributors, and the existing team. It&rsquo;s easy to be inclusive towards one of those groups while still being hostile to other groups. In the Linux kernel, for example, end-users are welcomed and documentation is provided for them. Contributors, on the other hand, are semi-regularly told that they are brainless imbeciles (or even worse language). Not so good on the welcoming, there.</p>\n<h3 id=\"include-your-users\">include your users</h3>\n<p>So, let&rsquo;s start with end-users. How can you be welcoming and inclusive of the people who use the software you work on? There are a lot of ways. People who are interested in using your software will pay attention to the other people working on it. They will pay attention to the other people using it. They will pay attention to whether your docs only ever say &ldquo;he&rdquo;. They will notice if reporting an error gets the response &ldquo;that&rsquo;s stupid&rdquo;. So here are some straightforward ways to make sure that people using the code you write feel included.</p>\n<h3 id=\"codes-of-conduct\">codes of conduct</h3>\n<p>The most straightforward way to include people who are interested in your project is to establish a code of conduct, and then enforce it. At this point, there are a lot of examples of projects that have a code of conduct. Bundler is one, the Rust programming language is one. Coraline Ada Ehmke has even created a template called the <a href=\"http://contributor-covenant.org\">Contributor Covenant</a> that any project can use.</p>\n<p>A code of conduct makes it clear that the project intends to provide a safe space for users and contributors that is free from harassment. Without one, anyone who experiences harassment on the internet will be much less likely to become involved in your project. That would be a tragedy, because most people who aren&rsquo;t white caucasian males on the internet experience at least some harassment.</p>\n<h3 id=\"write-great-documentation\">write great documentation</h3>\n<p>Next up, documentation. It&rsquo;s a thing. You probably don&rsquo;t have as much as you think you should, and chances are high you&rsquo;re embarrassed about it. That&rsquo;s okay. I work on a lot of projects, and none of them have perfect documentation. The ground rules for helpful, inclusive documentation for end-users are pretty straightforward: use &ldquo;she&rdquo; or &ldquo;they&rdquo; instead of &ldquo;he&rdquo;, and write some documentation that is aimed at newcomers to the project. Just that, and nothing else, is enough to get you off to a good start.</p>\n<p>You can do better than that, though! For a fantastic explanation of how to write documentation that is genuinely helpful to users, check out the talk <a href=\"http://www.slideshare.net/jacobian/writing-great-documentation-codeconf-2011\">Writing Great Documentation</a> by Jacob Kaplan-Moss. He outlines four kinds of documentation that your project needs to be useful to all the people who will look for it.</p>\n<p>The first type of documentation is tutorials. Tutorials should explain how to accomplish a specific task. It should be possible to follow them start to finish in 30 minutes or less, and they should link to other tutorials or topic guides whenever they are relevant. Next up, topic guides. Topic guides explain a single topic in detail, mentioning the options, details, and unexpected bits that are specific to that topic. Topic guides should refer to other tutorials and topic guides as they&rsquo;re relevant. Then there&rsquo;s reference documentation. This is the API docs for your project. They are the most likely to already exist, but they are also completely useless for someone who doesn&rsquo;t already know what they&rsquo;re looking for. This is the reason that Rails has both extensive API docs and a guides project full of topic guides.</p>\n<p>Finally, troubleshooting documentation. There will be problems that come up again and again—if you can, fix them in your code. If you can&rsquo;t, add them to the troubleshooting guide. Write down the steps that you suggest to people having problems, and explain exactly what information you will need to be able to help someone who has a problem that they can&rsquo;t fix on their own. As you write it, keep in mind that the person reading it will be frustrated at minimum and raging at worst. Keep it straightforward, don&rsquo;t condescend, and try to be helpful.</p>\n<p>Speaking of troubleshooting, how your project handles issue reports is a huge indicator of how inclusive your project is. If you respond to a new issue with &ldquo;that doesn&rsquo;t make sense&rdquo; or &ldquo;why were trying to do that&rdquo;, the hostility implicit in those statements will be heard loud and clear. Start with the idea that whatever the reporter is trying to makes sense to them. Then, try to figure out what context they have that you don&rsquo;t (or that you have that they don&rsquo;t) that can help solve the issue.</p>\n<h3 id=\"give-better-answers\">give better answers</h3>\n<p>The talk <a href=\"http://pewpewthespells.com/conf/altconf/2015.html\">Giving Better Answers</a>, given by Sam Marshall at Alterconf Chicago in 2015, has really great concrete examples of how to give answers that are helpful without being condescending or hostile. Finally, always thank users for taking the time to report the error to you (they didn&rsquo;t have to!). Always let them know than even if their problem isn&rsquo;t a bug, you&rsquo;ll try to help them, or at least suggest somewhere else that they can go to get more help.</p>\n<p>Closely related to issue reports: IRC channels, Slack rooms, emails, and Twitter. Don&rsquo;t just follow the code of conduct in all of those places, make sure that the code of conduct is being followed by others. Anyone that you have allowed to officially represent your project, whether core team or just a contributor, needs to understand that harassment and condescension is not okay. Listen and speak respectfully. Follow the code of conduct yourself, and call out anyone who doesn&rsquo;t.</p>\n<p>Finally, and this is true for every project: you have to enforce the code of conduct with everyone, whether user, contributor, core member, or yourself. If you have a code of conduct but let people slide, your project will become known as not just hostile, but hypocritically hostile.</p>\n<h3 id=\"include-your-occasional-contributors\">include your occasional contributors</h3>\n<p>Now let&rsquo;s talk about how to welcome contributors to your project. Everything that I&rsquo;ve said so far about welcoming users applies to welcoming contributors, too. A safe environment without harassment is a requirement. Documentation helps a lot. Just having those things, though, isn&rsquo;t enough. There&rsquo;s more you can do to encourage contributions to your project.</p>\n<p>First, and I can&rsquo;t say this enough times, the biggest thing you can do to encourage contributions to your project is to ask for help. (Let me take this moment to say that I run the <a href=\"bundler.io\">Bundler</a> project, and we would <em>love</em> to <a href=\"https://github.com/bundler/bundler/blob/master/CONTRIBUTING.md\">have your help</a>!) Most people think that they need to be total experts on something before they can even begin to help.</p>\n<p>Let me make it extremely clear: I have never been an expert on any project that I have started to work on. I have also learned more by working on open source projects than any other way that I have learned anything about programming. When you ask for help, and when you write a document explaining how people can contribute, make it clear that they don&rsquo;t need to be an expert to be able to help you. Schedule times to pair with contributors at any skill level, and then actually pair with them so they can get started.</p>\n<p>Write development documentation. This is completely different from the documentation that targets end-users. It needs to explain how to set up the project locally for development and how to run the tests. It needs to explain who runs the project, what policies there are for contributions, and how to contact the other contributors on the project.</p>\n<p>Don&rsquo;t stop there, though. If there is anything you have to do repeatedly, like triaging issues, write a document that explains the steps. Write a list of every type of help you would like to receive. For Bundler, that includes fixing typos, writing docs, triaging tickets, refactoring code, fixing bugs, implementing features, and lots more. We have a dedicated document that not only lists each kind of help we would like to get, but links to guides for that specific type of help.</p>\n<p>Treat pull requests the same way you treat issue reports: the person opening the pull request has context that means what they are doing makes sense to them. Even if there is no chance that you will ever accept the PR, thank them for making it (they didn&rsquo;t have to contribute!). Explain your reasoning. Try to understand the underlying problem that drove them to send the PR, and see if there is a way you can help them even if you aren&rsquo;t willing to accept the patch. Respect their intelligence and their time, and they&rsquo;re likely to keep contributing.</p>\n<h3 id=\"include-your-team\">include your team</h3>\n<p>It&rsquo;s important to apply all of these principles to your entire team. Respond to code of conduct violations aimed at your team members. Make it clear that you have their back. Tell them that you appreciate their help. Apply the exact same principles that we just discussed for users and contributors: assume that what they are doing makes sense to them, and work to find the context that isn&rsquo;t shared so that you can understand each other. Finally, give positive feedback and make requests. Collaborate with your team, discuss decisions, and listen to their input.</p>\n<h3 id=\"apologize-for-real-though\">apologize. for real, though.</h3>\n<p>I screw up on a regular basis. Chances are also pretty good that you will screw this up at some point—we all make mistakes, and that’s okay! Being inclusive is an ongoing process, and part of that process is learning from mistakes. When you act in a way that excludes or alienates people, they might talk to you about it. Appreciate them for it! You’re being given a chance to learn and improve.</p>\n<p>Most human beings respond with defensiveness when they hear they made a mistake. It’s really important to recognize that urge, acknowledge that it is normal, and completely suppress it. If you learn you screwed up, a defensive response is not okay, and not appropriate.</p>\n<p>Keep in mind that the person you hurt is under no obligation to even inform you, let alone put up with a shitty response. Listen respectfully, then go process your feelings somewhere else. Apply <a href=\"http://articles.latimes.com/2013/apr/07/opinion/la-oe-0407-silk-ring-theory-20130407\">ring theory</a>, and keep your own problems aimed outward at people who are less impacted by the situation than you are.</p>\n<p>Once you’ve managed to process your mistake, it’s time to apologize. Apologies absolutely cannot include conditionals like “if I offended you” or “those who might have been offended”. They cannot include any mention of intentions, like “we didn’t intend”. They cannot include any caveats, like “sorry, but”. Any apology with one of those elements is doubling down on your mistake and making the situation worse. Don’t do it.</p>\n<p>Research from Ohio State University identified <a href=\"https://facultyombuds.ncsu.edu/apology-research-how-to-do-it-well/\">elements that make an apology most effective</a>: Express your regret, identify the harms you caused, acknowledge your responsibility, apologize specifically for the harm you caused, ask if you can do anything to resolve the situation. If there is a thing you can do and you are able to, do that thing without arguing, bargaining, or complaining. For a more thorough explanation of this topic, go read <a href=\"http://juliepagano.com/blog/2014/01/06/on-making-mistakes/\">On Making Mistakes</a> by Julie Pagano for a detailed, step by step guide.</p>\n<h3 id=\"respect--empathy-are-all-that-matters\">respect &amp; empathy are all that matters</h3>\n<p>In the end, everything that I&rsquo;ve suggested comes from the underlying principle of having respect and empathy for other people. Other people don&rsquo;t have the context you do, and they don&rsquo;t have the skills you do. Almost all of the time, they&rsquo;re just trying to do their job. They can tell when you&rsquo;re trying to help, and they&rsquo;ll be very happy to receive that help. On the other hand, they can tell when you don&rsquo;t like them and don&rsquo;t care about them, and they will respond to that as well.</p>\n<p>Treating others the way that you personally would like to be treated isn&rsquo;t enough. Read about <a href=\"http://juliepagano.com/blog/2014/05/10/so-you-want-to-be-an-ally/\">how to be a good ally</a>. Listen to people in underrepresented groups. Pay attention to how tech as a field mistreats those underrepresented people, and actively work to fix it. Call out people violating codes of conduct. Let them know that what they&rsquo;re doing isn&rsquo;t okay. Tech as a field is biased and exclusive, and the only way it will get better is if all of us act together to change it.</p>\n<h3 id=\"improving-inclusion\">improving inclusion</h3>\n<p>Let me finish by taking one of my own suggestions: I run a popular open source project, <a href=\"https://bundler.io\">Bundler</a>. We want to be more inclusive! The Bundler team has committed to offering pairing time to any developers who are willing to contribute.</p>\n<p>I also want to improve this talk, and improve inclusion in tech. If you have ever thought about contributing to open source, but felt intimidated or excluded, please let me know about your experiences—I want to change things so that everyone will feel capable, welcome, and empowered to help make things better.</p>\n<p><small>Special thanks to <a href=\"https://twitter.com/juliepagano\">@juliepagano</a> for <a href=\"http://juliepagano.com/blog/2014/01/06/on-making-mistakes/\">On Making Mistakes</a> and <a href=\"http://juliepagano.com/blog/2014/05/10/so-you-want-to-be-an-ally/\">So You Want To Be An Ally</a>, <a href=\"https://twitter.com/taylor_atx\">@taylor_atx</a> and <a href=\"https://twitter.com/rockbot\">@rockbot</a> for permission to use their tweets, and <a href=\"https://twitter.com/sailorhg\">@sailorhg</a> for editing, vetting, and extensive feedback.</small></p>\n",
				"content_text": "<small>This post was also given as a conference talk at [Ruby on Ales 2016](http://ruby.onales.com) and [Ruby Remote Conf 2016](https://allremoteconfs.com/ruby-2016). If you prefer, you can [watch the video](https://www.youtube.com/watch?v=MrPtHogES6k) or [see the slides](https://speakerdeck.com/indirect/including-people-1).</small>\n\n<script async class=\"speakerdeck-embed\" data-id=\"42eecdbf2fcd4f188426726cf7e4e013\" data-ratio=\"1.33333333333333\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n\nThis is about including people in projects, teams, and communities, but it would be more accurate to say “not excluding people”. Tech is a highly exclusive field, and we need to stop driving people away before we can make them feel welcome and included.\n\nTo be clear: I’m not an expert at this, but I have (sometimes painfully) learned many ways to exclude people. I’ll tell you about them, and then suggest ways to avoid them! That’s not enough to guarantee inclusiveness, but it’s at least a start. That said, I can't be sure my suggestions will work for everyone. Keep your eyes open, form hypotheses, experiment, and evaluate the results.\n\n### wait, why is a white dude talking about inclusion\n\nRight off the bat, I’d like to point out that it’s kind of weird for me to be talking about anything related to diversity or inclusion—I’m cis, straight, white, and male. People assume that I know what I’m talking about (whether I do or not), and my experience with the programming community has been largely positive. As a result, these suggestions are the result of research, reading, and listening to excluded people about their experiences.\n\nBut the main reason why I’m writing, as a white male, is that anyone else would suffer backlash for it. As reported in the Harvard Business Review, underrepresented people who bring up theses issues are [penalized at work](https://hbr.org/2016/03/women-and-minorities-are-penalized-for-promoting-diversity). When women and minorities try to surface these issues, they lose social status and prestige, and their coworkers and managers question their competence.\n\nNow that you know why I’m the one bringing this up, let’s talk about it. It’s really important, we’ve been ignoring it for a long time, and everyone needs to act thoughtfully and considerately for us to have any chance of improving things.\n\n### let’s talk about “diversity”\n\nThe topic of \"diversity\" has gotten a lot of press lately, both in tech in general and in the Ruby community in particular. The history of pervasive sexism and harassment in the Ruby community has even gone mainstream—it was the biggest fact about Ruby in a Bloomberg magazine article by Paul Ford called “[What is Code?](http://www.bloomberg.com/graphics/2015-paul-ford-what-is-code/)”, aimed at non-technical readers.\n\nOver the last few years, I've put a lot of time and energy into research. I've also written about [how to be an ally](/2013/12/04/how-to-be-an-ally/), and about [why diversity matters](http://www.ashedryden.com/la-conf-tbd). First, I’ll briefly summarize the current situation.\n\n> How to start increasing inclusion in tech: s/diversity/inclusion/g — [@rockbot](https://twitter.com/rockbot/status/622109542533300224)\n\nTo start with, diversity is the wrong word to use while talking about these problems. The reason that underrepresented people are underrepresented is not because they don’t exist—graduating computer science majors include many women and many people of color, to name just two underrepresented groups.\n\n> “Diversity is being asked to sit at the table while inclusivity is being asked your opinion at the table you are sitting at.” — [@taylor\\_atx](https://twitter.com/taylor_atx/status/623598547837939712)\n\nEven though these people absolutely exist, they are not included in tech. Their opinions are not listened to, their feedback is not acted on, and their technical skills are suspect based purely on their background.\n\nThe lack of diversity in tech is unsolvable as long as we remain exclusive. We need to become inclusive people, creating an inclusive community. In an inclusive community, there is no “diversity problem”, because everyone feels equally welcome.\n\nI talk about this in conversations with a lot of people, and this is usually the point where someone tells me that they feel very welcome and included, so this can’t possibly be the reason for the lack of diversity in tech. That position is demonstrably wrong.\n\n### we have a bias & exclusion problem\n\nOne underlying problem is people being denied education, jobs, and promotions solely because of bias. Coworkers, managers, executives, and VCs have all been shown to unreasonably overvalue work by young, white, straight men, while unreasonably undervaluing work by anyone else.\n\nThere is a huge amount of repeated research showing these biases, because of gender, [disabilities](http://news.rutgers.edu/research-news/rutgers-study-employers-discriminate-against-qualified-workers-disabilities/20151104#.VorAgZMrKRs), [sexual orientation or status](http://williamsinstitute.law.ucla.edu/wp-content/uploads/Sears-Mallory-Discrimination-July-20111.pdf), [race](http://www.latimes.com/business/technology/la-fi-tn-asian-american-tech-20150506-story.html), age, weight, or any of a thousand other things. Just changing the name, gender, or race on a resume is enough to change a “good” resume to an “unacceptable” one.\n\nEven though this is a thoroughly researched phenomenon, people disagree with me every time I talk about this. They say things like \"[but tech is a meritocracy](butitsameritocracy.com) and everyone is valued based on their contributions to the whole\". Well, about that…\n\n### meritocracy is a lie told by rich white men\n\nSuccessful people often [assume they caused their success](https://en.wikipedia.org/wiki/Actor%E2%80%93observer_asymmetry), and then [wrongly conclude](https://en.wikipedia.org/wiki/Survivorship_bias) that anyone less successful [somehow deserved to fail](https://en.wikipedia.org/wiki/Just-world_hypothesis). This is not only the [fundamental attribution error](https://en.wikipedia.org/wiki/Fundamental_attribution_error), but holding this idea actively causes discrimination! At companies that claim to be a meritocracy, performance evaluations and hiring decisions [are _even more_ biased](http://www.tarahunt.com/blog/2013/10/28/meritocracy-is-almost-as-real-as-this-unicorn).\n\nOnce that’s out of the way, everyone tells me that I should “obviously” focus on [the pipeline problem](isitapipelineproblem.com). If new developers are diverse, then all of tech will become diverse eventually, right? Again: nope.\n\n### the pipeline leads straight to a sewage plant\n\nLet’s start with the most obvious counterpoint: when the first electronic computers were built, [all programmers were women](http://labweb.education.wisc.edu/elpa940/readings/Light.pdf). The title “Computer” referred to a woman who performed calculations. The job of “Programmer” was created by those women as they wrote programs to automate their computational work. Despite starting at 100% women, programmers today are less than 25% women. If the trend continues, within a few decades no women at all will be programmers.\n\nYoung women already like computers and are interested in working with them. [There isn't anything wrong with the pipeline](https://mfbt.ca/your-diversity-problem-isnt-the-pipeline-s-fault-820e8e7e2b77#.pv80ijfnj), but tech is so hostile that it drives even interested women away. This is why we need inclusion—we had diversity already, but we fucked it up.\n\nAt this point, objectors usually pivot to another tactic. I can't even count the number of times I've heard the comeback “including everyone is a good idea, but it would lower the bar”.\n\n### the bar is already on the floor\n\nFirst, the very phrase \"lowering the bar\" is [full of racist assumptions](https://byrslf.co/what-you-re-really-saying-when-you-talk-about-lowering-the-bar-in-hiring-a30d9b12430f#.gfv372a53). Second, research into bias during job applications shows that [white men have a huge advantage](http://whatwouldkingleonidasdo.tumblr.com/post/54989171152/how-i-discovered-gender-discrimination) while experience and competence are dismissed in  [women](http://gender.stanford.edu/news/2014/why-does-john-get-stem-job-rather-jennifer) or [people of color](http://www.nber.org/digest/sep03/w9873.html).\n\nDespite the research showing otherwise, big tech companies claim they value everyone equally. Their dismal diversity reports are explained away as part of the pipeline problem (which, as we just covered, doesn't exist!) On top of that, employees confirm these companies [continue to treat white men as inherently better](https://42hire.com/the-big-lie-tech-companies-and-diversity-hiring-f52fb82abfbf#.i4qwkryed), no matter what they say.\n\n> Hiring Manager: \"We <3 diversity, but can't lower the bar for our devs\"  \n> Me: \"No, I was suggesting raising it. But, for your hiring managers\"  \n> — [@josh\\_cheek](https://twitter.com/josh_cheek/status/770624979436199936)\n\nIn America in general and in tech specifically, the bar for white dudes is already unbelievably low. The reality is the exact opposite of the claim: inclusion requires _raising_ the bar&mdash;up above incompetent white men. At that point, more jobs can go to underrepresented competent people.\n\nAt this point, arguments for exclusion pivot to one more extremely popular claim: companies would love to be inclusive, but they can't because it would be costly and hurt profits.\n\n### the economic argument is diversionary bullshit\n\nI've written about [the economic argument for diversity](/2014/05/24/the-economic-argument-for-diversity/) before, but the short response is “no, wrong again”. Research shows that [diverse teams produce better results in less time](http://www.scientificamerican.com/article/how-diversity-makes-us-smarter/).  Diversity is measurably beneficial whether it is [different expertise](https://hbr.org/2014/03/the-case-for-team-diversity-gets-even-better/) or merely [different backgrounds or perspectives](https://hbr.org/2013/12/how-diversity-can-drive-innovation). Companies choose to uphold the exclusive status quo even when it means harming themselves.\n\nBut the underlying problem is even worse: there is no excuse for treating a group of people as inferior. Ever. Mercenary attempts to include underrepresented groups for the purpose of \"improving productivity\" are doomed to fail from the start. Companies motivated solely by profit will never be inclusive, because that requires valuing people as human beings rather than interchangeable cogs in a machine that creates value for investors.\n\n### inclusion is an attitude, and a philosophy of interaction\n\nSince you're still here, I'm guessing that you're interested in ways to be genuinely inclusive (especially in open source projects, with no profits to worry about). Being inclusive isn't just a checklist of things for you to do: it's an attitude to adopt in every social interaction. That attitude needs to not just be present, but supported and defended against the condescending, hostile attitudes that are all too common in software.\n\n### people are why you are here\n\nSoftware is made by people for use by people. Keep that in mind while you work on it! Projects usually involve three groups of people: end-users, potential contributors, and the existing team. It's easy to be inclusive towards one of those groups while still being hostile to other groups. In the Linux kernel, for example, end-users are welcomed and documentation is provided for them. Contributors, on the other hand, are semi-regularly told that they are brainless imbeciles (or even worse language). Not so good on the welcoming, there.\n\n### include your users\n\nSo, let's start with end-users. How can you be welcoming and inclusive of the people who use the software you work on? There are a lot of ways. People who are interested in using your software will pay attention to the other people working on it. They will pay attention to the other people using it. They will pay attention to whether your docs only ever say \"he\". They will notice if reporting an error gets the response \"that's stupid\". So here are some straightforward ways to make sure that people using the code you write feel included.\n\n### codes of conduct\n\nThe most straightforward way to include people who are interested in your project is to establish a code of conduct, and then enforce it. At this point, there are a lot of examples of projects that have a code of conduct. Bundler is one, the Rust programming language is one. Coraline Ada Ehmke has even created a template called the [Contributor Covenant](http://contributor-covenant.org) that any project can use.\n\nA code of conduct makes it clear that the project intends to provide a safe space for users and contributors that is free from harassment. Without one, anyone who experiences harassment on the internet will be much less likely to become involved in your project. That would be a tragedy, because most people who aren't white caucasian males on the internet experience at least some harassment.\n\n### write great documentation\n\nNext up, documentation. It's a thing. You probably don't have as much as you think you should, and chances are high you're embarrassed about it. That's okay. I work on a lot of projects, and none of them have perfect documentation. The ground rules for helpful, inclusive documentation for end-users are pretty straightforward: use \"she\" or \"they\" instead of \"he\", and write some documentation that is aimed at newcomers to the project. Just that, and nothing else, is enough to get you off to a good start.\n\nYou can do better than that, though! For a fantastic explanation of how to write documentation that is genuinely helpful to users, check out the talk [Writing Great Documentation](http://www.slideshare.net/jacobian/writing-great-documentation-codeconf-2011) by Jacob Kaplan-Moss. He outlines four kinds of documentation that your project needs to be useful to all the people who will look for it.\n\nThe first type of documentation is tutorials. Tutorials should explain how to accomplish a specific task. It should be possible to follow them start to finish in 30 minutes or less, and they should link to other tutorials or topic guides whenever they are relevant. Next up, topic guides. Topic guides explain a single topic in detail, mentioning the options, details, and unexpected bits that are specific to that topic. Topic guides should refer to other tutorials and topic guides as they're relevant. Then there's reference documentation. This is the API docs for your project. They are the most likely to already exist, but they are also completely useless for someone who doesn't already know what they're looking for. This is the reason that Rails has both extensive API docs and a guides project full of topic guides.\n\nFinally, troubleshooting documentation. There will be problems that come up again and again—if you can, fix them in your code. If you can't, add them to the troubleshooting guide. Write down the steps that you suggest to people having problems, and explain exactly what information you will need to be able to help someone who has a problem that they can't fix on their own. As you write it, keep in mind that the person reading it will be frustrated at minimum and raging at worst. Keep it straightforward, don't condescend, and try to be helpful.\n\nSpeaking of troubleshooting, how your project handles issue reports is a huge indicator of how inclusive your project is. If you respond to a new issue with \"that doesn't make sense\" or \"why were trying to do that\", the hostility implicit in those statements will be heard loud and clear. Start with the idea that whatever the reporter is trying to makes sense to them. Then, try to figure out what context they have that you don't (or that you have that they don't) that can help solve the issue.\n\n### give better answers\n\nThe talk [Giving Better Answers](http://pewpewthespells.com/conf/altconf/2015.html), given by Sam Marshall at Alterconf Chicago in 2015, has really great concrete examples of how to give answers that are helpful without being condescending or hostile. Finally, always thank users for taking the time to report the error to you (they didn't have to!). Always let them know than even if their problem isn't a bug, you'll try to help them, or at least suggest somewhere else that they can go to get more help.\n\nClosely related to issue reports: IRC channels, Slack rooms, emails, and Twitter. Don't just follow the code of conduct in all of those places, make sure that the code of conduct is being followed by others. Anyone that you have allowed to officially represent your project, whether core team or just a contributor, needs to understand that harassment and condescension is not okay. Listen and speak respectfully. Follow the code of conduct yourself, and call out anyone who doesn't.\n\nFinally, and this is true for every project: you have to enforce the code of conduct with everyone, whether user, contributor, core member, or yourself. If you have a code of conduct but let people slide, your project will become known as not just hostile, but hypocritically hostile.\n\n### include your occasional contributors\n\nNow let's talk about how to welcome contributors to your project. Everything that I've said so far about welcoming users applies to welcoming contributors, too. A safe environment without harassment is a requirement. Documentation helps a lot. Just having those things, though, isn't enough. There's more you can do to encourage contributions to your project.\n\nFirst, and I can't say this enough times, the biggest thing you can do to encourage contributions to your project is to ask for help. (Let me take this moment to say that I run the [Bundler](bundler.io) project, and we would _love_ to [have your help](https://github.com/bundler/bundler/blob/master/CONTRIBUTING.md)!) Most people think that they need to be total experts on something before they can even begin to help.\n\nLet me make it extremely clear: I have never been an expert on any project that I have started to work on. I have also learned more by working on open source projects than any other way that I have learned anything about programming. When you ask for help, and when you write a document explaining how people can contribute, make it clear that they don't need to be an expert to be able to help you. Schedule times to pair with contributors at any skill level, and then actually pair with them so they can get started.\n\nWrite development documentation. This is completely different from the documentation that targets end-users. It needs to explain how to set up the project locally for development and how to run the tests. It needs to explain who runs the project, what policies there are for contributions, and how to contact the other contributors on the project.\n\nDon't stop there, though. If there is anything you have to do repeatedly, like triaging issues, write a document that explains the steps. Write a list of every type of help you would like to receive. For Bundler, that includes fixing typos, writing docs, triaging tickets, refactoring code, fixing bugs, implementing features, and lots more. We have a dedicated document that not only lists each kind of help we would like to get, but links to guides for that specific type of help.\n\nTreat pull requests the same way you treat issue reports: the person opening the pull request has context that means what they are doing makes sense to them. Even if there is no chance that you will ever accept the PR, thank them for making it (they didn't have to contribute!). Explain your reasoning. Try to understand the underlying problem that drove them to send the PR, and see if there is a way you can help them even if you aren't willing to accept the patch. Respect their intelligence and their time, and they're likely to keep contributing.\n\n### include your team\n\nIt's important to apply all of these principles to your entire team. Respond to code of conduct violations aimed at your team members. Make it clear that you have their back. Tell them that you appreciate their help. Apply the exact same principles that we just discussed for users and contributors: assume that what they are doing makes sense to them, and work to find the context that isn't shared so that you can understand each other. Finally, give positive feedback and make requests. Collaborate with your team, discuss decisions, and listen to their input.\n\n### apologize. for real, though.\n\nI screw up on a regular basis. Chances are also pretty good that you will screw this up at some point—we all make mistakes, and that’s okay! Being inclusive is an ongoing process, and part of that process is learning from mistakes. When you act in a way that excludes or alienates people, they might talk to you about it. Appreciate them for it! You’re being given a chance to learn and improve.\n\nMost human beings respond with defensiveness when they hear they made a mistake. It’s really important to recognize that urge, acknowledge that it is normal, and completely suppress it. If you learn you screwed up, a defensive response is not okay, and not appropriate.\n\nKeep in mind that the person you hurt is under no obligation to even inform you, let alone put up with a shitty response. Listen respectfully, then go process your feelings somewhere else. Apply [ring theory](http://articles.latimes.com/2013/apr/07/opinion/la-oe-0407-silk-ring-theory-20130407), and keep your own problems aimed outward at people who are less impacted by the situation than you are.\n\nOnce you’ve managed to process your mistake, it’s time to apologize. Apologies absolutely cannot include conditionals like “if I offended you” or “those who might have been offended”. They cannot include any mention of intentions, like “we didn’t intend”. They cannot include any caveats, like “sorry, but”. Any apology with one of those elements is doubling down on your mistake and making the situation worse. Don’t do it.\n\nResearch from Ohio State University identified [elements that make an apology most effective](https://facultyombuds.ncsu.edu/apology-research-how-to-do-it-well/): Express your regret, identify the harms you caused, acknowledge your responsibility, apologize specifically for the harm you caused, ask if you can do anything to resolve the situation. If there is a thing you can do and you are able to, do that thing without arguing, bargaining, or complaining. For a more thorough explanation of this topic, go read [On Making Mistakes](http://juliepagano.com/blog/2014/01/06/on-making-mistakes/) by Julie Pagano for a detailed, step by step guide.\n\n### respect & empathy are all that matters\n\nIn the end, everything that I've suggested comes from the underlying principle of having respect and empathy for other people. Other people don't have the context you do, and they don't have the skills you do. Almost all of the time, they're just trying to do their job. They can tell when you're trying to help, and they'll be very happy to receive that help. On the other hand, they can tell when you don't like them and don't care about them, and they will respond to that as well.\n\nTreating others the way that you personally would like to be treated isn't enough. Read about [how to be a good ally](http://juliepagano.com/blog/2014/05/10/so-you-want-to-be-an-ally/). Listen to people in underrepresented groups. Pay attention to how tech as a field mistreats those underrepresented people, and actively work to fix it. Call out people violating codes of conduct. Let them know that what they're doing isn't okay. Tech as a field is biased and exclusive, and the only way it will get better is if all of us act together to change it.\n\n### improving inclusion\n\nLet me finish by taking one of my own suggestions: I run a popular open source project, [Bundler](https://bundler.io). We want to be more inclusive! The Bundler team has committed to offering pairing time to any developers who are willing to contribute.\n\nI also want to improve this talk, and improve inclusion in tech. If you have ever thought about contributing to open source, but felt intimidated or excluded, please let me know about your experiences—I want to change things so that everyone will feel capable, welcome, and empowered to help make things better.\n\n<small>Special thanks to [@juliepagano](https://twitter.com/juliepagano) for [On Making Mistakes](http://juliepagano.com/blog/2014/01/06/on-making-mistakes/) and [So You Want To Be An Ally](http://juliepagano.com/blog/2014/05/10/so-you-want-to-be-an-ally/), [@taylor\\_atx](https://twitter.com/taylor_atx) and [@rockbot](https://twitter.com/rockbot) for permission to use their tweets, and [@sailorhg](https://twitter.com/sailorhg) for editing, vetting, and extensive feedback.</small>\n",
				"date_published": "2016-08-31T00:00:00-08:00",
				"url": "https://andre.arko.net/2016/08/31/including-people/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2016/07/02/andreconcatamy/",
				"title": "andre.concat(amy)",
				"content_html": "<img src=\"https://indirect.micro.blog/uploads/2025/8f5c545a1b.jpg\">\n",
				"content_text": "<img src=\"https://indirect.micro.blog/uploads/2025/8f5c545a1b.jpg\">\n",
				"date_published": "2016-07-02T00:00:00-08:00",
				"url": "https://andre.arko.net/2016/07/02/andreconcatamy/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2015/04/28/how-does-bundler-work-anyway/",
				"title": "How does Bundler work, anyway?",
				"content_html": "<h3 id=\"a-history-of-ruby-dependency-management\">a history of ruby dependency management</h3>\n<script async class=\"speakerdeck-embed\" data-id=\"7eaa2724d0624961bc4423a100036ce5\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n<p><small>This post was originally given as a presentation at <a href=\"http://confreaks.tv/videos/railsconf2015-how-does-bundler-work-anyway\">RailsConf 2015</a>.</small></p>\n<p>Using Ruby code written by other developers is easy! Just add it to your Gemfile, run <code>bundle install</code>, and start using it. But what&rsquo;s really happening when you do that? How can use you someone else&rsquo;s code just by putting it in your Gemfile? To answer that question, I&rsquo;m going to take you back in time. We&rsquo;re going on a tour of the history of dependencies in Ruby, from the beginning to the present day. When we&rsquo;re done, you&rsquo;ll not only understand what happens when you use Bundler, you&rsquo;ll understand why things work the way they do.</p>\n<p>Starting with good old-fashioned <code>require</code>, we&rsquo;ll discuss how Ruby allows you to load code from files and directories. Next, we&rsquo;ll look at <code>setup.rb</code>, the first way Ruby developers were able to share code with one another. After that, Rubygems and the revolutionary ability to install multiple versions of the same library. Finally, we&rsquo;ll look at Bundler and exactly how dependency management for a single project is very different from just managing libraries.</p>\n<p>Even though the <code>require</code> method has been around since at least 1997, which is the oldest Ruby code we have in version control, it can still be broken down into even smaller concepts. Using code from another file is functionally the same as inserting that file into your code at the place you wrote <code>require</code>.  As a result, it&rsquo;s possible to implement a naive require function with just one line of code.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">require</span>(filename)\n</span></span><span style=\"display:flex;\"><span>  eval <span style=\"color:#66d9ef\">File</span><span style=\"color:#f92672\">.</span>read(filename)\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span>\n</span></span></code></pre></div><p>If you start thinking about how this function would work in practice, though, you&rsquo;ll probably notice a couple of problems. First, if you call this <code>require</code> more than once, the file will be run multiple times. It&rsquo;s generally a bad idea to run required code multiple times, even if the file is required more than once in different places. The second require could disrupt your program by re-initializing values, and every class and method has already been created by the first require.</p>\n<p>How can we avoid requiring the same file twice? The simplest answer is to keep track of every file that we have ever required before, and only eval files that are being required for the first time. An implementation of this better require function would still be very simple.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>$LOADED_FEATURES <span style=\"color:#f92672\">=</span> <span style=\"color:#f92672\">[]</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">require</span>(filename)\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">return</span> <span style=\"color:#66d9ef\">true</span> <span style=\"color:#66d9ef\">if</span> $LOADED_FEATURES<span style=\"color:#f92672\">.</span>include?(filename)\n</span></span><span style=\"display:flex;\"><span>  eval <span style=\"color:#66d9ef\">File</span><span style=\"color:#f92672\">.</span>read(filename)\n</span></span><span style=\"display:flex;\"><span>  $LOADED_FEATURES <span style=\"color:#f92672\">&lt;&lt;</span> filename\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span>\n</span></span></code></pre></div><p>Although global variables are usually considered evil, in this case it&rsquo;s the only kind of tracking that makes sense—required files create constants in the global namespace, and so the list of required files should be global, too. In fact, Ruby does provide a global variable named <code>$LOADED_FEATURES</code>, and it holds a list of every file that has been required, just like our example!</p>\n<p>The second problem that you might have noticed by now is that the argument to <code>require</code> has to be an absolute path on the filesystem. That&rsquo;s probably okay if you know exactly where every file on your machine is, but that won&rsquo;t work between lots and lots of different developers. The easiest way to allow requires that aren&rsquo;t absolute is to just treat every filename as if it&rsquo;s relative to the directory the program was started from. That&rsquo;s easy, but doesn&rsquo;t work well if you want to combine Ruby files from a bunch of different directories.</p>\n<p>To allow required files to be in different directories, we could create a list of directories to look inside whenever require is called. Here&rsquo;s what an implementation of load paths might look like:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>$LOAD_PATH <span style=\"color:#f92672\">=</span> <span style=\"color:#f92672\">[</span><span style=\"color:#e6db74\">&#34;/path/to/code&#34;</span>, <span style=\"color:#e6db74\">&#34;/other/path/to/code&#34;</span><span style=\"color:#f92672\">]</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">require</span>(filename)\n</span></span><span style=\"display:flex;\"><span>  full_path <span style=\"color:#f92672\">=</span> $LOAD_PATH<span style=\"color:#f92672\">.</span>find <span style=\"color:#66d9ef\">do</span> <span style=\"color:#f92672\">|</span>path<span style=\"color:#f92672\">|</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">File</span><span style=\"color:#f92672\">.</span>exist?(<span style=\"color:#66d9ef\">File</span><span style=\"color:#f92672\">.</span>join(path, filename))\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>  eval <span style=\"color:#66d9ef\">File</span><span style=\"color:#f92672\">.</span>read(full_path)\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span>\n</span></span></code></pre></div><p>You may then wonder if these two things can be combined. They can! Here&rsquo;s a version of the function that only loads files once, and looks in all <code>$LOAD_PATH</code> directories.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>$LOAD_PATH <span style=\"color:#f92672\">=</span> <span style=\"color:#f92672\">[</span><span style=\"color:#e6db74\">&#34;/path/to/code&#34;</span>, <span style=\"color:#e6db74\">&#34;/other/path/to/code&#34;</span><span style=\"color:#f92672\">]</span>\n</span></span><span style=\"display:flex;\"><span>$LOADED_FEATURES <span style=\"color:#f92672\">=</span> <span style=\"color:#f92672\">[]</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">require</span>(filename)\n</span></span><span style=\"display:flex;\"><span>  full_path <span style=\"color:#f92672\">=</span> $LOAD_PATH<span style=\"color:#f92672\">.</span>find <span style=\"color:#66d9ef\">do</span> <span style=\"color:#f92672\">|</span>path<span style=\"color:#f92672\">|</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">File</span><span style=\"color:#f92672\">.</span>exist?(<span style=\"color:#66d9ef\">File</span><span style=\"color:#f92672\">.</span>join(path, filename))\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">return</span> <span style=\"color:#66d9ef\">true</span> <span style=\"color:#66d9ef\">if</span> $LOADED_FEATURES<span style=\"color:#f92672\">.</span>include?(full_path)\n</span></span><span style=\"display:flex;\"><span>  eval <span style=\"color:#66d9ef\">File</span><span style=\"color:#f92672\">.</span>read(full_path)\n</span></span><span style=\"display:flex;\"><span>  $LOADED_FEATURES <span style=\"color:#f92672\">&lt;&lt;</span> full_path\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span>\n</span></span></code></pre></div><p>Anyway, adding a load path allows us to find Ruby libraries even if they are spread across multiple directories. At this point, we can add the directory that holds the Ruby standard library to that list, and it becomes very easy to require those files. Loading <code>net/http</code>? No problem, now you can just <code>require 'net/http'</code>, and Ruby wil automatically check the directory where it lives.</p>\n<p>At this point, we&rsquo;ve already caught up to the state of the art in Ruby libraries circa 2004. The final piece of the puzzle for developers who wanted to share Ruby code in 2004 was the combination of <code>setup.rb</code> and the RAA, or Ruby Application Archive. While the RAA is no longer around (it was shut down 2014 due to lack of use), <code>setup.rb</code> is amazingly <a href=\"http://i.loveruby.net/en/projects/setup/\">still around on the internet</a>, and you can even download it if you like. Just to warn you, though, it hasn&rsquo;t been updated since 2005.</p>\n<p>At its core, <code>setup.rb</code> is a Ruby implementation of the classic unix trinity of commands to install software: <code>./configure &amp;&amp; make &amp;&amp; make install</code>. When using <code>setup.rb</code>, the commands to run are <code>ruby setup.rb config &amp;&amp; ruby setup.rb setup &amp;&amp; ruby setup.rb install</code>. Installing a Ruby library was then as simple as:</p>\n<ol>\n<li>Browse the RAA looking for a library that did what you wanted.</li>\n<li>Find a library, click a link to download the .tar.gz file containing the library.</li>\n<li>Decompress the tarball, <code>cd</code> into the directory, and run <code>ruby setup.rb</code>.</li>\n</ol>\n<p>At that point, you would have installed the library! Because <code>setup.rb</code> installed the Ruby files into a single, well-known directory that was already added to the load path, you could even require the library immediately. Libraries could be found, downloaded, installed, and used. It was pretty good.</p>\n<p>It doesn&rsquo;t take too long to figure out that there are some possible problems with this plan, though. For example, if a library is updated, and you want the new version, how do you get it? Well, you had to browse back to the RAA, find the new version, download it, decompress the tarball, and then manually run <code>ruby setup.rb</code> again. Then you&rsquo;d have the new version.</p>\n<p>Does this sound tedious to you? Speaking as someone who was there, let me tell you: it was tedious. It took a lot of time, it was error prone, there was no good way to know when new versions came out. In a word, it was&hellip; not great. Even worse, what if you had one Ruby application that only worked with the old version of tha library? You just overwrote the old version with the new one, and your script that needs the old version doesn&rsquo;t work anymore. Oops.</p>\n<p>In 2004, RubyGems came racing to the rescue, fixing all of these problems. Installing libraries was a single command: <code>gem install</code>. Checking to see what gems were available was also a single command: <code>gem list</code>. Just that was enough to revolutionize sharing code in Ruby, but RubyGems had yet another trick up its sleeve: multiple gem versions.</p>\n<p>Unhappy with the way that <code>setup.rb</code> only allowed one version of each library to be installed at a time, RubyGems allow multiple versions of a gem to be installed at the same time. RubyGems adds its own patches to the <code>require</code> method, checking to see if a gem is installed that provides the file <code>rack.rb</code>. If no version of that gem has been activated, RubyGems will automatically activate the newest version that is already installed.</p>\n<p>It&rsquo;s even possible to load a specific version of a gem, rather than the newest version that has been installed using the <code>gem</code> method provided by RubyGems.</p>\n<p>Here&rsquo;s how to load a specific version of a gem using the <code>gem</code> method, followed by <code>require</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>gem <span style=\"color:#e6db74\">&#34;rack&#34;</span>, <span style=\"color:#e6db74\">&#34;1.0&#34;</span>\n</span></span><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#34;rack&#34;</span>\n</span></span></code></pre></div><p>Calling the <code>gem</code> method adds a specific version of that gem to the load path. At that point, a regular <code>require</code> is enough to load exactly that version of that gem.</p>\n<p>If you running a command that&rsquo;s provided by a gem, like <code>rackup</code>, it&rsquo;s also possible to run that command using a specific version of the gem. RubyGems checks to see if the first argument is a version number surrounded by underscores, and uses that version if so. So to run <code>rackup</code> using rack version 1.2.2 on port 3000, you would run:</p>\n<pre tabindex=\"0\"><code>$ rackup _1.2.2_ -p 3000\n</code></pre><p>RubyGems made installing, upgrading, and using libraries easy. So easy, in fact, that there was an explosion of libraries. Today, there are almost 100,000 gems, and almost 1 million released versions of every gem.</p>\n<p>This explosion revealed a new problem with dependencies: it&rsquo;s hard to coordinate them. If one developer ran <code>gem install foo</code> and started using a new gem in the application, other developers on the project would have to be told to run <code>gem install foo</code>. Then each of the production servers would also have to run <code>gem install foo</code>.</p>\n<p>Even worse, setting up a new machine might mean that <code>gem install foo</code> installed a different version than the one that the application knew how to use. Adding new developers to a team could be a week-long process as gems were installed, checked, and fixes were made.</p>\n<p>Around  2008, developers started to create solutions for managing lists of gems. Rails started offering the <code>config.gem</code> setting, and the <code>gem_installer</code> gem offered another option for installing many gems at once.</p>\n<p>Unfortunately, it didn&rsquo;t take long to discover problems with that system, either. Because RubyGems automatically used the newest version of each gem, simply having older versions of gems installed wasn&rsquo;t enough to mean that they would be used. Days, and even weeks of developer time was spent trying to figure out why projects would mysteriously work on one machine only to fail on another machine.</p>\n<p>Anyone with more than one large Rails application quickly discovered exactly how hard it is to manage gems by hand: upgrade a gem in any application, and all the other applications on your machine stop working until you upgrade them, too. Upgrading gems in those apps spread the upgrade pain to all the other developers on those applications, and so on.</p>\n<p>Ultimately, a large chunk of any Ruby developer&rsquo;s time was spent managing and upgrading gems, and it sucked. Just fixing that would be a big enough reason to create Bundler right there, but there was another, even more insidious, problem as well: gem activation errors.</p>\n<p>Gem activation errors mean that, somehow, RubyGems was asked to activate a gem, and then later on asked to activate a different version of the same gem. Since it&rsquo;s not possible to have two versions of the same gem loaded at the same time, this raises an exception. At this point, you&rsquo;re probably thinking &ldquo;Surely that wasn&rsquo;t very common, André! That sounds like a complicated and rare situation.&rdquo;</p>\n<p>If only that were true. :( Almost every Ruby application with more than a handful of gems would eventually start to experience this problem. RubyGems loads the newest installed gem by default, and so when another dependency declared that it only worked with a version slightly older than the newest one, everything would explode.</p>\n<p>The underlying reason for activation errors is simple: RubyGems does what we usually call &ldquo;runtime dependency resolution&rdquo;. It loads the gems you ask for, when you ask for them, and doesn&rsquo;t know in advance if you&rsquo;re going to need a different version later. To prevent runtime dependency errors, we need to do dependency resolution <em>before</em> runtime. We need to know every gem and every version, and know that they all work together, before we start to load any of them.</p>\n<p>This is the ultimate problem that Bundler exists to solve: how do you figure out which versions of all the gems that you want can actually work together? Each gem depends on other gems, and those gems depend on other gems, and so on. Before Bundler, this process was done entirely by hand, and simply involved trying things until the exceptions stopped.</p>\n<p>Unsurprisingly, computers are more accurate than humans at sytematically trying a mutitude of options and reporting back which options work. They&rsquo;re also much, much faster at it. Thanks to Bundler, Ruby developers have been able to simply list the gems they want to use and count on Bundler to find versions for all of them that actually work together.</p>\n<p>This problem is called &ldquo;dependency graph resolution&rdquo;, and it is an example of a Well-Known Hard Problem™, also known as NP-Complete problems. In theory, it is possible to construct a set of existing gems and a Gemfile such that it could take until past the heat death of the universe to find versions that all work together.</p>\n<p>Since most developers don&rsquo;t have that long to spare, Bundler&rsquo;s resolver uses a lot of tricks, shortcuts, and heuristics to prioritize which gem versions to try first. We&rsquo;ve built up a pretty large library of tricks over the years, and most Gemfiles now resolve within a few seconds.</p>\n<p>After finding the versions that work together, Bundler records the exact versions of every gem into another file, named <code>Gemfile.lock</code>. This lock file is what makes it possible to install the exact same versions on to every machine that runs this application, whether that machine belongs to a developer, a production server, or a CI server.</p>\n<p>At the end of the day, the way Bundler works boils down to two separate steps, <code>bundle install</code> and <code>bundle exec</code>. The steps for <code>bundle install</code> are pretty simple to explain:</p>\n<ol>\n<li>Read the Gemfile (and lock, if it&rsquo;s there)</li>\n<li>Ask RubyGems.org for a list of every version of every gem we want</li>\n<li>If needed, find gem versions allowed by the Gemfile that work together</li>\n<li>If found, write down those versions in the lock for future installs</li>\n<li>Install gems as needed until every gem in the lock is installed</li>\n</ol>\n<p>The process for <code>bundle exec</code> is similar, with two important changes. First, it is just setting up Ruby to load gems that are already installed, so it doesn&rsquo;t ask RubyGems.org for a list of gem versions. Second, it doesn&rsquo;t install gems if any are missing, it just prints out an error asking you to install the gems, instead. Here are the steps:</p>\n<ol>\n<li>Read the Gemfile (and lock, if it&rsquo;s there)</li>\n<li>If needed, find gem versions allowed by the Gemfile that work together</li>\n<li>If found, write down those versions in the lock for future installs</li>\n<li>Remove any existing gems from the <code>$LOAD_PATH</code> array</li>\n<li>Add each gem version listed in the lock file to the <code>$LOAD_PATH</code></li>\n</ol>\n<p>That&rsquo;s it! While there are a lot of other details, those are the underlying pieces Bundler uses to let you get your work done: <code>require</code>, the <code>$LOAD_PATH</code> array, and RubyGems. Each one is built on top of the ones that came before, and each one fixes problems that only became apparent after the new system was created.</p>\n<p>Even after Bundler was created, the pattern continues. The biggest problem left for users of Bundler 1.0 was how long it took to run <code>bundle install</code>. To fix that, Bundler 1.1 created a completely new way to get information about gems from RubyGems.org and it sped things up. We continue to work on Bundler today, and we released <a href=\"http://bundler.io/blog/2015/03/21/hello-bundler-19.html\">Bundler 1.9</a> just this month. We have big improvements in the pipeline as well, so keep an eye on the <a href=\"http://bundler.io\">Bundler website</a> or <a href=\"https://twitter.com/bundlerio\">@bundlerio</a> on Twitter for updates!</p>\n<p>If you use Bundler, or if your company uses Bundler, support Bundler&rsquo;s development. For those of you with time but not money, we&rsquo;d love your help on Bundler! You can tweet at us <a href=\"https://twitter.com/bundlerio\">@bundlerio</a> or email us at <a href=\"mailto:team@bundler.io\">team@bundler.io</a> and set you up with ways you can help. On the other hand, if you have money but not time, you can still make sure Bundler keeps getting better by joining <a href=\"https://rubytogether.org/\">Ruby Together</a> as a paying member.</p>\n<p>Ruby Together membership fees go directly to support developers working on Bundler, RubyGems, and RubyGems.org. We&rsquo;re working to make sure that you can spend time on your own work, rather than solving dependency problems. As Ruby Together grows, we&rsquo;ll be tackling more community issues, including gem mirrors, better public benchmarks for Ruby and Rails, and more.</p>\n",
				"content_text": "### a history of ruby dependency management\n\n<script async class=\"speakerdeck-embed\" data-id=\"7eaa2724d0624961bc4423a100036ce5\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n\n<small>This post was originally given as a presentation at [RailsConf 2015](http://confreaks.tv/videos/railsconf2015-how-does-bundler-work-anyway).</small>\n\nUsing Ruby code written by other developers is easy! Just add it to your Gemfile, run `bundle install`, and start using it. But what's really happening when you do that? How can use you someone else's code just by putting it in your Gemfile? To answer that question, I'm going to take you back in time. We're going on a tour of the history of dependencies in Ruby, from the beginning to the present day. When we're done, you'll not only understand what happens when you use Bundler, you'll understand why things work the way they do.\n\nStarting with good old-fashioned `require`, we'll discuss how Ruby allows you to load code from files and directories. Next, we'll look at `setup.rb`, the first way Ruby developers were able to share code with one another. After that, Rubygems and the revolutionary ability to install multiple versions of the same library. Finally, we'll look at Bundler and exactly how dependency management for a single project is very different from just managing libraries.\n\nEven though the `require` method has been around since at least 1997, which is the oldest Ruby code we have in version control, it can still be broken down into even smaller concepts. Using code from another file is functionally the same as inserting that file into your code at the place you wrote `require`.  As a result, it's possible to implement a naive require function with just one line of code.\n\n```ruby\ndef require(filename)\n  eval File.read(filename)\nend\n```\n\nIf you start thinking about how this function would work in practice, though, you'll probably notice a couple of problems. First, if you call this `require` more than once, the file will be run multiple times. It's generally a bad idea to run required code multiple times, even if the file is required more than once in different places. The second require could disrupt your program by re-initializing values, and every class and method has already been created by the first require.\n\nHow can we avoid requiring the same file twice? The simplest answer is to keep track of every file that we have ever required before, and only eval files that are being required for the first time. An implementation of this better require function would still be very simple.\n\n```ruby\n$LOADED_FEATURES = []\n\ndef require(filename)\n  return true if $LOADED_FEATURES.include?(filename)\n  eval File.read(filename)\n  $LOADED_FEATURES << filename\nend\n```\n\nAlthough global variables are usually considered evil, in this case it's the only kind of tracking that makes sense—required files create constants in the global namespace, and so the list of required files should be global, too. In fact, Ruby does provide a global variable named `$LOADED_FEATURES`, and it holds a list of every file that has been required, just like our example!\n\nThe second problem that you might have noticed by now is that the argument to `require` has to be an absolute path on the filesystem. That's probably okay if you know exactly where every file on your machine is, but that won't work between lots and lots of different developers. The easiest way to allow requires that aren't absolute is to just treat every filename as if it's relative to the directory the program was started from. That's easy, but doesn't work well if you want to combine Ruby files from a bunch of different directories.\n\nTo allow required files to be in different directories, we could create a list of directories to look inside whenever require is called. Here's what an implementation of load paths might look like:\n\n```ruby\n$LOAD_PATH = [\"/path/to/code\", \"/other/path/to/code\"]\n\ndef require(filename)\n  full_path = $LOAD_PATH.find do |path|\n    File.exist?(File.join(path, filename))\n  end\n  eval File.read(full_path)\nend\n```\n\nYou may then wonder if these two things can be combined. They can! Here's a version of the function that only loads files once, and looks in all `$LOAD_PATH` directories.\n\n```ruby\n$LOAD_PATH = [\"/path/to/code\", \"/other/path/to/code\"]\n$LOADED_FEATURES = []\n\ndef require(filename)\n  full_path = $LOAD_PATH.find do |path|\n    File.exist?(File.join(path, filename))\n  end\n  return true if $LOADED_FEATURES.include?(full_path)\n  eval File.read(full_path)\n  $LOADED_FEATURES << full_path\nend\n```\n\nAnyway, adding a load path allows us to find Ruby libraries even if they are spread across multiple directories. At this point, we can add the directory that holds the Ruby standard library to that list, and it becomes very easy to require those files. Loading `net/http`? No problem, now you can just `require 'net/http'`, and Ruby wil automatically check the directory where it lives.\n\nAt this point, we've already caught up to the state of the art in Ruby libraries circa 2004. The final piece of the puzzle for developers who wanted to share Ruby code in 2004 was the combination of `setup.rb` and the RAA, or Ruby Application Archive. While the RAA is no longer around (it was shut down 2014 due to lack of use), `setup.rb` is amazingly [still around on the internet](http://i.loveruby.net/en/projects/setup/), and you can even download it if you like. Just to warn you, though, it hasn't been updated since 2005.\n\nAt its core, `setup.rb` is a Ruby implementation of the classic unix trinity of commands to install software: `./configure && make && make install`. When using `setup.rb`, the commands to run are `ruby setup.rb config && ruby setup.rb setup && ruby setup.rb install`. Installing a Ruby library was then as simple as:\n\n  1. Browse the RAA looking for a library that did what you wanted.\n  2. Find a library, click a link to download the .tar.gz file containing the library.\n  3. Decompress the tarball, `cd` into the directory, and run `ruby setup.rb`.\n\nAt that point, you would have installed the library! Because `setup.rb` installed the Ruby files into a single, well-known directory that was already added to the load path, you could even require the library immediately. Libraries could be found, downloaded, installed, and used. It was pretty good.\n\nIt doesn't take too long to figure out that there are some possible problems with this plan, though. For example, if a library is updated, and you want the new version, how do you get it? Well, you had to browse back to the RAA, find the new version, download it, decompress the tarball, and then manually run `ruby setup.rb` again. Then you'd have the new version.\n\nDoes this sound tedious to you? Speaking as someone who was there, let me tell you: it was tedious. It took a lot of time, it was error prone, there was no good way to know when new versions came out. In a word, it was... not great. Even worse, what if you had one Ruby application that only worked with the old version of tha library? You just overwrote the old version with the new one, and your script that needs the old version doesn't work anymore. Oops.\n\nIn 2004, RubyGems came racing to the rescue, fixing all of these problems. Installing libraries was a single command: `gem install`. Checking to see what gems were available was also a single command: `gem list`. Just that was enough to revolutionize sharing code in Ruby, but RubyGems had yet another trick up its sleeve: multiple gem versions.\n\nUnhappy with the way that `setup.rb` only allowed one version of each library to be installed at a time, RubyGems allow multiple versions of a gem to be installed at the same time. RubyGems adds its own patches to the `require` method, checking to see if a gem is installed that provides the file `rack.rb`. If no version of that gem has been activated, RubyGems will automatically activate the newest version that is already installed.\n\nIt's even possible to load a specific version of a gem, rather than the newest version that has been installed using the `gem` method provided by RubyGems.\n\nHere's how to load a specific version of a gem using the `gem` method, followed by `require`:\n\n```ruby\ngem \"rack\", \"1.0\"\nrequire \"rack\"\n```\n\nCalling the `gem` method adds a specific version of that gem to the load path. At that point, a regular `require` is enough to load exactly that version of that gem.\n\nIf you running a command that's provided by a gem, like `rackup`, it's also possible to run that command using a specific version of the gem. RubyGems checks to see if the first argument is a version number surrounded by underscores, and uses that version if so. So to run `rackup` using rack version 1.2.2 on port 3000, you would run:\n\n```\n$ rackup _1.2.2_ -p 3000\n```\n\nRubyGems made installing, upgrading, and using libraries easy. So easy, in fact, that there was an explosion of libraries. Today, there are almost 100,000 gems, and almost 1 million released versions of every gem.\n\nThis explosion revealed a new problem with dependencies: it's hard to coordinate them. If one developer ran `gem install foo` and started using a new gem in the application, other developers on the project would have to be told to run `gem install foo`. Then each of the production servers would also have to run `gem install foo`.\n\nEven worse, setting up a new machine might mean that `gem install foo` installed a different version than the one that the application knew how to use. Adding new developers to a team could be a week-long process as gems were installed, checked, and fixes were made.\n\nAround  2008, developers started to create solutions for managing lists of gems. Rails started offering the `config.gem` setting, and the `gem_installer` gem offered another option for installing many gems at once.\n\nUnfortunately, it didn't take long to discover problems with that system, either. Because RubyGems automatically used the newest version of each gem, simply having older versions of gems installed wasn't enough to mean that they would be used. Days, and even weeks of developer time was spent trying to figure out why projects would mysteriously work on one machine only to fail on another machine.\n\nAnyone with more than one large Rails application quickly discovered exactly how hard it is to manage gems by hand: upgrade a gem in any application, and all the other applications on your machine stop working until you upgrade them, too. Upgrading gems in those apps spread the upgrade pain to all the other developers on those applications, and so on.\n\nUltimately, a large chunk of any Ruby developer's time was spent managing and upgrading gems, and it sucked. Just fixing that would be a big enough reason to create Bundler right there, but there was another, even more insidious, problem as well: gem activation errors.\n\nGem activation errors mean that, somehow, RubyGems was asked to activate a gem, and then later on asked to activate a different version of the same gem. Since it's not possible to have two versions of the same gem loaded at the same time, this raises an exception. At this point, you're probably thinking \"Surely that wasn't very common, André! That sounds like a complicated and rare situation.\"\n\nIf only that were true. :( Almost every Ruby application with more than a handful of gems would eventually start to experience this problem. RubyGems loads the newest installed gem by default, and so when another dependency declared that it only worked with a version slightly older than the newest one, everything would explode.\n\nThe underlying reason for activation errors is simple: RubyGems does what we usually call \"runtime dependency resolution\". It loads the gems you ask for, when you ask for them, and doesn't know in advance if you're going to need a different version later. To prevent runtime dependency errors, we need to do dependency resolution _before_ runtime. We need to know every gem and every version, and know that they all work together, before we start to load any of them.\n\nThis is the ultimate problem that Bundler exists to solve: how do you figure out which versions of all the gems that you want can actually work together? Each gem depends on other gems, and those gems depend on other gems, and so on. Before Bundler, this process was done entirely by hand, and simply involved trying things until the exceptions stopped.\n\nUnsurprisingly, computers are more accurate than humans at sytematically trying a mutitude of options and reporting back which options work. They're also much, much faster at it. Thanks to Bundler, Ruby developers have been able to simply list the gems they want to use and count on Bundler to find versions for all of them that actually work together.\n\nThis problem is called \"dependency graph resolution\", and it is an example of a Well-Known Hard Problem™, also known as NP-Complete problems. In theory, it is possible to construct a set of existing gems and a Gemfile such that it could take until past the heat death of the universe to find versions that all work together.\n\nSince most developers don't have that long to spare, Bundler's resolver uses a lot of tricks, shortcuts, and heuristics to prioritize which gem versions to try first. We've built up a pretty large library of tricks over the years, and most Gemfiles now resolve within a few seconds.\n\nAfter finding the versions that work together, Bundler records the exact versions of every gem into another file, named `Gemfile.lock`. This lock file is what makes it possible to install the exact same versions on to every machine that runs this application, whether that machine belongs to a developer, a production server, or a CI server.\n\nAt the end of the day, the way Bundler works boils down to two separate steps, `bundle install` and `bundle exec`. The steps for `bundle install` are pretty simple to explain:\n\n1. Read the Gemfile (and lock, if it's there)\n2. Ask RubyGems.org for a list of every version of every gem we want\n2. If needed, find gem versions allowed by the Gemfile that work together\n3. If found, write down those versions in the lock for future installs\n4. Install gems as needed until every gem in the lock is installed\n\nThe process for `bundle exec` is similar, with two important changes. First, it is just setting up Ruby to load gems that are already installed, so it doesn't ask RubyGems.org for a list of gem versions. Second, it doesn't install gems if any are missing, it just prints out an error asking you to install the gems, instead. Here are the steps:\n\n1. Read the Gemfile (and lock, if it's there)\n2. If needed, find gem versions allowed by the Gemfile that work together\n3. If found, write down those versions in the lock for future installs\n4. Remove any existing gems from the `$LOAD_PATH` array\n5. Add each gem version listed in the lock file to the `$LOAD_PATH`\n\nThat's it! While there are a lot of other details, those are the underlying pieces Bundler uses to let you get your work done: `require`, the `$LOAD_PATH` array, and RubyGems. Each one is built on top of the ones that came before, and each one fixes problems that only became apparent after the new system was created.\n\nEven after Bundler was created, the pattern continues. The biggest problem left for users of Bundler 1.0 was how long it took to run `bundle install`. To fix that, Bundler 1.1 created a completely new way to get information about gems from RubyGems.org and it sped things up. We continue to work on Bundler today, and we released [Bundler 1.9](http://bundler.io/blog/2015/03/21/hello-bundler-19.html) just this month. We have big improvements in the pipeline as well, so keep an eye on the [Bundler website](http://bundler.io) or [@bundlerio](https://twitter.com/bundlerio) on Twitter for updates!\n\nIf you use Bundler, or if your company uses Bundler, support Bundler's development. For those of you with time but not money, we'd love your help on Bundler! You can tweet at us [@bundlerio](https://twitter.com/bundlerio) or email us at [team@bundler.io](mailto:team@bundler.io) and set you up with ways you can help. On the other hand, if you have money but not time, you can still make sure Bundler keeps getting better by joining [Ruby Together](https://rubytogether.org/) as a paying member.\n\nRuby Together membership fees go directly to support developers working on Bundler, RubyGems, and RubyGems.org. We're working to make sure that you can spend time on your own work, rather than solving dependency problems. As Ruby Together grows, we'll be tackling more community issues, including gem mirrors, better public benchmarks for Ruby and Rails, and more.\n",
				"date_published": "2015-04-28T00:00:00-08:00",
				"url": "https://andre.arko.net/2015/04/28/how-does-bundler-work-anyway/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2015/04/09/the-ruby-way-rd-edition/",
				"title": "The Ruby Way, 3rd Edition",
				"content_html": "<p>A surprising thing happened to me last year: I wrote a book.</p>\n<p>Well, I didn&rsquo;t quite write a book. I coauthored a book. I revised a book. I updated a book. While the starting point was much farther along than usual, it still felt an awful lot like writing a book.</p>\n<p>In the end, though, after a year of writing and rewriting and editing and checking: I wrote a book! I&rsquo;m as surprised as anyone.</p>\n<p>It&rsquo;s all Sandi Metz&rsquo;s fault. Years ago, at GoRuCo, I said hi to Sandi during lunch, and she asked if I had met her publisher, Debra Williams-Cauley. We chatted pleasantly for a few minutes, and then Debra asked what I thought of <em>The Ruby Way</em>.</p>\n<p>I said it was my favorite Ruby book, but I was sad that it as out of date. At that point, Sandi immediately declared &ldquo;I deny all responsibility for this&rdquo;, and walked away. Next, of course, Debra asked if I would be interested in coauthoring the new edition.</p>\n<p>It was a great way to satisfy my urge to have written a book without having to (quite) do all of the work required for writing a book. Every page had to be tested against new versions of Ruby, and almost every page had to be updated. About a quarter of the pages in this edition were written by me, out of the one third of the pages that were new for this edition.</p>\n<p>Surprisingly, writing a book somehow produced ideas for other books. Given the monumental amount of time that went into revising this book, though, writing any new books from scratch is going to take a really long time.</p>\n<p>Ultimately, I guess I have to accept that it&rsquo;s my fault, since I did have to do the writing. But it wouldn&rsquo;t have happened without Sandi. So thanks, Sandi. This edition&rsquo;s for you.</p>\n<p>Check out <a href=\"http://www.amazon.com/gp/product/0321714636/?tag=indirect0b-20\">The Ruby Way, 3rd Edition</a> on Amazon, or get a sample chapter of the book from <a href=\"http://therubyway.io\">therubyway.io</a>.</p>\n<p><a href=\"http://www.amazon.com/gp/product/0321714636/?tag=indirect0b-20\"><img src=\"https://indirect.micro.blog/uploads/2025/8ce0f103a6.jpg\" style=\"width: 336px;\"></a></p>\n",
				"content_text": "A surprising thing happened to me last year: I wrote a book.\n\nWell, I didn't quite write a book. I coauthored a book. I revised a book. I updated a book. While the starting point was much farther along than usual, it still felt an awful lot like writing a book.\n\nIn the end, though, after a year of writing and rewriting and editing and checking: I wrote a book! I'm as surprised as anyone.\n\nIt's all Sandi Metz's fault. Years ago, at GoRuCo, I said hi to Sandi during lunch, and she asked if I had met her publisher, Debra Williams-Cauley. We chatted pleasantly for a few minutes, and then Debra asked what I thought of _The Ruby Way_.\n\nI said it was my favorite Ruby book, but I was sad that it as out of date. At that point, Sandi immediately declared \"I deny all responsibility for this\", and walked away. Next, of course, Debra asked if I would be interested in coauthoring the new edition.\n\nIt was a great way to satisfy my urge to have written a book without having to (quite) do all of the work required for writing a book. Every page had to be tested against new versions of Ruby, and almost every page had to be updated. About a quarter of the pages in this edition were written by me, out of the one third of the pages that were new for this edition.\n\nSurprisingly, writing a book somehow produced ideas for other books. Given the monumental amount of time that went into revising this book, though, writing any new books from scratch is going to take a really long time.\n\nUltimately, I guess I have to accept that it's my fault, since I did have to do the writing. But it wouldn't have happened without Sandi. So thanks, Sandi. This edition's for you.\n\nCheck out [The Ruby Way, 3rd Edition](http://www.amazon.com/gp/product/0321714636/?tag=indirect0b-20) on Amazon, or get a sample chapter of the book from [therubyway.io](http://therubyway.io).\n\n[<img src=\"https://indirect.micro.blog/uploads/2025/8ce0f103a6.jpg\" style=\"width: 336px;\">](http://www.amazon.com/gp/product/0321714636/?tag=indirect0b-20)\n",
				"date_published": "2015-04-09T00:00:00-08:00",
				"url": "https://andre.arko.net/2015/04/09/the-ruby-way-rd-edition/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2015/03/18/announcing-ruby-together/",
				"title": "Announcing Ruby Together",
				"content_html": "<p>All of the infrastructure used by Ruby developers today, including <a href=\"https://bundler.io\">Bundler</a>, <a href=\"https://rubygems.org/pages/download\">RubyGems</a>, and <a href=\"https://rubygems.org/\">RubyGems.org</a> is maintained and developed by volunteers.  While it&rsquo;s good that no one company controls resources shared by the community, it&rsquo;s terrible that the only people who work on our shared infrastructure are doing so for free and in their spare time.</p>\n<p><a href=\"https://rubytogether.org\">Ruby Together</a> was founded so the community can cooperate to solve that problem. It will fund on-call rotations, maintenance work, and improvements to the <a href=\"https://rubytogether.org/projects\">shared, public infrastructure</a> used by the entire community.</p>\n<p>We&rsquo;ll also provide <a href=\"https://rubytogether.org/benefits\">benefits just for members</a>. To begin with, opportunities to connect and exchange knowledge, turning <a href=\"https://rubybench.org\">RubyBench.org</a> into a fully-fledged community performance benchmarking resource, and tracking and reporting on important deprecations and security issues for members.</p>\n<p>The community that has grown up around Ruby is amazing, and provides many fantastic (and fantastically useful) tools that any developer can use. Let&rsquo;s work together to make sure those tools (and the community) are the best that they can be, for everyone.</p>\n<p>If you or your company has benefited from the free and open-source tools available in Ruby, support those tools by becoming a member of Ruby Together. Membership dues directly fund work that benefits everyone using Ruby, and are usually tax-deductible. Both individual and corporate memberships are available. <a href=\"https://rubytogether.org/join\">Join us today</a>!</p>\n<img src=\"https://indirect.micro.blog/uploads/2025/12fccf6258.jpg\" alt=\"Ruby Together Logo\" style=\"width: 200px\">\n<p><small>This post was originally written for <a href=\"https://rubytogether.org/news/2015-03-17-announcing-ruby-together\">Ruby Together</a>.</small></p>\n",
				"content_text": "\nAll of the infrastructure used by Ruby developers today, including [Bundler][bundler], [RubyGems][rubygems], and [RubyGems.org](https://rubygems.org/) is maintained and developed by volunteers.  While it's good that no one company controls resources shared by the community, it's terrible that the only people who work on our shared infrastructure are doing so for free and in their spare time.\n\n[Ruby Together][rubytogether] was founded so the community can cooperate to solve that problem. It will fund on-call rotations, maintenance work, and improvements to the [shared, public infrastructure][projects] used by the entire community.\n\nWe'll also provide [benefits just for members][benefits]. To begin with, opportunities to connect and exchange knowledge, turning [RubyBench.org](https://rubybench.org) into a fully-fledged community performance benchmarking resource, and tracking and reporting on important deprecations and security issues for members.\n\nThe community that has grown up around Ruby is amazing, and provides many fantastic (and fantastically useful) tools that any developer can use. Let's work together to make sure those tools (and the community) are the best that they can be, for everyone.\n\nIf you or your company has benefited from the free and open-source tools available in Ruby, support those tools by becoming a member of Ruby Together. Membership dues directly fund work that benefits everyone using Ruby, and are usually tax-deductible. Both individual and corporate memberships are available. [Join us today][join]!\n\n<img src=\"https://indirect.micro.blog/uploads/2025/12fccf6258.jpg\" alt=\"Ruby Together Logo\" style=\"width: 200px\">\n\n<small>This post was originally written for [Ruby Together][post].</small>\n\n[rubytogether]: https://rubytogether.org\n[bundler]: https://bundler.io\n[rubygems]: https://rubygems.org/pages/download\n[projects]: https://rubytogether.org/projects\n[benefits]: https://rubytogether.org/benefits\n[join]: https://rubytogether.org/join\n[post]: https://rubytogether.org/news/2015-03-17-announcing-ruby-together\n",
				"date_published": "2015-03-18T00:00:00-08:00",
				"url": "https://andre.arko.net/2015/03/18/announcing-ruby-together/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2014/09/29/rails-from-the-ground-up/",
				"title": "Rails from the ground up: HTML",
				"content_html": "<p>Last time, we learned about <a href=\"/2014/07/22/rails-from-the-ground-up-headers--status-codes/\">status codes</a>, and created an HTTP server that was able to serve a response to a web browser. This time, we&rsquo;re going to change the response body to be <em>HTML</em>. HTML stands for &ldquo;HyperText Markup Language&rdquo;. HTML is written as plain text, but <em>rendered</em> by web browsers.</p>\n<p>What each browser renders is theoretically determined by the HTML sent to the browser (and CSS sent to the browser, but we&rsquo;ll talk about that later). In real life, HTML may be rendered slightly differently by different browsers. For now, though, we&rsquo;re going to ignore those minor differences, and focus on getting our HTTP server sending HTML in its responses.</p>\n<p>Here&rsquo;s an expanded version of our HTTP server from last time that returns HTML instead of plaintext.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#39;socket&#39;</span>\n</span></span><span style=\"display:flex;\"><span>server <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">TCPServer</span><span style=\"color:#f92672\">.</span>new <span style=\"color:#ae81ff\">3000</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">loop</span> <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>  socket <span style=\"color:#f92672\">=</span> server<span style=\"color:#f92672\">.</span>accept\n</span></span><span style=\"display:flex;\"><span>  socket<span style=\"color:#f92672\">.</span>write <span style=\"color:#e6db74\">&#34;HTTP/1.0 200 OK</span><span style=\"color:#ae81ff\">\\r\\n</span><span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>  socket<span style=\"color:#f92672\">.</span>write <span style=\"color:#e6db74\">&#34;Content-Length: 38</span><span style=\"color:#ae81ff\">\\r\\n</span><span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>  socket<span style=\"color:#f92672\">.</span>write <span style=\"color:#e6db74\">&#34;</span><span style=\"color:#ae81ff\">\\r\\n</span><span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>  socket<span style=\"color:#f92672\">.</span>write <span style=\"color:#e6db74\">&#34;&lt;html&gt;&lt;body&gt;&lt;h1&gt;hi&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;</span><span style=\"color:#ae81ff\">\\n</span><span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>  socket<span style=\"color:#f92672\">.</span>close\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span>\n</span></span></code></pre></div><p>Go ahead and run that code and then brows to <a href=\"http://localhost:3000\">localhost:3000</a> in your browser. You&rsquo;ll notice that a couple of things have changed since last time. The word &ldquo;hi&rdquo; should be much bigger and bolder than it was in the example from last time. In the code, we&rsquo;ve changed the <code>Content-Length</code> header to match the length of our new body, and we&rsquo;ve changed the body to contain some new words inside the <code>&lt;&gt;</code> characters, which are normally referred to as <em>angle brackets</em>.</p>\n<p>The combination of a word inside angle brackets, some text, and the same word (after a slash <code>/</code>) inside angle brackets again is called an HTML <em>tag</em>. The initial word is called an <em>opening tag</em>, and the final slash plus word is called a <em>closing tag</em>. Tags are the way that HTML provides instructions to web browsers. (There are some tags that don&rsquo;t need closing tags, but we&rsquo;ll get to them for later). Our new HTTP response body contains three tags: an <code>html</code> tag, a <code>body</code> tag, and an <code>h1</code> tag.</p>\n<p>The <code>html</code> tag is required for all HTML documents, and simply serves as the beginning and end of the text that a browser should render. Like HTTP has headers and then a body, HTML can also have optional headers and then a body. We&rsquo;ve skipped the <code>head</code> tag and HTML headers for now, but we&rsquo;ll come back to them later. The <code>body</code> tag tells the browser to render the HTML inside it. Finally, the <code>h1</code> tag tells the browser that the text inside it is a header. There are several header tags, starting with <code>h1</code> (the biggest) and going  down to <code>h6</code> (the smallest, but still bigger than regular text).</p>\n<p>HTML contains dozens of tags, and each one tells the web browser to do something different. Tags can be invisible (like <code>span</code> tags) or they can be extremely visible (like <code>h1</code> tags). Either way, they add what is typically called <em>semantic meaning</em> to the text. Semantic meaning is just a fancy way of saying that the tags add information about what the text means that you wouldn&rsquo;t have if the tags weren&rsquo;t there.</p>\n<p>There are a lot of pieces to HTML, and it would take a lot of posts to cover them all. We&rsquo;re focusing on Rails here, but if you&rsquo;re interested in learning more about HTML itself, check out <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML\">Mozilla developer documentation</a> as an online reference, or <a href=\"http://www.amazon.com/gp/product/0596159900/?tag=indirect0b-20\">Head First HTML &amp; CSS</a> as a book to get started with.</p>\n<p>If you&rsquo;ve been experimenting with the HTTP server code, you&rsquo;ve probably noticed that the <code>Content-Length</code> header has to exactly match the number of characters in the response body, or web browsers won&rsquo;t show it. Next time, we&rsquo;ll fix that by making our program count the characters for us. We&rsquo;ll also start to return <em>dynamic content</em>, which means that the page you see in your browser won&rsquo;t be the same every time.</p>\n",
				"content_text": "Last time, we learned about [status codes](/2014/07/22/rails-from-the-ground-up-headers--status-codes/), and created an HTTP server that was able to serve a response to a web browser. This time, we're going to change the response body to be _HTML_. HTML stands for \"HyperText Markup Language\". HTML is written as plain text, but _rendered_ by web browsers.\n\nWhat each browser renders is theoretically determined by the HTML sent to the browser (and CSS sent to the browser, but we'll talk about that later). In real life, HTML may be rendered slightly differently by different browsers. For now, though, we're going to ignore those minor differences, and focus on getting our HTTP server sending HTML in its responses.\n\nHere's an expanded version of our HTTP server from last time that returns HTML instead of plaintext.\n\n```ruby\nrequire 'socket'\nserver = TCPServer.new 3000\nloop do\n  socket = server.accept\n  socket.write \"HTTP/1.0 200 OK\\r\\n\"\n  socket.write \"Content-Length: 38\\r\\n\"\n  socket.write \"\\r\\n\"\n  socket.write \"<html><body><h1>hi</h1></body></html>\\n\"\n  socket.close\nend\n```\nGo ahead and run that code and then brows to [localhost:3000](http://localhost:3000) in your browser. You'll notice that a couple of things have changed since last time. The word \"hi\" should be much bigger and bolder than it was in the example from last time. In the code, we've changed the `Content-Length` header to match the length of our new body, and we've changed the body to contain some new words inside the `<>` characters, which are normally referred to as _angle brackets_.\n\nThe combination of a word inside angle brackets, some text, and the same word (after a slash `/`) inside angle brackets again is called an HTML _tag_. The initial word is called an _opening tag_, and the final slash plus word is called a _closing tag_. Tags are the way that HTML provides instructions to web browsers. (There are some tags that don't need closing tags, but we'll get to them for later). Our new HTTP response body contains three tags: an `html` tag, a `body` tag, and an `h1` tag.\n\nThe `html` tag is required for all HTML documents, and simply serves as the beginning and end of the text that a browser should render. Like HTTP has headers and then a body, HTML can also have optional headers and then a body. We've skipped the `head` tag and HTML headers for now, but we'll come back to them later. The `body` tag tells the browser to render the HTML inside it. Finally, the `h1` tag tells the browser that the text inside it is a header. There are several header tags, starting with `h1` (the biggest) and going  down to `h6` (the smallest, but still bigger than regular text).\n\nHTML contains dozens of tags, and each one tells the web browser to do something different. Tags can be invisible (like `span` tags) or they can be extremely visible (like `h1` tags). Either way, they add what is typically called _semantic meaning_ to the text. Semantic meaning is just a fancy way of saying that the tags add information about what the text means that you wouldn't have if the tags weren't there.\n\nThere are a lot of pieces to HTML, and it would take a lot of posts to cover them all. We're focusing on Rails here, but if you're interested in learning more about HTML itself, check out [Mozilla developer documentation](https://developer.mozilla.org/en-US/docs/Web/HTML) as an online reference, or [Head First HTML & CSS](http://www.amazon.com/gp/product/0596159900/?tag=indirect0b-20) as a book to get started with.\n\nIf you've been experimenting with the HTTP server code, you've probably noticed that the `Content-Length` header has to exactly match the number of characters in the response body, or web browsers won't show it. Next time, we'll fix that by making our program count the characters for us. We'll also start to return _dynamic content_, which means that the page you see in your browser won't be the same every time.\n",
				"date_published": "2014-09-29T00:00:00-08:00",
				"url": "https://andre.arko.net/2014/09/29/rails-from-the-ground-up/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2014/09/26/ios-keyboards/",
				"title": "iOS 8 Keyboards",
				"content_html": "<p>This week I tried a lot of iOS 8 third-party keyboards. They all had problems so big that I gave up. Here&rsquo;s what I noticed.</p>\n<p><strong>Fleksy</strong> (free)</p>\n<p><img src=\"https://indirect.micro.blog/uploads/2025/1021a436e9.jpg\" alt=\"Fleksy\" style=\"float: right; width: 40%; height: 40%; margin: 0 0 5px 25px;\"></p>\n<p>Swipe left to delete! Swipe right for space! So good. Emoji by holding down the enter key is also better than trying to find the emoji keyboard in a list of keyboards. Unfortunately, Fleksy isn&rsquo;t a very good keyboard. 😔 Swipe right twice to get punctuation is weird (and really slow). The &ldquo;compact&rdquo; mode takes up exactly the same space, but expands each row of keys and drops the spacebar row. The delete key has been moved to the spacebar row because ???. The &ldquo;change keyboard&rdquo; globe button is holding down on the numeric keyboard shift, and there is no menu of keyboards. The key locations don&rsquo;t quite match the default keyboard locations. Finally, but sooo frustratingly: the keyboard colors don&rsquo;t change to match the application theme, so you end up with a blinding white keyboard while your app is in dark mode.</p>\n<br style=\"clear:both;\">\n<p><strong>SwiftKey</strong> (free)</p>\n<p><img src=\"https://indirect.micro.blog/uploads/2025/ed33eae45b.jpg\" alt=\"SwiftKey\" style=\"float: right; width: 40%; height: 40%; margin: 0 0 5px 25px;\"></p>\n<p>When you only have one hand available to use your phone, swiping to type is completely magical and amazing. It makes me giddy to be able to type with only one hand. The dedicated swipe punctuation button is also pretty genius, since a single swipe is all you need for common characters. That said, anytime time I have two hands, it&rsquo;s awful. It&rsquo;s worse at predictive typing than the default keyboard, it also can&rsquo;t change themes when an app does, and its globe button to change keyboards also doesn&rsquo;t show the menu of keyboards. Also, the spacebar is too tall and has a big ugly logo on it. Ick.</p>\n<br style=\"clear:both;\">\n<p><strong>TouchPal</strong> (free)</p>\n<p><img src=\"https://indirect.micro.blog/uploads/2025/f440bbe9ea.jpg\" alt=\"TouchPal\" style=\"float: right; width: 40%; height: 40%; margin: 0 0 5px 25px;\"></p>\n<blockquote>\n<p>Has swipe-typing. Has emoji when holding enter. Has gestures for numbers and punctuation. Looks almost exactly like the default keyboard. Seemed perfect&hellip; and then I tried to actually use it. The swiping is barely functional, and nowhere close to as accurate as SwiftKey. Every single word (whether swiped, typed, or gestured) appears only in the autocomplete options bar <em>until you type a space or the next word</em>. It&rsquo;s really hard to describe how flow-destroying that is, so if you&rsquo;re curious, try it out. As usual, can&rsquo;t change colors to match the application.</p>\n</blockquote>\n<br style=\"clear:both;\">\n<p><strong>Swype</strong> ($0.99)</p>\n<p><img src=\"https://indirect.micro.blog/uploads/2025/190c5aa152.jpg\" alt=\"Swype\" style=\"float: right; width: 40%; height: 40%; margin: 0 0 5px 25px;\"></p>\n<p>Super nice swiping. Manages spaces between words and punctuation better than every other option. The delete key removes an entire swiped word so you can try again. That&rsquo;s super great, and I love it. The Swype button that has a menu for changing keyboards and settings is super fast to use; no hold required. It has a numpad keyboard! I don&rsquo;t use numpads, ever, but that&rsquo;s cute, and kinda neat. Adds a dedicated comma button to the left of the spacebar. Pretty gutsy. Inexplicably has a dedicated button on the 123 keyboard layer that types ☺️. You can&rsquo;t type any other emoji directly from Swype, but that one has just become your new friend. That said, I kept accidentally hitting a word in the autocomplete bar when I meant to type a letter in the top row. The colors are static, as usual. The change keyboard button doesn&rsquo;t show a menu. It still doesn&rsquo;t look as nice as the default keyboard. Sigh.</p>\n<p>For comparison, here is the default keyboard and input menu that only the default keyboard has.</p>\n<p><img src=\"https://indirect.micro.blog/uploads/2025/87af5659d9.jpg\" alt=\"Default\" style=\"float: left; width: 40%; height: 40%; margin: 0 0 5px 25px;\"></p>\n<p><img src=\"https://indirect.micro.blog/uploads/2025/037f693982.jpg\" alt=\"Input Menu\" style=\"float: left; width: 40%; height: 40%; margin: 0 0 5px 25px;\"></p>\n<br style=\"clear:both;\">\n<p>This is what the default keyboard&rsquo;s automatic color-switching looks like, and Fleksy failing to change colors in the same place.</p>\n<p><img src=\"https://indirect.micro.blog/uploads/2025/57cff4b343.jpg\" alt=\"Color Swap\" style=\"float: left; width: 40%; height: 40%; margin: 0 0 5px 25px;\"></p>\n<p><img src=\"https://indirect.micro.blog/uploads/2025/a082a18d60.jpg\" alt=\"Fleksy Color\" style=\"float: left; width: 40%; height: 40%; margin: 0 0 5px 25px;\"></p>\n<br style=\"clear:both;\">\n<p>On top of being ugly and variously broken, none of these keyboards support Japanese input. I&rsquo;m just going to keep using the OS keyboard for that.</p>\n<p>Ultimately, it turns out I want a very specific thing: a keyboard that is as visually attractive as the default keyboard, switches colors with the app, offers gestures as shortcuts for typing, includes emoji as a layer, switches to other keyboards quickly with an optional menu of available keyboards, and has a fraction of a second toggle to enable functional and well-implemented swipe-style typing for when I only have one hand.</p>\n<p>If there is a one true keyboard somewhere out there, please oh please tell me about it, but in the meantime, I&rsquo;ll keep using the default keyboard when I have two hands, SwiftKey when I have one hand, and the emoji and 10-key kana Japanese keyboards all the time. Crankily.</p>\n",
				"content_text": "This week I tried a lot of iOS 8 third-party keyboards. They all had problems so big that I gave up. Here's what I noticed.\n\n**Fleksy** (free)\n\n<p><img src=\"https://indirect.micro.blog/uploads/2025/1021a436e9.jpg\" alt=\"Fleksy\" style=\"float: right; width: 40%; height: 40%; margin: 0 0 5px 25px;\"></p>\n\nSwipe left to delete! Swipe right for space! So good. Emoji by holding down the enter key is also better than trying to find the emoji keyboard in a list of keyboards. Unfortunately, Fleksy isn't a very good keyboard. 😔 Swipe right twice to get punctuation is weird (and really slow). The \"compact\" mode takes up exactly the same space, but expands each row of keys and drops the spacebar row. The delete key has been moved to the spacebar row because ???. The \"change keyboard\" globe button is holding down on the numeric keyboard shift, and there is no menu of keyboards. The key locations don't quite match the default keyboard locations. Finally, but sooo frustratingly: the keyboard colors don't change to match the application theme, so you end up with a blinding white keyboard while your app is in dark mode.\n\n<br style=\"clear:both;\">\n\n**SwiftKey** (free)\n\n<p><img src=\"https://indirect.micro.blog/uploads/2025/ed33eae45b.jpg\" alt=\"SwiftKey\" style=\"float: right; width: 40%; height: 40%; margin: 0 0 5px 25px;\"></p>\n\nWhen you only have one hand available to use your phone, swiping to type is completely magical and amazing. It makes me giddy to be able to type with only one hand. The dedicated swipe punctuation button is also pretty genius, since a single swipe is all you need for common characters. That said, anytime time I have two hands, it's awful. It's worse at predictive typing than the default keyboard, it also can't change themes when an app does, and its globe button to change keyboards also doesn't show the menu of keyboards. Also, the spacebar is too tall and has a big ugly logo on it. Ick.\n\n<br style=\"clear:both;\">\n\n**TouchPal** (free)\n\n<p><img src=\"https://indirect.micro.blog/uploads/2025/f440bbe9ea.jpg\" alt=\"TouchPal\" style=\"float: right; width: 40%; height: 40%; margin: 0 0 5px 25px;\"></p>\n\n> Has swipe-typing. Has emoji when holding enter. Has gestures for numbers and punctuation. Looks almost exactly like the default keyboard. Seemed perfect... and then I tried to actually use it. The swiping is barely functional, and nowhere close to as accurate as SwiftKey. Every single word (whether swiped, typed, or gestured) appears only in the autocomplete options bar _until you type a space or the next word_. It's really hard to describe how flow-destroying that is, so if you're curious, try it out. As usual, can't change colors to match the application.\n\n<br style=\"clear:both;\">\n\n**Swype** ($0.99)\n\n<p><img src=\"https://indirect.micro.blog/uploads/2025/190c5aa152.jpg\" alt=\"Swype\" style=\"float: right; width: 40%; height: 40%; margin: 0 0 5px 25px;\"></p>\n\nSuper nice swiping. Manages spaces between words and punctuation better than every other option. The delete key removes an entire swiped word so you can try again. That's super great, and I love it. The Swype button that has a menu for changing keyboards and settings is super fast to use; no hold required. It has a numpad keyboard! I don't use numpads, ever, but that's cute, and kinda neat. Adds a dedicated comma button to the left of the spacebar. Pretty gutsy. Inexplicably has a dedicated button on the 123 keyboard layer that types ☺️. You can't type any other emoji directly from Swype, but that one has just become your new friend. That said, I kept accidentally hitting a word in the autocomplete bar when I meant to type a letter in the top row. The colors are static, as usual. The change keyboard button doesn't show a menu. It still doesn't look as nice as the default keyboard. Sigh.\n\nFor comparison, here is the default keyboard and input menu that only the default keyboard has.\n\n<p><img src=\"https://indirect.micro.blog/uploads/2025/87af5659d9.jpg\" alt=\"Default\" style=\"float: left; width: 40%; height: 40%; margin: 0 0 5px 25px;\"></p>\n\n<p><img src=\"https://indirect.micro.blog/uploads/2025/037f693982.jpg\" alt=\"Input Menu\" style=\"float: left; width: 40%; height: 40%; margin: 0 0 5px 25px;\"></p>\n\n<br style=\"clear:both;\">\n\nThis is what the default keyboard's automatic color-switching looks like, and Fleksy failing to change colors in the same place.\n\n<p><img src=\"https://indirect.micro.blog/uploads/2025/57cff4b343.jpg\" alt=\"Color Swap\" style=\"float: left; width: 40%; height: 40%; margin: 0 0 5px 25px;\"></p>\n\n<p><img src=\"https://indirect.micro.blog/uploads/2025/a082a18d60.jpg\" alt=\"Fleksy Color\" style=\"float: left; width: 40%; height: 40%; margin: 0 0 5px 25px;\"></p>\n\n<br style=\"clear:both;\">\n\nOn top of being ugly and variously broken, none of these keyboards support Japanese input. I'm just going to keep using the OS keyboard for that.\n\nUltimately, it turns out I want a very specific thing: a keyboard that is as visually attractive as the default keyboard, switches colors with the app, offers gestures as shortcuts for typing, includes emoji as a layer, switches to other keyboards quickly with an optional menu of available keyboards, and has a fraction of a second toggle to enable functional and well-implemented swipe-style typing for when I only have one hand.\n\nIf there is a one true keyboard somewhere out there, please oh please tell me about it, but in the meantime, I'll keep using the default keyboard when I have two hands, SwiftKey when I have one hand, and the emoji and 10-key kana Japanese keyboards all the time. Crankily.\n",
				"date_published": "2014-09-26T00:00:00-08:00",
				"url": "https://andre.arko.net/2014/09/26/ios-keyboards/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2014/09/20/how-to-safely-store-user/",
				"title": "How to Safely Store User Data",
				"content_html": "<p>So, you have a web application, and you want to store some data &ldquo;securely&rdquo;. Right now, you&rsquo;re you probably thinking &ldquo;I know this already, we should encrypt it!&rdquo;. No.</p>\n<p>Encryption is a last ditch option, and the least secure of the legitimate options for handling sensitive data. Only do encryption if you absolutely have to and there is no other option. Let&rsquo;s look at all the options for secure storage, and which one you should use. We&rsquo;ll start with the better options.</p>\n<h3 id=\"dont-store-it\">Don&rsquo;t store it</h3>\n<p>First option: don&rsquo;t. Don&rsquo;t store it, don&rsquo;t look at it, don&rsquo;t let employees see it, don&rsquo;t let yourself see it. The only way for the data to be absolutely secure is if you absolutely don&rsquo;t have it. There&rsquo;s a pretty good FAQ about the problems with storing data at <a href=\"http://plaintextoffenders.com/faq/devs\">plaintextoffenders.com</a>, but the short version is: do everything you can to not store things. It is not possible to lose data that you don&rsquo;t have.</p>\n<h3 id=\"use-bcrypt\">Use bcrypt</h3>\n<p>If you need to authenticate someone or something, <em>don&rsquo;t store passwords</em>. <a href=\"http://codahale.com/how-to-safely-store-a-password/\">Use bcrypt</a>. Never store your users&rsquo; passwords in plaintext. Never store your users&rsquo; passwords encrypted. Never store your users&rsquo; passwords, period. Use bcrypt to hash the password, and store the hash. Don&rsquo;t write your own implementation of bcrypt, you&rsquo;ll probably get it wrong. Just use the bcrypt library for your language, like the <a href=\"https://github.com/codahale/bcrypt-ruby\">bcrypt</a> gem for Ruby. If you&rsquo;re writing a Rails app, you could just use <a href=\"https://github.com/plataformatec/devise\">devise</a> for user accounts and get bcrypt for passwords for free.</p>\n<p>Bcrypt is deliberately designed to be adjustably hard: as computers get faster, the password hashes stored in the database can be replaced by hashes that are harder to calculate. This keeps the time needed to brute force the password impossibly long, even as computers get faster. Whenever you use bcrypt, increase the bcrypt difficulty factor as high as you can without inconveniencing your users when they log in. Whatever difficulty takes your servers a few hundred milliseconds is probably about right. Every year or two, benchmark your password hash calculations, and <a href=\"http://security.stackexchange.com/questions/15847/is-it-possible-to-increase-the-cost-of-bcrypt-or-pbkdf2-when-its-already-calcula\">increase the difficulty</a> if necessary.</p>\n<h3 id=\"replace-foreign-keys-with-hashes\">Replace foreign keys with hashes</h3>\n<p>If you store data in your database that isn&rsquo;t an actual secret, but can be dangerous when connected to a specific user, don&rsquo;t store the foreign key. This is especially relevant for things like a search history, or a history of browsed items, or anything that could be abused by staff or hackers with access to the data. If you really need to to connect two pieces of sensitive data, don&rsquo;t store the foreign key directly.</p>\n<p>When I worked at <a href=\"http://en.wikipedia.org/wiki/Wesabe\">Wesabe</a>, we stored financial transactions, but we didn&rsquo;t save the foreign key that connected a user profile to their accounts and transaction records. Instead, we used a hash of user information, including their password, as the foreign key. Since we didn&rsquo;t store the password, <a href=\"http://web.archive.org/web/20100731183631/http://blog.wesabe.com/2007/02/23/safeguarding-your-data-the-privacy-wall/\">even employees couldn&rsquo;t connect purchases to a name or email address</a>.</p>\n<p>If you must store sensitive data, don&rsquo;t allow that data to be connected to any names or email addresses. Instead, store a hash that can be reconstructed when the user logs in with their password. Only keep that hash in memory, and expired it after a few minutes of inactivity or as soon as the user logs out, whichever comes first.</p>\n<h3 id=\"keyless-encryption\">Keyless encryption</h3>\n<p>We ran into another, harder problem at Wesabe as well: storing usernames and passwords for bank accounts. Wesabe used those credentials to simply download a list of transactions, but they could also be used to send payments or transfer money out of accounts. We did encrypt those usernames and passwords, but we deliberately did it using a key generated from the user&rsquo;s password.</p>\n<p>Similar to the foreign key hashes I mentioned above, this approach meant that even Wesabe employees couldn&rsquo;t decrypt a user&rsquo;s credentials. When a user logged in, their Wesabe password would be used to regenerate the key that could decrypt their bank usernames and passwords. Then, the bank usernames and passwords would be used, once, to update their transaction data stored in Wesabe. The credentials were only stored in memory, and never saved, logged, or accessible in any other way.</p>\n<p>This was a great first-pass approach to the problem of keeping secrets, but we came up with an even better one later. I&rsquo;ll talk about that option next.</p>\n<h3 id=\"pubkey-encryption\">Pubkey encryption</h3>\n<p>In short, we realized that features we wanted (primarily sharing data between family members&rsquo; accounts and background transaction updates) required better access controls for secret data. Wesabe didn&rsquo;t last long enough to use it, but we (and by we, I mean almost entirely <a href=\"https://twitter.com/coda\">Coda Hale</a> and <a href=\"https://twitter.com/emerose\">Sam Quigley</a>) managed to release a <a href=\"https://github.com/wesabe/grendel\">proof of concept implementation</a> that we called Grendel.</p>\n<p>Grendel uses a public key encryption scheme where secret keys can only be accessed by a user who knows their password. Using this setup, it is possible for Alice to explicitly grant permissions for Bob to see her data. By decrypting the data using her secret key, and then encrypting it with Bob&rsquo;s public key, Alice can allow Bob to see it too.</p>\n<p>In real life, the cryptography was a little bit more complicated than that: the real secret keys were encrypted using keys derived from passwords, and the data shared between accounts was actually the encryption key needed to decrypt the data, instead of the data itself. But the principle was the same, and it worked out pretty well.</p>\n<p>In addition, it meant that anyone who wanted to could also choose to share their secret with our automatic updating system. For those people, a sufficiently determined employee likely could have eventually figured out how to decrypt a copy of someone&rsquo;s credentials. The tradeoff was that it allowed our system to update account and transaction data anytime, even when that person wasn&rsquo;t visiting our site. It wasn&rsquo;t perfect, but we made sure it was an opt-in decision for those who valued convenience over the strictest possible security.</p>\n<p>A system like Grendel is complex, and might not be a gem that you can just drop in to your Rails project in five minutes. That said, it&rsquo;s seriously important to have something like that in a situation where how secure your shit is really matters. Systems like these get used by big websites in the real world. If your goal is to be a grown-up website one day, start thinking about this stuff now.</p>\n<h3 id=\"plain-old-encryption\">Plain old encryption</h3>\n<p>Finally, if you absolutely need access to secret data while a user is not logged in, that is the only time that it is acceptable to actually encrypt the data. Again, avoid this if you possibly can. It provides a juicer target for hacking, and makes it possible for a breach to disclose <em>all</em> of the data you have stored without any additional per-user work by if you are hacked. If there just isn&rsquo;t any way to avoid encrypting some of the data, make sure that you&rsquo;re doing it as safely as you can. Always generate your key using a tool that is known to produce good randomness (in Ruby, use the <code>ActiveSupport::SecureRandom</code> class). Generate a different key for every environment. Never share any your keys between development, test, staging, qa, and production environments.</p>\n<p>Also, very importantly, split your key up. Store part of it in the database as part of the user profile, different for every user in your system. This means brute forcing a single key will not allow you to decrypt every other user&rsquo;s data. Store part of the key in your codebase (but use a different value in each environment). This means that a copy of the database (without a copy of the application source code) will not allow all the data to be decrypted. Finally, store another part of the key in an environment variable that is only available to the application servers while they are running. Generate that part of the key separately for each environment (test, staging, qa, production), and don&rsquo;t store it anywhere your database or source code is stored.</p>\n<p>Splitting the key up this way means that even attacks that reveal the source code of your running application (which are sadly somewhat common), combined with a complete copy of the database (likewise sadly common) still does not reveal the entire key. Every one of these partitions is important: database breaches, code breaches, and code execution breaches often require different security holes. If all three are required to get the entire encryption key, it is significantly less likely that a single breach will result in all of your users&rsquo; &ldquo;secure&rdquo; data being leaked.</p>\n<p>In addition to splitting your keys, rotating your keys is an excellent practice to ensure that former employees or contractors will not retain access to the encrypted data after they&rsquo;ve left. <a href=\"http://product.reverb.com/2015/01/20/encryption-on-rails-a-primer\">Encryption on Rails: A Primer</a> outlines a structure that allows encrypting arbitrary data as well as rotating the keys used to encrypt that data on a regular basis.</p>\n<p>You really don&rsquo;t want your users&rsquo; data to be leaked. California and many other states require notification of data breaches to every user in a timely manner or companies face stiff fines. If your company is sued over the breach and found liable, the resulting judgement could be huge.</p>\n<h3 id=\"secure-your-admin-keys-too\">Secure your admin keys, too</h3>\n<p>One last note: treat all of your own internal credentials as carefully as your users&rsquo; credentials! Careless handling of your own usernames, passwords, and keys can allow a relatively minor hack to escalate into something that destroys your company. Just ask Code Spaces, whose entire company was <a href=\"http://arstechnica.com/security/2014/06/aws-console-breach-leads-to-demise-of-service-with-proven-backup-plan/\">destroyed by a leak of a minor key</a>. They just used the key to upload files to AWS S3, but that key was also able to delete every server, file, database, and database backup that the company had. A single hack of a &ldquo;minor&rdquo; key destroyed the entire company. Don&rsquo;t let that happen to you!</p>\n<p>This stuff is company making or breaking, so think about what you&rsquo;re doing and why you&rsquo;re doing it. Ask for advice from people who know their stuff. Fix small security issues before they combine with other small security issues to become a huge security issue. And definitely follow (at a minimum!) the steps outlined above.</p>\n<p><small>2014-09-22: updated with separate sections for the safer hash and encryption options, thanks <a href=\"http://twitter.com/bgreenlee\">@bgreenlee</a>!</small></p>\n<p><small>2015-01-24: updated with a link to <i>Encryption on Rails</i></small></p>\n",
				"content_text": "\nSo, you have a web application, and you want to store some data \"securely\". Right now, you're you probably thinking \"I know this already, we should encrypt it!\". No.\n\nEncryption is a last ditch option, and the least secure of the legitimate options for handling sensitive data. Only do encryption if you absolutely have to and there is no other option. Let's look at all the options for secure storage, and which one you should use. We'll start with the better options.\n\n### Don't store it\n\nFirst option: don't. Don't store it, don't look at it, don't let employees see it, don't let yourself see it. The only way for the data to be absolutely secure is if you absolutely don't have it. There's a pretty good FAQ about the problems with storing data at [plaintextoffenders.com](http://plaintextoffenders.com/faq/devs), but the short version is: do everything you can to not store things. It is not possible to lose data that you don't have.\n\n### Use bcrypt\n\nIf you need to authenticate someone or something, _don't store passwords_. [Use bcrypt](http://codahale.com/how-to-safely-store-a-password/). Never store your users' passwords in plaintext. Never store your users' passwords encrypted. Never store your users' passwords, period. Use bcrypt to hash the password, and store the hash. Don't write your own implementation of bcrypt, you'll probably get it wrong. Just use the bcrypt library for your language, like the [bcrypt](https://github.com/codahale/bcrypt-ruby) gem for Ruby. If you're writing a Rails app, you could just use [devise](https://github.com/plataformatec/devise) for user accounts and get bcrypt for passwords for free.\n\nBcrypt is deliberately designed to be adjustably hard: as computers get faster, the password hashes stored in the database can be replaced by hashes that are harder to calculate. This keeps the time needed to brute force the password impossibly long, even as computers get faster. Whenever you use bcrypt, increase the bcrypt difficulty factor as high as you can without inconveniencing your users when they log in. Whatever difficulty takes your servers a few hundred milliseconds is probably about right. Every year or two, benchmark your password hash calculations, and [increase the difficulty](http://security.stackexchange.com/questions/15847/is-it-possible-to-increase-the-cost-of-bcrypt-or-pbkdf2-when-its-already-calcula) if necessary.\n\n### Replace foreign keys with hashes\n\nIf you store data in your database that isn't an actual secret, but can be dangerous when connected to a specific user, don't store the foreign key. This is especially relevant for things like a search history, or a history of browsed items, or anything that could be abused by staff or hackers with access to the data. If you really need to to connect two pieces of sensitive data, don't store the foreign key directly. \n\nWhen I worked at [Wesabe][wesabe], we stored financial transactions, but we didn't save the foreign key that connected a user profile to their accounts and transaction records. Instead, we used a hash of user information, including their password, as the foreign key. Since we didn't store the password, [even employees couldn't connect purchases to a name or email address][blog.wesabe.com].\n\n[wesabe]: http://en.wikipedia.org/wiki/Wesabe\n[blog.wesabe.com]: http://web.archive.org/web/20100731183631/http://blog.wesabe.com/2007/02/23/safeguarding-your-data-the-privacy-wall/\n\nIf you must store sensitive data, don't allow that data to be connected to any names or email addresses. Instead, store a hash that can be reconstructed when the user logs in with their password. Only keep that hash in memory, and expired it after a few minutes of inactivity or as soon as the user logs out, whichever comes first.\n\n### Keyless encryption\n\nWe ran into another, harder problem at Wesabe as well: storing usernames and passwords for bank accounts. Wesabe used those credentials to simply download a list of transactions, but they could also be used to send payments or transfer money out of accounts. We did encrypt those usernames and passwords, but we deliberately did it using a key generated from the user's password.\n\nSimilar to the foreign key hashes I mentioned above, this approach meant that even Wesabe employees couldn't decrypt a user's credentials. When a user logged in, their Wesabe password would be used to regenerate the key that could decrypt their bank usernames and passwords. Then, the bank usernames and passwords would be used, once, to update their transaction data stored in Wesabe. The credentials were only stored in memory, and never saved, logged, or accessible in any other way.\n\nThis was a great first-pass approach to the problem of keeping secrets, but we came up with an even better one later. I'll talk about that option next.\n\n### Pubkey encryption\n\nIn short, we realized that features we wanted (primarily sharing data between family members' accounts and background transaction updates) required better access controls for secret data. Wesabe didn't last long enough to use it, but we (and by we, I mean almost entirely [Coda Hale](https://twitter.com/coda) and [Sam Quigley](https://twitter.com/emerose)) managed to release a [proof of concept implementation](https://github.com/wesabe/grendel) that we called Grendel.\n\nGrendel uses a public key encryption scheme where secret keys can only be accessed by a user who knows their password. Using this setup, it is possible for Alice to explicitly grant permissions for Bob to see her data. By decrypting the data using her secret key, and then encrypting it with Bob's public key, Alice can allow Bob to see it too.\n\nIn real life, the cryptography was a little bit more complicated than that: the real secret keys were encrypted using keys derived from passwords, and the data shared between accounts was actually the encryption key needed to decrypt the data, instead of the data itself. But the principle was the same, and it worked out pretty well.\n\nIn addition, it meant that anyone who wanted to could also choose to share their secret with our automatic updating system. For those people, a sufficiently determined employee likely could have eventually figured out how to decrypt a copy of someone's credentials. The tradeoff was that it allowed our system to update account and transaction data anytime, even when that person wasn't visiting our site. It wasn't perfect, but we made sure it was an opt-in decision for those who valued convenience over the strictest possible security.\n\nA system like Grendel is complex, and might not be a gem that you can just drop in to your Rails project in five minutes. That said, it's seriously important to have something like that in a situation where how secure your shit is really matters. Systems like these get used by big websites in the real world. If your goal is to be a grown-up website one day, start thinking about this stuff now.\n\n### Plain old encryption\n\nFinally, if you absolutely need access to secret data while a user is not logged in, that is the only time that it is acceptable to actually encrypt the data. Again, avoid this if you possibly can. It provides a juicer target for hacking, and makes it possible for a breach to disclose _all_ of the data you have stored without any additional per-user work by if you are hacked. If there just isn't any way to avoid encrypting some of the data, make sure that you're doing it as safely as you can. Always generate your key using a tool that is known to produce good randomness (in Ruby, use the `ActiveSupport::SecureRandom` class). Generate a different key for every environment. Never share any your keys between development, test, staging, qa, and production environments.\n\nAlso, very importantly, split your key up. Store part of it in the database as part of the user profile, different for every user in your system. This means brute forcing a single key will not allow you to decrypt every other user's data. Store part of the key in your codebase (but use a different value in each environment). This means that a copy of the database (without a copy of the application source code) will not allow all the data to be decrypted. Finally, store another part of the key in an environment variable that is only available to the application servers while they are running. Generate that part of the key separately for each environment (test, staging, qa, production), and don't store it anywhere your database or source code is stored.\n\nSplitting the key up this way means that even attacks that reveal the source code of your running application (which are sadly somewhat common), combined with a complete copy of the database (likewise sadly common) still does not reveal the entire key. Every one of these partitions is important: database breaches, code breaches, and code execution breaches often require different security holes. If all three are required to get the entire encryption key, it is significantly less likely that a single breach will result in all of your users' \"secure\" data being leaked.\n\nIn addition to splitting your keys, rotating your keys is an excellent practice to ensure that former employees or contractors will not retain access to the encrypted data after they've left. [Encryption on Rails: A Primer](http://product.reverb.com/2015/01/20/encryption-on-rails-a-primer) outlines a structure that allows encrypting arbitrary data as well as rotating the keys used to encrypt that data on a regular basis.\n\nYou really don't want your users' data to be leaked. California and many other states require notification of data breaches to every user in a timely manner or companies face stiff fines. If your company is sued over the breach and found liable, the resulting judgement could be huge.\n\n### Secure your admin keys, too\n\nOne last note: treat all of your own internal credentials as carefully as your users' credentials! Careless handling of your own usernames, passwords, and keys can allow a relatively minor hack to escalate into something that destroys your company. Just ask Code Spaces, whose entire company was [destroyed by a leak of a minor key](http://arstechnica.com/security/2014/06/aws-console-breach-leads-to-demise-of-service-with-proven-backup-plan/). They just used the key to upload files to AWS S3, but that key was also able to delete every server, file, database, and database backup that the company had. A single hack of a \"minor\" key destroyed the entire company. Don't let that happen to you!\n\nThis stuff is company making or breaking, so think about what you're doing and why you're doing it. Ask for advice from people who know their stuff. Fix small security issues before they combine with other small security issues to become a huge security issue. And definitely follow (at a minimum!) the steps outlined above.\n\n<small>2014-09-22: updated with separate sections for the safer hash and encryption options, thanks [@bgreenlee](http://twitter.com/bgreenlee)!</small>\n\n<small>2015-01-24: updated with a link to <i>Encryption on Rails</i></small>\n",
				"date_published": "2014-09-20T00:00:00-08:00",
				"url": "https://andre.arko.net/2014/09/20/how-to-safely-store-user/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2014/08/01/tcp-delays-and-retransmissions-on/",
				"title": "TCP delays and retransmissions on Illumos",
				"content_html": "<p>The other day, I helped debug an issue on some production Joyent Cloud servers (which use SmartOS, based on Illumos, the open-source successor to Solaris). The solution turned out to be so non-obvious, and the cause pretty interesting, so I thought it was worth writing up.</p>\n<p>While checking through the production request logs, I noticed a specific (and very strange) anomaly. During the day, as traffic increased towards the usual daily peak, the amount of time that requests spent waiting to be processed increased dramatically.</p>\n<p>The average time spent in queueing per request went up from ~2ms all the way up to ~12ms&hellip; with only a 3x increase in traffic at peak times. A quick check of the servers showed they had plenty of CPU available, and many web server workers that weren&rsquo;t busy. So what could be causing requests to spend time waiting around, instead of getting responded to right away?</p>\n<p>The problem was tricky to even notice, since it was bad enough to depress the request averages dashboard, but rare enough that it took careful and specific searching to see the slow requests. Making requests against the load balancer machines revealed that roughly 10% of all traffic experienced a delay before it was answered. No matter how long the request took normally, about 9% of traffic would be delayed by 1.1 seconds, and about 1% of traffic would be delayed by 3.4 seconds.</p>\n<p>We eventually narrowed it down to the app servers themselves, writing a script to fire off around 10,000 requests as quickly as they could be served by a single set of unicorn server processes. Even when the requests were coming one at a time, and even when they were coming from the same machine as the unicorns were running on, the problem would appear again and again.</p>\n<p>We tried the usual UNIX voodoo of checking ulimits, file descriptors, open sockets, and the like, but weren&rsquo;t able to find anything that even looked like it might be the problem. According to the internet, a common culprit for long delays before connections are accepted is having too many sockets in the <code>TIME_WAIT</code> state.</p>\n<p>The <code>TIME_WAIT</code> state (or TIME-WAIT, if you ask <a href=\"http://tools.ietf.org/html/rfc793\">RFC 793</a>), is the final state for one side of every TCP/IP connection. After a connection, sockets are held in <code>TIME_WAIT</code> for up to 4 minutes. Keeping one side of the TCP connection around after it has been closed sounds a little weird at first, but there are reasons to do it. I&rsquo;ll explain briefly, and for more details I would suggest <a href=\"http://www.serverframework.com/asynchronousevents/2011/01/time-wait-and-its-design-implications-for-protocols-and-scalable-servers.html\">this explanation of TCP client and server states</a>, as well as the RFC.</p>\n<p>One reason for <code>TIME_WAIT</code> is to be able to catch delayed packets that were sent while the connection was open, but arrive after it is closed. Waiting until the network timeout for any packets still traveling means that the next connection to use the socket won&rsquo;t get confused by stray late packets. This is incredibly unlikely under most circumstances.</p>\n<p>The main reason is to close TCP connections fully and cleanly. The final ACK, acknowledging that the connection is closed, needs to be accepted as a valid packet, even though the connection has already been closed by both sides. Without <code>TIME_WAIT</code>, that final ACK would be rejected, since it would be part of a connection that no longer exists. That&rsquo;s the upside. The downside is that because sockets in <code>TIME_WAIT</code> count as used, opening many connections to a single server can block it from opening new connections until existing <code>TIME_WAIT</code>s time out.</p>\n<p>Since we knew that was a likely possible source for the problem, we checked for that problem first: <code>netstat -sP tcp</code> prints statistics for all TCP connections on the entire system. One of those statistics is <code>tcpTimKeepaliveDrop</code>, a counter of the number of times a connection could not be accepted due to too many other connections being alive. We never saw that number increment, and so assumed that the (high) number of connections in the <code>TIME_WAIT</code> state just wasn&rsquo;t the problem. Big mistake.</p>\n<p>Over the course of an entire day of researching TCP tuning on Solaris, on Illumos, and on SmartOS, we tried adjusting nearly every single TCP stack setting. Some suggestions sounded like they might be related to our problem, others sounded completely unrelated. Eventually, we tried changing every single one we could find, and none of them fixed the problem.</p>\n<p>While researching issues, we ran <code>netstat -sP tcp</code> over and over, looking for some indicator of what the problem was. Near the end of the day, we noticed one specific counter that incremented every time we saw a delayed response: <code>tcpRetransSegs</code>. That counter is incremented anytime a TCP segment is sent more than once.</p>\n<p>So, we finally learned something concrete: a TCP segment has to be sent again, even though the client and server are on the same computer. That immediately became the new problem, though, because that shouldn&rsquo;t ever happen! Retransmission is supposed to happen if data gets lost and isn&rsquo;t delivered. It&rsquo;s not supposed to happen when a machine is trying to talk to itself.</p>\n<p>Armed with new search terms, we were able to find <a href=\"http://lists.illumos.org/pipermail/developer/2011-April/001958.html\">a mailing list post from 2011</a> that described the exact same 1.1 second and 3.4 second delays that we were witnessing. The post was on the Illumos-Developer mailing list, and described the background for a patch to Illumos supplied by Joyent. The entire post is interesting to read, but I&rsquo;ll summarize what they found.</p>\n<p>First, a little necessary background on TCP. It&rsquo;s not terribly obvious, but every network connection using IP (and therefore every TCP/IP connection) happens between a source address and port and a destination address and port. For example, for your computer to display <code>google.com</code>, it connects to that address on port 80 for HTTP or port 443 for HTTPS.</p>\n<p>But that&rsquo;s just the destination. The source is your computer&rsquo;s IP address, on a port that was randomly chosen when you opened the connection. Chances are good that the port number is in the range 49152–65535, which are are the ports officially set aside for this kind of temporary use by IANA (the Internet Assigned Numbers Authority).</p>\n<p>When a single host connects to another single host many, many times (as in the case of our load balancer talking to our app server, for example), it connects from one of those temporary ports to the specific port that the app server is listening on. Thanks to <code>TIME_WAIT</code>, the app server keeps listening for packets coming from each temporary port for up to the default <code>tcp_time_wait_interval</code> of 60 seconds.</p>\n<p>Since the source&rsquo;s temporary port is chosen randomly, with many requests per second it is highly likely that a single temporary port will be used for another connection with the same app server on the same listening port. At that point, the server is supposed to realize a new connection is being opened, and the previous connection is no longer being used.</p>\n<p>Normally, every TCP packet includes a <em>sequence number</em> that starts at a random-ish value and then goes up by one each time another packet is sent. When a connection is in <code>TIME_WAIT</code>, the server will check this number and notice that it is so much higher than the old connection&rsquo;s sequence number that this must be a new request.</p>\n<p>Seems straightforward, but TCP sequence numbers are a single 32-bit value. The default settings on Solaris mean that each new connection starts with a new sequence number that is <em>much</em> higher than the last connection. So much higher, in fact, that after several connections the 32-bit number wraps around and starts again from zero.</p>\n<p>Now the surprising part: when a new connection is created with the same source address, source port, destination address, and destination port, and the TCP sequence number has also wrapped around, the request for a new connection is interpreted as if it were a delayed request to create the connection that is now in <code>TIME_WAIT</code>.</p>\n<p>Under those (very specific) circumstances, the server tries to reset the connection instead of opening a new one. Since the connection it asked for was never opened, the client waits a set amount of time and then tries again. The second time, the connection request succeeds. The amount of time before the client retries is set by the parameters <code>tcp_rexmit_interval_initial</code> and <code>tcp_rexmit_interval_min</code> added together.</p>\n<p>While this set of circumstances sounds extremely complicated and unlikely, we were seeing this exact problem happen in production very frequently. As much as 1% of our traffic (many thousands of requests) was being delayed by 3.4 seconds, even if the actual time needed to deliver the response was only 10 milliseconds.</p>\n<p>Based on the Illumos blog post, some <a href=\"http://www.sean.de/Solaris/soltune.html\">old Solaris tuning advice</a> and some <a href=\"http://www.princeton.edu/~unix/Solaris/troubleshoot/\">new Solaris tuning advice</a>, we were able to find a new TCP configuration that reduces the problem so much we don&rsquo;t notice it anymore. Note, however, that this problem is not &ldquo;fixable&rdquo;, since all we can do is change settings to make it less and less likely.</p>\n<p><strong>Using these settings is a terrible idea on servers exposed to the internet.</strong> That said, running these commands to change TCP tunable settings fixed this issue for us. You&rsquo;ll need root permissions to run these commands on a Solaris/Illumos host.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># 1) Reduce smallest_anon_port for more ephemeral ports, which reduces collisions</span>\n</span></span><span style=\"display:flex;\"><span>ndd -set /dev/tcp tcp_smallest_anon_port <span style=\"color:#ae81ff\">8192</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># 2) Reduce time_wait_interval so less connections are in TIME_WAIT</span>\n</span></span><span style=\"display:flex;\"><span>ndd -set /dev/tcp tcp_time_wait_interval <span style=\"color:#ae81ff\">5000</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># 3) Change strong_iss to 0 so that sequence numbers increment by only a fixed amount</span>\n</span></span><span style=\"display:flex;\"><span>ndd -set /dev/tcp tcp_strong_iss <span style=\"color:#ae81ff\">0</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># 4) Change iss_incr to 25000 as per the linkedm post to reduce wrap-around</span>\n</span></span><span style=\"display:flex;\"><span>ndd -set /dev/tcp tcp_iss_incr <span style=\"color:#ae81ff\">25000</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># 5) Reduce rexmit_interval_initial, min, and max so that collisions take less time</span>\n</span></span><span style=\"display:flex;\"><span>ndd -set /dev/tcp tcp_rexmit_interval_initial <span style=\"color:#ae81ff\">150</span>\n</span></span><span style=\"display:flex;\"><span>ndd -set /dev/tcp tcp_rexmit_interval_min <span style=\"color:#ae81ff\">25</span>\n</span></span><span style=\"display:flex;\"><span>ndd -set /dev/tcp tcp_rexmit_interval_max <span style=\"color:#ae81ff\">15000</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># 6) Reduce ip_abort_interval to 4x the rexmit_interval_initial, as recommended</span>\n</span></span><span style=\"display:flex;\"><span>ndd -set /dev/tcp tcp_ip_abort_interval <span style=\"color:#ae81ff\">60000</span>\n</span></span></code></pre></div>",
				"content_text": "\nThe other day, I helped debug an issue on some production Joyent Cloud servers (which use SmartOS, based on Illumos, the open-source successor to Solaris). The solution turned out to be so non-obvious, and the cause pretty interesting, so I thought it was worth writing up.\n\nWhile checking through the production request logs, I noticed a specific (and very strange) anomaly. During the day, as traffic increased towards the usual daily peak, the amount of time that requests spent waiting to be processed increased dramatically.\n\nThe average time spent in queueing per request went up from ~2ms all the way up to ~12ms... with only a 3x increase in traffic at peak times. A quick check of the servers showed they had plenty of CPU available, and many web server workers that weren't busy. So what could be causing requests to spend time waiting around, instead of getting responded to right away?\n\nThe problem was tricky to even notice, since it was bad enough to depress the request averages dashboard, but rare enough that it took careful and specific searching to see the slow requests. Making requests against the load balancer machines revealed that roughly 10% of all traffic experienced a delay before it was answered. No matter how long the request took normally, about 9% of traffic would be delayed by 1.1 seconds, and about 1% of traffic would be delayed by 3.4 seconds.\n\nWe eventually narrowed it down to the app servers themselves, writing a script to fire off around 10,000 requests as quickly as they could be served by a single set of unicorn server processes. Even when the requests were coming one at a time, and even when they were coming from the same machine as the unicorns were running on, the problem would appear again and again.\n\nWe tried the usual UNIX voodoo of checking ulimits, file descriptors, open sockets, and the like, but weren't able to find anything that even looked like it might be the problem. According to the internet, a common culprit for long delays before connections are accepted is having too many sockets in the `TIME_WAIT` state.\n\nThe `TIME_WAIT` state (or TIME-WAIT, if you ask [RFC 793][ietf]), is the final state for one side of every TCP/IP connection. After a connection, sockets are held in `TIME_WAIT` for up to 4 minutes. Keeping one side of the TCP connection around after it has been closed sounds a little weird at first, but there are reasons to do it. I'll explain briefly, and for more details I would suggest [this explanation of TCP client and server states][timewait], as well as the RFC.\n\nOne reason for `TIME_WAIT` is to be able to catch delayed packets that were sent while the connection was open, but arrive after it is closed. Waiting until the network timeout for any packets still traveling means that the next connection to use the socket won't get confused by stray late packets. This is incredibly unlikely under most circumstances.\n\nThe main reason is to close TCP connections fully and cleanly. The final ACK, acknowledging that the connection is closed, needs to be accepted as a valid packet, even though the connection has already been closed by both sides. Without `TIME_WAIT`, that final ACK would be rejected, since it would be part of a connection that no longer exists. That's the upside. The downside is that because sockets in `TIME_WAIT` count as used, opening many connections to a single server can block it from opening new connections until existing `TIME_WAIT`s time out.\n\nSince we knew that was a likely possible source for the problem, we checked for that problem first: `netstat -sP tcp` prints statistics for all TCP connections on the entire system. One of those statistics is `tcpTimKeepaliveDrop`, a counter of the number of times a connection could not be accepted due to too many other connections being alive. We never saw that number increment, and so assumed that the (high) number of connections in the `TIME_WAIT` state just wasn't the problem. Big mistake.\n\nOver the course of an entire day of researching TCP tuning on Solaris, on Illumos, and on SmartOS, we tried adjusting nearly every single TCP stack setting. Some suggestions sounded like they might be related to our problem, others sounded completely unrelated. Eventually, we tried changing every single one we could find, and none of them fixed the problem.\n\nWhile researching issues, we ran `netstat -sP tcp` over and over, looking for some indicator of what the problem was. Near the end of the day, we noticed one specific counter that incremented every time we saw a delayed response: `tcpRetransSegs`. That counter is incremented anytime a TCP segment is sent more than once.\n\nSo, we finally learned something concrete: a TCP segment has to be sent again, even though the client and server are on the same computer. That immediately became the new problem, though, because that shouldn't ever happen! Retransmission is supposed to happen if data gets lost and isn't delivered. It's not supposed to happen when a machine is trying to talk to itself.\n\nArmed with new search terms, we were able to find [a mailing list post from 2011][illumos] that described the exact same 1.1 second and 3.4 second delays that we were witnessing. The post was on the Illumos-Developer mailing list, and described the background for a patch to Illumos supplied by Joyent. The entire post is interesting to read, but I'll summarize what they found.\n\nFirst, a little necessary background on TCP. It's not terribly obvious, but every network connection using IP (and therefore every TCP/IP connection) happens between a source address and port and a destination address and port. For example, for your computer to display `google.com`, it connects to that address on port 80 for HTTP or port 443 for HTTPS.\n\nBut that's just the destination. The source is your computer's IP address, on a port that was randomly chosen when you opened the connection. Chances are good that the port number is in the range 49152–65535, which are are the ports officially set aside for this kind of temporary use by IANA (the Internet Assigned Numbers Authority).\n\nWhen a single host connects to another single host many, many times (as in the case of our load balancer talking to our app server, for example), it connects from one of those temporary ports to the specific port that the app server is listening on. Thanks to `TIME_WAIT`, the app server keeps listening for packets coming from each temporary port for up to the default `tcp_time_wait_interval` of 60 seconds.\n\nSince the source's temporary port is chosen randomly, with many requests per second it is highly likely that a single temporary port will be used for another connection with the same app server on the same listening port. At that point, the server is supposed to realize a new connection is being opened, and the previous connection is no longer being used.\n\nNormally, every TCP packet includes a _sequence number_ that starts at a random-ish value and then goes up by one each time another packet is sent. When a connection is in `TIME_WAIT`, the server will check this number and notice that it is so much higher than the old connection's sequence number that this must be a new request.\n\nSeems straightforward, but TCP sequence numbers are a single 32-bit value. The default settings on Solaris mean that each new connection starts with a new sequence number that is _much_ higher than the last connection. So much higher, in fact, that after several connections the 32-bit number wraps around and starts again from zero.\n\nNow the surprising part: when a new connection is created with the same source address, source port, destination address, and destination port, and the TCP sequence number has also wrapped around, the request for a new connection is interpreted as if it were a delayed request to create the connection that is now in `TIME_WAIT`.\n\nUnder those (very specific) circumstances, the server tries to reset the connection instead of opening a new one. Since the connection it asked for was never opened, the client waits a set amount of time and then tries again. The second time, the connection request succeeds. The amount of time before the client retries is set by the parameters `tcp_rexmit_interval_initial` and `tcp_rexmit_interval_min` added together.\n\nWhile this set of circumstances sounds extremely complicated and unlikely, we were seeing this exact problem happen in production very frequently. As much as 1% of our traffic (many thousands of requests) was being delayed by 3.4 seconds, even if the actual time needed to deliver the response was only 10 milliseconds.\n\nBased on the Illumos blog post, some [old Solaris tuning advice](http://www.sean.de/Solaris/soltune.html) and some [new Solaris tuning advice](http://www.princeton.edu/~unix/Solaris/troubleshoot/), we were able to find a new TCP configuration that reduces the problem so much we don't notice it anymore. Note, however, that this problem is not \"fixable\", since all we can do is change settings to make it less and less likely.\n\n**Using these settings is a terrible idea on servers exposed to the internet.** That said, running these commands to change TCP tunable settings fixed this issue for us. You'll need root permissions to run these commands on a Solaris/Illumos host.\n\n```bash\n# 1) Reduce smallest_anon_port for more ephemeral ports, which reduces collisions\nndd -set /dev/tcp tcp_smallest_anon_port 8192\n# 2) Reduce time_wait_interval so less connections are in TIME_WAIT\nndd -set /dev/tcp tcp_time_wait_interval 5000\n# 3) Change strong_iss to 0 so that sequence numbers increment by only a fixed amount\nndd -set /dev/tcp tcp_strong_iss 0\n# 4) Change iss_incr to 25000 as per the linkedm post to reduce wrap-around\nndd -set /dev/tcp tcp_iss_incr 25000\n# 5) Reduce rexmit_interval_initial, min, and max so that collisions take less time\nndd -set /dev/tcp tcp_rexmit_interval_initial 150\nndd -set /dev/tcp tcp_rexmit_interval_min 25\nndd -set /dev/tcp tcp_rexmit_interval_max 15000\n# 6) Reduce ip_abort_interval to 4x the rexmit_interval_initial, as recommended\nndd -set /dev/tcp tcp_ip_abort_interval 60000\n```\n\n[bug]: https://www.illumos.org/issues/5011\n[ietf]: http://tools.ietf.org/html/rfc793\n[illumos]: http://lists.illumos.org/pipermail/developer/2011-April/001958.html\n[timewait]: http://www.serverframework.com/asynchronousevents/2011/01/time-wait-and-its-design-implications-for-protocols-and-scalable-servers.html\n",
				"date_published": "2014-08-01T00:00:00-08:00",
				"url": "https://andre.arko.net/2014/08/01/tcp-delays-and-retransmissions-on/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2014/07/22/rails-from-the-ground-up/",
				"title": "Rails from the ground up: Status Codes \u0026 Headers",
				"content_html": "<p>Now that we know what the tiniest possible response looks like, we can talk about what a valid and correct HTTP response looks like. That means explaining <em>status codes</em> and <em>headers</em>.</p>\n<p>To send a fully correct HTTP response, we need to first tell the client our HTTP version and the status code of our response. Status codes tell the client what kind of response is going to be sent.</p>\n<p>For the smallest possible response, we only need one status code: <code>200 OK</code>. That code means everything is okay on the server, and a response is about to be sent.</p>\n<p>Other common status codes include <code>302 Found</code>, <code>404 Not Found</code>, and various other messages that are useful to send to a browser or other HTTP client. There are <a href=\"http://httpstatus.es\">a lot of status codes</a>, so feel free to read more about all of them if you&rsquo;re interested.</p>\n<p>So, the first line of a valid response simply declares the version of HTTP that the server supports and the status code:</p>\n<pre tabindex=\"0\"><code>HTTP/1.0 200 OK\n</code></pre><p>The next line (and subsequent lines, until there is a completely blank line) are all headers. Similar to the client headers that we saw in the previous post, response headers consist of a name, a colon and space, and then a value. They contain information about the response that is being sent, but are not considered part of the response itself.</p>\n<p>While HTTP version 1.0 technically doesn&rsquo;t require any headers, browsers usually don&rsquo;t work unless you supply at least the <code>Content-Length</code> header. It should be set to the number of bytes in the response body.</p>\n<p>After all that, we know how to create the smallest possible valid HTTP response.</p>\n<pre tabindex=\"0\"><code>HTTP/1.0 200 OK\nContent-Length: 2\n\nhi\n</code></pre><p>Armed with this knowledge, it is now easy to write a Ruby server we can navigate to in our browser:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#39;socket&#39;</span>\n</span></span><span style=\"display:flex;\"><span>server <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">TCPServer</span><span style=\"color:#f92672\">.</span>new <span style=\"color:#ae81ff\">3000</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">loop</span> <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>  socket <span style=\"color:#f92672\">=</span> server<span style=\"color:#f92672\">.</span>accept\n</span></span><span style=\"display:flex;\"><span>  socket<span style=\"color:#f92672\">.</span>write <span style=\"color:#e6db74\">&#34;HTTP/1.0 200 OK</span><span style=\"color:#ae81ff\">\\r\\n</span><span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>  socket<span style=\"color:#f92672\">.</span>write <span style=\"color:#e6db74\">&#34;Content-Length: 2</span><span style=\"color:#ae81ff\">\\r\\n</span><span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>  socket<span style=\"color:#f92672\">.</span>write <span style=\"color:#e6db74\">&#34;</span><span style=\"color:#ae81ff\">\\r\\n</span><span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>  socket<span style=\"color:#f92672\">.</span>write <span style=\"color:#e6db74\">&#34;hi</span><span style=\"color:#ae81ff\">\\n</span><span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>  socket<span style=\"color:#f92672\">.</span>close\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span>\n</span></span></code></pre></div><p>You can run this server on OS X by copying the code to the clipboard and then running <code>pbpaste | ruby</code>. That will start a server at <a href=\"http://localhost:3000\">http://localhost:3000</a> that you can then navigate to in your browser. You should see our message, &ldquo;hi&rdquo;!</p>\n<p>Other than <code>Content-Length</code>, common repsonse headers include:</p>\n<ul>\n<li><code>Content-Type</code>, for distinguishing between <code>text/plain</code>, <code>text/html</code>, and <code>application/json</code></li>\n<li><code>Date</code>, a timestamp telling when the server replied</li>\n<li><code>Connection</code>, to let the client know whether to <code>close</code> or <code>keep-alive</code> this connection</li>\n<li><code>Server</code>, the name of the server software that sent this reply</li>\n<li><code>Set-Cookie</code>, data for the client to send back to the server so the server knows who you are</li>\n</ul>\n<p>All of these headers (and others besides) allow HTTP clients and servers to coordinate requests and responses in more and more complicated ways.</p>\n<p>Use <code>curl</code> to experiement and check out response headers for yourself. Run <code>curl -I</code> on your favorite websites, and see what response headers they send. Discover the secrets of the internet.</p>\n<p>Now that we have code for a working HTTP server, we&rsquo;ll talk about how to serve HTML next.</p>\n",
				"content_text": "Now that we know what the tiniest possible response looks like, we can talk about what a valid and correct HTTP response looks like. That means explaining *status codes* and *headers*.\n\nTo send a fully correct HTTP response, we need to first tell the client our HTTP version and the status code of our response. Status codes tell the client what kind of response is going to be sent.\n\nFor the smallest possible response, we only need one status code: `200 OK`. That code means everything is okay on the server, and a response is about to be sent.\n\nOther common status codes include `302 Found`, `404 Not Found`, and various other messages that are useful to send to a browser or other HTTP client. There are [a lot of status codes](http://httpstatus.es), so feel free to read more about all of them if you're interested.\n\nSo, the first line of a valid response simply declares the version of HTTP that the server supports and the status code:\n\n```\nHTTP/1.0 200 OK\n```\n\nThe next line (and subsequent lines, until there is a completely blank line) are all headers. Similar to the client headers that we saw in the previous post, response headers consist of a name, a colon and space, and then a value. They contain information about the response that is being sent, but are not considered part of the response itself.\n\nWhile HTTP version 1.0 technically doesn't require any headers, browsers usually don't work unless you supply at least the `Content-Length` header. It should be set to the number of bytes in the response body.\n\nAfter all that, we know how to create the smallest possible valid HTTP response.\n\n```\nHTTP/1.0 200 OK\nContent-Length: 2\n\nhi\n```\n\nArmed with this knowledge, it is now easy to write a Ruby server we can navigate to in our browser:\n\n```ruby\nrequire 'socket'\nserver = TCPServer.new 3000\nloop do\n  socket = server.accept\n  socket.write \"HTTP/1.0 200 OK\\r\\n\"\n  socket.write \"Content-Length: 2\\r\\n\"\n  socket.write \"\\r\\n\"\n  socket.write \"hi\\n\"\n  socket.close\nend\n```\n\nYou can run this server on OS X by copying the code to the clipboard and then running `pbpaste | ruby`. That will start a server at [http://localhost:3000](http://localhost:3000) that you can then navigate to in your browser. You should see our message, \"hi\"!\n\nOther than `Content-Length`, common repsonse headers include:\n\n  - `Content-Type`, for distinguishing between `text/plain`, `text/html`, and `application/json`\n  - `Date`, a timestamp telling when the server replied\n  - `Connection`, to let the client know whether to `close` or `keep-alive` this connection\n  - `Server`, the name of the server software that sent this reply\n  - `Set-Cookie`, data for the client to send back to the server so the server knows who you are\n\nAll of these headers (and others besides) allow HTTP clients and servers to coordinate requests and responses in more and more complicated ways.\n\nUse `curl` to experiement and check out response headers for yourself. Run `curl -I` on your favorite websites, and see what response headers they send. Discover the secrets of the internet.\n\nNow that we have code for a working HTTP server, we'll talk about how to serve HTML next.\n",
				"date_published": "2014-07-22T00:00:00-08:00",
				"url": "https://andre.arko.net/2014/07/22/rails-from-the-ground-up/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2014/07/16/bundler-api-outages/",
				"title": "Bundler API Outages",
				"content_html": "<p><small>This was also <a href=\"http://bundler.io/blog/2014/07/16/bundler-api-outages.html\">crossposted on the Bundler blog</a>.</small></p>\n<p>In the last couple of days, the Bundler API has seen some downtime: 53 minutes on July 15 and 3 hours  and 16 minutes on July 16. Here&rsquo;s what happened, and how we&rsquo;re working to keep it from happening again in the future.</p>\n<p>The Bundler API provides information about specific gems, allowing <code>bundle install</code> to run more quickly. Without the API installing multiple gems is much slower, because information about every single gem has to be downloaded, instead of just information about the gems that are needed.</p>\n<p>Starting with Rubygems version 2.2.0 (which is included with Ruby 2.1), the <code>gem install</code> also uses the API to download gem metadata more quickly. As users upgraded Rubygems (or were upgraded by upgrading their Ruby version), the load on the API slowly started to increase. We are usually able to successfully handle the increased load, but only up to a point. When enough large, slow requests are made simultaneously (some of you have really big Gemfiles!), unanswered API requests started to pile up while the slow requests ran. At that point, the problem snowballs pretty naturally, and the API starts responding very sluggishly, if it even manages to respond at all inside the hard 30 second timeout imposed by Heroku.</p>\n<p>This snowball effect happens on occasion, and the solution is often as simple as dropping all the requests in the queue so that incoming requests can be served immediately. Yesterday, that wasn&rsquo;t enough. There were too many requests to handle even after dropping the backed up queue of waiting requests. Adding application servers is extremely straightforward, but there was a problem: the database server had reached its connection limit. If we added any more application servers, we would need additional database connections, and our current server was at the limits of its hardware.</p>\n<p>To increase the number of allowed database connections, we had to upgrade to a bigger database. Fortunately, Heroku&rsquo;s Postgres tools make it simple to create a bigger database that follows the existing database, and then turn off replication and switch to using that bigger database as the new main database. Yesterday, I took down the API, created a new follower database that was able to support more connections, and then failed over to use it as the primary database. This resulted in roughly 53 minutes of downtime on July 15.</p>\n<p>Unfortunately, replication to the new, bigger database was only partially complete when I manually failed over to that database. The automated process that synchronizes the main Rubygems.org database to the API database wasn&rsquo;t able to fill in all of the missing data due to the way replication had copied only part of the information about some gems.</p>\n<p>Today, while investigating reports of failures during <code>bundle install</code>, we discovered the missing database data, and took down the API entirely to force everyone to use the accurate (but slower) full gem index. To repair the missing data, we restored a database backup from yesterday, and then synchronized the API database with the main Rubygems.org database to catch up on new gems. This caused roughly 3 hours and 16 minutes of downtime on July 16.</p>\n<p>Since the API was already down, we took the opportunity to improve the API database infrastructure. With the bigger database, we were able to remove our replica setup and instead just use a single database. We were also able to upgrade from Postgres 9.2.4 to 9.3.4, with performance enhancements and automatic failover. In the future, primary database failures should now be handled automatically.</p>\n<p>At this point, we have successfully upgraded to the latest version of Postgres, dramatically increased the hardware the database runs on, and increased the number of application servers from 15 to 20. We believe this fully armed and operational database will be faster and more reliable. Sorry for the downtime. Happy Bundling!</p>\n",
				"content_text": "<small>This was also <a href=\"http://bundler.io/blog/2014/07/16/bundler-api-outages.html\">crossposted on the Bundler blog</a>.</small>\n\nIn the last couple of days, the Bundler API has seen some downtime: 53 minutes on July 15 and 3 hours  and 16 minutes on July 16. Here's what happened, and how we're working to keep it from happening again in the future.\n\nThe Bundler API provides information about specific gems, allowing `bundle install` to run more quickly. Without the API installing multiple gems is much slower, because information about every single gem has to be downloaded, instead of just information about the gems that are needed.\n\nStarting with Rubygems version 2.2.0 (which is included with Ruby 2.1), the `gem install` also uses the API to download gem metadata more quickly. As users upgraded Rubygems (or were upgraded by upgrading their Ruby version), the load on the API slowly started to increase. We are usually able to successfully handle the increased load, but only up to a point. When enough large, slow requests are made simultaneously (some of you have really big Gemfiles!), unanswered API requests started to pile up while the slow requests ran. At that point, the problem snowballs pretty naturally, and the API starts responding very sluggishly, if it even manages to respond at all inside the hard 30 second timeout imposed by Heroku.\n\nThis snowball effect happens on occasion, and the solution is often as simple as dropping all the requests in the queue so that incoming requests can be served immediately. Yesterday, that wasn't enough. There were too many requests to handle even after dropping the backed up queue of waiting requests. Adding application servers is extremely straightforward, but there was a problem: the database server had reached its connection limit. If we added any more application servers, we would need additional database connections, and our current server was at the limits of its hardware.\n\nTo increase the number of allowed database connections, we had to upgrade to a bigger database. Fortunately, Heroku's Postgres tools make it simple to create a bigger database that follows the existing database, and then turn off replication and switch to using that bigger database as the new main database. Yesterday, I took down the API, created a new follower database that was able to support more connections, and then failed over to use it as the primary database. This resulted in roughly 53 minutes of downtime on July 15.\n\nUnfortunately, replication to the new, bigger database was only partially complete when I manually failed over to that database. The automated process that synchronizes the main Rubygems.org database to the API database wasn't able to fill in all of the missing data due to the way replication had copied only part of the information about some gems.\n\nToday, while investigating reports of failures during `bundle install`, we discovered the missing database data, and took down the API entirely to force everyone to use the accurate (but slower) full gem index. To repair the missing data, we restored a database backup from yesterday, and then synchronized the API database with the main Rubygems.org database to catch up on new gems. This caused roughly 3 hours and 16 minutes of downtime on July 16.\n\nSince the API was already down, we took the opportunity to improve the API database infrastructure. With the bigger database, we were able to remove our replica setup and instead just use a single database. We were also able to upgrade from Postgres 9.2.4 to 9.3.4, with performance enhancements and automatic failover. In the future, primary database failures should now be handled automatically.\n\nAt this point, we have successfully upgraded to the latest version of Postgres, dramatically increased the hardware the database runs on, and increased the number of application servers from 15 to 20. We believe this fully armed and operational database will be faster and more reliable. Sorry for the downtime. Happy Bundling!\n",
				"date_published": "2014-07-16T00:00:00-08:00",
				"url": "https://andre.arko.net/2014/07/16/bundler-api-outages/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2014/07/14/rails-from-the-ground-up/",
				"title": "Rails from the ground up: HTTP",
				"content_html": "<p>Welcome to Rails from the ground up! In this series of posts, I&rsquo;m going to talk about the many pieces that underlie every Ruby on Rails application. By the end, anyone who has read every post will hopefully understand both how each layer of the Rails stack works, why it&rsquo;s there, and how to implement it themselves.</p>\n<p>The entire Ruby on Rails web application framework is designed around accepting requests and generating responses. Those requests follow (or are at least supposed to follow) the rules of the Hyper-Text Transport Protocol. HTTP is used by every web browser and web server, as well as the vast majority of applications on computers, phones, and tablets that communicate with a server.</p>\n<h3 id=\"requests\">Requests</h3>\n<p>Fortunately, it turns out that HTTP at its simplest is just a few lines of text. Here is a valid HTTP 1.1 request:</p>\n<pre><code>GET / HTTP/1.1\n    \n</code></pre>\n<p><code>GET</code> is an <em>HTTP verb</em>, and tells the server what the client is trying to do. The single forward slash by itself (<code>/</code>) is the <em>path</em>, identifying the resource that the client is requesting from the server. The <code>HTTP/1.1</code> tells the server that this client knows how to use the HTTP protocol version 1.1. Version 1.1 added some handy things that we&rsquo;ll talk about later, but don&rsquo;t matter for now. The last line is required to be blank as part of the protocol, and indicates that the client is done sending the request.</p>\n<p>It&rsquo;s possible to see the plain text of a request using the netcat command-line tool and any web browser. First, open a terminal window and start netcat listening by running <code>nc -lp 3000</code>. Then, open a web browser and browse to <a href=\"http://localhost:3000\">http://localhost:3000</a>. Here&rsquo;s an example using Safari 7.</p>\n<pre tabindex=\"0\"><code>$ nc -lp 3000\nGET / HTTP/1.1\nHost: localhost:3000\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: en-us\nConnection: keep-alive\nAccept-Encoding: gzip, deflate\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.77.4 (KHTML, like Gecko) Version/7.0.5 Safari/537.77.4\n</code></pre><p>We&rsquo;ll talk more about the extra lines following the first line (called <em>request headers</em>) in the next post in this series. For now, this serves to show that even the newest and fanciest web browsers use easily readable plain-text HTTP.</p>\n<h3 id=\"responses\">Responses</h3>\n<p>In HTTP, responses are sent back over the same connection. The first lines are <em>response headers</em>, and after two newlines comes the <em>body</em>. The body contains the stuff that you actually see in your browser.</p>\n<p>Using netcat, it&rsquo;s possible to send a response to your web browser just by typing. The absolute minimum that you can send is a line of text, followed by ⌃C to quit netcat and close the connection.</p>\n<pre tabindex=\"0\"><code>$ nc -lp 3000\nGET / HTTP/1.1\nHost: localhost:3000\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: en-us\nConnection: keep-alive\nAccept-Encoding: gzip, deflate\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.77.4 (KHTML, like Gecko) Version/7.0.5 Safari/537.77.4\n\nhi\n^C\n</code></pre><p>This is a complete HTTP request and response! (Although the response is technically not valid HTTP.) Nonetheless, in Safari 7, it produces a screen that looks like this:</p>\n<p><img src=\"https://cl.ly/image/0K0X3y0Y0w2r/Image%202014-07-14%20at%201.10.23%20AM.png\" alt=\"Safari window containing &ldquo;hi&rdquo;\"></p>\n<p>Now that we&rsquo;ve got the basics of HTTP out of the way, everything from here on out will be a piece of cake. Probably.</p>\n<p>Next up: HTTP headers.</p>\n",
				"content_text": "Welcome to Rails from the ground up! In this series of posts, I'm going to talk about the many pieces that underlie every Ruby on Rails application. By the end, anyone who has read every post will hopefully understand both how each layer of the Rails stack works, why it's there, and how to implement it themselves.\n\nThe entire Ruby on Rails web application framework is designed around accepting requests and generating responses. Those requests follow (or are at least supposed to follow) the rules of the Hyper-Text Transport Protocol. HTTP is used by every web browser and web server, as well as the vast majority of applications on computers, phones, and tablets that communicate with a server.\n\n### Requests\n\nFortunately, it turns out that HTTP at its simplest is just a few lines of text. Here is a valid HTTP 1.1 request:\n\n<pre><code>GET / HTTP/1.1\n    \n</code></pre>\n\n`GET` is an _HTTP verb_, and tells the server what the client is trying to do. The single forward slash by itself (`/`) is the _path_, identifying the resource that the client is requesting from the server. The `HTTP/1.1` tells the server that this client knows how to use the HTTP protocol version 1.1. Version 1.1 added some handy things that we'll talk about later, but don't matter for now. The last line is required to be blank as part of the protocol, and indicates that the client is done sending the request.\n\nIt's possible to see the plain text of a request using the netcat command-line tool and any web browser. First, open a terminal window and start netcat listening by running `nc -lp 3000`. Then, open a web browser and browse to [http://localhost:3000](http://localhost:3000). Here's an example using Safari 7.\n\n```\n$ nc -lp 3000\nGET / HTTP/1.1\nHost: localhost:3000\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: en-us\nConnection: keep-alive\nAccept-Encoding: gzip, deflate\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.77.4 (KHTML, like Gecko) Version/7.0.5 Safari/537.77.4\n\n```\n\nWe'll talk more about the extra lines following the first line (called _request headers_) in the next post in this series. For now, this serves to show that even the newest and fanciest web browsers use easily readable plain-text HTTP.\n\n### Responses\n\nIn HTTP, responses are sent back over the same connection. The first lines are _response headers_, and after two newlines comes the _body_. The body contains the stuff that you actually see in your browser.\n\nUsing netcat, it's possible to send a response to your web browser just by typing. The absolute minimum that you can send is a line of text, followed by ⌃C to quit netcat and close the connection.\n\n```\n$ nc -lp 3000\nGET / HTTP/1.1\nHost: localhost:3000\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: en-us\nConnection: keep-alive\nAccept-Encoding: gzip, deflate\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.77.4 (KHTML, like Gecko) Version/7.0.5 Safari/537.77.4\n\nhi\n^C\n```\n\nThis is a complete HTTP request and response! (Although the response is technically not valid HTTP.) Nonetheless, in Safari 7, it produces a screen that looks like this:\n\n![Safari window containing \"hi\"](https://cl.ly/image/0K0X3y0Y0w2r/Image%202014-07-14%20at%201.10.23%20AM.png)\n\nNow that we've got the basics of HTTP out of the way, everything from here on out will be a piece of cake. Probably.\n\nNext up: HTTP headers.\n",
				"date_published": "2014-07-14T00:00:00-08:00",
				"url": "https://andre.arko.net/2014/07/14/rails-from-the-ground-up/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2014/06/27/rails-in-seconds/",
				"title": "Rails in 0.5 seconds",
				"content_html": "<p>You too can boot a Rails app in as little as 500ms! Here&rsquo;s how, using lazy require statements and not loading Bundler or Rubygems at runtime.</p>\n<p>🏃💨💫💞</p>\n<p>A couple of weeks ago, I had a great conversation with a Ruby developer in Paris, and we talked about the things we liked and didn&rsquo;t like about Ruby and Rails when compared to other options. He had just tried Node.js and Python for the first time, and was very excited about how fast everything was in development. It surprised me that he thought all Ruby projects were slow, and as I asked him more about what he meant, it became clear that he had never used Ruby without a Rails app, a huge Gemfile, and a 5-10 second boot time before being able to do anything at all.</p>\n<p>To me, the saddest part of this is that Ruby is really fast! The Ruby interpreter only takes about 35ms to start up. It&rsquo;s entirely possible to have scripts or even test suites that run so quickly that they seem instant. Knowing that Ruby itself was already quite fast, I decided to see just how quickly Rails could load and provide a running application.</p>\n<p>Getting a Rails 4 app booting without Bundler or Rubygems requires a few changes to the way things load, and a shim that pretends to be Rubygems so that parts of Rails that expect Rubygems to be there won&rsquo;t throw exceptions. Let&rsquo;s start with a new Rails 4.1.1 application:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span>$ rails new fast-rails --skip-bundle --skip-spring\n</span></span><span style=\"display:flex;\"><span>      create\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#f92672\">[</span>...<span style=\"color:#f92672\">]</span>\n</span></span><span style=\"display:flex;\"><span>$ cd fast-rails\n</span></span></code></pre></div><p>Let&rsquo;s get a baseline, so we know how long it takes to boot our Rails environment, and we know how much of a difference our changes make:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span>$ time bin/rails runner <span style=\"color:#e6db74\">&#39;0&#39;</span>\n</span></span><span style=\"display:flex;\"><span>real\t0m1.066s\n</span></span></code></pre></div><p>Now that we have a baseline, let&rsquo;s set up our hard-coded replacement for all the work that Bundler and Rubygems do at runtime. Bundler includes a little-known feature called standalone mode. This mode creates a Ruby file that simply adds each gem from your bundle to the <code>$LOAD_PATH</code>. Let&rsquo;s start there.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span>$ bundle install --standalone --path .bundle --jobs <span style=\"color:#ae81ff\">0</span>\n</span></span><span style=\"display:flex;\"><span>Fetching gem metadata from https://rubygems.org/.........\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#f92672\">[</span>...<span style=\"color:#f92672\">]</span>\n</span></span><span style=\"display:flex;\"><span>Your bundle is complete!\n</span></span><span style=\"display:flex;\"><span>It was installed into ./.bundle\n</span></span></code></pre></div><p>If you&rsquo;d like to see the contents of the standalone file, you can look at <code>.bundle/bundler/setup.rb</code>. That&rsquo;s the file that we&rsquo;re going to use to set up the app environment, instead of having Bundler do it every time the app starts up.</p>\n<p>This approach comes with a <em>big</em> disadvantage, though: Bundler doesn&rsquo;t set up the app environment every time the app starts up. That means that any change to your <code>Gemfile</code> requires running <code>bundle install --standalone --path .bundle --jobs 0</code> again after the change, to write out a new standalone file that honors those changes.</p>\n<p>This is similar to the approach the Bundler team took in Bundler version 0.9, but it caused a lot of confusion when changes to the Gemfile didn&rsquo;t take effect until after another <code>bundle install</code>. So keep that in mind if you decide to do this! I recommend creating a bootstrapping script that you can run to make sure the application is ready to go. I called mine <code>bin/setup</code>.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\">#!/bin/bash\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#75715e\"></span>gem list -i bundler &gt; /dev/null <span style=\"color:#f92672\">||</span> gem install bundler\n</span></span><span style=\"display:flex;\"><span>bundle install --standalone --path .bundle --jobs <span style=\"color:#ae81ff\">0</span>\n</span></span><span style=\"display:flex;\"><span>bin/rake db:create db:schema:load db:seed\n</span></span></code></pre></div><p>If your application has other requirements, you could potentially install them using that script as well. Just remember that you need to re-run <code>bundle install --standalone</code> every time you change your Gemfile!</p>\n<p>Now that we have a standalone bundle, we need to tell Rails how to load it instead of using Rubygems. We can do that using <code>config/boot.rb</code>.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># config/boot.rb</span>\n</span></span><span style=\"display:flex;\"><span>require_relative <span style=\"color:#e6db74\">&#34;../.bundle/bundler/setup&#34;</span>\n</span></span><span style=\"display:flex;\"><span>require_relative <span style=\"color:#e6db74\">&#34;rubygems_shim&#34;</span>\n</span></span></code></pre></div><p>Next step (you probably guessed it already) is <code>config/rubygems_shim.rb</code>. It&rsquo;s the absolute minimum set of Rubygems constants and methods that mean that Rails will function. Using this shim instead of Rubygems removes one specific thing from Rails: the version of your database adapter will not be checked by ActiveRecord. Make sure you have the latest compatible version!</p>\n<p>When I first figured out how to do this, I was using Ruby 2.1.1. About halfway through, I decided that I should upgrade to Ruby 2.1.2, and I discovered a very sad thing: the version of RDoc that ships with Ruby 2.1.2 has a file, <code>rdoc/tasks.rb</code>, that actually has the statement <code>require &quot;rubygems&quot;</code> inside it. The only way to work around it was to monkeypatch require, and block Rubygems from loading. This is a terrible idea, and honestly I don&rsquo;t recommend that anyone do it. I couldn&rsquo;t figure out how to make this work on Ruby 2.1.2 without it, though, so it&rsquo;s here in the shim.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># config/rubygems_shim.rb</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">if</span> defined?(<span style=\"color:#66d9ef\">Gem</span>)\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">STDERR</span><span style=\"color:#f92672\">.</span>puts <span style=\"color:#e6db74\">&#34;Running without Rubygems, but Rubygems is already loaded!&#34;</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">else</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">module</span> Gem\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#75715e\"># ActiveRecord requires Gem::LoadError to load</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">class</span> <span style=\"color:#a6e22e\">LoadError</span> <span style=\"color:#f92672\">&lt;</span> <span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">LoadError</span>; <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#75715e\"># BacktraceCleaner wants path and default_dir</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">self</span><span style=\"color:#f92672\">.</span><span style=\"color:#a6e22e\">path</span>; <span style=\"color:#f92672\">[]</span>; <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">self</span><span style=\"color:#f92672\">.</span><span style=\"color:#a6e22e\">default_dir</span>\n</span></span><span style=\"display:flex;\"><span>      @default_dir <span style=\"color:#f92672\">||=</span> <span style=\"color:#66d9ef\">File</span><span style=\"color:#f92672\">.</span>expand_path(<span style=\"color:#e6db74\">&#34;../../.bundle/</span><span style=\"color:#e6db74\">#{</span><span style=\"color:#66d9ef\">RUBY_ENGINE</span><span style=\"color:#e6db74\">}</span><span style=\"color:#e6db74\">/</span><span style=\"color:#e6db74\">#{</span><span style=\"color:#66d9ef\">RbConfig</span><span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">CONFIG</span><span style=\"color:#f92672\">[</span><span style=\"color:#e6db74\">&#34;ruby_version&#34;</span><span style=\"color:#f92672\">]</span><span style=\"color:#e6db74\">}</span><span style=\"color:#e6db74\">&#34;</span>, __FILE__)\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#75715e\"># irb/locale.rb calls this if defined?(Gem)</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">self</span><span style=\"color:#f92672\">.</span><span style=\"color:#a6e22e\">try_activate</span>(<span style=\"color:#f92672\">*</span>); <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">module</span> Kernel\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#75715e\"># ActiveSupport requires Kernel.gem to load</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">gem</span>(<span style=\"color:#f92672\">*</span>); <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#75715e\"># rdoc/task.rb in Ruby 2.1.2 requires rubygems itself :(</span>\n</span></span><span style=\"display:flex;\"><span>    alias_method <span style=\"color:#e6db74\">:require</span>, <span style=\"color:#e6db74\">:orig_require</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">require</span>(<span style=\"color:#f92672\">*</span>args); args<span style=\"color:#f92672\">.</span>first <span style=\"color:#f92672\">==</span> <span style=\"color:#e6db74\">&#34;rubygems&#34;</span> <span style=\"color:#f92672\">||</span> orig_require(<span style=\"color:#f92672\">*</span>args); <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span>\n</span></span></code></pre></div><p>Next, we need to stop calling the <code>Bundler.require</code> method, which is how Rails automatically requires every gem that is listed in <code>Gemfile</code>. From here on out, we&rsquo;ll have to require everything that we use ourselves, by hand. The upside to this additional work is that we can wait to load things until we need them. If we only need to load part of our application (for example, to test a specific thing), we won&rsquo;t have to load anything else. Edit the <code>config/application.rb</code> file, and delete the line <code>Bundler.require(*Rails.groups)</code>.</p>\n<p>Last, we need to stop loading Rubygems whenever we run Ruby. The easiest way to do that, at least if you&rsquo;re on OS X, is by editing <code>bin/rails</code> and <code>bin/rake</code>. Change the first line of each file from <code>#!/usr/bin/env ruby</code> to <code>#!/usr/bin/env ruby --disable-gems</code>.</p>\n<p>If everything went as expected, you&rsquo;re now able to boot your Rails application in less than a second:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span>$ time bin/rails runner <span style=\"color:#e6db74\">&#39;0&#39;</span>\n</span></span><span style=\"display:flex;\"><span>real    0m0.728s\n</span></span></code></pre></div><p>Cutting out Bundler and Rubygems has saved us about 300ms so far, but it&rsquo;s done something else much more important: it&rsquo;s removed <code>Bundler.require</code>. Now simply adding gems to your Gemfile won&rsquo;t increase the amount of time that your application takes to load. As long as you put your <code>require</code> statements in strategicly lazy places, you&rsquo;ll be able to keep the base loading time for your app down extremely low. Being able to start a console, server, or Rake task in less than a second is pretty amazing.</p>\n<p>If you plan to write tests for your Rails app (and you do, right?), I highly recommend using the brand-new RSpec 3.0. The rspec-rails gem creates two helpers, <code>spec_helper.rb</code> and <code>rails_helper.rb</code>. RSpec automatically requires <code>spec_helper</code>, but lets you only require <code>rails_helper</code> inside tests for your models or controllers. That way, you can run Rails tests with less than a second of waiting for Rails to load, but you can run tests on regular Ruby classes with only 50ms of waiting for RSpec to load! After a week like this, I want all my projects to be this fast and responsive while I work on them.</p>\n<p>In order to make the <code>rspec</code> command work without loading Bundler or Rubygems, you&rsquo;ll need to create a binstub that knows exactly where the rspec gem&rsquo;s command is located. If you don&rsquo;t have a binstub for a command, you can <code>bundle exec gem_command</code> exactly like you normally would, and everything will still work. It&rsquo;ll just be a little bit slower. Here&rsquo;s a <code>bin/rspec</code> file that will load RSpec without Bundler or Rubygems.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\">#!/usr/bin/env ruby --disable-gems</span>\n</span></span><span style=\"display:flex;\"><span>require_relative <span style=\"color:#e6db74\">&#39;../config/boot&#39;</span>\n</span></span><span style=\"display:flex;\"><span>load <span style=\"color:#66d9ef\">Dir</span><span style=\"color:#f92672\">[</span><span style=\"color:#66d9ef\">File</span><span style=\"color:#f92672\">.</span>join(<span style=\"color:#66d9ef\">Gem</span><span style=\"color:#f92672\">.</span>default_dir, <span style=\"color:#e6db74\">&#34;gems/rspec-core-*/exe/rspec&#34;</span>)<span style=\"color:#f92672\">].</span>first\n</span></span></code></pre></div><p>&ldquo;That&rsquo;s cool, but hang on!&rdquo;, I can hear you saying. &ldquo;Where is the 500ms Rails application I was promised?&rdquo;. To get that, we need to do a little bit of creative surgery on the application, cutting out more things that are expensive to load. We&rsquo;re going to remove two lines from <code>config/application.rb</code>:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#34;action_mailer/railtie&#34;</span>\n</span></span><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#34;sprockets/railtie&#34;</span>\n</span></span></code></pre></div><p>Building an API service in Rails that returns JSON means that I don&rsquo;t need ActionMailer or Sprockets. If you do, you&rsquo;ll either need to figure out how to load those frameworks lazily, when you need to send mail or generate assets, or just bite the bullet and load your Rails app in 700ms instead of 500. At last, loading Rails (including ActiveRecord, ActionController, and ActionView) takes less than 500ms:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span>$ time bin/rails runner <span style=\"color:#e6db74\">&#39;0&#39;</span>\n</span></span><span style=\"display:flex;\"><span>real    0m0.498s\n</span></span></code></pre></div><p>🎉🎊🌟</p>\n",
				"content_text": "You too can boot a Rails app in as little as 500ms! Here's how, using lazy require statements and not loading Bundler or Rubygems at runtime.\n\n🏃💨💫💞\n\nA couple of weeks ago, I had a great conversation with a Ruby developer in Paris, and we talked about the things we liked and didn't like about Ruby and Rails when compared to other options. He had just tried Node.js and Python for the first time, and was very excited about how fast everything was in development. It surprised me that he thought all Ruby projects were slow, and as I asked him more about what he meant, it became clear that he had never used Ruby without a Rails app, a huge Gemfile, and a 5-10 second boot time before being able to do anything at all.\n\nTo me, the saddest part of this is that Ruby is really fast! The Ruby interpreter only takes about 35ms to start up. It's entirely possible to have scripts or even test suites that run so quickly that they seem instant. Knowing that Ruby itself was already quite fast, I decided to see just how quickly Rails could load and provide a running application.\n\nGetting a Rails 4 app booting without Bundler or Rubygems requires a few changes to the way things load, and a shim that pretends to be Rubygems so that parts of Rails that expect Rubygems to be there won't throw exceptions. Let's start with a new Rails 4.1.1 application:\n\n```bash\n$ rails new fast-rails --skip-bundle --skip-spring\n      create\n[...]\n$ cd fast-rails\n```\n\nLet's get a baseline, so we know how long it takes to boot our Rails environment, and we know how much of a difference our changes make:\n\n```bash\n$ time bin/rails runner '0'\nreal\t0m1.066s\n```\n\nNow that we have a baseline, let's set up our hard-coded replacement for all the work that Bundler and Rubygems do at runtime. Bundler includes a little-known feature called standalone mode. This mode creates a Ruby file that simply adds each gem from your bundle to the `$LOAD_PATH`. Let's start there.\n\n```bash\n$ bundle install --standalone --path .bundle --jobs 0\nFetching gem metadata from https://rubygems.org/.........\n[...]\nYour bundle is complete!\nIt was installed into ./.bundle\n```\n\nIf you'd like to see the contents of the standalone file, you can look at `.bundle/bundler/setup.rb`. That's the file that we're going to use to set up the app environment, instead of having Bundler do it every time the app starts up.\n\nThis approach comes with a _big_ disadvantage, though: Bundler doesn't set up the app environment every time the app starts up. That means that any change to your `Gemfile` requires running `bundle install --standalone --path .bundle --jobs 0` again after the change, to write out a new standalone file that honors those changes.\n\nThis is similar to the approach the Bundler team took in Bundler version 0.9, but it caused a lot of confusion when changes to the Gemfile didn't take effect until after another `bundle install`. So keep that in mind if you decide to do this! I recommend creating a bootstrapping script that you can run to make sure the application is ready to go. I called mine `bin/setup`.\n\n```bash\n#!/bin/bash\ngem list -i bundler > /dev/null || gem install bundler\nbundle install --standalone --path .bundle --jobs 0\nbin/rake db:create db:schema:load db:seed\n```\n\nIf your application has other requirements, you could potentially install them using that script as well. Just remember that you need to re-run `bundle install --standalone` every time you change your Gemfile!\n\nNow that we have a standalone bundle, we need to tell Rails how to load it instead of using Rubygems. We can do that using `config/boot.rb`.\n\n```ruby\n# config/boot.rb\nrequire_relative \"../.bundle/bundler/setup\"\nrequire_relative \"rubygems_shim\"\n```\n\nNext step (you probably guessed it already) is `config/rubygems_shim.rb`. It's the absolute minimum set of Rubygems constants and methods that mean that Rails will function. Using this shim instead of Rubygems removes one specific thing from Rails: the version of your database adapter will not be checked by ActiveRecord. Make sure you have the latest compatible version!\n\nWhen I first figured out how to do this, I was using Ruby 2.1.1. About halfway through, I decided that I should upgrade to Ruby 2.1.2, and I discovered a very sad thing: the version of RDoc that ships with Ruby 2.1.2 has a file, `rdoc/tasks.rb`, that actually has the statement `require \"rubygems\"` inside it. The only way to work around it was to monkeypatch require, and block Rubygems from loading. This is a terrible idea, and honestly I don't recommend that anyone do it. I couldn't figure out how to make this work on Ruby 2.1.2 without it, though, so it's here in the shim. \n\n```ruby\n# config/rubygems_shim.rb\nif defined?(Gem)\n  STDERR.puts \"Running without Rubygems, but Rubygems is already loaded!\"\nelse\n  module Gem\n    # ActiveRecord requires Gem::LoadError to load\n    class LoadError < ::LoadError; end\n\n    # BacktraceCleaner wants path and default_dir\n    def self.path; []; end\n    def self.default_dir\n      @default_dir ||= File.expand_path(\"../../.bundle/#{RUBY_ENGINE}/#{RbConfig::CONFIG[\"ruby_version\"]}\", __FILE__)\n    end\n\n    # irb/locale.rb calls this if defined?(Gem)\n    def self.try_activate(*); end\n  end\n\n  module Kernel\n    # ActiveSupport requires Kernel.gem to load\n    def gem(*); end\n    # rdoc/task.rb in Ruby 2.1.2 requires rubygems itself :(\n    alias_method :require, :orig_require\n    def require(*args); args.first == \"rubygems\" || orig_require(*args); end\n  end\nend\n```\n\nNext, we need to stop calling the `Bundler.require` method, which is how Rails automatically requires every gem that is listed in `Gemfile`. From here on out, we'll have to require everything that we use ourselves, by hand. The upside to this additional work is that we can wait to load things until we need them. If we only need to load part of our application (for example, to test a specific thing), we won't have to load anything else. Edit the `config/application.rb` file, and delete the line `Bundler.require(*Rails.groups)`.\n\nLast, we need to stop loading Rubygems whenever we run Ruby. The easiest way to do that, at least if you're on OS X, is by editing `bin/rails` and `bin/rake`. Change the first line of each file from `#!/usr/bin/env ruby` to `#!/usr/bin/env ruby --disable-gems`.\n\nIf everything went as expected, you're now able to boot your Rails application in less than a second:\n\n```bash\n$ time bin/rails runner '0'\nreal    0m0.728s\n```\n\nCutting out Bundler and Rubygems has saved us about 300ms so far, but it's done something else much more important: it's removed `Bundler.require`. Now simply adding gems to your Gemfile won't increase the amount of time that your application takes to load. As long as you put your `require` statements in strategicly lazy places, you'll be able to keep the base loading time for your app down extremely low. Being able to start a console, server, or Rake task in less than a second is pretty amazing.\n\nIf you plan to write tests for your Rails app (and you do, right?), I highly recommend using the brand-new RSpec 3.0. The rspec-rails gem creates two helpers, `spec_helper.rb` and `rails_helper.rb`. RSpec automatically requires `spec_helper`, but lets you only require `rails_helper` inside tests for your models or controllers. That way, you can run Rails tests with less than a second of waiting for Rails to load, but you can run tests on regular Ruby classes with only 50ms of waiting for RSpec to load! After a week like this, I want all my projects to be this fast and responsive while I work on them.\n\nIn order to make the `rspec` command work without loading Bundler or Rubygems, you'll need to create a binstub that knows exactly where the rspec gem's command is located. If you don't have a binstub for a command, you can `bundle exec gem_command` exactly like you normally would, and everything will still work. It'll just be a little bit slower. Here's a `bin/rspec` file that will load RSpec without Bundler or Rubygems.\n\n```ruby\n#!/usr/bin/env ruby --disable-gems\nrequire_relative '../config/boot'\nload Dir[File.join(Gem.default_dir, \"gems/rspec-core-*/exe/rspec\")].first\n```\n\n\"That's cool, but hang on!\", I can hear you saying. \"Where is the 500ms Rails application I was promised?\". To get that, we need to do a little bit of creative surgery on the application, cutting out more things that are expensive to load. We're going to remove two lines from `config/application.rb`:\n\n```ruby\nrequire \"action_mailer/railtie\"\nrequire \"sprockets/railtie\"\n```\n\nBuilding an API service in Rails that returns JSON means that I don't need ActionMailer or Sprockets. If you do, you'll either need to figure out how to load those frameworks lazily, when you need to send mail or generate assets, or just bite the bullet and load your Rails app in 700ms instead of 500. At last, loading Rails (including ActiveRecord, ActionController, and ActionView) takes less than 500ms:\n\n```bash\n$ time bin/rails runner '0'\nreal    0m0.498s\n```\n\n🎉🎊🌟\n",
				"date_published": "2014-06-27T00:00:00-08:00",
				"url": "https://andre.arko.net/2014/06/27/rails-in-seconds/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2014/05/24/the-economic-argument-for-diversity/",
				"title": "The \"Economic Argument\" For Diversity",
				"content_html": "<p>This week, I gave <a href=\"http://ashedryden.com/la-conf-tbd\">a talk with Ashe Dryden at La Conf</a> advocating for diversity in tech. In the talk, we addressed a wide range of questions and concerns that commonly come up when talking about diversity. Afterwards, I had an extended conversation with a gentleman who was quite insistent that I should spend less time talking about equality, empathy, and opportunity. Instead, he thought, I should emphasise the <a href=\"http://asr.sagepub.com/content/74/2/208.abstract\">well-documented business advantages of diversity</a>: increased quality, productivity, efficiency, and of course the resulting higher profits. This would, he said, cause businessmen like himself to want to hire diverse teams.</p>\n<p>The problem with that entire idea is that it assumes that the only action needed to produce a diverse industry is to hire diverse teams. While the business advantages of diversity are real, I don&rsquo;t mention them to encourage diverse hiring. I mention them so I can debunk the common belief that increasing diversity somehow requires &ldquo;lower standards&rdquo; or &ldquo;charitable handouts&rdquo;. Diversity means exactly the opposite. It means higher standards for less-competent men who nonetheless make more money than their female coworkers. Diversity means jobs stop going to less qualified applicants because their name sounds male, or white. Fair treatment and equal opportunities based on skill and experince would mean <em>higher</em> standards and <em>less</em> handouts.</p>\n<p>The problem with arguing for diversity based on economics is that it doesn&rsquo;t solve the underlying problem. Seeking diversity to increase profits perpetuates the existing, abusive system. If a business seeks to hire women or minorities solely to make more money, they still won&rsquo;t be motivated to treat them fairly. Those businesses will continue to dismiss minorities&rsquo; needs (that would lower profits), pay minorities poorly (lower expenses, too!), and then claim the credit for more profitable results.</p>\n<p>That hypothetical situation is the present day. We already have businesses willing to exploit minorities for higher profits. Unsurprisingly, women and minorities <a href=\"http://www.lpfi.org/sites/default/files/tilted_playing_field_lpfi_9_29_11.pdf\">notice they are being treated poorly</a>, and <a href=\"http://www.ncwit.org/sites/default/files/legacy/pdf/NCWIT_TheFacts_rev2010.pdf\">leave both their jobs and the entire industry</a>. Correcting diversity&rsquo;s downward trend requires more than simply hiring more diverse people. It means changes at every stage to counteract bias and remove mistreatment.</p>\n<p>We need ungendered expectations and role models for children, and equal access to computers and other equipment across gender and race. We need to stop teachers and classmates who &ldquo;know&rdquo; minorities are incapable and unmotivated. We need everyone to be treated fairly and not challenged, attacked, or threatened if they are different. We need opportunities to be given fairly to everyone instead of automatically going to white dudes who already get more than their share. We need hiring and pay to be based on skill and ability, not gender or race.</p>\n<p>Focusing on the so-called economic argument for diversity is just a distraction tactic. It takes attention away from the ways minorities are already being abused and claims it&rsquo;s &ldquo;helping&rdquo; to mistreat them slightly differently. I can already see what the economic argument for diversity does by looking at how terrible tech is today. Fuck that. Diversity means every person is treated not just equally but well, with empathy, consideration, and understanding. That&rsquo;s a future worth fighting to reach.</p>\n",
				"content_text": "\nThis week, I gave [a talk with Ashe Dryden at La Conf][talk] advocating for diversity in tech. In the talk, we addressed a wide range of questions and concerns that commonly come up when talking about diversity. Afterwards, I had an extended conversation with a gentleman who was quite insistent that I should spend less time talking about equality, empathy, and opportunity. Instead, he thought, I should emphasise the [well-documented business advantages of diversity][advantages]: increased quality, productivity, efficiency, and of course the resulting higher profits. This would, he said, cause businessmen like himself to want to hire diverse teams.\n\nThe problem with that entire idea is that it assumes that the only action needed to produce a diverse industry is to hire diverse teams. While the business advantages of diversity are real, I don't mention them to encourage diverse hiring. I mention them so I can debunk the common belief that increasing diversity somehow requires \"lower standards\" or \"charitable handouts\". Diversity means exactly the opposite. It means higher standards for less-competent men who nonetheless make more money than their female coworkers. Diversity means jobs stop going to less qualified applicants because their name sounds male, or white. Fair treatment and equal opportunities based on skill and experince would mean _higher_ standards and _less_ handouts.\n\nThe problem with arguing for diversity based on economics is that it doesn't solve the underlying problem. Seeking diversity to increase profits perpetuates the existing, abusive system. If a business seeks to hire women or minorities solely to make more money, they still won't be motivated to treat them fairly. Those businesses will continue to dismiss minorities' needs (that would lower profits), pay minorities poorly (lower expenses, too!), and then claim the credit for more profitable results.\n\nThat hypothetical situation is the present day. We already have businesses willing to exploit minorities for higher profits. Unsurprisingly, women and minorities [notice they are being treated poorly][treatment], and [leave both their jobs and the entire industry][leaving]. Correcting diversity's downward trend requires more than simply hiring more diverse people. It means changes at every stage to counteract bias and remove mistreatment.\n\nWe need ungendered expectations and role models for children, and equal access to computers and other equipment across gender and race. We need to stop teachers and classmates who \"know\" minorities are incapable and unmotivated. We need everyone to be treated fairly and not challenged, attacked, or threatened if they are different. We need opportunities to be given fairly to everyone instead of automatically going to white dudes who already get more than their share. We need hiring and pay to be based on skill and ability, not gender or race.\n\nFocusing on the so-called economic argument for diversity is just a distraction tactic. It takes attention away from the ways minorities are already being abused and claims it's \"helping\" to mistreat them slightly differently. I can already see what the economic argument for diversity does by looking at how terrible tech is today. Fuck that. Diversity means every person is treated not just equally but well, with empathy, consideration, and understanding. That's a future worth fighting to reach.\n\n[talk]: http://ashedryden.com/la-conf-tbd\n[advantages]: http://asr.sagepub.com/content/74/2/208.abstract\n[treatment]: http://www.lpfi.org/sites/default/files/tilted_playing_field_lpfi_9_29_11.pdf\n[leaving]: http://www.ncwit.org/sites/default/files/legacy/pdf/NCWIT_TheFacts_rev2010.pdf\n",
				"date_published": "2014-05-24T00:00:00-08:00",
				"url": "https://andre.arko.net/2014/05/24/the-economic-argument-for-diversity/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2014/03/28/the-new-rubygems-index-format/",
				"title": "The New Rubygems Index Format",
				"content_html": "<p>This post is a part news, part technical documentation, and part request for comment. I&rsquo;m going to explain the technical nitty-gritty details of the planned next generation index that allows Bundler and Rubygems to know what gems exist and how to install them.</p>\n<p>The current index is a mishmash of different files that were created at different times to serve different needs. I&rsquo;ll walk you through the reasons that each of the current index files exists, and then explain how the new index plans to handle all those needs at the same time, in a simple and fast way.</p>\n<p>In the beginning was <code>Marshal.4.8.gz</code>. It wasn&rsquo;t actually 4.8, because that&rsquo;s the current version of Ruby&rsquo;s marshalling format, but it was certainly marshalled and gzipped. It contains an array of every <code>Gem::Specification</code> object for every gem that exists. As you might expect, that file is pretty big now that we have almost 100,000 gems.</p>\n<p>Once that file got big and unweildy, the <code>specs.4.8.gz</code> file was introduced. Confusingly, it doesn&rsquo;t actually contain any specs, but is instead an array of smaller arrays. Each smaller array contains the name, version, and platform of a single gem, so one item in that specs array is <code>[&quot;rack&quot;, Gem::Version.new(&quot;1.0.0&quot;), &quot;ruby&quot;]</code>. This was much, much faster to download and decompress.</p>\n<p>In addition, those three items could be used to download only the gemspec for the single gem that had been given to <code>gem install</code>. Those files are YAML serializations of Gem::Specification objects, and are served at URLs like <code>/specs/rack-1.0.0.gemspec</code>.</p>\n<p>At some later point, Rubygems gained support for &ldquo;prerelease&rdquo; versions, which are not installed by default, but must be opted into using <code>gem install --pre</code>. A similar marshalled array listing those gems can be found as <code>prerelease_specs.4.8.gz</code>. Gemspecs for prerelease versions are downloaded in the same way as regular versions.</p>\n<p>Eventually, even <code>specs.4.8.gz</code> got too big and unweildy, though, and so <code>latest_specs.4.8.gz</code> was added. It contains an array with one entry for the latest non-prerelease version of each gem that exists. Since that array does not grow with each release of each gem, its size has stayed more manageable. However, it&rsquo;s only useful if you are absolutely sure that you want to install the very latest version of a gem, which may or may not contain the code you are expecting.</p>\n<p>This is the world that Bundler was born into. Bundler did something new: it started with the latest version of a gem, but would successively try earlier versions of the gem, looking for one that was compatible with the dependencies of every other gem included in the bundle. In order for this to be done in a reasonable amount of time, Bundler was forced to ignore all the &ldquo;faster&rdquo; indexes, and just rely on <code>Marshal.4.8.gz</code>. That single file contained all the gemspecs, enabling Bundler to check the dependencies of every version that it tried while resolving the Gemfile.</p>\n<p>As many people noticed and pointed out, though, this was suuuuper slow. Downloading every single gemspec of every gem ever when the Gemfile only contained Rack was a bit wasteful. On top of that, it could even start swapping on servers with around 128MB of RAM, increasing a minute or two install to ten or twenty minutes. After some discussion with the Rubygems.org team, they suggested a dedicated Bundler API.</p>\n<p>The Bundler API is served at <code>/api/v1/dependenices</code>, and returns the same array-of-small-arrays format that <code>specs.4.8.gz</code> does. But it only includes the gems that are specifically asked for by name. That means that when a Gemfile only includes Rack, Bundler only downloads a list of all the versions of Rack. Using that list, Bundler can then try various versions looking for one that fits the bundle. Then it can download the gemspec of that version to make sure there are no dependency conflicts.</p>\n<p>Unfortunately, the Bundler API is much, much more demanding on the server than serving static marshalled files. After six months of Bundler users upgrading to the version that used the API, the server load was simply too much for Rubygems.org to handle, and it went down. The API had to be disabled to bring Rubygems.org back up again. As a result, the Bundler team built a dedicated app, deployed separately from Rubygems.org, that serves nothing but the Bundler API.</p>\n<p>While the foundations for the current index files are entirely pragmatic, those files definitely have some shortcomings. They&rsquo;re not security friendly, because they store gemspecs as Ruby marshal or YAML marshal data, which has repeatedly been shown to be vulnerable to various remote exploit scenarios. They&rsquo;re not local-cache friendly, beacuse the entire marshalled file changes if an item is appended to the array. They&rsquo;re not server friendly, because the API requires thousands of dollars per month of server infrastructure just to stay up. They&rsquo;re not Bundler friendly, because even the Bundler API doesn&rsquo;t provide critical information like required Ruby version or required Rubygems version, leading to some <code>bundle install</code> commands that seem to succeed, but then explode. They don&rsquo;t contain any checksum information, making it extremely difficult to tell if the downloaded or locally cached copy of a .gem file has been corrupted. They&rsquo;re not high-latency friendly, because the &ldquo;efficient&rdquo; Bundler API requires many, many round trip requests to the server that is currently hosted solely in EC2&rsquo;s US-East-1. They&rsquo;re not even low-bandwidth friendly, because the data is requested over and over again, even if it was just sent seconds ago.</p>\n<p>The new index tries to address all of these issues. First, it&rsquo;s just plaintext. That saves us from all the YAML and Marshal security concerns right away. It&rsquo;s caching friendly, because the line-based format allows new data to simply be appended at the end of the file. It&rsquo;s local-cache friendly, because the client can download any file once and then simply download new lines that have been appended since the last check. It&rsquo;s server friendly, because it&rsquo;s just flat files that are generated once when a gem is pushed. It&rsquo;s Bundler friendly, because it includes dependency information as well as required Ruby and Rubygems versions. It contains checksums, so that it is trivial to ensure that a local <code>.gem</code> file is the correct one before it is used. It&rsquo;s both high-latency and low-bandwidth friendly, by reducing the number of requests needed and the number of bytes downloaded thanks to local caching on each client. Finally, it&rsquo;s still fast: parsing plain text files using <code>split()</code> is on par with loading serialized data in our benchmarks.</p>\n<p>The new index provides three files: <code>names</code>, which is just a plaintext list of the name of every existing gem, one per line. It&rsquo;s not used by anything at present, but could be cached locally to be used for autocomplete or other similar things.\nThen there&rsquo;s the main index file, <code>versions</code>. That file is simply the name of a gem and a list of each version/platform pair for the gem, comma separated. A simple versions file might look like this:</p>\n<pre tabindex=\"0\"><code># /versions\nrack 0.9.2,1.0.0,1.0.1,1.1.0\nsinatra 1.0,1.0.1,1.0.1-jruby,1.1\n</code></pre><p>Knowing which versions exist makes it straightforward to dowload the gemspec of the corresponding gem. That&rsquo;s not usually needed, though, thanks to the other files, <code>/deps/GEM_NAME</code>. One file per named gem that exists, and each line contains a version and dependency information. A simple deps file might look like this.</p>\n<pre tabindex=\"0\"><code># /deps/nokogiri\n1.1.5 |checksum:abc123\n1.1.6 rake:&gt;= 0.7.1,activesupport:= 1.3.1|ruby:&gt; 1.8.7,checksum:bcd234\n1.1.7.rc2 rake:&gt;= 0.7.1|ruby:&gt;= 1.8.7,rubygems:&gt; 1.3.1,checksum:cde345\n1.1.7.rc3 |rubygems:&gt; 1.3.1,checksum:def456\n1.2.0-java mini_portile:~&gt; 0.5.0|checksum:fgh567\n</code></pre><p>Using this format, these files can be saved on the client machine the first time they are requested. After that, it is simply a matter of checking for changes using an HTTP etag header, and requesting any missing data using a Range header in the HTTP request. If the <code>versions</code> file is already up to date, there are no new versions of any gems, and no further work needs to be done. If there are new gems, the newly pushed versions can be inspected, and the <code>deps</code> files for only those gems can be updated as necessary when the bundle is resolved. Ruby version, Rubygems version, and checksums take care of the remaining issues with the Bundler API format.</p>\n<p>Now it&rsquo;s time for a confession: I said this was the planned format, but it&rsquo;s already implemented. There is existing code in Bundler and the Bundler API that uses this index format to install gems. It&rsquo;s not terribly optimized yet, but it&rsquo;s working, and it&rsquo;s going to get better as we got closer to releasing it. The Rubygems and Rubygems.org teams have agreed that this format is an improvement for everyone. The Bundler team will lead an integration effort once the client code is complete, and Bundler and Rubygems will share and improve the same index client library.</p>\n<p>If you have ideas for how the next-generation Rubygems index format could be even better, let me know on Github at <a href=\"https://github.com/bundler/new-index/issues/new\">https://github.com/bundler/new-index/issues/new</a> or on Twitter, where I am @indirect. Even better, if you want to help integrate it into Rubygems and Rubygems.org, let me know! It would be great to work together.</p>\n<p class=\"aside\">This post was also crossposted to the <a href=\"https://blog.engineyard.com/2014/new-rubygems-index-format\">Engine Yard blog</a>.</p>\n",
				"content_text": "This post is a part news, part technical documentation, and part request for comment. I'm going to explain the technical nitty-gritty details of the planned next generation index that allows Bundler and Rubygems to know what gems exist and how to install them.\n\nThe current index is a mishmash of different files that were created at different times to serve different needs. I'll walk you through the reasons that each of the current index files exists, and then explain how the new index plans to handle all those needs at the same time, in a simple and fast way.\n\nIn the beginning was `Marshal.4.8.gz`. It wasn't actually 4.8, because that's the current version of Ruby's marshalling format, but it was certainly marshalled and gzipped. It contains an array of every `Gem::Specification` object for every gem that exists. As you might expect, that file is pretty big now that we have almost 100,000 gems.\n\nOnce that file got big and unweildy, the `specs.4.8.gz` file was introduced. Confusingly, it doesn't actually contain any specs, but is instead an array of smaller arrays. Each smaller array contains the name, version, and platform of a single gem, so one item in that specs array is `[\"rack\", Gem::Version.new(\"1.0.0\"), \"ruby\"]`. This was much, much faster to download and decompress.\n\nIn addition, those three items could be used to download only the gemspec for the single gem that had been given to `gem install`. Those files are YAML serializations of Gem::Specification objects, and are served at URLs like `/specs/rack-1.0.0.gemspec`.\n\nAt some later point, Rubygems gained support for \"prerelease\" versions, which are not installed by default, but must be opted into using `gem install --pre`. A similar marshalled array listing those gems can be found as `prerelease_specs.4.8.gz`. Gemspecs for prerelease versions are downloaded in the same way as regular versions.\n\nEventually, even `specs.4.8.gz` got too big and unweildy, though, and so `latest_specs.4.8.gz` was added. It contains an array with one entry for the latest non-prerelease version of each gem that exists. Since that array does not grow with each release of each gem, its size has stayed more manageable. However, it's only useful if you are absolutely sure that you want to install the very latest version of a gem, which may or may not contain the code you are expecting.\n\nThis is the world that Bundler was born into. Bundler did something new: it started with the latest version of a gem, but would successively try earlier versions of the gem, looking for one that was compatible with the dependencies of every other gem included in the bundle. In order for this to be done in a reasonable amount of time, Bundler was forced to ignore all the \"faster\" indexes, and just rely on `Marshal.4.8.gz`. That single file contained all the gemspecs, enabling Bundler to check the dependencies of every version that it tried while resolving the Gemfile.\n\nAs many people noticed and pointed out, though, this was suuuuper slow. Downloading every single gemspec of every gem ever when the Gemfile only contained Rack was a bit wasteful. On top of that, it could even start swapping on servers with around 128MB of RAM, increasing a minute or two install to ten or twenty minutes. After some discussion with the Rubygems.org team, they suggested a dedicated Bundler API.\n\nThe Bundler API is served at `/api/v1/dependenices`, and returns the same array-of-small-arrays format that `specs.4.8.gz` does. But it only includes the gems that are specifically asked for by name. That means that when a Gemfile only includes Rack, Bundler only downloads a list of all the versions of Rack. Using that list, Bundler can then try various versions looking for one that fits the bundle. Then it can download the gemspec of that version to make sure there are no dependency conflicts.\n\nUnfortunately, the Bundler API is much, much more demanding on the server than serving static marshalled files. After six months of Bundler users upgrading to the version that used the API, the server load was simply too much for Rubygems.org to handle, and it went down. The API had to be disabled to bring Rubygems.org back up again. As a result, the Bundler team built a dedicated app, deployed separately from Rubygems.org, that serves nothing but the Bundler API.\n\nWhile the foundations for the current index files are entirely pragmatic, those files definitely have some shortcomings. They're not security friendly, because they store gemspecs as Ruby marshal or YAML marshal data, which has repeatedly been shown to be vulnerable to various remote exploit scenarios. They're not local-cache friendly, beacuse the entire marshalled file changes if an item is appended to the array. They're not server friendly, because the API requires thousands of dollars per month of server infrastructure just to stay up. They're not Bundler friendly, because even the Bundler API doesn't provide critical information like required Ruby version or required Rubygems version, leading to some `bundle install` commands that seem to succeed, but then explode. They don't contain any checksum information, making it extremely difficult to tell if the downloaded or locally cached copy of a .gem file has been corrupted. They're not high-latency friendly, because the \"efficient\" Bundler API requires many, many round trip requests to the server that is currently hosted solely in EC2's US-East-1. They're not even low-bandwidth friendly, because the data is requested over and over again, even if it was just sent seconds ago.\n\nThe new index tries to address all of these issues. First, it's just plaintext. That saves us from all the YAML and Marshal security concerns right away. It's caching friendly, because the line-based format allows new data to simply be appended at the end of the file. It's local-cache friendly, because the client can download any file once and then simply download new lines that have been appended since the last check. It's server friendly, because it's just flat files that are generated once when a gem is pushed. It's Bundler friendly, because it includes dependency information as well as required Ruby and Rubygems versions. It contains checksums, so that it is trivial to ensure that a local `.gem` file is the correct one before it is used. It's both high-latency and low-bandwidth friendly, by reducing the number of requests needed and the number of bytes downloaded thanks to local caching on each client. Finally, it's still fast: parsing plain text files using `split()` is on par with loading serialized data in our benchmarks.\n\nThe new index provides three files: `names`, which is just a plaintext list of the name of every existing gem, one per line. It's not used by anything at present, but could be cached locally to be used for autocomplete or other similar things.\nThen there's the main index file, `versions`. That file is simply the name of a gem and a list of each version/platform pair for the gem, comma separated. A simple versions file might look like this:\n\n```\n# /versions\nrack 0.9.2,1.0.0,1.0.1,1.1.0\nsinatra 1.0,1.0.1,1.0.1-jruby,1.1\n```\n\nKnowing which versions exist makes it straightforward to dowload the gemspec of the corresponding gem. That's not usually needed, though, thanks to the other files, `/deps/GEM_NAME`. One file per named gem that exists, and each line contains a version and dependency information. A simple deps file might look like this.\n\n```\n# /deps/nokogiri\n1.1.5 |checksum:abc123\n1.1.6 rake:>= 0.7.1,activesupport:= 1.3.1|ruby:> 1.8.7,checksum:bcd234\n1.1.7.rc2 rake:>= 0.7.1|ruby:>= 1.8.7,rubygems:> 1.3.1,checksum:cde345\n1.1.7.rc3 |rubygems:> 1.3.1,checksum:def456\n1.2.0-java mini_portile:~> 0.5.0|checksum:fgh567\n```\n\nUsing this format, these files can be saved on the client machine the first time they are requested. After that, it is simply a matter of checking for changes using an HTTP etag header, and requesting any missing data using a Range header in the HTTP request. If the `versions` file is already up to date, there are no new versions of any gems, and no further work needs to be done. If there are new gems, the newly pushed versions can be inspected, and the `deps` files for only those gems can be updated as necessary when the bundle is resolved. Ruby version, Rubygems version, and checksums take care of the remaining issues with the Bundler API format.\n\nNow it's time for a confession: I said this was the planned format, but it's already implemented. There is existing code in Bundler and the Bundler API that uses this index format to install gems. It's not terribly optimized yet, but it's working, and it's going to get better as we got closer to releasing it. The Rubygems and Rubygems.org teams have agreed that this format is an improvement for everyone. The Bundler team will lead an integration effort once the client code is complete, and Bundler and Rubygems will share and improve the same index client library.\n\nIf you have ideas for how the next-generation Rubygems index format could be even better, let me know on Github at https://github.com/bundler/new-index/issues/new or on Twitter, where I am @indirect. Even better, if you want to help integrate it into Rubygems and Rubygems.org, let me know! It would be great to work together.\n\n<p class=\"aside\">This post was also crossposted to the <a href=\"https://blog.engineyard.com/2014/new-rubygems-index-format\">Engine Yard blog</a>.</p>\n",
				"date_published": "2014-03-28T00:00:00-08:00",
				"url": "https://andre.arko.net/2014/03/28/the-new-rubygems-index-format/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2014/02/19/rubyfuza/",
				"title": "RubyFuza 2014",
				"content_html": "<h3 id=\"how-to-build-the-web-with-empathy-for-non-americans\">How to build the web with empathy for non-Americans</h3>\n<p><img src=\"https://indirect.micro.blog/uploads/2025/13bb0f74b9.jpg\" alt=\"Cape Town Beach\"></p>\n<p>Hey there everyone! Here&rsquo;s another entry in my ongoing conference travelogues. This time, I went to Cape Town in South Africa, to speak at the RubyFuza conference. It was a great experience, and really brought home some important development concepts that I&rsquo;ve heard about in the abstract but rarely experienced myself. The conference had speakers from all over the world, including Oregon, California, and Massachusetts, as well as Denmark, India, and (of course) South Africa.</p>\n<p>The community in South Africa isn&rsquo;t as mature as it is in the US, but it didn&rsquo;t feel as jaded, either. As I met locals and talked to them, I was reminded most strongly of the early days of the American Ruby community. No one was buying their own custom sports cars (at least, not yet), but everyone was excited about using Ruby and excited about building things.</p>\n<p>That excitement translated into both projects tailored for South African needs and projects intended for anyone to use wherever they are, and that was great to see. I was impressed that even with a nascent development community, companies like Microsoft, Google, and Amazon have already founded local offices. Microsoft even announced a local startup accelerator, offering seed funding for companies building on their Azure cloud platform.</p>\n<p>While there were many excellent talks, the highlights for me were Zachary Scott&rsquo;s <em>Contributing to Ruby</em>, and the complementary talks <em>The Art of Ruby</em> by Matthew Mongeau and <em>Clean Code: Applying Object Oriented Principles</em> by Kevin McKelvin.</p>\n<p>In Zak&rsquo;s talk, it was very encouraging to hear a clear explanation of the actual requirements to be able to contribute to Ruby. There was even someone present who had already become a Ruby contributor after hearing a previous version of the same talk at another conference!</p>\n<p>The refactoring talks were thoughtful, smart, and absolutely inspirational to me. The influence of Sandi Metz and <em>Practical Object Oriented Development with Ruby</em> could definitely be felt. Overall, the Ruby community seems to be reaching a consensus about some aspects of modular and understandable code that I find both encouraging and motivational.</p>\n<p>The talk that left the strongest impression on me, though, was the keynote. Charlene Tshitoka spoke on <em>Software Development in South Africa</em>. Almost every aspect of developing software there is challenging, and not just for the reasons that it&rsquo;s challenging for American developers. African developers (and their users) have to deal with limited or nonexistent resources on every level.</p>\n<p>As she spoke about development process and product management, it became clear to me that I (without even realizing) rely on developers around me to provide solutions to my problems. I can get advice and suggestions quickly and easily thanks to where I live and work. Although South Africa clearly has excellent developers, there is a much smaller pool of expertise available. As a result, they face greater challenges in solving not just technical problems, but organizational and social problems too.</p>\n<p>&ldquo;But these are software developers&rdquo;, you&rsquo;re probably thinking, &ldquo;they can get advice and suggestions from developers all over the world!&rdquo;. While that is technically possible, internet access is another one of the challenges faced by developers in South Africa. It&rsquo;s hard for me to describe, because the internet isn&rsquo;t just slow (which it is, very), but it&rsquo;s also extremely laggy. A single request for a page, or an image, can take upwards of two seconds to make its way to a server in the US and back.</p>\n<p>A complex <code>bundle install</code> run in the US can sometimes take up to 30 or 45 seconds. In South Africa, it was common to hear about 5 or 10 <em>minute</em> bundle installs. Compounding the problem is the expense and unreliability of data connections. In a country where many people live on as little as $60 per month, internet access is an extreme luxury.</p>\n<p>One developer I talked to pays $100 USD per month for a high-speed connection: 4mbits down, 512kbits up, with a usage cap under 100GB. Just watching Netflix for a couple of hours per night could be enough to use up that entire limit, to say nothing of actually doing work. Cellular data is equally distressing, with costs around $15 USD per gigabyte, and signal that is frequently weak and degraded or completely absent. I lost count of how many places there was no signal at all within just a day or two.</p>\n<p>Developing countries are already a big market, and they are expected to grow into a truly enormous one. As big as the opportunity is, though, any developer targeting users in developing countries faces extreme challenges in accomodating circumstancesa they have never experienced. (Some South African users don&rsquo;t have their own source of electricity!) In order to build products and services that work well for every human, we need to account for these challenges.\n  \nStay tuned. Next time: Sydney, Australia, and RubyConf AU!</p>\n",
				"content_text": "\n### How to build the web with empathy for non-Americans\n\n![Cape Town Beach](https://indirect.micro.blog/uploads/2025/13bb0f74b9.jpg)\n\nHey there everyone! Here's another entry in my ongoing conference travelogues. This time, I went to Cape Town in South Africa, to speak at the RubyFuza conference. It was a great experience, and really brought home some important development concepts that I've heard about in the abstract but rarely experienced myself. The conference had speakers from all over the world, including Oregon, California, and Massachusetts, as well as Denmark, India, and (of course) South Africa.\n\nThe community in South Africa isn't as mature as it is in the US, but it didn't feel as jaded, either. As I met locals and talked to them, I was reminded most strongly of the early days of the American Ruby community. No one was buying their own custom sports cars (at least, not yet), but everyone was excited about using Ruby and excited about building things. \n\nThat excitement translated into both projects tailored for South African needs and projects intended for anyone to use wherever they are, and that was great to see. I was impressed that even with a nascent development community, companies like Microsoft, Google, and Amazon have already founded local offices. Microsoft even announced a local startup accelerator, offering seed funding for companies building on their Azure cloud platform.\n\nWhile there were many excellent talks, the highlights for me were Zachary Scott's _Contributing to Ruby_, and the complementary talks _The Art of Ruby_ by Matthew Mongeau and _Clean Code: Applying Object Oriented Principles_ by Kevin McKelvin.\n\nIn Zak's talk, it was very encouraging to hear a clear explanation of the actual requirements to be able to contribute to Ruby. There was even someone present who had already become a Ruby contributor after hearing a previous version of the same talk at another conference!\n\nThe refactoring talks were thoughtful, smart, and absolutely inspirational to me. The influence of Sandi Metz and _Practical Object Oriented Development with Ruby_ could definitely be felt. Overall, the Ruby community seems to be reaching a consensus about some aspects of modular and understandable code that I find both encouraging and motivational.\n\nThe talk that left the strongest impression on me, though, was the keynote. Charlene Tshitoka spoke on _Software Development in South Africa_. Almost every aspect of developing software there is challenging, and not just for the reasons that it's challenging for American developers. African developers (and their users) have to deal with limited or nonexistent resources on every level.\n\nAs she spoke about development process and product management, it became clear to me that I (without even realizing) rely on developers around me to provide solutions to my problems. I can get advice and suggestions quickly and easily thanks to where I live and work. Although South Africa clearly has excellent developers, there is a much smaller pool of expertise available. As a result, they face greater challenges in solving not just technical problems, but organizational and social problems too.\n\n\"But these are software developers\", you're probably thinking, \"they can get advice and suggestions from developers all over the world!\". While that is technically possible, internet access is another one of the challenges faced by developers in South Africa. It's hard for me to describe, because the internet isn't just slow (which it is, very), but it's also extremely laggy. A single request for a page, or an image, can take upwards of two seconds to make its way to a server in the US and back.\n\nA complex `bundle install` run in the US can sometimes take up to 30 or 45 seconds. In South Africa, it was common to hear about 5 or 10 _minute_ bundle installs. Compounding the problem is the expense and unreliability of data connections. In a country where many people live on as little as $60 per month, internet access is an extreme luxury.\n\nOne developer I talked to pays $100 USD per month for a high-speed connection: 4mbits down, 512kbits up, with a usage cap under 100GB. Just watching Netflix for a couple of hours per night could be enough to use up that entire limit, to say nothing of actually doing work. Cellular data is equally distressing, with costs around $15 USD per gigabyte, and signal that is frequently weak and degraded or completely absent. I lost count of how many places there was no signal at all within just a day or two. \n\nDeveloping countries are already a big market, and they are expected to grow into a truly enormous one. As big as the opportunity is, though, any developer targeting users in developing countries faces extreme challenges in accomodating circumstancesa they have never experienced. (Some South African users don't have their own source of electricity!) In order to build products and services that work well for every human, we need to account for these challenges.\n  \nStay tuned. Next time: Sydney, Australia, and RubyConf AU!\n",
				"date_published": "2014-02-19T00:00:00-08:00",
				"url": "https://andre.arko.net/2014/02/19/rubyfuza/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2014/01/20/restoring-backed-up-cookies-on/",
				"title": "Restoring backed up cookies on Mac OS X 10.9 Mavericks",
				"content_html": "<p>You can restore a backup of your <code>Cookies</code> file on 10.9 by quitting Safari and then running these commands in the Terminal:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span>$ killall cookied\n</span></span><span style=\"display:flex;\"><span>$ mv ~/path/to/backed/up/Cookies.binarycookies ~/Library/Cookies/\n</span></span><span style=\"display:flex;\"><span>$ open -a Safari\n</span></span></code></pre></div><p>For reasons that aren&rsquo;t clear to me, every once in a while OS X will clear my entire cookies file, logging me out of every website in Safari and every other Mac app that uses a system WebKit browser window. It&rsquo;s pretty annoying, but in the past I&rsquo;ve always been able to fix it by restoring the <code>Cookies</code> file from my Time Machine backup. (You have a Time Machine backup, right? If not, please <a href=\"http://www.apple.com/airport-time-capsule/\">go buy one</a>.)</p>\n<p>That seems to have changed with OS X 10.9, with the <code>cookied</code> process that runs and seems to act as a central control point for Safari and other applications to access your cookies. It stays running even when Safari is closed, so restoring a backup of your cookies after quitting Safari doesn&rsquo;t help. <code>cookied</code> still has your old cookies in memory, and the next time you open Safari, it erases your carefully restored backup and overwrites it with your newly empty cookies file.</p>\n<p>As a result, the only way to restore a backup of your cookies is to quit Safari (so that cookied won&rsquo;t automatically get run again), then run <code>killall cookied</code> in the Terminal, <em>then</em> copy your backup into <code>~/Library/Cookies</code>, and finally re-open Safari. I searched the internet for instructions on how to do that for about 20 minutes before giving up and figuring it out myself. Hopefully this post will mean that future me and other people like him will just see the answer right away when they search.</p>\n",
				"content_text": "\nYou can restore a backup of your `Cookies` file on 10.9 by quitting Safari and then running these commands in the Terminal:\n\n```bash\n$ killall cookied\n$ mv ~/path/to/backed/up/Cookies.binarycookies ~/Library/Cookies/\n$ open -a Safari\n```\n\nFor reasons that aren't clear to me, every once in a while OS X will clear my entire cookies file, logging me out of every website in Safari and every other Mac app that uses a system WebKit browser window. It's pretty annoying, but in the past I've always been able to fix it by restoring the `Cookies` file from my Time Machine backup. (You have a Time Machine backup, right? If not, please [go buy one][1].)\n\nThat seems to have changed with OS X 10.9, with the `cookied` process that runs and seems to act as a central control point for Safari and other applications to access your cookies. It stays running even when Safari is closed, so restoring a backup of your cookies after quitting Safari doesn't help. `cookied` still has your old cookies in memory, and the next time you open Safari, it erases your carefully restored backup and overwrites it with your newly empty cookies file.\n\nAs a result, the only way to restore a backup of your cookies is to quit Safari (so that cookied won't automatically get run again), then run `killall cookied` in the Terminal, *then* copy your backup into `~/Library/Cookies`, and finally re-open Safari. I searched the internet for instructions on how to do that for about 20 minutes before giving up and figuring it out myself. Hopefully this post will mean that future me and other people like him will just see the answer right away when they search.\n\n[1]: http://www.apple.com/airport-time-capsule/\n",
				"date_published": "2014-01-20T00:00:00-08:00",
				"url": "https://andre.arko.net/2014/01/20/restoring-backed-up-cookies-on/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2013/12/09/extreme-makeover-rubygems-edition/",
				"title": "Extreme Makeover: Rubygems Edition",
				"content_html": "<p><small>This was also a talk given at RubyConf 2013 in Miami Beach, FL. If you prefer, you can <a href=\"http://www.confreaks.com/videos/2885-rubyconf2013-extreme-makeover-rubygems-edition\">watch the video</a> from the talk. This post contains the slide deck from the talk, and a written version of the content.</small></p>\n<script async class=\"speakerdeck-embed\" data-id=\"5951b48031690131904d529dfbcfdd99\" data-ratio=\"1.33333333333333\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n<p>Bundler, Rubygems, and rubygems.org are vital infrastructure that every Rubyist uses just about every day. Over the last year, that infrastructure has seen a huge amount of change. This is an overview of the changes, an update on where things are now, and an explanation of where we’re going soon.</p>\n<h2 id=\"so-what-happened-last-year\">So, what happened last year?</h2>\n<p>Playing it a little bit fast and loose with the definition of year, last October rubygems.org went down, in a big way. Bringing the site back up only lasted a few seconds before everything went down again. We eventually discovered the problem was the dependency API used by Bundler to speed up installs. The dependency API is database and CPU intensive, and there were so many users that the rubygems.org server couldn’t handle the load anymore. I gave a talk at Gotham Ruby with a lot of detail about what the problems were and how we fixed them. Essentially, the Bundler API was rebuilt as a separate Sinatra app, and we now throw a lot more CPU and database resources at it than we used to.</p>\n<p>The next major event was at the end of January, when someone exploited a YAML security vulnerability to get unauthorized access to the server hosting rubygems.org. That meant that, potentially, any gem could have been replaced with a trojan horse that compromised machines as it was installed. Every gem had to be verified to match another copy of that gem from mirrors that were not compromised. So, the single server hosting all of rubygems.org was decomissioned. New infrastructure was built on Amazon’s EC2, with redundant servers for failover, managed by Chef recipes that are open source in the rubygems/rubygems-aws repo. One significant upside to this change is the community can now contribute fixes and improvements to the servers that rubygems.org runs on, which was never possible before.</p>\n<p>The other significant issues this year have been more diffuse and inconsistent. Is everybody familiar with Travis, the hosted continuous integration service? Travis runs the tests for many open source projects, and they experienced seriously degraded network connections to rubygems.org. This caused a huge number of builds to fail just because of dropped or failed connections. After a thorough investigation, it turned out that the Travis network issues were a DNS configuration problem. The Travis VMs were hard-coded to use DNS servers that were on the opposite side of the country. As you may already know, gems are hosted on Amazon’s S3 storage service, and served via Amazon’s CloudFront content delivery network. CloudFront uses the location of your DNS servers to know which server it should tell you to download from. That meant Travis jobs were told to download every single gem from across the country, instead of from servers in a nearby datacenter. After the DNS issue was resolved, Travis build reliability shot up and has been steady since.</p>\n<p>The final major issues this year were all related to SSL, the system used to provide secure HTTP connections. In order to make a HTTPS connection, the client machine must have a certificate that it can use to verify the corresponding certificate used by the server. While recent Macs had most of those certificates built-in, many Linux and Windows machines did not. Compounding the problem, some S3 endpoints recently started using new certificates that couldn’t be verified by every Mac, either. Making everything more confusing, right around the time that the certificate issue happened, there was another issue that caused connections to fail right as they were started. That issue looked similar, but had a completely different cause. We solved the certificate issue by including the needed certificates in Rubygems and Bundler directly. The connection failure issue was a connection timeout set to only a few seconds, which was not enough time to allow connections to set up over lagging internet connections. We resolved that issue by Increasing the timeout.</p>\n<h3 id=\"how-does-rubygemsorg-work-now\">How does rubygems.org work now?</h3>\n<p>Today, Bundler and Rubygems both get information about the gems available from rubygems.org. Right now, there are two ways to do that: either download the entire list of every gem that exists, or ask for just some specific gems by name using the Bundler API. When you run gem install, your computer downloads the entire list of all the gems that exist. That takes a long time and needs a lot of memory. The list is pretty big already, and only getting bigger over time. When you run bundle install, Bundler tries ask just for the gems that you need to install. Using the Bundler API, it asks for just the gems it knows about, and then just the gems those gems need, etc. If you have a fast connection to rubygems.org, that takes less time than downloading the full index.</p>\n<p>If you live outside the US, however, that can sometimes take even more time than downloading the full index, because each round trip to AWS US-East in Virginia takes such a long time. Because all of these lists are sent as Ruby arrays, turned into strings via Marshal.dump, it’s not even possible to cache the list and update it with only the changes that have happened since the last download. So both Rubygems and Bundler download all this information again, from scratch, every time you install something.</p>\n<p>As you can probably guess from that description (and I’m sure you know if you have ever installed gems before), this is not the fastest situation ever. Earlier this year, after setting up the new Bundler API, I spent a lot of time discussing this problem with members of the Bundler, Rubygems, and Rubygems.org teams. After incorporating all of their feedback, I devised a plan for the next generation of Rubygems infrastructure. The single goal of that plan is to make installing gems as fast as possible, using every technique that we have been able to come up with.</p>\n<h3 id=\"the-plan-stan\">The plan, Stan</h3>\n<p>The plan I came up with was relatively straightforward, but a big departure from how we have been doing things until now. Instead of using marshalled arrays, gems will be listed in plaintext files. Those plaintext files can simply be added to when new gems are created. They can also easily be cached on each machine that installs gems. Using plaintext instead of marshal means that the gem index will not need to change even if Ruby updates the marshal format. It also removes the security issues around marshal and YAML, like the one that took down rubygems.org in January, at least on client machines.</p>\n<p>If no new gems have been released the server can reply with 304 Not Modified, and no data needs to be downloaded. Since the list of gems and the detailed information about each gem are separated into different files, requests for details can be limited to the gems that have been updated since the last update. This strategy hugely reduces both the size of the data that needs to be transferred, but also reduces the number of requests that need to be made. Those changes improve things for all Rubyists, but especially improve things for those far away from the rubygems.org servers.</p>\n<p>An additional improvement for everyone, but especially big for those outside the US, is using the Fastly content delivery network. Not just for gems, like we do today with CloudFront, but for all the plaintext gem index files as well. That means it will no longer be necessary to make a request all the way to US-East to install gems, which will be a huge improvement.</p>\n<p>Finally, we are working to expand the open source application that provides the Bundler API. It will provide the new index format, and it will be able to act as a mirror for Rubygems.org, caching copies of the gems that you need in your own datacenter, next to your servers. At companies with enough servers or enough paranoia to care, gem installs can be both fast and independent of the public rubygems.org server infrastructure.</p>\n<h3 id=\"what-have-we-done\">What have we done?</h3>\n<p>This summer, after I publicly announced this plan, Ruby Central was gracious enough to give me a grant to work on it. For the last several months, I have been spending one or two days a week working on the plan I just outlined. It’s starting to come together, and I’m excited to share the progress that we’ve made.</p>\n<p>We have implemented the plaintext index in the Bundler API server. The Bundler codebase contains a client library that can download and cache the new plaintext index files. It can use the cached index files to resolve Gemfiles, and do a bundle install. We’ve made it possible to tell the server what version of each file we have, and avoid downloading the file again if we already have it. Fastly is now the CDN serving gems and gemspecs when requests are made to rubygems.org.</p>\n<h3 id=\"what-do-we-still-need-to-do\">What do we still need to do?</h3>\n<p>As significant as that progress is, we still have a ways to go. We will improve the client to always use persistent HTTP connections and request pipelining to speed up requests for updating index files. We will add the new index format to rubygems.org, so that everyone around the world will be able to use it. In parallel with adding the new index format to rubygems.org, we’re going to add support for the new index format to Rubygems.</p>\n<p>Then everyone installing gems, using Bundler or Rubygems itself, will be able to benefit from the improvements that I’ve just outlined. Finally, we’re going to get all gems, gemspecs, and index files hosted by Fastly, so that requests for the gem index and requests for gems themselves can be returned by servers physically close to the person requesting them.</p>\n<h3 id=\"the-bright-future\">The bright future</h3>\n<p>I’m extremely excited about these changes and very grateful to the Rubygems team, the Rubygems.org team, the Bundler team, and of course Ruby Central for funding this work. Installing gems will be hugely improved as a result of this system and the work that we’ve done together.</p>\n<p>There is a Bundler release candidate, available right now, that can install gems much more quickly by using multple cores. Please test it, marvel at its speed, and let us know if it works (or doesn’t work) for you. As soon as that version is out into the wild, the next prelease version will include the index format improvements that I’ve outlined.</p>\n<p>The work isn’t done yet, though. If you’re willing to help, we can absolutely use that help to get done more quickly. <a href=\"mailto:andre@arko.net\">Email</a> or <a href=\"http://twitter.com/indirect\">tweet at me</a> if you’d like to get involved. We can make Rubygems better for everyone, together.</p>\n",
				"content_text": "\n<small>This was also a talk given at RubyConf 2013 in Miami Beach, FL. If you prefer, you can [watch the video](http://www.confreaks.com/videos/2885-rubyconf2013-extreme-makeover-rubygems-edition) from the talk. This post contains the slide deck from the talk, and a written version of the content.</small>\n\n<script async class=\"speakerdeck-embed\" data-id=\"5951b48031690131904d529dfbcfdd99\" data-ratio=\"1.33333333333333\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n\nBundler, Rubygems, and rubygems.org are vital infrastructure that every Rubyist uses just about every day. Over the last year, that infrastructure has seen a huge amount of change. This is an overview of the changes, an update on where things are now, and an explanation of where we’re going soon.\n\n## So, what happened last year?\n\nPlaying it a little bit fast and loose with the definition of year, last October rubygems.org went down, in a big way. Bringing the site back up only lasted a few seconds before everything went down again. We eventually discovered the problem was the dependency API used by Bundler to speed up installs. The dependency API is database and CPU intensive, and there were so many users that the rubygems.org server couldn’t handle the load anymore. I gave a talk at Gotham Ruby with a lot of detail about what the problems were and how we fixed them. Essentially, the Bundler API was rebuilt as a separate Sinatra app, and we now throw a lot more CPU and database resources at it than we used to.\n\nThe next major event was at the end of January, when someone exploited a YAML security vulnerability to get unauthorized access to the server hosting rubygems.org. That meant that, potentially, any gem could have been replaced with a trojan horse that compromised machines as it was installed. Every gem had to be verified to match another copy of that gem from mirrors that were not compromised. So, the single server hosting all of rubygems.org was decomissioned. New infrastructure was built on Amazon’s EC2, with redundant servers for failover, managed by Chef recipes that are open source in the rubygems/rubygems-aws repo. One significant upside to this change is the community can now contribute fixes and improvements to the servers that rubygems.org runs on, which was never possible before.\n\nThe other significant issues this year have been more diffuse and inconsistent. Is everybody familiar with Travis, the hosted continuous integration service? Travis runs the tests for many open source projects, and they experienced seriously degraded network connections to rubygems.org. This caused a huge number of builds to fail just because of dropped or failed connections. After a thorough investigation, it turned out that the Travis network issues were a DNS configuration problem. The Travis VMs were hard-coded to use DNS servers that were on the opposite side of the country. As you may already know, gems are hosted on Amazon’s S3 storage service, and served via Amazon’s CloudFront content delivery network. CloudFront uses the location of your DNS servers to know which server it should tell you to download from. That meant Travis jobs were told to download every single gem from across the country, instead of from servers in a nearby datacenter. After the DNS issue was resolved, Travis build reliability shot up and has been steady since.\n\nThe final major issues this year were all related to SSL, the system used to provide secure HTTP connections. In order to make a HTTPS connection, the client machine must have a certificate that it can use to verify the corresponding certificate used by the server. While recent Macs had most of those certificates built-in, many Linux and Windows machines did not. Compounding the problem, some S3 endpoints recently started using new certificates that couldn’t be verified by every Mac, either. Making everything more confusing, right around the time that the certificate issue happened, there was another issue that caused connections to fail right as they were started. That issue looked similar, but had a completely different cause. We solved the certificate issue by including the needed certificates in Rubygems and Bundler directly. The connection failure issue was a connection timeout set to only a few seconds, which was not enough time to allow connections to set up over lagging internet connections. We resolved that issue by Increasing the timeout.\n\n### How does rubygems.org work now?\n\nToday, Bundler and Rubygems both get information about the gems available from rubygems.org. Right now, there are two ways to do that: either download the entire list of every gem that exists, or ask for just some specific gems by name using the Bundler API. When you run gem install, your computer downloads the entire list of all the gems that exist. That takes a long time and needs a lot of memory. The list is pretty big already, and only getting bigger over time. When you run bundle install, Bundler tries ask just for the gems that you need to install. Using the Bundler API, it asks for just the gems it knows about, and then just the gems those gems need, etc. If you have a fast connection to rubygems.org, that takes less time than downloading the full index.\n\nIf you live outside the US, however, that can sometimes take even more time than downloading the full index, because each round trip to AWS US-East in Virginia takes such a long time. Because all of these lists are sent as Ruby arrays, turned into strings via Marshal.dump, it’s not even possible to cache the list and update it with only the changes that have happened since the last download. So both Rubygems and Bundler download all this information again, from scratch, every time you install something.\n\nAs you can probably guess from that description (and I’m sure you know if you have ever installed gems before), this is not the fastest situation ever. Earlier this year, after setting up the new Bundler API, I spent a lot of time discussing this problem with members of the Bundler, Rubygems, and Rubygems.org teams. After incorporating all of their feedback, I devised a plan for the next generation of Rubygems infrastructure. The single goal of that plan is to make installing gems as fast as possible, using every technique that we have been able to come up with.\n\n### The plan, Stan\n\nThe plan I came up with was relatively straightforward, but a big departure from how we have been doing things until now. Instead of using marshalled arrays, gems will be listed in plaintext files. Those plaintext files can simply be added to when new gems are created. They can also easily be cached on each machine that installs gems. Using plaintext instead of marshal means that the gem index will not need to change even if Ruby updates the marshal format. It also removes the security issues around marshal and YAML, like the one that took down rubygems.org in January, at least on client machines.\n\nIf no new gems have been released the server can reply with 304 Not Modified, and no data needs to be downloaded. Since the list of gems and the detailed information about each gem are separated into different files, requests for details can be limited to the gems that have been updated since the last update. This strategy hugely reduces both the size of the data that needs to be transferred, but also reduces the number of requests that need to be made. Those changes improve things for all Rubyists, but especially improve things for those far away from the rubygems.org servers.\n\nAn additional improvement for everyone, but especially big for those outside the US, is using the Fastly content delivery network. Not just for gems, like we do today with CloudFront, but for all the plaintext gem index files as well. That means it will no longer be necessary to make a request all the way to US-East to install gems, which will be a huge improvement.\n\nFinally, we are working to expand the open source application that provides the Bundler API. It will provide the new index format, and it will be able to act as a mirror for Rubygems.org, caching copies of the gems that you need in your own datacenter, next to your servers. At companies with enough servers or enough paranoia to care, gem installs can be both fast and independent of the public rubygems.org server infrastructure.\n\n### What have we done?\n\nThis summer, after I publicly announced this plan, Ruby Central was gracious enough to give me a grant to work on it. For the last several months, I have been spending one or two days a week working on the plan I just outlined. It’s starting to come together, and I’m excited to share the progress that we’ve made.\n\nWe have implemented the plaintext index in the Bundler API server. The Bundler codebase contains a client library that can download and cache the new plaintext index files. It can use the cached index files to resolve Gemfiles, and do a bundle install. We’ve made it possible to tell the server what version of each file we have, and avoid downloading the file again if we already have it. Fastly is now the CDN serving gems and gemspecs when requests are made to rubygems.org.\n\n### What do we still need to do?\n\nAs significant as that progress is, we still have a ways to go. We will improve the client to always use persistent HTTP connections and request pipelining to speed up requests for updating index files. We will add the new index format to rubygems.org, so that everyone around the world will be able to use it. In parallel with adding the new index format to rubygems.org, we’re going to add support for the new index format to Rubygems.\n\nThen everyone installing gems, using Bundler or Rubygems itself, will be able to benefit from the improvements that I’ve just outlined. Finally, we’re going to get all gems, gemspecs, and index files hosted by Fastly, so that requests for the gem index and requests for gems themselves can be returned by servers physically close to the person requesting them.\n\n### The bright future\n\nI’m extremely excited about these changes and very grateful to the Rubygems team, the Rubygems.org team, the Bundler team, and of course Ruby Central for funding this work. Installing gems will be hugely improved as a result of this system and the work that we’ve done together.\n\nThere is a Bundler release candidate, available right now, that can install gems much more quickly by using multple cores. Please test it, marvel at its speed, and let us know if it works (or doesn’t work) for you. As soon as that version is out into the wild, the next prelease version will include the index format improvements that I’ve outlined.\n\nThe work isn’t done yet, though. If you’re willing to help, we can absolutely use that help to get done more quickly. [Email](mailto:andre@arko.net) or [tweet at me](http://twitter.com/indirect) if you’d like to get involved. We can make Rubygems better for everyone, together.\n",
				"date_published": "2013-12-09T00:00:00-08:00",
				"url": "https://andre.arko.net/2013/12/09/extreme-makeover-rubygems-edition/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2013/12/07/the-rumors-of-bundlers-death/",
				"title": "The rumors of Bundler's death have been greatly exaggerated",
				"content_html": "<p>So this week there was some excitement on <a href=\"https://github.com/jruby/jruby/issues/1146\">Github</a>, <a href=\"https://news.ycombinator.com/item?id=6841855\">Hacker News</a>, and <a href=\"http://rubyweekly.com/archive/173.html\">Ruby Weekly</a> about the news that Bundler will (eventually) be merged into Rubygems. Before that comment, which was a side point on a different topic, the idea of merging the two projects had not been announced or explained.  As lead of the Bundler project, I&rsquo;d like to explain the merger plan, such as it is, and the reasoning behind it.</p>\n<p>The underlying motivation is simple: Rubygems and Bundler do a lot of the same things. Installing gems, downloading gems, resolving dependencies, and the like. It&rsquo;s a pretty long list of duplicated functionality. Since the Rubygems team now values many features that started in Bundler, they are also adding them, leading to even more duplicated functionality.</p>\n<p>During RubyConf this year in Miami, the Bundler team (represented by Terence Lee and myself) and the Rubygems team (represented by Eric Hodel and Evan Phoenix) met to talk about the increasing duplication. In that meeting, we agreed it made sense to decrease the duplication by merging the Rubygems and Bundler projects into one codebase and one team. Eventually. One day.</p>\n<p>But not yet! Because still there&rsquo;s a lot of stuff going on. Bundler and Rubygems already have separate feature road maps for the next 6 months, including things like parallel installation of gems and the new, more efficient index format that I talked about in my <a href=\"https://www.youtube.com/watch?v=-kyhz_VZMMM\">talk at RubyConf</a>. As a result of those plans, the merger work of combining code, tests, teams, and policies is not expected to be done for a year or even two years. At the earliest.</p>\n<p>While we&rsquo;re talking about potential changes that are this big, I&rsquo;d also like to make a point about what this means for Bundler. The Bundler team has taken great pains over the years so that Bundler can be stable, reliable, and dependable. We have also gone out of our way to conform strictly to semantic versioning. This has all been for a simple reason: trust. We want you to be able to trust Bundler. To keep that trust, we are willing to make changes much more slowly and cautiously than many other projects. While we all agree that reducing duplicate work is good, keeping Bundler trustworthy is an even higher priority than combining our work.</p>\n<p>Even with the plan to eventually merge projects, Bundler will continue to adhere to semantic versioning. We will continue to run the exhaustive Bundler integration RSpec suite. We will continue working just as hard to make sure Bundler updates are reliable and useful to the entire Ruby community. While everyone involved agrees this merger seems good, it also involves a huge amount of work. At a minimum, we need a unified release process and a functional single codebase, but we haven&rsquo;t even started working on either of those yet!</p>\n<p>In the meantime, we are enjoying higher cooperation than ever before between the Rubygems and Bundler projects. We are deliberately sharing both server-side and client-side code for the new index format. The Rubygems team has said they will begin using Bundler&rsquo;s integration tests along side their unit tests.</p>\n<p>I&rsquo;m excited about the idea of sharing work between both projects and eliminating duplicated code. We have a common goal of streamlining and simplifying the work of using gems for everyone in Ruby. Even though we aren&rsquo;t working on merging projects yet, keep an eye out for releases of both Bundler and Rubygems that showcase the work we are cooperating to do together.</p>\n<p>And we will be happy to announce the merger once it has happened.</p>\n",
				"content_text": "So this week there was some excitement on [Github][gh], [Hacker News][hn], and [Ruby Weekly][rw] about the news that Bundler will (eventually) be merged into Rubygems. Before that comment, which was a side point on a different topic, the idea of merging the two projects had not been announced or explained.  As lead of the Bundler project, I'd like to explain the merger plan, such as it is, and the reasoning behind it.\n\nThe underlying motivation is simple: Rubygems and Bundler do a lot of the same things. Installing gems, downloading gems, resolving dependencies, and the like. It's a pretty long list of duplicated functionality. Since the Rubygems team now values many features that started in Bundler, they are also adding them, leading to even more duplicated functionality.\n\nDuring RubyConf this year in Miami, the Bundler team (represented by Terence Lee and myself) and the Rubygems team (represented by Eric Hodel and Evan Phoenix) met to talk about the increasing duplication. In that meeting, we agreed it made sense to decrease the duplication by merging the Rubygems and Bundler projects into one codebase and one team. Eventually. One day.\n\nBut not yet! Because still there's a lot of stuff going on. Bundler and Rubygems already have separate feature road maps for the next 6 months, including things like parallel installation of gems and the new, more efficient index format that I talked about in my [talk at RubyConf][rc]. As a result of those plans, the merger work of combining code, tests, teams, and policies is not expected to be done for a year or even two years. At the earliest. \n\nWhile we're talking about potential changes that are this big, I'd also like to make a point about what this means for Bundler. The Bundler team has taken great pains over the years so that Bundler can be stable, reliable, and dependable. We have also gone out of our way to conform strictly to semantic versioning. This has all been for a simple reason: trust. We want you to be able to trust Bundler. To keep that trust, we are willing to make changes much more slowly and cautiously than many other projects. While we all agree that reducing duplicate work is good, keeping Bundler trustworthy is an even higher priority than combining our work.\n\nEven with the plan to eventually merge projects, Bundler will continue to adhere to semantic versioning. We will continue to run the exhaustive Bundler integration RSpec suite. We will continue working just as hard to make sure Bundler updates are reliable and useful to the entire Ruby community. While everyone involved agrees this merger seems good, it also involves a huge amount of work. At a minimum, we need a unified release process and a functional single codebase, but we haven't even started working on either of those yet!\n\nIn the meantime, we are enjoying higher cooperation than ever before between the Rubygems and Bundler projects. We are deliberately sharing both server-side and client-side code for the new index format. The Rubygems team has said they will begin using Bundler's integration tests along side their unit tests.\n\nI'm excited about the idea of sharing work between both projects and eliminating duplicated code. We have a common goal of streamlining and simplifying the work of using gems for everyone in Ruby. Even though we aren't working on merging projects yet, keep an eye out for releases of both Bundler and Rubygems that showcase the work we are cooperating to do together.\n\nAnd we will be happy to announce the merger once it has happened.\n\n[hn]: https://news.ycombinator.com/item?id=6841855\n[gh]: https://github.com/jruby/jruby/issues/1146\n[rw]: http://rubyweekly.com/archive/173.html\n[rc]: https://www.youtube.com/watch?v=-kyhz_VZMMM\n",
				"date_published": "2013-12-07T00:00:00-08:00",
				"url": "https://andre.arko.net/2013/12/07/the-rumors-of-bundlers-death/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2013/12/04/how-to-be-an-ally/",
				"title": "How to be an ally",
				"content_html": "<p><small>This was also a talk during the RailsBridge lightning talks at Github. See below for the slides, as well as the content of the talk.</small></p>\n<script async class=\"speakerdeck-embed\" data-id=\"664ea2803fca013162053a28d6c1e062\" data-ratio=\"1.33333333333333\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n<p>So I titled this &ldquo;How to be an ally&rdquo;, but that&rsquo;s a lie. You can&rsquo;t be an ally. No one can. Ally-ness isn&rsquo;t something that you can have intrinsically, any more than you can inherently be kindness, or rudeness. You can do ally actions. So probably a better name for this is How To Do Ally Work. But I&rsquo;m getting a bit ahead of myself.</p>\n<h3 id=\"convince-me-i-should-care\">Convince me I should care</h3>\n<p>Tech as a field (and software development in particular) have a stunning gender imbalance. While gender imbalance isn&rsquo;t inherently a bad thing, it has been getting worse, not better, for the last ten years. The few women who do make it into the field experience omnipresent sexist behaviour, from the tiny to the completely unbelievable. The scale starts with small (but still clearly exclusive) things like documentation that only uses &ldquo;he&rdquo;, includes being told &ldquo;oh, you don&rsquo;t LOOK like an engineer&rdquo; every single introduction, and can get as absurd as bosses who demand that the female engineer serve everyone coffee. Yes, that really happens. In real life. This is completely unreasonable, unfair, and demoralizing.</p>\n<p>But wait! It gets worse. Even if you&rsquo;re not a woman, you can still be attacked for being unacceptable. That can include (but isn&rsquo;t limited to) homosexuality, transsexuality, genderqueerness, having skin that isn&rsquo;t white enough, or just pointing out those groups are mistreated. Male-dominated culture doesn&rsquo;t just automatically degrade women. It also normalizes treating women as objects, unwanted sexual advances, and even sexual assault and rape. Right now tech (and American culture in general) is a place where it sucks to be anything but white and male.</p>\n<p>Even if you don&rsquo;t have a personal beef with the dominant culture, there are two big reasons to care about this stuff. First, diverse work groups are well-documented to produce better results in less time than homogenous groups. Second, the only time you can expect to be treated with empathy and understanding is when everyone is expected to have empathy and compassion. So ultimately, your success in business, in life, and in relationships depends on this stuff. No big deal.</p>\n<h3 id=\"what-even-is-allies\">What even is allies</h3>\n<p>Now that you know why you should care, back to allies. Ally, in this context, is a description of how you act. Are your actions inclusive and encouraging towards those who aren&rsquo;t like you? Do they show that you couldn&rsquo;t care less about them? Or worse yet, do they show that you are willing to harass, exploit, or attack people who aren&rsquo;t like you?</p>\n<p>It&rsquo;s important to note at this point that I am NOT saying that anyone (including you) is a bad person, or even sexist. I&rsquo;m saying that it&rsquo;s important to think about our actions, and how they impact the people around us, regardless of our intentions.</p>\n<p>As a result, ally work isn&rsquo;t something you can just do once. It&rsquo;s an attitude and a series of actions, and doing ally work consistently means CONSTANT VIGILANCE. Not just against others, but against yourself as well. Culture has done the best it can to make you automatically assist with the harassment and oppression of people who aren&rsquo;t like you in some way.</p>\n<h3 id=\"fucking-allies-how-do-they-work\">Fucking allies how do they work</h3>\n<p>So now that you care, what can you do? Primarily, it is this: shut the fuck up and listen. There are a lot of people who have been mistreated. Listen to them. You have not experienced what has happened to them. Listen to them so you can understand their experiences. They will probably be angry. They should be! The culture says it&rsquo;s fine to mistreat them. Ugh.</p>\n<p>Now that you&rsquo;re listening, you&rsquo;ll probably only last a few seconds before you&rsquo;re tempted to say something. Don&rsquo;t. Don&rsquo;t change the subject. Don&rsquo;t talk about how you have problems too. Don&rsquo;t tell them they&rsquo;re being too angry. Don&rsquo;t tell them being angry could have bad repercussions. Definitely don&rsquo;t say anything that includes &ldquo;I&rsquo;m only trying to help&rdquo;. You didn&rsquo;t have their experience, and your problems aren&rsquo;t directly comparable to theirs.</p>\n<p>Learn about the things that are upsetting to the people that you care about. When those situations come up in the future, speak up. Make it clear that the behaviour you just witnessed isn&rsquo;t okay. The cultural status quo is that bigoted actions are okay. Because of that, any witness who does nothing gives both attacked and attacker the impression that they condone the attack. Likewise, a vast body of research makes it clear that humans are willing to silently accept situations they cannot stand until just one other person speaks up. Be that person, and make it possible for others to speak up as well.</p>\n<p>You won&rsquo;t always get ally actions right, but that&rsquo;s okay. No one does. We need to change the norm to expect that everyone be treated well. You can do that. Once you are doing that, though, don&rsquo;t expect special treatment. There isn&rsquo;t a cookie jar or a bag of candy for the guys who are so amazing that they treat women and other minorities like regular human beings. This is minimum humanity, here. Aspire to actively do more, so that you can in fact be awesome.</p>\n<h3 id=\"what-about-the-mens\">What about the mens!</h3>\n<p>At this point, you&rsquo;re about to meet the opposition. Those people and their allies will fight unbelievably hard against the idea that their actions are damaging. &ldquo;What about the mens!&rdquo;, they cry. &ldquo;Reverse racism!&rdquo; is another popular one, as is &ldquo;but tech is a meritocracy!&rdquo;. Then there&rsquo;s always my favorite, &ldquo;feminists hate men and also fun&rdquo;.</p>\n<p>I&rsquo;ll just cover responses to the top few that I run into:</p>\n<ol>\n<li>&ldquo;Women can&rsquo;t computer because biology!&rdquo; There is no biological difference between the genders in IQ, problem solving, or any of the other skills needed to type all day while being angry at computers.</li>\n<li>&ldquo;Tech is a meritocracy!&rdquo; Meritocracy doesn&rsquo;t solve subconscious biases. In fact, a study found that performance evaluations in so-called meritocracies were more biased against women and minorities than evaluations outside them.</li>\n<li>&ldquo;That&rsquo;s reverse sexism (or reverse racism)!&rdquo; Helping minorities is not unfair to the majority—the majority is already better off than those who need help are even after they get helped.</li>\n<li>&ldquo;Not ALL men are like that!&rdquo; Of course not all men are like this. Anyone who feels the need to say this is just trying to distract from the very real problems caused by SOME men.</li>\n</ol>\n<p>Fortunately, the entire list of complaints is so common that there are already well-researched responses to all of them. A wonderfully concise list can be found on Julie Pagano&rsquo;s blog, in her post <a href=\"http://juliepagano.com/blog/2013/11/02/101-off-limits/\">101 Off Limits</a>. In it, she quickly covers the most common arguments against diversity, feminism, and women. Most answers also include links to other responses with tons of detail and research.</p>\n<p>There&rsquo;s a ton more to learn about this, but it&rsquo;s hard to tackle all at once. If you&rsquo;d like to learn how deep the rabbit hole goes, I&rsquo;ve collected a few links that you can use as starting points to learn more.</p>\n<h3 id=\"further-resources-on\">Further resources on</h3>\n<h4 id=\"sexism\">Sexism</h4>\n<ul>\n<li><a href=\"http://juliepagano.com/blog/2013/11/02/101-off-limits/\">101 Off Limits</a></li>\n<li><a href=\"http://www.creativebloq.com/netmag/primer-sexism-tech-industry-10126040\">A primer on sexism in the tech industry</a></li>\n<li><a href=\"http://finallyfeminism101.wordpress.com/the-faqs/faq-roundup/\">Finally, Feminism 101: The FAQs</a></li>\n<li><a href=\"https://medium.com/about-work/935a550ddd02\">Reading List for a New Kind of Manager</a></li>\n</ul>\n<h4 id=\"allies\">Allies</h4>\n<ul>\n<li><a href=\"http://juliepagano.com/blog/2014/05/10/so-you-want-to-be-an-ally/\">How to be an ally</a></li>\n<li><a href=\"https://medium.com/tech-culture-briefs/a1e93d985af0\">What Can Men Do?</a></li>\n<li><a href=\"http://geekfeminism.wikia.com/wiki/Allies\">Allies</a></li>\n<li><a href=\"http://www.anamardoll.com/2012/11/deconstruction-how-to-be-male-ally.html\">How To Be A (Male) Ally</a></li>\n<li><a href=\"http://theangryblackwoman.com/2009/10/01/the-dos-and-donts-of-being-a-good-ally/\">The Do&rsquo;s and Don&rsquo;ts of Being a Good Ally</a></li>\n<li><a href=\"http://sophiaserpentia.livejournal.com/838741.html\">listening to anger</a></li>\n</ul>\n<h4 id=\"open-source\">Open Source</h4>\n<ul>\n<li><a href=\"http://ashedryden.com/blog/the-ethics-of-unpaid-labor-and-the-oss-community\">The Ethics of Unpaid Labor and the OSS Community</a></li>\n<li><a href=\"https://blog.jcoglan.com/2013/11/15/why-github-is-not-your-cv/\">Why Github is not your CV</a></li>\n<li><a href=\"http://tarahunt.com/2013/10/28/meritocracy-is-almost-as-real-as-this-unicorn/\">Meritocracy is almost as real as this unicorn</a></li>\n<li><a href=\"http://www.garann.com/dev/2012/you-keep-using-that-word/\">You Keep Using That Word</a></li>\n</ul>\n",
				"content_text": "<small>This was also a talk during the RailsBridge lightning talks at Github. See below for the slides, as well as the content of the talk.</small>\n\n<script async class=\"speakerdeck-embed\" data-id=\"664ea2803fca013162053a28d6c1e062\" data-ratio=\"1.33333333333333\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n\nSo I titled this \"How to be an ally\", but that's a lie. You can't be an ally. No one can. Ally-ness isn't something that you can have intrinsically, any more than you can inherently be kindness, or rudeness. You can do ally actions. So probably a better name for this is How To Do Ally Work. But I'm getting a bit ahead of myself.\n\n### Convince me I should care\n\nTech as a field (and software development in particular) have a stunning gender imbalance. While gender imbalance isn't inherently a bad thing, it has been getting worse, not better, for the last ten years. The few women who do make it into the field experience omnipresent sexist behaviour, from the tiny to the completely unbelievable. The scale starts with small (but still clearly exclusive) things like documentation that only uses \"he\", includes being told \"oh, you don't LOOK like an engineer\" every single introduction, and can get as absurd as bosses who demand that the female engineer serve everyone coffee. Yes, that really happens. In real life. This is completely unreasonable, unfair, and demoralizing.\n\nBut wait! It gets worse. Even if you're not a woman, you can still be attacked for being unacceptable. That can include (but isn't limited to) homosexuality, transsexuality, genderqueerness, having skin that isn't white enough, or just pointing out those groups are mistreated. Male-dominated culture doesn't just automatically degrade women. It also normalizes treating women as objects, unwanted sexual advances, and even sexual assault and rape. Right now tech (and American culture in general) is a place where it sucks to be anything but white and male.\n\nEven if you don't have a personal beef with the dominant culture, there are two big reasons to care about this stuff. First, diverse work groups are well-documented to produce better results in less time than homogenous groups. Second, the only time you can expect to be treated with empathy and understanding is when everyone is expected to have empathy and compassion. So ultimately, your success in business, in life, and in relationships depends on this stuff. No big deal.\n\n### What even is allies\n\nNow that you know why you should care, back to allies. Ally, in this context, is a description of how you act. Are your actions inclusive and encouraging towards those who aren't like you? Do they show that you couldn't care less about them? Or worse yet, do they show that you are willing to harass, exploit, or attack people who aren't like you?\n\nIt's important to note at this point that I am NOT saying that anyone (including you) is a bad person, or even sexist. I'm saying that it's important to think about our actions, and how they impact the people around us, regardless of our intentions.\n\nAs a result, ally work isn't something you can just do once. It's an attitude and a series of actions, and doing ally work consistently means CONSTANT VIGILANCE. Not just against others, but against yourself as well. Culture has done the best it can to make you automatically assist with the harassment and oppression of people who aren't like you in some way.\n\n### Fucking allies how do they work\n\nSo now that you care, what can you do? Primarily, it is this: shut the fuck up and listen. There are a lot of people who have been mistreated. Listen to them. You have not experienced what has happened to them. Listen to them so you can understand their experiences. They will probably be angry. They should be! The culture says it's fine to mistreat them. Ugh.\n\nNow that you're listening, you'll probably only last a few seconds before you're tempted to say something. Don't. Don't change the subject. Don't talk about how you have problems too. Don't tell them they're being too angry. Don't tell them being angry could have bad repercussions. Definitely don't say anything that includes \"I'm only trying to help\". You didn't have their experience, and your problems aren't directly comparable to theirs.\n\nLearn about the things that are upsetting to the people that you care about. When those situations come up in the future, speak up. Make it clear that the behaviour you just witnessed isn't okay. The cultural status quo is that bigoted actions are okay. Because of that, any witness who does nothing gives both attacked and attacker the impression that they condone the attack. Likewise, a vast body of research makes it clear that humans are willing to silently accept situations they cannot stand until just one other person speaks up. Be that person, and make it possible for others to speak up as well.\n\nYou won't always get ally actions right, but that's okay. No one does. We need to change the norm to expect that everyone be treated well. You can do that. Once you are doing that, though, don't expect special treatment. There isn't a cookie jar or a bag of candy for the guys who are so amazing that they treat women and other minorities like regular human beings. This is minimum humanity, here. Aspire to actively do more, so that you can in fact be awesome.\n\n### What about the mens!\n\nAt this point, you're about to meet the opposition. Those people and their allies will fight unbelievably hard against the idea that their actions are damaging. \"What about the mens!\", they cry. \"Reverse racism!\" is another popular one, as is \"but tech is a meritocracy!\". Then there's always my favorite, \"feminists hate men and also fun\".\n\nI'll just cover responses to the top few that I run into:\n\n  1. \"Women can't computer because biology!\" There is no biological difference between the genders in IQ, problem solving, or any of the other skills needed to type all day while being angry at computers.\n  2. \"Tech is a meritocracy!\" Meritocracy doesn't solve subconscious biases. In fact, a study found that performance evaluations in so-called meritocracies were more biased against women and minorities than evaluations outside them.\n  3. \"That's reverse sexism (or reverse racism)!\" Helping minorities is not unfair to the majority—the majority is already better off than those who need help are even after they get helped.\n  4. \"Not ALL men are like that!\" Of course not all men are like this. Anyone who feels the need to say this is just trying to distract from the very real problems caused by SOME men.\n\nFortunately, the entire list of complaints is so common that there are already well-researched responses to all of them. A wonderfully concise list can be found on Julie Pagano's blog, in her post [101 Off Limits](http://juliepagano.com/blog/2013/11/02/101-off-limits/). In it, she quickly covers the most common arguments against diversity, feminism, and women. Most answers also include links to other responses with tons of detail and research.\n\nThere's a ton more to learn about this, but it's hard to tackle all at once. If you'd like to learn how deep the rabbit hole goes, I've collected a few links that you can use as starting points to learn more.\n\n### Further resources on\n\n#### Sexism\n\n  - [101 Off Limits](http://juliepagano.com/blog/2013/11/02/101-off-limits/)\n  - [A primer on sexism in the tech industry](http://www.creativebloq.com/netmag/primer-sexism-tech-industry-10126040)\n  - [Finally, Feminism 101: The FAQs](http://finallyfeminism101.wordpress.com/the-faqs/faq-roundup/)\n  - [Reading List for a New Kind of Manager](https://medium.com/about-work/935a550ddd02)\n\n#### Allies\n\n  - [How to be an ally](http://juliepagano.com/blog/2014/05/10/so-you-want-to-be-an-ally/)\n  - [What Can Men Do?](https://medium.com/tech-culture-briefs/a1e93d985af0)\n  - [Allies](http://geekfeminism.wikia.com/wiki/Allies)\n  - [How To Be A (Male) Ally](http://www.anamardoll.com/2012/11/deconstruction-how-to-be-male-ally.html)\n  - [The Do's and Don'ts of Being a Good Ally](http://theangryblackwoman.com/2009/10/01/the-dos-and-donts-of-being-a-good-ally/)\n  - [listening to anger](http://sophiaserpentia.livejournal.com/838741.html)\n\n#### Open Source\n\n  - [The Ethics of Unpaid Labor and the OSS Community](http://ashedryden.com/blog/the-ethics-of-unpaid-labor-and-the-oss-community)\n  - [Why Github is not your CV](https://blog.jcoglan.com/2013/11/15/why-github-is-not-your-cv/)\n  - [Meritocracy is almost as real as this unicorn](http://tarahunt.com/2013/10/28/meritocracy-is-almost-as-real-as-this-unicorn/)\n  - [You Keep Using That Word](http://www.garann.com/dev/2012/you-keep-using-that-word/)\n",
				"date_published": "2013-12-04T00:00:00-08:00",
				"url": "https://andre.arko.net/2013/12/04/how-to-be-an-ally/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2013/12/01/strings-in-ruby-are-utf/",
				"title": "Strings in Ruby are UTF-8 now… right?",
				"content_html": "<p>Ruby strings! In Ruby 1.8, Strings were (basically) just arrays of bytes with some extra methods, but Ruby 1.9 added explicit encoding support. Now every string knows how it is encoded! This fixes all of our issues with non-ASCII characters, right? Maybe? Hopefully? Possibly?</p>\n<p>Nope. :(</p>\n<p>Behold, as narrated by <a href=\"http://mortoray.com/2013/11/27/the-string-type-is-broken\">The String Type Is Broken</a>, exactly how many ways Ruby completely fails to comprehend UTF-8 strings.</p>\n<p>First, special characters that have to be handled in a way that is aware of how they work. 100% fail.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">&#34;noël&#34;</span><span style=\"color:#f92672\">.</span>encoding <span style=\"color:#75715e\"># =&gt; UTF-8</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">&#34;noël&#34;</span><span style=\"color:#f92672\">.</span>reverse <span style=\"color:#f92672\">==</span> <span style=\"color:#e6db74\">&#34;lëon&#34;</span> <span style=\"color:#75715e\"># =&gt; false</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">&#34;noël&#34;</span><span style=\"color:#f92672\">[</span><span style=\"color:#ae81ff\">0</span><span style=\"color:#f92672\">..</span><span style=\"color:#ae81ff\">2</span><span style=\"color:#f92672\">]</span> <span style=\"color:#f92672\">==</span> <span style=\"color:#e6db74\">&#34;noë&#34;</span> <span style=\"color:#75715e\"># =&gt; false</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">&#34;noël&#34;</span><span style=\"color:#f92672\">.</span>length <span style=\"color:#f92672\">==</span> <span style=\"color:#ae81ff\">4</span> <span style=\"color:#75715e\"># =&gt; false</span>\n</span></span></code></pre></div><p>Perhaps unsurprisingly, Ruby is able to handle emoji correctly. So that&rsquo;s good! I gather that handling multibyte characters as characters was one of the focuses of Ruby&rsquo;s encoding support. Of course, handling these incorrectly probably requires UTF-16 strings, and Ruby doesn&rsquo;t have those at all. So maybe it&rsquo;s just accidentally correct.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">&#34;😸😾&#34;</span><span style=\"color:#f92672\">.</span>encoding <span style=\"color:#75715e\"># =&gt; UTF-8</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">&#34;😸😾&#34;</span><span style=\"color:#f92672\">.</span>length <span style=\"color:#f92672\">==</span> <span style=\"color:#ae81ff\">2</span> <span style=\"color:#75715e\"># =&gt; true</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">&#34;😸😾&#34;</span><span style=\"color:#f92672\">[</span><span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">..</span><span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">]</span> <span style=\"color:#f92672\">==</span> <span style=\"color:#e6db74\">&#34;😾&#34;</span> <span style=\"color:#75715e\"># =&gt; true</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">&#34;😸😾&#34;</span><span style=\"color:#f92672\">.</span>reverse <span style=\"color:#f92672\">==</span> <span style=\"color:#e6db74\">&#34;😾😸&#34;</span> <span style=\"color:#75715e\"># =&gt; true</span>\n</span></span></code></pre></div><p>Finally, composition in the form of ligatures isn&rsquo;t handled at all. Bummer.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">&#34;baﬄe&#34;</span><span style=\"color:#f92672\">.</span>encoding <span style=\"color:#75715e\"># =&gt; UTF-8</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#e6db74\">&#34;baﬄe&#34;</span><span style=\"color:#f92672\">.</span>upcase <span style=\"color:#f92672\">==</span> <span style=\"color:#e6db74\">&#34;BAFFLE&#34;</span> <span style=\"color:#75715e\"># =&gt; false</span>\n</span></span></code></pre></div><p>😢</p>\n<p>To totally upgrade your 😢 into a full on 😭, though, just check out the way that <a href=\"https://github.com/elixir-lang/elixir/blob/d95a7d1a58bddcbbfec62a17c16a53dc1d6a3543/lib/elixir/test/elixir/string_test.exs#L18-L29\">the Elixir language handles unicode strings</a>. I mean, I understand that Erlang is only 5 years older than Ruby is, but I sure wish we had added support for unicode when we added support for strings to claim they are unicode.</p>\n",
				"content_text": "Ruby strings! In Ruby 1.8, Strings were (basically) just arrays of bytes with some extra methods, but Ruby 1.9 added explicit encoding support. Now every string knows how it is encoded! This fixes all of our issues with non-ASCII characters, right? Maybe? Hopefully? Possibly?\n\nNope. :(\n\nBehold, as narrated by [The String Type Is Broken](http://mortoray.com/2013/11/27/the-string-type-is-broken), exactly how many ways Ruby completely fails to comprehend UTF-8 strings.\n\nFirst, special characters that have to be handled in a way that is aware of how they work. 100% fail.\n\n```ruby\n\"noël\".encoding # => UTF-8\n\"noël\".reverse == \"lëon\" # => false\n\"noël\"[0..2] == \"noë\" # => false\n\"noël\".length == 4 # => false\n```\n\nPerhaps unsurprisingly, Ruby is able to handle emoji correctly. So that's good! I gather that handling multibyte characters as characters was one of the focuses of Ruby's encoding support. Of course, handling these incorrectly probably requires UTF-16 strings, and Ruby doesn't have those at all. So maybe it's just accidentally correct.\n\n```ruby\n\"😸😾\".encoding # => UTF-8\n\"😸😾\".length == 2 # => true\n\"😸😾\"[1..1] == \"😾\" # => true\n\"😸😾\".reverse == \"😾😸\" # => true\n```\n\nFinally, composition in the form of ligatures isn't handled at all. Bummer.\n\n```ruby\n\"baﬄe\".encoding # => UTF-8\n\"baﬄe\".upcase == \"BAFFLE\" # => false\n```\n\n😢\n\nTo totally upgrade your 😢 into a full on 😭, though, just check out the way that [the Elixir language handles unicode strings](https://github.com/elixir-lang/elixir/blob/d95a7d1a58bddcbbfec62a17c16a53dc1d6a3543/lib/elixir/test/elixir/string_test.exs#L18-L29). I mean, I understand that Erlang is only 5 years older than Ruby is, but I sure wish we had added support for unicode when we added support for strings to claim they are unicode.\n",
				"date_published": "2013-12-01T00:00:00-08:00",
				"url": "https://andre.arko.net/2013/12/01/strings-in-ruby-are-utf/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2013/09/11/vim-is-the-worst-editor/",
				"title": "Vim is the worst editor",
				"content_html": "<h3 id=\"hellipexcept-all-the-other-editors\">…except all the other editors</h3>\n<p>I want to like Vim so badly! I&rsquo;ve been using it full-time for just over three months now. I finished <a href=\"http://vim-adventures.com/\">Vim Adventures</a>, I&rsquo;ve been reading <a href=\"http://pragprog.com/book/dnvim/practical-vim\">Practical Vim</a>, and watching some <a href=\"http://vimcasts.org/\">Vimcasts</a>. I&rsquo;ve even been asking my <a href=\"http://twitter.com/lmarburger\">smart</a> <a href=\"http://twitter.com/hone02\">vim</a> <a href=\"http://twitter.com/tpope\">friends</a> for help.</p>\n<p>Unfortunately, it&rsquo;s still driving me insane. The things it does badly make me want to stop using it almost every single day. That makes me really sad, because the things it does well, it does <em>phenomenally</em> well. After 3 months in Vim, I get incredibly angry trying to use other text editors because they do so many editing tasks so poorly in comparsion to Vim.</p>\n<p>So, in no particular order, here&rsquo;s a list of things that drive me insane about Vim. After reading this list, feel free to tell me how I&rsquo;m doing it wrong <a href=\"http://twitter.com/indirect\">on twitter</a>.</p>\n<ol>\n<li>Janus</li>\n</ol>\n<pre><code>&quot;Here, we've set up everything you need to use Vim.&quot; Except it's way too much stuff, it's fiddly, it's semi-broken in various places, and having it installed makes Vim _incredibly slow_. It takes more than one second to open a new window in MacVim, and it takes more than one second to open a file in terminal Vim. That is crazy and unacceptable. That said, I'm still using it because I don't want to take the time to figure out how to manually add back in all of the pieces I actually do want, like support for all the languages I care about, and syntax checking on save, and the other stuff like that.\n</code></pre>\n<ol>\n<li>Files that change.</li>\n</ol>\n<pre><code>Argh, argh, argh. This is so frustrating. If I edit a file in another process, or if I edit a file using [pry-rescue][github]'s `edit` command, Vim screeches loudly that my file has been changed and do I want to reload it from disk. I have to read the message, decide what to do, and then continue. Every. Single. Time.\n\nWhile it's possible to turn on `autoload`, something still has to trigger Vim's `:checktime` command. Some people do things like trigger it on every cursor move, or every 1-2 seconds. That is _completely insane_. Every invocation stats the file on disk. If you have 100 buffers open, that is 100 stats per second. NO, because no.\n\nOther editors automatically save when they lose focus, and register for file system notifications when any file that's open changes. When the file is open, they immediately read the file again, whether in the foreground or background. I never have to think about it. It's great.\n</code></pre>\n<ol>\n<li>Indenting pastes.</li>\n</ol>\n<pre><code>You're all about to start yelling &quot;use `p` instead of `⌘V`, you idiot&quot;, or &quot;bind `set paste` to F2`, but that's not what I'm talking about at all. TextMate has an _incredibly_ smart paste system. Every paste is automatically re-indented to match the indentation level of the cursor at the moment of pasting. It never even occurred to me that indentation of pastes was a thing that ever required thought of any kind.\n\nThen I switched to Vim, and discovered that [leading Vim minds][twitter] [suggest][uniqpath] that you &quot;simply&quot; run ``V`]=`` after _every goddamn paste_. No, no, and also: no.\n\nI guess I can try to figure out how to hook vim to automatically run that after every time I use `p`, but that seems terrible, and would forcibly re-align sub-line pastes.\n\nTo add insult to injury, Vim's automatic indentation, triggered by `=`, is frequently just wrong. Then I have to manually adjust by running ``V`]&gt;`` or whatever, and that is also... pretty terrible.\n</code></pre>\n<ol>\n<li>Project-wide find and replace</li>\n</ol>\n<pre><code>I am aware of both `:Ack` and `:Ag`, but there's a reason that I rarely used them before: printing out lines with matches is not that great. Jumping to matches inside files, having the results live update as I make changes and then save, and being able to do a project-wide find and replace where I can live-preview the results of the replace operation for every single line in the project that matched? _That_ is a good project search.\n\n*Update:* The fantastic [Larry][twitter 2] tells me that this is possible in Vim:\n\n    /Pragmatic\\ze Vim\n    :vimgrep /&lt;C-r&gt;// **/*.txt\n    :Qargs\n    :argdo %s//Practical/g\n    :argdo update\n\nWhile I am incredibly impressed that Vim has primitives that allow this, the last four commands is a pretty crappy replacement for `⇥Practical`, in my opinion.\n</code></pre>\n<ol>\n<li>Opening files inside my project</li>\n</ol>\n<pre><code>This seems completely insane, but there is no file opener that is as good as TextMate's decade-old ⌘T. The ctrl-p and command-t plugins both try, but neither one is as fast, they are both quite hard to exclude files from, and they have no facility for per-project excludes. In fact, anyone who wants per-project excludes is told to manually set up their own per-project vimrc files and then write the Vim configuration lines required to cause those files to be ignored. You have got to be kidding me.\n</code></pre>\n<ol>\n<li>Navigating inside files</li>\n</ol>\n<pre><code>Jump to method definition. Jump to CSS selector. Jump to class. Fuzzy string matching. All these things and more are what I am used to. Vim just _can't do them_. Everyone says to use `ctags`, which is a great idea, but completely isn't the thing that I am talking about. The tiny, tiny percentage of the time that I want to jump to the definition of a method that is named in my buffer, `ctags` is fantastic.\n\nThe rest of the time, I want to jump to something that is _not_ written out inside this buffer, but is instead simply elsewhere in the file. Or is a CSS selector. Using `/` to search is not fuzzy matching. It is _terrible_ in comparison.\n</code></pre>\n<ol>\n<li>\n<p>Soft-wrapped text</p>\n<p>Welcome to the modern world! Text does not need to be hard wrapped at 80 columns, and paragraphs do not need to be indicated by two line-breaks in a row. In fact, it&rsquo;s entirely possible to edit plaintext files that are <em>softwrapped</em> in any modern text editor. But not in Vim. In Vim, your options are automatic hardwrapping or remapping <code>j</code> and <code>k</code> to <code>gj</code> and <code>gk</code> so you can still use them in softwrapped text. Grrrr.</p>\n<p>If you then expect something completely absurd like your editor to have indentation awareness while softwrapping is turned on, I am afraid you are about to be highly disappointed. Beacuse Vim <em>literally cannot do that</em>. There&rsquo;s a <a href=\"https://retracile.net/wiki/VimBreakIndent\">public and well-known patch</a> that you can apply to the Vim sourcetree and then compile your own fork of Vim if you want that feature. Of course, even if you do that, the patch is broken in several ways that are also well-known, which is why it&rsquo;s not part of mainstream Vim. Good luck with that.</p>\n</li>\n</ol>\n<p>In conclusion, Vim is a terrible editor, but it makes me less angry than all the other editors, so I&rsquo;m still using it full time. Yay?</p>\n",
				"content_text": "### &hellip;except all the other editors\n\nI want to like Vim so badly! I've been using it full-time for just over three months now. I finished [Vim Adventures][vim-adventures], I've been reading [Practical Vim][pragprog], and watching some [Vimcasts][vimcasts]. I've even been asking my [smart][twitter 2] [vim][twitter 3] [friends][twitter 4] for help.\n\nUnfortunately, it's still driving me insane. The things it does badly make me want to stop using it almost every single day. That makes me really sad, because the things it does well, it does _phenomenally_ well. After 3 months in Vim, I get incredibly angry trying to use other text editors because they do so many editing tasks so poorly in comparsion to Vim.\n\nSo, in no particular order, here's a list of things that drive me insane about Vim. After reading this list, feel free to tell me how I'm doing it wrong [on twitter][twitter 5].\n\n  1. Janus\n\n    \"Here, we've set up everything you need to use Vim.\" Except it's way too much stuff, it's fiddly, it's semi-broken in various places, and having it installed makes Vim _incredibly slow_. It takes more than one second to open a new window in MacVim, and it takes more than one second to open a file in terminal Vim. That is crazy and unacceptable. That said, I'm still using it because I don't want to take the time to figure out how to manually add back in all of the pieces I actually do want, like support for all the languages I care about, and syntax checking on save, and the other stuff like that.\n\n  1. Files that change.\n\n    Argh, argh, argh. This is so frustrating. If I edit a file in another process, or if I edit a file using [pry-rescue][github]'s `edit` command, Vim screeches loudly that my file has been changed and do I want to reload it from disk. I have to read the message, decide what to do, and then continue. Every. Single. Time.\n\n    While it's possible to turn on `autoload`, something still has to trigger Vim's `:checktime` command. Some people do things like trigger it on every cursor move, or every 1-2 seconds. That is _completely insane_. Every invocation stats the file on disk. If you have 100 buffers open, that is 100 stats per second. NO, because no.\n\n    Other editors automatically save when they lose focus, and register for file system notifications when any file that's open changes. When the file is open, they immediately read the file again, whether in the foreground or background. I never have to think about it. It's great.\n\n  1. Indenting pastes.\n\n    You're all about to start yelling \"use `p` instead of `⌘V`, you idiot\", or \"bind `set paste` to F2`, but that's not what I'm talking about at all. TextMate has an _incredibly_ smart paste system. Every paste is automatically re-indented to match the indentation level of the cursor at the moment of pasting. It never even occurred to me that indentation of pastes was a thing that ever required thought of any kind.\n\n    Then I switched to Vim, and discovered that [leading Vim minds][twitter] [suggest][uniqpath] that you \"simply\" run ``V`]=`` after _every goddamn paste_. No, no, and also: no.\n\n    I guess I can try to figure out how to hook vim to automatically run that after every time I use `p`, but that seems terrible, and would forcibly re-align sub-line pastes.\n\n    To add insult to injury, Vim's automatic indentation, triggered by `=`, is frequently just wrong. Then I have to manually adjust by running ``V`]>`` or whatever, and that is also... pretty terrible.\n\n  1. Project-wide find and replace\n\n    I am aware of both `:Ack` and `:Ag`, but there's a reason that I rarely used them before: printing out lines with matches is not that great. Jumping to matches inside files, having the results live update as I make changes and then save, and being able to do a project-wide find and replace where I can live-preview the results of the replace operation for every single line in the project that matched? _That_ is a good project search.\n\n    *Update:* The fantastic [Larry][twitter 2] tells me that this is possible in Vim:\n\n        /Pragmatic\\ze Vim\n        :vimgrep /<C-r>// **/*.txt\n        :Qargs\n        :argdo %s//Practical/g\n        :argdo update\n    \n    While I am incredibly impressed that Vim has primitives that allow this, the last four commands is a pretty crappy replacement for `⇥Practical`, in my opinion.\n\n  1. Opening files inside my project\n\n    This seems completely insane, but there is no file opener that is as good as TextMate's decade-old ⌘T. The ctrl-p and command-t plugins both try, but neither one is as fast, they are both quite hard to exclude files from, and they have no facility for per-project excludes. In fact, anyone who wants per-project excludes is told to manually set up their own per-project vimrc files and then write the Vim configuration lines required to cause those files to be ignored. You have got to be kidding me.\n\n  1. Navigating inside files\n\n    Jump to method definition. Jump to CSS selector. Jump to class. Fuzzy string matching. All these things and more are what I am used to. Vim just _can't do them_. Everyone says to use `ctags`, which is a great idea, but completely isn't the thing that I am talking about. The tiny, tiny percentage of the time that I want to jump to the definition of a method that is named in my buffer, `ctags` is fantastic.\n    \n    The rest of the time, I want to jump to something that is _not_ written out inside this buffer, but is instead simply elsewhere in the file. Or is a CSS selector. Using `/` to search is not fuzzy matching. It is _terrible_ in comparison.\n\n  1. Soft-wrapped text\n\n      Welcome to the modern world! Text does not need to be hard wrapped at 80 columns, and paragraphs do not need to be indicated by two line-breaks in a row. In fact, it's entirely possible to edit plaintext files that are _softwrapped_ in any modern text editor. But not in Vim. In Vim, your options are automatic hardwrapping or remapping `j` and `k` to `gj` and `gk` so you can still use them in softwrapped text. Grrrr.\n      \n      If you then expect something completely absurd like your editor to have indentation awareness while softwrapping is turned on, I am afraid you are about to be highly disappointed. Beacuse Vim _literally cannot do that_. There's a [public and well-known patch][retracile] that you can apply to the Vim sourcetree and then compile your own fork of Vim if you want that feature. Of course, even if you do that, the patch is broken in several ways that are also well-known, which is why it's not part of mainstream Vim. Good luck with that.\n\n\nIn conclusion, Vim is a terrible editor, but it makes me less angry than all the other editors, so I'm still using it full time. Yay?\n\n\n\n[github]: https://github.com/ConradIrwin/pry-rescue\n[pragprog]: http://pragprog.com/book/dnvim/practical-vim\n[retracile]: https://retracile.net/wiki/VimBreakIndent\n[twitter]: https://twitter.com/mislav\n[twitter 2]: http://twitter.com/lmarburger\n[twitter 3]: http://twitter.com/hone02\n[twitter 4]: http://twitter.com/tpope\n[twitter 5]: http://twitter.com/indirect\n[uniqpath]: http://mislav.uniqpath.com/2011/12/vim-revisited/\n[vim-adventures]: http://vim-adventures.com/\n[vimcasts]: http://vimcasts.org/\n",
				"date_published": "2013-09-11T00:00:00-08:00",
				"url": "https://andre.arko.net/2013/09/11/vim-is-the-worst-editor/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2013/08/22/security-is-hard-but-we/",
				"title": "Security is hard, but we can't go shopping",
				"content_html": "<p><img src=\"https://indirect.micro.blog/uploads/2025/a4aaba309a.jpg\" alt=\"hard\"></p>\n<p><small>This post was also given as a conference talk, originally at <a href=\"http://ruby.onales.com\">Ruby on Ales 2013</a> (<a href=\"https://speakerdeck.com/indirect/security-is-hard-but-we-cant-go-shopping-ruby-on-ales-2013\">slides</a>, <a href=\"http://confreaks.com/videos/2317-roa2013-security-is-hard-but-we-cant-go-shopping\">video</a>), <a href=\"http://www.railsconf.com\">RailsConf 2013</a> (<a href=\"https://speakerdeck.com/indirect/security-is-hard-but-we-cant-go-shopping-railsconf-2013\">slides</a>, <a href=\"http://www.confreaks.com/videos/2430-railsconf2013-security-is-hard-but-we-cant-go-shopping\">video</a>) and <a href=\"http://rubykaigi.org\">RubyKaigi 2013</a> (<a href=\"https://speakerdeck.com/indirect/security-is-hard-but-we-cant-go-shopping-rubykaigi-2013\">slides</a>, <a href=\"http://www.ustream.tv/recorded/33562443\">video</a>), then updated for <a href=\"http://madisonpl.us/ruby\">Madison+Ruby 2014</a> (<a href=\"https://speakerdeck.com/indirect/security-is-hard-but-we-cant-go-shopping-madison-ruby-2014\">slides</a>) and <a href=\"http://www.reddotrubyconf.com\">RedDot RubyConf 2015</a> (<a href=\"https://speakerdeck.com/indirect/security-is-hard-reddotrubyconf-2015\">slides</a>).</small></p>\n<h3 id=\"uhoh-security\">Uhoh, security?</h3>\n<p>Security is a hard topic. It&rsquo;s an especially hard topic in the Ruby community, where the security situation has historically been so great that hardly anyone has had to care about it. You may not know this, depending on how long you’ve been a rubyist, but Ruby security issues usually only come up once or maybe twice per year. They&rsquo;re usually relatively benign, as those things go, so everyone updates as soon as it&rsquo;s convenient, and life goes on.</p>\n<p>Things changed pretty dramatically this year. In rapid succession, there was a flurry of security releases in both Ruby and Rails. Almost every release fixed a huge vulnerability, and security updates were suddenly extremely important.\nThere were more Rails security releases by May of 2013 than there had been in 2011 and 2012 <em>combined</em>. This year, Rails has issued 12 CVEs, and Ruby has issued 5 CVEs.</p>\n<h3 id=\"cve-and-me\">CVE and me</h3>\n<p>The most common question that I get at this point is “what is a CVE, exactly?” General knowledge among Rubyists seems to be that they are &ldquo;about security problems&rdquo;, but not anything more concrete than that. Fortunately, they&rsquo;re pretty straightforward. CVE stands for “Common Vulnerabilities and Exposures”, but each CVE is ultimately just a number issued by the Mitre corporation. Many groups, from corporations to governments, have agreed to use CVE numbers issued by Mitre as the canonical reference for security vulnerabilities.</p>\n<p>Mitre assigns blocks of CVE numbers to a list of big software companies, like Oracle, Apple, Microsoft, RedHat, and others. Those companies then assign CVEs from their blocks to issues that arise in software they use or develop. The Ruby community has been assisted by RedHat’s Security Operations Group, mostly in the form of Karl Seifried, and they have provided CVE numbers for most Ruby security issues. Both Mitre and NIST (the government’s National Institute of Standards and Technology) host their own websites providing a list of all CVEs, including detailed information about the issues involved and links to any workarounds or patches that have been provided.</p>\n<h3 id=\"the-drone-of-the-swarm\">The drone of the swarm</h3>\n<p>Unfortunately, the Ruby and Rails bugs found earlier this year acted to pique the interest of security researchers in Ruby and Rails. Historically, Ruby has been a (relatively) nice place, full of pretty nice people. MINASWAN, after all. Unfortunately, a nice mindset means that we don&rsquo;t spend time attacking or defending all the things that we are making.</p>\n<p>That means we haven&rsquo;t really faced any concerted effort to find holes in our ecosystem before this year. As you can probably tell from the number of CVEs, that has changed. Security researchers saw the unusual size of the problems in Ruby and Rails, and realized there would probably be other similar problems that hadn&rsquo;t yet been found. That&rsquo;s what set off the huge number of security fixes all at once, as researchers found other low-hanging vulnerability fruit.</p>\n<p>Luckily, the major security issues have come from security researchers on the white-ish side of the line, and we’ve been able to announce fixes as most problems have become public. We can’t count on that luck to hold forever, though, and that’s why I think it’s so important to start talking about this stuff now.</p>\n<p>Rails is clearly leading the ruby community in dealing with security problems, but it&rsquo;s mostly because Rails used to be almost the entire attack surface! Rails 3 modularized everything, and that was great. Bundler added the ability to plug in any fork of any project with one line of code, and that was also great.</p>\n<p>The thing we didn&rsquo;t realize at the time is that now there are dozens, if not hundreds, of gems that can all provide security vulnerabilities to applications that use them. For example, in just 8 weeks there were security vulnerabilities fixed in new releases of: rubygems, bundler, rdoc (rdoc?! yes, rdoc), json, rexml, rack, arel, activerecord, actionpack, and activesupport.</p>\n<h3 id=\"suits-dont-like-updates\">Suits don&rsquo;t like updates</h3>\n<p>Just keeping up with the critical security releases has been hard. How are we supposed to handle problems in other people&rsquo;s code? Bosses and managers can be reluctant to dedicate time to updating, and it can be hard to explain exactly what the business case is for spending that time. On the other hand, there&rsquo;s definitely a strong cultural consensus among developers that security updates are important. The cultural consensus is great, but it rarely includes an explanation of exactly how and why it&rsquo;s important.</p>\n<p>The reasoning turns out to be fairly straightforward: time spent on security updates is insurance payments. You pay a (relatively small) amount of effort over time in order to avoid a (relatively small) chance that you&rsquo;ll be attacked. Without that investment, though, an attack could be catastrophic. Just cleaning up from a single security breach can have huge unanticipated costs. Hacks can take down your site for an impossible-to-estimate amount of time (just look at the recent hack against Apple&rsquo;s developer center), and cost months of engineering time to diagnose, analyze, and fix.</p>\n<p>And that&rsquo;s just the engineering repurcussions. If you are hacked, you&rsquo;ll also need to spend time and effort explaining what happened to your customers and trying to reassure them that it won&rsquo;t happen again. Even beyond that, nearly every state in the US has a law requiring that any compromised company inform every user, directly and individually, in writing. Some states have civil and criminal penalties if that notification is delayed. You really don&rsquo;t want to have to deal with that. An ongoing low-level cost of security maintinance is hugely preferable to a huge incident that your business might not be able to recover from.</p>\n<h3 id=\"dont-shoot-the-messenger\">Don&rsquo;t shoot the messenger</h3>\n<p>So now that I’ve (hopefully) convinced you that keeping up with the tide of security releases is important, what about our actions as developers? How should security issues be reported? Happily, there is already a best process that the security community has developed after years of trying different options. It&rsquo;s called responsible disclosure. (You can tell it’s the best option because it makes everyone involved feel upset, but protects them from each other.) Responsible disclosure fundamentally has just two parts:</p>\n<p>Disclosure just means publicly announcing the problem and anything bad that happened because of the problem. It is the part that generally pisses off the software companies. It means they can’t keep things quiet and pretend that nothing happened. Responsible disclosure, on the other hand, means that the security researchers who find the problem will contact the developers and let them know about the problem privately, so that they have a chance to fix it before it becomes public. This part pisses off most security researchers. Not only did they do some very clever work, but they’re unlikely to get paid for it, and they can’t tell anyone about it until the fix is ready and the embargo ends.</p>\n<p>Some companies are trying to improve that unhappiness by providing monetary rewards for responsible disclosures. Different companies occupy the entire spectrum on the issue of rewards. Engine Yard provides responsible disclosure guidelines, but explicitly states that monetary rewards are out of the question. Github has no stated policy for or against rewards. Facebook provides a minimum of $500 (with no maximum!) for each security vulnerability that is reported according to their documented process. Google recently paid a record $31,000 at once for three bugs in Chrome.</p>\n<h3 id=\"a-bug-walks-into-a-bar\">A bug walks into a bar…</h3>\n<p>So. Practical application time: what if you find a bug? First, congratulations! You figured out there’s a bug. Reporting that bug is a great way to make the software that you&rsquo;re using better for everyone. Once you have the bug, and you’re ready to report it, stop and think for a second. The fact that it&rsquo;s a bug obviously means something isn&rsquo;t working as intended. But beyond that, there are two important questions to ask yourself: does this bug allow you to get information or privileges you shouldn&rsquo;t have? Alternatively, does it allow you to disable the system for anyone other than yourself?</p>\n<p>If the answer to either one of those questions is yes, then you should report the bug, in private, to the security team. Maybe it&rsquo;ll be a false alarm, and that would be great. If it&rsquo;s not a false alarm, though, you probably just saved both developers and users a ton of time and hassle. Even if bugs aren&rsquo;t actually exploited by anyone, once they are public they need to be patched for all the reasons I talked about earlier. Reporting to the security team means that patches can be written, tested, and ready to go along with the announcement.</p>\n<h3 id=\"im-on-the-security-team\">I&rsquo;m on the security team</h3>\n<p>Now comes the slightly tricky part—what do you do if there is no security team? I&rsquo;ve seen gem security issues range from &ldquo;can&rsquo;t find the author&rdquo; up to &ldquo;I committed a fix to github but can&rsquo;t push to rubygems, and the person who can is out of the country&rdquo;. Definitely try to contact the author if you possibly can. If you can&rsquo;t, or they don&rsquo;t reply to you for a while (at least 48 hours, and please be patient on weekends), then you are faced with a decision.</p>\n<p>Should you drop it entirely? (This mostly only works for very small problems.) Should you file a public bug and hope the author fixes it before someone else exploits it? Or should you fix it yourself, and make the fixed version available? If you fix it yourself, reporting the bug and the fix publicly can also work. Only do that in a worst-case situation where the author has vanished, though.</p>\n<p>Now that we&rsquo;ve covered the worst case, let&rsquo;s talk about a better one. If you hear back from an author, try to work with them. If you can, offer to work on the fix together (but be prepared to be turned down). Most authors, most of the time, will be happy to work with you, fix the bug, and release the fix and announce the issue at the same time.</p>\n<p>You should avoid publicizing the issue until a fix is completed and available. Above all, have empathy with the developer! They&rsquo;re a person, too, who hopefully has a life. Treat them the way you would want to be treated if the problem was with your code.</p>\n<h3 id=\"you-are-the-security-team\">You are the security team</h3>\n<p>Before I wrap things up, I’d also like to talk about how to be responsible about security when you are the author of the code in question. How many of you have ever released a public gem? A lot of you. And if other people use that code, you might be on the receiving end of a vulnerability disclosure. Let me run through the scenarios when your own code has a vulnerability, and the communal wisdom that has emerged as the best way to deal with those situations.</p>\n<p>Easiest to handle is a responsible disclosure from a sympathetic researcher or developer. Make sure to reply to them and address their concerns. Keep them updated on your progress as you work on a fix. Check your eventual fix with them to make sure that it fully addresses the issue that they found.</p>\n<p>The other straightforward case is a vulnerability that you only discover because it’s already being exploited by malicious attackers. If that happens, good form is to fix it as quickly as possible. If fixing it will take time, and especially if there is a possible workaround, publicly announce the issue even before the fix is ready. If additional publicity will cause more damage, don’t announce until you also have a fix.</p>\n<p>Now the complicated case: pushy, impatient security researchers who want publicity to show how awesome they are, but follow the letter of the responsible disclosure law so that they stay legally in the clear. Keeping those kinds of people happy is the truly hard work of being responsible about security as an author. While that does sound kind of scary, it’s completely possible to manage your project so that they help you instead of hurting you.</p>\n<h3 id=\"help-them-help-you\">Help them help you</h3>\n<p>As a project author or manager, make sure that the contact information in your gemspec is accurate. Add your email address to your github profile. If the project is big enough to have a team, set up a security address and put trusted team members on it. If you have a security address, create a PGP key and a disclosure policy. Put them in a static page, even if it’s just a file in your git repo. It’s only a few minutes of work, and can save you hours or days of stress later on! Respond to security reports within 24-48 hours, even if it’s just to acknowledge the email. Follow up with status reports, every 24-48 hours, even if the status report just says “nothing to report yet, but I’m working on it”. Finally, plan on crediting the reporter when you announce the issue and your fix (although you should of course check with them before doing so).</p>\n<p>All those steps combined are not a huge amount of work. Honestly, they’re only a few minutes for most projects, even relatively big ones. It may not seem like a big deal if you’ve never had a security issue before, but just ask the Rails team about how things were before! Implementing those simple steps can save you all kinds of unpleasantness.</p>\n<p><small>This is a cross-post from the <a href=\"http://blog.cloudcity.io/2013/08/22/security-is-hard-but-we-cant-go-shopping/\">Cloud City Development Blog</a>, where you can <a href=\"http://cloudcity.io/#contact\">hire us</a> to help you build web applications.</p>\n",
				"content_text": "\n![hard](https://indirect.micro.blog/uploads/2025/a4aaba309a.jpg)\n\n<small>This post was also given as a conference talk, originally at [Ruby on Ales 2013][roa] ([slides][roas], [video][roav]), [RailsConf 2013][rc] ([slides][rcs], [video][rcv]) and [RubyKaigi 2013][rk] ([slides][rks], [video][rkv]), then updated for [Madison+Ruby 2014][mr] ([slides][mrs]) and [RedDot RubyConf 2015][rd] ([slides][rds]).</small>\n\n[roa]: http://ruby.onales.com\n[roas]: https://speakerdeck.com/indirect/security-is-hard-but-we-cant-go-shopping-ruby-on-ales-2013\n[roav]: http://confreaks.com/videos/2317-roa2013-security-is-hard-but-we-cant-go-shopping\n[rc]: http://www.railsconf.com\n[rcs]: https://speakerdeck.com/indirect/security-is-hard-but-we-cant-go-shopping-railsconf-2013\n[rcv]: http://www.confreaks.com/videos/2430-railsconf2013-security-is-hard-but-we-cant-go-shopping\n[rk]: http://rubykaigi.org\n[rks]: https://speakerdeck.com/indirect/security-is-hard-but-we-cant-go-shopping-rubykaigi-2013\n[rkv]: http://www.ustream.tv/recorded/33562443\n[mr]: http://madisonpl.us/ruby\n[mrs]: https://speakerdeck.com/indirect/security-is-hard-but-we-cant-go-shopping-madison-ruby-2014\n[rd]: http://www.reddotrubyconf.com\n[rds]: https://speakerdeck.com/indirect/security-is-hard-reddotrubyconf-2015\n\n### Uhoh, security?\n\nSecurity is a hard topic. It's an especially hard topic in the Ruby community, where the security situation has historically been so great that hardly anyone has had to care about it. You may not know this, depending on how long you’ve been a rubyist, but Ruby security issues usually only come up once or maybe twice per year. They're usually relatively benign, as those things go, so everyone updates as soon as it's convenient, and life goes on.\n\nThings changed pretty dramatically this year. In rapid succession, there was a flurry of security releases in both Ruby and Rails. Almost every release fixed a huge vulnerability, and security updates were suddenly extremely important.\nThere were more Rails security releases by May of 2013 than there had been in 2011 and 2012 _combined_. This year, Rails has issued 12 CVEs, and Ruby has issued 5 CVEs.\n\n### CVE and me\n\nThe most common question that I get at this point is “what is a CVE, exactly?” General knowledge among Rubyists seems to be that they are \"about security problems\", but not anything more concrete than that. Fortunately, they're pretty straightforward. CVE stands for “Common Vulnerabilities and Exposures”, but each CVE is ultimately just a number issued by the Mitre corporation. Many groups, from corporations to governments, have agreed to use CVE numbers issued by Mitre as the canonical reference for security vulnerabilities.\n\nMitre assigns blocks of CVE numbers to a list of big software companies, like Oracle, Apple, Microsoft, RedHat, and others. Those companies then assign CVEs from their blocks to issues that arise in software they use or develop. The Ruby community has been assisted by RedHat’s Security Operations Group, mostly in the form of Karl Seifried, and they have provided CVE numbers for most Ruby security issues. Both Mitre and NIST (the government’s National Institute of Standards and Technology) host their own websites providing a list of all CVEs, including detailed information about the issues involved and links to any workarounds or patches that have been provided.\n\n### The drone of the swarm\n\nUnfortunately, the Ruby and Rails bugs found earlier this year acted to pique the interest of security researchers in Ruby and Rails. Historically, Ruby has been a (relatively) nice place, full of pretty nice people. MINASWAN, after all. Unfortunately, a nice mindset means that we don't spend time attacking or defending all the things that we are making.\n\nThat means we haven't really faced any concerted effort to find holes in our ecosystem before this year. As you can probably tell from the number of CVEs, that has changed. Security researchers saw the unusual size of the problems in Ruby and Rails, and realized there would probably be other similar problems that hadn't yet been found. That's what set off the huge number of security fixes all at once, as researchers found other low-hanging vulnerability fruit.\n\nLuckily, the major security issues have come from security researchers on the white-ish side of the line, and we’ve been able to announce fixes as most problems have become public. We can’t count on that luck to hold forever, though, and that’s why I think it’s so important to start talking about this stuff now.\n\nRails is clearly leading the ruby community in dealing with security problems, but it's mostly because Rails used to be almost the entire attack surface! Rails 3 modularized everything, and that was great. Bundler added the ability to plug in any fork of any project with one line of code, and that was also great.\n\nThe thing we didn't realize at the time is that now there are dozens, if not hundreds, of gems that can all provide security vulnerabilities to applications that use them. For example, in just 8 weeks there were security vulnerabilities fixed in new releases of: rubygems, bundler, rdoc (rdoc?! yes, rdoc), json, rexml, rack, arel, activerecord, actionpack, and activesupport. \n\n### Suits don't like updates\n\nJust keeping up with the critical security releases has been hard. How are we supposed to handle problems in other people's code? Bosses and managers can be reluctant to dedicate time to updating, and it can be hard to explain exactly what the business case is for spending that time. On the other hand, there's definitely a strong cultural consensus among developers that security updates are important. The cultural consensus is great, but it rarely includes an explanation of exactly how and why it's important.\n\nThe reasoning turns out to be fairly straightforward: time spent on security updates is insurance payments. You pay a (relatively small) amount of effort over time in order to avoid a (relatively small) chance that you'll be attacked. Without that investment, though, an attack could be catastrophic. Just cleaning up from a single security breach can have huge unanticipated costs. Hacks can take down your site for an impossible-to-estimate amount of time (just look at the recent hack against Apple's developer center), and cost months of engineering time to diagnose, analyze, and fix.\n\nAnd that's just the engineering repurcussions. If you are hacked, you'll also need to spend time and effort explaining what happened to your customers and trying to reassure them that it won't happen again. Even beyond that, nearly every state in the US has a law requiring that any compromised company inform every user, directly and individually, in writing. Some states have civil and criminal penalties if that notification is delayed. You really don't want to have to deal with that. An ongoing low-level cost of security maintinance is hugely preferable to a huge incident that your business might not be able to recover from.\n\n### Don't shoot the messenger\n\nSo now that I’ve (hopefully) convinced you that keeping up with the tide of security releases is important, what about our actions as developers? How should security issues be reported? Happily, there is already a best process that the security community has developed after years of trying different options. It's called responsible disclosure. (You can tell it’s the best option because it makes everyone involved feel upset, but protects them from each other.) Responsible disclosure fundamentally has just two parts:\n\nDisclosure just means publicly announcing the problem and anything bad that happened because of the problem. It is the part that generally pisses off the software companies. It means they can’t keep things quiet and pretend that nothing happened. Responsible disclosure, on the other hand, means that the security researchers who find the problem will contact the developers and let them know about the problem privately, so that they have a chance to fix it before it becomes public. This part pisses off most security researchers. Not only did they do some very clever work, but they’re unlikely to get paid for it, and they can’t tell anyone about it until the fix is ready and the embargo ends.\n\nSome companies are trying to improve that unhappiness by providing monetary rewards for responsible disclosures. Different companies occupy the entire spectrum on the issue of rewards. Engine Yard provides responsible disclosure guidelines, but explicitly states that monetary rewards are out of the question. Github has no stated policy for or against rewards. Facebook provides a minimum of $500 (with no maximum!) for each security vulnerability that is reported according to their documented process. Google recently paid a record $31,000 at once for three bugs in Chrome.\n\n### A bug walks into a bar…\n\nSo. Practical application time: what if you find a bug? First, congratulations! You figured out there’s a bug. Reporting that bug is a great way to make the software that you're using better for everyone. Once you have the bug, and you’re ready to report it, stop and think for a second. The fact that it's a bug obviously means something isn't working as intended. But beyond that, there are two important questions to ask yourself: does this bug allow you to get information or privileges you shouldn't have? Alternatively, does it allow you to disable the system for anyone other than yourself?\n\nIf the answer to either one of those questions is yes, then you should report the bug, in private, to the security team. Maybe it'll be a false alarm, and that would be great. If it's not a false alarm, though, you probably just saved both developers and users a ton of time and hassle. Even if bugs aren't actually exploited by anyone, once they are public they need to be patched for all the reasons I talked about earlier. Reporting to the security team means that patches can be written, tested, and ready to go along with the announcement.\n\n### I'm on the security team\n\nNow comes the slightly tricky part—what do you do if there is no security team? I've seen gem security issues range from \"can't find the author\" up to \"I committed a fix to github but can't push to rubygems, and the person who can is out of the country\". Definitely try to contact the author if you possibly can. If you can't, or they don't reply to you for a while (at least 48 hours, and please be patient on weekends), then you are faced with a decision.\n\nShould you drop it entirely? (This mostly only works for very small problems.) Should you file a public bug and hope the author fixes it before someone else exploits it? Or should you fix it yourself, and make the fixed version available? If you fix it yourself, reporting the bug and the fix publicly can also work. Only do that in a worst-case situation where the author has vanished, though.\n\nNow that we've covered the worst case, let's talk about a better one. If you hear back from an author, try to work with them. If you can, offer to work on the fix together (but be prepared to be turned down). Most authors, most of the time, will be happy to work with you, fix the bug, and release the fix and announce the issue at the same time.\n\nYou should avoid publicizing the issue until a fix is completed and available. Above all, have empathy with the developer! They're a person, too, who hopefully has a life. Treat them the way you would want to be treated if the problem was with your code.\n\n### You are the security team\n\nBefore I wrap things up, I’d also like to talk about how to be responsible about security when you are the author of the code in question. How many of you have ever released a public gem? A lot of you. And if other people use that code, you might be on the receiving end of a vulnerability disclosure. Let me run through the scenarios when your own code has a vulnerability, and the communal wisdom that has emerged as the best way to deal with those situations.\n\nEasiest to handle is a responsible disclosure from a sympathetic researcher or developer. Make sure to reply to them and address their concerns. Keep them updated on your progress as you work on a fix. Check your eventual fix with them to make sure that it fully addresses the issue that they found.\n\nThe other straightforward case is a vulnerability that you only discover because it’s already being exploited by malicious attackers. If that happens, good form is to fix it as quickly as possible. If fixing it will take time, and especially if there is a possible workaround, publicly announce the issue even before the fix is ready. If additional publicity will cause more damage, don’t announce until you also have a fix.\n\nNow the complicated case: pushy, impatient security researchers who want publicity to show how awesome they are, but follow the letter of the responsible disclosure law so that they stay legally in the clear. Keeping those kinds of people happy is the truly hard work of being responsible about security as an author. While that does sound kind of scary, it’s completely possible to manage your project so that they help you instead of hurting you.\n\n### Help them help you\n\nAs a project author or manager, make sure that the contact information in your gemspec is accurate. Add your email address to your github profile. If the project is big enough to have a team, set up a security address and put trusted team members on it. If you have a security address, create a PGP key and a disclosure policy. Put them in a static page, even if it’s just a file in your git repo. It’s only a few minutes of work, and can save you hours or days of stress later on! Respond to security reports within 24-48 hours, even if it’s just to acknowledge the email. Follow up with status reports, every 24-48 hours, even if the status report just says “nothing to report yet, but I’m working on it”. Finally, plan on crediting the reporter when you announce the issue and your fix (although you should of course check with them before doing so).\n\nAll those steps combined are not a huge amount of work. Honestly, they’re only a few minutes for most projects, even relatively big ones. It may not seem like a big deal if you’ve never had a security issue before, but just ask the Rails team about how things were before! Implementing those simple steps can save you all kinds of unpleasantness.\n\n<small>This is a cross-post from the [Cloud City Development Blog](http://blog.cloudcity.io/2013/08/22/security-is-hard-but-we-cant-go-shopping/), where you can [hire us](http://cloudcity.io/#contact) to help you build web applications.\n",
				"date_published": "2013-08-22T00:00:00-08:00",
				"url": "https://andre.arko.net/2013/08/22/security-is-hard-but-we/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2013/06/12/cloud-city-citizen/",
				"title": "Cloud City Citizen",
				"content_html": "<p><img src=\"https://indirect.micro.blog/uploads/2025/4b6d9c6156.jpg\" alt=\"Cloud City Logo\"></p>\n<p>This week I&rsquo;m joining the team at <a href=\"http://www.cloudcitydevelopment.com\">Cloud City Development</a>. I&rsquo;ve known <a href=\"https://twitter.com/timocratic\">Tim Connor</a> for what seems like forever, and we&rsquo;ve collaborated on some great projects recently. I&rsquo;d love to continue that, so here&rsquo;s to great projects yet to come! 🎉</p>\n<p>If you&rsquo;re looking for someone to help build your website or webapp, I strongly recommend <a href=\"http://www.cloudcitydevelopment.com/#contact\">hiring us</a>. 😇</p>\n",
				"content_text": "![Cloud City Logo](https://indirect.micro.blog/uploads/2025/4b6d9c6156.jpg)\n\nThis week I'm joining the team at [Cloud City Development](http://www.cloudcitydevelopment.com). I've known [Tim Connor](https://twitter.com/timocratic) for what seems like forever, and we've collaborated on some great projects recently. I'd love to continue that, so here's to great projects yet to come! 🎉\n\nIf you're looking for someone to help build your website or webapp, I strongly recommend [hiring us](http://www.cloudcitydevelopment.com/#contact). 😇\n",
				"date_published": "2013-06-12T00:00:00-08:00",
				"url": "https://andre.arko.net/2013/06/12/cloud-city-citizen/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2013/05/31/illegal-anonymous-lifetime/",
				"title": "Illegal anonymous lifetime",
				"content_html": "<p>So I’ve been playing with <a href=\"http://rust-lang.org\">Rust</a> lately. It’s pretty fun, in a “let’s learn about four types of pointers” kind of way.</p>\n<p>I managed to work my way through <a href=\"http://www.rustforrubyists.com\">Rust for Rubyists</a>, <a href=\"http://static.rust-lang.org/doc/tutorial.html\">the Rust language tutorial</a>, <a href=\"http://www.darkcoding.net/software/rust-what-i-learnt-so-far\">What I Learnt So Far</a>, and big chunks of the <a href=\"http://static.rust-lang.org/doc/rust.html\">Reference Guide</a>, <a href=\"http://doc.rust-lang.org/doc/core/index.html\">core docs</a>, and <a href=\"http://doc.rust-lang.org/doc/std/index.html\">stdlib docs</a>. (If you’re interested in Rust, that turns out to be a pretty decent order for starting out.)</p>\n<p>I wound up having to compile Rust from the <code>incoming</code> branch so that all the functions that I wanted to use would be available, but it turns out that telling homebrew to compile from a branch other than <code>master</code> just requires a single additional hash argument supplied during <code>brew edit rust</code>.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-diff\" data-lang=\"diff\"><span style=\"display:flex;\"><span><span style=\"color:#f92672\">-  head &#39;https://github.com/mozilla/rust.git&#39;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#f92672\"></span><span style=\"color:#a6e22e\">+  head &#39;https://github.com/mozilla/rust.git&#39;, :branch =&gt; &#34;incoming&#34;\n</span></span></span></code></pre></div><p>Anyway, this blog post is about anonymous lifetimes, so I should probably get to that. If you’re just starting off, and you think you’d like to create your own traits, chances are high that you wrote something like this:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-rust\" data-lang=\"rust\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">trait</span> MyTrait {\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">fn</span> <span style=\"color:#a6e22e\">do_stuff</span>(<span style=\"color:#f92672\">&amp;</span>self) -&gt; ();\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><p>Then you tried to write an implementation, and it probably looked like this:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-rust\" data-lang=\"rust\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">impl</span> MyTrait <span style=\"color:#66d9ef\">for</span> <span style=\"color:#f92672\">&amp;</span><span style=\"color:#66d9ef\">str</span> {\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">fn</span> <span style=\"color:#a6e22e\">do_stuff</span>(<span style=\"color:#f92672\">&amp;</span>self) -&gt; () { () };\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><p>Turns out that that isn’t possible. Bummer. The reason it’s not possible is pretty interesting, though. When you try to compile that implementation, rustc will give you a very specific error, namely<code>Illegal anonymous lifetime: anonymous lifetimes are not permitted here</code>. I eventually found the <a href=\"http://static.rust-lang.org/doc/tutorial-borrowed-ptr.html\">borrowed pointer tutorial</a> to explain what that error actually means.</p>\n<p>Anonymous lifetimes are what rustc calls borrowed pointers (like <code>&amp;str</code>) that will be valid for an indeterminate amount of time. Since rustc has no idea how long that pointer will need to be valid, it may or may not be garbage collected, go out of scope, get invalidated by an assignment, or anything else that would result in a dangling pointer. Rust doesn’t let you compile code that it knows could allow a dangling pointer, so it errors out there.</p>\n<p>I naively thought that might mean I need to write two separate implementations, one for <code>~str</code> and one for <code>@str</code>, but that turns out to also be no good. Even if you write both of those implementations, when you try to call the method on your <code>str</code> object, you’ll get the error <code>type `&amp;str` does not implement any method in scope named `do_stuff`</code>. And then I was back where I started two paragraphs ago, trying to implement for <code>&amp;str</code>, with an anonymous lifetime error.</p>\n<p>The fix is surprisingly simple. All you have to do is tell rustc how long the borrowed pointer will need to last. The easiest way to do that is to say that the borrowed pointer will need to last for the same amount of time as the pointer to <code>&amp;self</code>. The syntax to do that has a lot of punctuation, but isn’t too bad once you know what’s going on.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-rust\" data-lang=\"rust\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">impl</span><span style=\"color:#f92672\">&lt;</span><span style=\"color:#a6e22e\">&#39;self</span><span style=\"color:#f92672\">&gt;</span> <span style=\"color:#66d9ef\">for</span> MyTrait <span style=\"color:#f92672\">&amp;</span><span style=\"color:#a6e22e\">&#39;self</span> <span style=\"color:#66d9ef\">str</span> {\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">fn</span> <span style=\"color:#a6e22e\">do_stuff</span>(<span style=\"color:#f92672\">&amp;</span>self) -&gt; () { () }\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><p>The angle brackets apply to the <code>impl</code> keyword, and can contain lifetime declarations or types. In this case, it simply tags the lifetime of the implementation’s functions with the name “self”. Later, the pointer <code>&amp;'self str</code> identifies the object that will be <code>&amp;self</code> inside this implementation block as also having the lifetime named “self”.</p>\n<p>All the pointers that have been tagged with the same name are said to have “intersecting lifetimes”. In practice, that means that Rust needs to keep pointers that are borrowed by this implementation alive for as long as the pointer to <code>&amp;self</code> is alive.</p>\n<p>And there you have it: in the end, rustc just needs to know how long the pointers that are being thrown around need to last, but the error messages might lead you around in a circle until you know that.</p>\n",
				"content_text": "So I’ve been playing with [Rust](http://rust-lang.org) lately. It’s pretty fun, in a “let’s learn about four types of pointers” kind of way.\n\nI managed to work my way through [Rust for Rubyists](http://www.rustforrubyists.com), [the Rust language tutorial](http://static.rust-lang.org/doc/tutorial.html), [What I Learnt So Far](http://www.darkcoding.net/software/rust-what-i-learnt-so-far), and big chunks of the [Reference Guide](http://static.rust-lang.org/doc/rust.html), [core docs](http://doc.rust-lang.org/doc/core/index.html), and [stdlib docs](http://doc.rust-lang.org/doc/std/index.html). (If you’re interested in Rust, that turns out to be a pretty decent order for starting out.)\n\nI wound up having to compile Rust from the `incoming` branch so that all the functions that I wanted to use would be available, but it turns out that telling homebrew to compile from a branch other than `master` just requires a single additional hash argument supplied during `brew edit rust`.\n\n```diff\n-  head 'https://github.com/mozilla/rust.git'\n+  head 'https://github.com/mozilla/rust.git', :branch => \"incoming\"\n```\n\nAnyway, this blog post is about anonymous lifetimes, so I should probably get to that. If you’re just starting off, and you think you’d like to create your own traits, chances are high that you wrote something like this:\n\n```rust\ntrait MyTrait {\n  fn do_stuff(&self) -> ();\n}\n```\n\nThen you tried to write an implementation, and it probably looked like this:\n\n```rust\nimpl MyTrait for &str {\n  fn do_stuff(&self) -> () { () };\n}\n```\n\nTurns out that that isn’t possible. Bummer. The reason it’s not possible is pretty interesting, though. When you try to compile that implementation, rustc will give you a very specific error, namely`Illegal anonymous lifetime: anonymous lifetimes are not permitted here`. I eventually found the [borrowed pointer tutorial](http://static.rust-lang.org/doc/tutorial-borrowed-ptr.html) to explain what that error actually means.\n\nAnonymous lifetimes are what rustc calls borrowed pointers (like `&str`) that will be valid for an indeterminate amount of time. Since rustc has no idea how long that pointer will need to be valid, it may or may not be garbage collected, go out of scope, get invalidated by an assignment, or anything else that would result in a dangling pointer. Rust doesn’t let you compile code that it knows could allow a dangling pointer, so it errors out there.\n\nI naively thought that might mean I need to write two separate implementations, one for `~str` and one for `@str`, but that turns out to also be no good. Even if you write both of those implementations, when you try to call the method on your `str` object, you’ll get the error `` type `&str` does not implement any method in scope named `do_stuff` ``. And then I was back where I started two paragraphs ago, trying to implement for `&str`, with an anonymous lifetime error.\n\nThe fix is surprisingly simple. All you have to do is tell rustc how long the borrowed pointer will need to last. The easiest way to do that is to say that the borrowed pointer will need to last for the same amount of time as the pointer to `&self`. The syntax to do that has a lot of punctuation, but isn’t too bad once you know what’s going on.\n\n```rust\nimpl<'self> for MyTrait &'self str {\n  fn do_stuff(&self) -> () { () }\n}\n```\n\nThe angle brackets apply to the `impl` keyword, and can contain lifetime declarations or types. In this case, it simply tags the lifetime of the implementation’s functions with the name “self”. Later, the pointer `&'self str` identifies the object that will be `&self` inside this implementation block as also having the lifetime named “self”.\n\nAll the pointers that have been tagged with the same name are said to have “intersecting lifetimes”. In practice, that means that Rust needs to keep pointers that are borrowed by this implementation alive for as long as the pointer to `&self` is alive.\n\nAnd there you have it: in the end, rustc just needs to know how long the pointers that are being thrown around need to last, but the error messages might lead you around in a circle until you know that.\n",
				"date_published": "2013-05-31T00:00:00-08:00",
				"url": "https://andre.arko.net/2013/05/31/illegal-anonymous-lifetime/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2013/05/31/ruby-central-grant/",
				"title": "Ruby Central Grant",
				"content_html": "<p>It&rsquo;s been a long time since I first started working on <a href=\"http://gembundler.com\">Bundler</a>, alongside Carl and Yehuda. Since then, the Bundler team has pushed three big feature releases, and I&rsquo;ve taken over as project lead. One of my major goals as lead has been to cooperate with both the Rubygems and rubygems.org teams to make Bundler and Rubygems more useful to the Ruby community at large.</p>\n<p>As part of those efforts, I&rsquo;ve worked on Bundler and other Ruby open source during my free time (and some paid time) for the last few years. More recently, I&rsquo;ve been <a href=\"/2013/05/12/deathmatch-bundler-vs-rubygemsorg\">recruiting volunteers</a> to help with Bundler and upcoming improvements to the Bundler API and overall Rubygems ecosystem infrastructure.</p>\n<p>I&rsquo;m extremely pleased (and excited) to announce that Ruby Central has funded a grant that will allow me to work on both Bundler and Rubygems for the next several months. To fully express my feelings when I announced the grant at my RubyKaigi talk about security, I (naturally) resorted to emoji.</p>\n<p><img src=\"https://indirect.micro.blog/uploads/2025/64bba2059e.jpg\" alt=\"ruby central loves bundler and rubygems. and sends them presents. presents of money.\"></p>\n<p>Planned improvements include a new index format that will speed up installing gems, improvements to the gem distribution infrastructure, and other things still in the planning stages. The grant also means I&rsquo;ll have some more time to pair with and help out volunteers. If you have some free time that you’d like to contribute, let me know! We like supporting new contributors to open source, and we would love to have your help.</p>\n<p>I&rsquo;ll be posting updates here and on the <a href=\"http://blog.rubygems.org\">Rubygems blog</a> as interesting bits are completed. It should be pretty great, and I&rsquo;m definitely looking forward to it.</p>\n",
				"content_text": "It's been a long time since I first started working on [Bundler](http://gembundler.com), alongside Carl and Yehuda. Since then, the Bundler team has pushed three big feature releases, and I've taken over as project lead. One of my major goals as lead has been to cooperate with both the Rubygems and rubygems.org teams to make Bundler and Rubygems more useful to the Ruby community at large.\n\nAs part of those efforts, I've worked on Bundler and other Ruby open source during my free time (and some paid time) for the last few years. More recently, I've been [recruiting volunteers](/2013/05/12/deathmatch-bundler-vs-rubygemsorg) to help with Bundler and upcoming improvements to the Bundler API and overall Rubygems ecosystem infrastructure.\n\nI'm extremely pleased (and excited) to announce that Ruby Central has funded a grant that will allow me to work on both Bundler and Rubygems for the next several months. To fully express my feelings when I announced the grant at my RubyKaigi talk about security, I (naturally) resorted to emoji.\n\n![ruby central loves bundler and rubygems. and sends them presents. presents of money.](https://indirect.micro.blog/uploads/2025/64bba2059e.jpg)\n\nPlanned improvements include a new index format that will speed up installing gems, improvements to the gem distribution infrastructure, and other things still in the planning stages. The grant also means I'll have some more time to pair with and help out volunteers. If you have some free time that you’d like to contribute, let me know! We like supporting new contributors to open source, and we would love to have your help.\n \nI'll be posting updates here and on the [Rubygems blog](http://blog.rubygems.org) as interesting bits are completed. It should be pretty great, and I'm definitely looking forward to it.\n",
				"date_published": "2013-05-31T00:00:00-08:00",
				"url": "https://andre.arko.net/2013/05/31/ruby-central-grant/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2013/05/12/deathmatch-bundler-vs-rubygemsorg/",
				"title": "Deathmatch: Bundler vs. Rubygems.org",
				"content_html": "<p class=\"aside\">This talk was given at <a href=\"http://2013.scottishrubyconference.com\">Scottish Ruby Conference 2013</a>, and the slides are <a href=\"https://speakerdeck.com/indirect/deathmatch-bundler-vs-rubygems-dot-org\">available on SpeakerDeck</a>, as well as <a href=\"/2013/05/12/deathmatch-bundler-vs-rubygemsorg/Deathmatch-Bundler-vs-Rubygems.pdf\">available for download as a PDF</a>. This post doesn't correlate exactly to the slides, but it has the same content.</p>\n<script async class=\"speakerdeck-embed\" data-id=\"57fa6d609d4901309d516e9dd498db92\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n<p>This talk is a story of hubris, disaster, overcoming adversity, and accidentally launching a distributed denial of service attack against rubygems.org. Sorry about that, everybody. But now, the story of how that came to pass, and things came back better than ever:</p>\n<h3 id=\"the-setup\">The setup</h3>\n<p>It all begins with the release Bundler 1.0, on August 30 2010. The great thing about Bundler was that you could run one command to install all the gems your application needed! The terrible thing about that one command was that it took… a really long time, unfortunately.</p>\n<p>In just a few short weeks, developers were already tweeting copies of the XKCD “compiling” comic, but with “compiling” replaced by “bundling”. Rubyists seem to have really high expectations as far as responsiveness goes (except when starting up Rails applications, I guess).</p>\n<p>Anyway, it was unacceptable that Bundler was slow. Slowness makes us sad. We can do better! We will do better! The estimable Nick Quaranto, creator and maintainer of the Rubygems.org Rails app, proposed an idea: instead of downloading the entire index of every gem that has ever been pushed, why not just ask for only the gems that are in the Gemfile? Just a few weeks of cooperative work later, the Bundler API was born. Rubygems.org provided an API that returned a much smaller set of dependency information, and <code>bundle install</code> could run in as little as a few seconds.</p>\n<h3 id=\"the-disaster\">The disaster</h3>\n<p>What we didn’t realize at the time, though, was that all the dynamic processing required significantly more CPU cycles than the rubygems.org server was accustomed to providing. Aside from the Bundler API, rubygems.org really only does two things: accept gem pushes, and update the database accordingly, and send 302 redirects to S3 for requests that are trying to download the actual .gem files. Neither one of those activities is particularly server-intensive, since redirects are cheap and pushes are infrequent.</p>\n<p>The Bundler API had to do a fair amount of work: looking up every requested gem in Postgres, then marshalling the arrays of output, and then caching the response into Redis before sending it. As the months wore on, more and more people upgraded to Bundler 1.1, and more and more people used the Bundler API every day. Back in 2012, there was just a single Rackspace VPS, running Postgres, Unicorn, and Redis. By October, we had reached the breaking point for that server, and it went down hard.</p>\n<p>It was impossible to install gems for almost an entire day while the server was down, and getting it back up proved to be a challenge, since Bundler requests were still overwhelming everything. In the end, the only way to get rubygems.org back was to disable the Bundler API.</p>\n<p>Fortunately, disabling the Bundler API didn’t actually stop anything from working — without the API, Bundler falls back on the old behaviour of fetching the entire source index. Unfortunately, this meant that all bundle installs were back to as slow as they had been on Bundler 1.0.</p>\n<h3 id=\"the-replacement\">The Replacement</h3>\n<p>Since hosting the Bundler API on the limited rubygems.org box was very impractical, the Bundler team decided to reimplement the Bundler API and host it by itself as a standalone app. Heroku offered to provide hosting, and we started to build a simple Sinatra app that spoke the same Bundler API that Rubygems.org had previously provided.</p>\n<p>After some trial and error, we had a working API, but now we were worried about how it would perform. Could it support every rubyist in the whole world?  How did it compare to the old API? Did we need more servers to handle all the requests? Had we overdone it, and we were wastefully running servers that we didn’t really need?</p>\n<p>To answer all of those questions we needed data — the only way to know if it was slow would be to measure it, and the only way to k ow if it was working would be to monitor it. So, we set up logging and instrumentation services. Librato and Papertrail have both gracefully donated accounts to the Bundler team, and we’ve been using them extensively to get a better idea of how the service is running.</p>\n<p>When combined with Paprtrail alerts and the Metriks gem, Librato provides some fancy and beautiful graphs that allows us to see what’s going on. We used Librato to track a lot of things: response times, database queries, request complexity, traffic, and server load. We use Papertrail to send errors from the Heroku infrastructure to Librato, since we don’t have a way to track those from inside our Ruby app.</p>\n<h3 id=\"discoveries\">Discoveries</h3>\n<p>As we started to track all of these things, which we hadn’t known anything about before, we discovered a few interesting trends. We knew, from the Rubygems.org team, that Bundler API requests typically took 2 seconds, and could take up to 20 seconds. As we instrumented everything on our new API, and tried to make it faster, we discovered some things that surprised us.</p>\n<p><em>Upgrading our Postgres server massively sped up response times</em></p>\n<p>In retrospect, that seems super obvious, right? We totally weren’t expecting this to be the case, becuase the dataset is so small. The entire rubygems.org database is only about 40MB after it’s been gzipped, and it’s around 250MB decompressed. Our initial setup used a small instance of Heroku Postgres, and seemed to be pretty fast. Queries returned in 5-15ms, and the median response time was around 80ms. 95th percentile response times were around 600ms.</p>\n<p>While the median was good, the 95th percentile was slow enough that we wanted to see if we could do better. One of the things we tried, thinking that it surely wouldn’t make any difference, was to switch to a much bigger Postgres than our dataset actually needed. Shockingly, it made a huge difference. Queries started coming back in 3-5ms. Median response time dropped to around 20ms, a 4x improvement. The 95th percentile responses dropped to about 225ms.</p>\n<p><em>Caching responses into Redis slowed down response times!</em></p>\n<p>Caching is supposed to make everything faster, right? Not so much, unfortunately. We discovered that the response caching that rubygems.org had been doing wasn’t actually helpful. The list of gems in a particular gemfile is almost never exactly the same as another gemfile, and new gems were being pushed every 2-3 minutes. As a result, the server was forced to calculate the responses for more or less every single request, and then spend extra time caching the result that it was never going to be able to use again!</p>\n<h3 id=\"enough-or-too-much\">Enough or too much?</h3>\n<p>Once we had worked out those somewhat surprising performance issues, we had a new question. How many Heroku dynos do we need to run to serve all the requests that we&rsquo;re getting? We could have simply run dozens of dynos just to be sure, but that would have been extremely rude to Heroku after they were gracious enough to donate hosting.</p>\n<p>We started to track the &ldquo;dynos in use&rdquo; header, provided by Heroku. Using that header, we were able to graph dyno usage over time, see the correlation between requests and dynos, and use enough dynos to serve our peak traffic successfully. While that worked well, there was recent public outcry over the inefficiencies of their public routing system for slow single-threaded apps. Bundler API was one of those single threaded-systems, and that caused us to make another surprising discovery that I’ll talk about in a minute.</p>\n<p>Back to the number of dynos we need — once Heroku removed that header when they reworked their router, we were at something of a loss for a while. Eventually, though, we realized that the router will tell us every time that we’re unable to serve a request in the Heroku logs. Once we realized that, it was straightforward to set up a Papertrail search that sends every hit to Librato.</p>\n<p>Now we can graph all the response types on a single graph, including success, not found, exception, and routing failures. If there aren’t any routing failures, we can be reasonably sure that we have a sufficient number of dynos. To make sure that things stay that way, we’ve set up Librato to send an alert to Pager Duty if there are persistent routing failures. When we see that alert, we’ll know that we’ve overrun our current crop of dynos, and need to add some more.</p>\n<h3 id=\"what-server-to-serve\">What server to serve</h3>\n<p>So, I mentioned that Heroku’s router isn’t that great for app servers that can only serve one request at a time. There’s been a ton of discussion around this topic recently, including posts on RapGenius and Heroku’s own blog. In case you missed it, the tl;dr is that random routing across all dynos really sucks if those dynos could be stuck serving a single very long request. When your app works like that, you need hundreds of dynos just to be able to serve the majority of requests that would (in an optimal world) only require 10 dynos.</p>\n<p>Bundler API had initially been deployed using Puma, the recently released server written by Evan Phoenix. It supports multithreaded app servers, and we were using that ability to great effect. Unfortunately, every day or two, one of the Puma servers would get itself into a deadlock and never respond to another request, even though it would keep accepting requests the entire time. In the end, we tried running the app on Thin, and it worked, so we moved on to other things.</p>\n<p>That all changed when Heroku talked about the routing issues, though. We benchmarked Bundler API from the outside world, and discovered that although the <em>internal</em> response times had a median of 20ms, external requests had a median response time of 400ms or higher, with something like 5% of requests timing out altogether. That was absolutely not acceptable, so we tried other app server concurrency options.</p>\n<p>In the end, we went with Unicorn, running 16 child processes per dyno. That provides the same concurrency on each dyno as we had with Puma, but without the occasional deadlocks. On the new unicorn-based concurrency system, the number of timed out connections dropped dramatically, and we were able to pay attention to some strange new bug reports that had started cropping up all over the place.</p>\n<h3 id=\"synchronizing-in-the-cloud-gooooood-luck\">Synchronizing in the cloud (gooooood luck)</h3>\n<p>After investigating reports of new and completely unknown bugs, we realized we had a pretty big problem: staying synchronized with rubygems.org as developers pushed and yanked gems over time. As part of the original system, we’d built a background process that would fetch the full index from rubygems.org and update our database as necessary, adding and removing gems that had changed.</p>\n<p>Unfortunately, it was a very slow process (taking upwards of 20 minutes), and tended to crash when it used more memory than Heroku was willing to give it. After many unhappy users couldn’t use Bundler to install the gems that they had pushed, we managed to settle on a solution: Rubygems.org now sends Bundler an HTTP request each time a gem is pushed, and Bundler immediately adds that gem to the Bundler API database.</p>\n<p>HTTP requests get lost sometimes, though, and so the Bundler API also runs the full updating task, albeit less frequently. By comibining both the webhooks and the background worker, the system is eventually consistent, but fully up to date as often as it can be without causing unbearable pressure on the rubygems.org servers. It is never more than an hour out of date, and most of the time updates happen within 5 seconds. Hopefully, everyone is happy.</p>\n<h3 id=\"we-can-do-better\">We can do better</h3>\n<p>At the end of all the monitoring, optimization, and tuning, the results were fantastic: responses are handled very quickly. Median response time is about 20ms, and 95th percentile response time is about 225ms. Bringing that down from 20s is pretty amazing, and I&rsquo;m extremely proud of what the team of volunteers working on Bundler API has accomplished so far. As great as what we have now is, we have plans to make things much, much better.</p>\n<p>A new index format has been proposed for the rubygems library. It will be append-only, and very easy to keep and update on each machine that installs gems. Once Bundler can start using it, running bundle install will suddenly only take a few seconds. Bundler will already know about almost all of the gems, and only have to download information about gems that have been pushed or yanked since the last time it ran. This will be… way, way, way faster.</p>\n<p>We&rsquo;ve planned a way to implement all the wonderful things that I&rsquo;ve just described. Now all we need is the manpower to accomplish it, but there simply aren&rsquo;t very many people actively helping with either the rubygems or Bundler infrastructure. I&rsquo;m hoping to change that. If you&rsquo;re interested in helping with the infrastructure that every rubyist uses every day, talk to either myself or Jessica Lynn Suttles, who is also on the Bundler core team. We&rsquo;d love your help.</p>\n",
				"content_text": "\n<p class=\"aside\">This talk was given at <a href=\"http://2013.scottishrubyconference.com\">Scottish Ruby Conference 2013</a>, and the slides are <a href=\"https://speakerdeck.com/indirect/deathmatch-bundler-vs-rubygems-dot-org\">available on SpeakerDeck</a>, as well as <a href=\"/2013/05/12/deathmatch-bundler-vs-rubygemsorg/Deathmatch-Bundler-vs-Rubygems.pdf\">available for download as a PDF</a>. This post doesn't correlate exactly to the slides, but it has the same content.</p>\n\n<script async class=\"speakerdeck-embed\" data-id=\"57fa6d609d4901309d516e9dd498db92\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n\nThis talk is a story of hubris, disaster, overcoming adversity, and accidentally launching a distributed denial of service attack against rubygems.org. Sorry about that, everybody. But now, the story of how that came to pass, and things came back better than ever:\n\n### The setup\n\nIt all begins with the release Bundler 1.0, on August 30 2010. The great thing about Bundler was that you could run one command to install all the gems your application needed! The terrible thing about that one command was that it took… a really long time, unfortunately.\n\nIn just a few short weeks, developers were already tweeting copies of the XKCD “compiling” comic, but with “compiling” replaced by “bundling”. Rubyists seem to have really high expectations as far as responsiveness goes (except when starting up Rails applications, I guess).\n\nAnyway, it was unacceptable that Bundler was slow. Slowness makes us sad. We can do better! We will do better! The estimable Nick Quaranto, creator and maintainer of the Rubygems.org Rails app, proposed an idea: instead of downloading the entire index of every gem that has ever been pushed, why not just ask for only the gems that are in the Gemfile? Just a few weeks of cooperative work later, the Bundler API was born. Rubygems.org provided an API that returned a much smaller set of dependency information, and `bundle install` could run in as little as a few seconds.\n\n### The disaster\n\nWhat we didn’t realize at the time, though, was that all the dynamic processing required significantly more CPU cycles than the rubygems.org server was accustomed to providing. Aside from the Bundler API, rubygems.org really only does two things: accept gem pushes, and update the database accordingly, and send 302 redirects to S3 for requests that are trying to download the actual .gem files. Neither one of those activities is particularly server-intensive, since redirects are cheap and pushes are infrequent. \n\nThe Bundler API had to do a fair amount of work: looking up every requested gem in Postgres, then marshalling the arrays of output, and then caching the response into Redis before sending it. As the months wore on, more and more people upgraded to Bundler 1.1, and more and more people used the Bundler API every day. Back in 2012, there was just a single Rackspace VPS, running Postgres, Unicorn, and Redis. By October, we had reached the breaking point for that server, and it went down hard.\n\nIt was impossible to install gems for almost an entire day while the server was down, and getting it back up proved to be a challenge, since Bundler requests were still overwhelming everything. In the end, the only way to get rubygems.org back was to disable the Bundler API.\n\nFortunately, disabling the Bundler API didn’t actually stop anything from working — without the API, Bundler falls back on the old behaviour of fetching the entire source index. Unfortunately, this meant that all bundle installs were back to as slow as they had been on Bundler 1.0.\n\n### The Replacement\n\nSince hosting the Bundler API on the limited rubygems.org box was very impractical, the Bundler team decided to reimplement the Bundler API and host it by itself as a standalone app. Heroku offered to provide hosting, and we started to build a simple Sinatra app that spoke the same Bundler API that Rubygems.org had previously provided.\n\nAfter some trial and error, we had a working API, but now we were worried about how it would perform. Could it support every rubyist in the whole world?  How did it compare to the old API? Did we need more servers to handle all the requests? Had we overdone it, and we were wastefully running servers that we didn’t really need?\n\nTo answer all of those questions we needed data — the only way to know if it was slow would be to measure it, and the only way to k ow if it was working would be to monitor it. So, we set up logging and instrumentation services. Librato and Papertrail have both gracefully donated accounts to the Bundler team, and we’ve been using them extensively to get a better idea of how the service is running.\n\nWhen combined with Paprtrail alerts and the Metriks gem, Librato provides some fancy and beautiful graphs that allows us to see what’s going on. We used Librato to track a lot of things: response times, database queries, request complexity, traffic, and server load. We use Papertrail to send errors from the Heroku infrastructure to Librato, since we don’t have a way to track those from inside our Ruby app.\n\n### Discoveries\n\nAs we started to track all of these things, which we hadn’t known anything about before, we discovered a few interesting trends. We knew, from the Rubygems.org team, that Bundler API requests typically took 2 seconds, and could take up to 20 seconds. As we instrumented everything on our new API, and tried to make it faster, we discovered some things that surprised us.\n\n*Upgrading our Postgres server massively sped up response times*\n\nIn retrospect, that seems super obvious, right? We totally weren’t expecting this to be the case, becuase the dataset is so small. The entire rubygems.org database is only about 40MB after it’s been gzipped, and it’s around 250MB decompressed. Our initial setup used a small instance of Heroku Postgres, and seemed to be pretty fast. Queries returned in 5-15ms, and the median response time was around 80ms. 95th percentile response times were around 600ms.\n\nWhile the median was good, the 95th percentile was slow enough that we wanted to see if we could do better. One of the things we tried, thinking that it surely wouldn’t make any difference, was to switch to a much bigger Postgres than our dataset actually needed. Shockingly, it made a huge difference. Queries started coming back in 3-5ms. Median response time dropped to around 20ms, a 4x improvement. The 95th percentile responses dropped to about 225ms.\n\n*Caching responses into Redis slowed down response times!*\n\nCaching is supposed to make everything faster, right? Not so much, unfortunately. We discovered that the response caching that rubygems.org had been doing wasn’t actually helpful. The list of gems in a particular gemfile is almost never exactly the same as another gemfile, and new gems were being pushed every 2-3 minutes. As a result, the server was forced to calculate the responses for more or less every single request, and then spend extra time caching the result that it was never going to be able to use again!\n\n### Enough or too much?\n\nOnce we had worked out those somewhat surprising performance issues, we had a new question. How many Heroku dynos do we need to run to serve all the requests that we're getting? We could have simply run dozens of dynos just to be sure, but that would have been extremely rude to Heroku after they were gracious enough to donate hosting.\n\nWe started to track the \"dynos in use\" header, provided by Heroku. Using that header, we were able to graph dyno usage over time, see the correlation between requests and dynos, and use enough dynos to serve our peak traffic successfully. While that worked well, there was recent public outcry over the inefficiencies of their public routing system for slow single-threaded apps. Bundler API was one of those single threaded-systems, and that caused us to make another surprising discovery that I’ll talk about in a minute.\n\nBack to the number of dynos we need — once Heroku removed that header when they reworked their router, we were at something of a loss for a while. Eventually, though, we realized that the router will tell us every time that we’re unable to serve a request in the Heroku logs. Once we realized that, it was straightforward to set up a Papertrail search that sends every hit to Librato.\n\nNow we can graph all the response types on a single graph, including success, not found, exception, and routing failures. If there aren’t any routing failures, we can be reasonably sure that we have a sufficient number of dynos. To make sure that things stay that way, we’ve set up Librato to send an alert to Pager Duty if there are persistent routing failures. When we see that alert, we’ll know that we’ve overrun our current crop of dynos, and need to add some more.\n\n### What server to serve\n\nSo, I mentioned that Heroku’s router isn’t that great for app servers that can only serve one request at a time. There’s been a ton of discussion around this topic recently, including posts on RapGenius and Heroku’s own blog. In case you missed it, the tl;dr is that random routing across all dynos really sucks if those dynos could be stuck serving a single very long request. When your app works like that, you need hundreds of dynos just to be able to serve the majority of requests that would (in an optimal world) only require 10 dynos.\n\nBundler API had initially been deployed using Puma, the recently released server written by Evan Phoenix. It supports multithreaded app servers, and we were using that ability to great effect. Unfortunately, every day or two, one of the Puma servers would get itself into a deadlock and never respond to another request, even though it would keep accepting requests the entire time. In the end, we tried running the app on Thin, and it worked, so we moved on to other things.\n\nThat all changed when Heroku talked about the routing issues, though. We benchmarked Bundler API from the outside world, and discovered that although the _internal_ response times had a median of 20ms, external requests had a median response time of 400ms or higher, with something like 5% of requests timing out altogether. That was absolutely not acceptable, so we tried other app server concurrency options.\n\nIn the end, we went with Unicorn, running 16 child processes per dyno. That provides the same concurrency on each dyno as we had with Puma, but without the occasional deadlocks. On the new unicorn-based concurrency system, the number of timed out connections dropped dramatically, and we were able to pay attention to some strange new bug reports that had started cropping up all over the place.\n\n### Synchronizing in the cloud (gooooood luck)\n\nAfter investigating reports of new and completely unknown bugs, we realized we had a pretty big problem: staying synchronized with rubygems.org as developers pushed and yanked gems over time. As part of the original system, we’d built a background process that would fetch the full index from rubygems.org and update our database as necessary, adding and removing gems that had changed.\n\nUnfortunately, it was a very slow process (taking upwards of 20 minutes), and tended to crash when it used more memory than Heroku was willing to give it. After many unhappy users couldn’t use Bundler to install the gems that they had pushed, we managed to settle on a solution: Rubygems.org now sends Bundler an HTTP request each time a gem is pushed, and Bundler immediately adds that gem to the Bundler API database.\n\nHTTP requests get lost sometimes, though, and so the Bundler API also runs the full updating task, albeit less frequently. By comibining both the webhooks and the background worker, the system is eventually consistent, but fully up to date as often as it can be without causing unbearable pressure on the rubygems.org servers. It is never more than an hour out of date, and most of the time updates happen within 5 seconds. Hopefully, everyone is happy.\n\n### We can do better\n\nAt the end of all the monitoring, optimization, and tuning, the results were fantastic: responses are handled very quickly. Median response time is about 20ms, and 95th percentile response time is about 225ms. Bringing that down from 20s is pretty amazing, and I'm extremely proud of what the team of volunteers working on Bundler API has accomplished so far. As great as what we have now is, we have plans to make things much, much better.\n\nA new index format has been proposed for the rubygems library. It will be append-only, and very easy to keep and update on each machine that installs gems. Once Bundler can start using it, running bundle install will suddenly only take a few seconds. Bundler will already know about almost all of the gems, and only have to download information about gems that have been pushed or yanked since the last time it ran. This will be… way, way, way faster.\n\nWe've planned a way to implement all the wonderful things that I've just described. Now all we need is the manpower to accomplish it, but there simply aren't very many people actively helping with either the rubygems or Bundler infrastructure. I'm hoping to change that. If you're interested in helping with the infrastructure that every rubyist uses every day, talk to either myself or Jessica Lynn Suttles, who is also on the Bundler core team. We'd love your help.\n",
				"date_published": "2013-05-12T00:00:00-08:00",
				"url": "https://andre.arko.net/2013/05/12/deathmatch-bundler-vs-rubygemsorg/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2013/05/10/hack-your-bundle-for-fun/",
				"title": "Hack your bundle for fun and profit",
				"content_html": "<p>Bundler has turned out to be an amazingly useful tool for installing and tracking gems a Ruby project needs. So useful, in fact, that nearly every Ruby project uses it. Even though it shows up practically everywhere, most people don’t know about Bundler’s built-in tools and helpers. In an attempt to increase awareness (and Ruby developer productivity), I’m going to tell you about them.</p>\n<h3 id=\"install-update-and-outdated\">Install, update, and outdated</h3>\n<p>You probably already know this, but I’m going to summarize for the people who are just getting started and don’t know yet. Run <code>bundle install</code> to install the bundle that’s requested by your project. If you’ve just run <code>git pull</code> and there are new gems? <code>bundle install</code>. If you’ve just added new gems or changed gem versions in the Gemfile? <code>bundle install</code>. It might seem like you want to <code>bundle update</code>, but that won’t just install gems — it will try to upgrade every single gem in your bundle. That’s usually a disaster unless you really meant to do it.</p>\n<p>The update command is for when gems you use has been updated, and you want your bundle to have the newest version that your Gemfile will allow. Run <code>bundle outdated</code> to print a list of gems that could be upgraded. If you want to upgrade a specific gem, run <code>bundle update GEM</code>, or run <code>bundle update</code> to update everything. After the update finishes, make sure all your tests pass before you commit your new Gemfile.lock!</p>\n<h3 id=\"show-and-open\">Show and open</h3>\n<p>Most people know about <code>bundle show</code>, which prints the full path to the location where a gem is installed (probably because it’s called out in the success message after installing!). Far more useful, however, is the <code>bundle open</code> command, which will open the gem itself directly into your EDITOR. Here’s a minimalist demo:</p>\n<pre tabindex=\"0\"><code>$ bundle install\nFetching gem metadata from https://rubygems.org/..........\nResolving dependencies...\nInstalling rack (1.5.2)\nUsing bundler (1.3.1)\nYour bundle is complete! Use `bundle show [gemname]` to see where a bundled gem is installed.\n$ echo $EDITOR\nmate -w\n$ bundle open rack\n</code></pre><p>That’s all you need to get the installed copy of rack open in your editor. Being able to edit gems without having to look for them can be an amazing debugging tool. It makes it possible to insert print or debugger statements in a few seconds. If you do change your gems, though, be sure to reset them afterwards! There will be a pristine command soon, but for now, just run <code>bundle exec gem pristine</code> to restore the gems that you edited.</p>\n<h3 id=\"searching\">Searching</h3>\n<p>The show command still has one more trick up it’s sleeve, though: <code>bundle show --paths</code>. Printing a list of paths may not sound terribly useful, but it makes it trivial to search through the source of every gem in your bundle. Want to know where ActionDispatch::RemoteIp is defined? It’s a one-liner:</p>\n<pre><code>$ grep ActionDispatch::RemoteIp `bundle show --paths`\n</code></pre>\n<p>Whether you use <code>grep</code>, <code>ack</code>, or <code>ag</code>, it’s very easy to set up a shell function that allows you to search the current bundle in just a few characters. Here’s mine:</p>\n<pre><code>function back () {\n  ack &quot;$@&quot; `bundle show --paths`\n}\n</code></pre>\n<p>With that function, searching becomes even easier:</p>\n<pre><code>$ back ActionDispatch::RemoteIp\n</code></pre>\n<h3 id=\"binstubs\">Binstubs</h3>\n<p>One of the most annoying things about using Bundler is the way that you (probably) have to run <code>bundle exec whatever</code> anytime you want to run a command. One of the easiest ways around that is installing Bundler binstubs. By running <code>bundle binstubs GEM</code>, you can generate stubs in the <code>bin/</code> directory. Those stubs will load your bundle, and the correct version of the gem, before running the command. Here&rsquo;s an example of setting up a binstub for rspec.</p>\n<pre tabindex=\"0\"><code>$ bundle binstubs rspec-core\n$ bin/rspec spec\nNo examples found.\nFinished in 0.00006 seconds\n0 examples, 0 failures\n</code></pre><p>Use binstubs for commands that you run often, or for commands that you might want to run from (say) a cronjob. Since the binstubs don&rsquo;t have to load as much code, they even run faster. Rails 4 adopts binstubs as an official convention, and ships with <code>bin/rails</code> and <code>bin/rake</code>, both set up to always run for that specific application.</p>\n<h3 id=\"creating-a-gemfile\">Creating a Gemfile</h3>\n<p>I&rsquo;ve seen some complaints recently that it&rsquo;s too much work to type <code>source 'https://rubygems.org'</code> when creating a new Gemfile. Happily, Bundler will do that for you! When you&rsquo;re starting a new project, you can create a new Gemfile with Rubygems.org as the source by running a single command:</p>\n<pre><code>$ bundle init\n</code></pre>\n<p>At that point, you&rsquo;re ready to add gems and install away!</p>\n<h3 id=\"git-local-gems\">Git local gems</h3>\n<p>A lot of people ask how they can use Bundler to modify and commit to a gem in their Gemfile. Thanks to work lead by José Valim, Bundler 1.2 allows this, in a pretty elegant way. With one setting, you can load your own git clone in development, but deploying to production will simply check out the last commit you used.</p>\n<p>Here&rsquo;s how to set up a git local copy of rack:</p>\n<pre><code>$ echo &quot;gem 'rack', :github =&gt; 'rack/rack', :branch =&gt; 'master'&quot; &gt;&gt; Gemfile\n$ bundle config local.rack ~/sw/gems/rack\n$ bundle show rack\n/Users/andre/sw/gems/rack\n</code></pre>\n<p>Now that it&rsquo;s set up, you can edit the code your application will use, but still commit in that repository as often as you like. Pretty sweet.</p>\n<h3 id=\"ruby-versions\">Ruby versions</h3>\n<p>Another feature of Bundler 1.2 is ruby version requirements. If you know that your application only works with one version of ruby, you can require that version.  Just add one line to your Gemfile specifying the version number as a string.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>ruby <span style=\"color:#e6db74\">&#39;1.9.3&#39;</span>\n</span></span></code></pre></div><p>Now Bundler will raise an exception if you try to run your application on a different version of ruby. Never worry about accidentally using the wrong version while developing again!</p>\n<h3 id=\"dependency-graph\">Dependency graph</h3>\n<p>Bundler uses your Gemfile to create what is technically called a &ldquo;dependency graph&rdquo;, where there are many gems that have various dependencies on eachother. It can be pretty cool to see that dependency graph drawn as a literal graph, and that&rsquo;s what the <code>bundle viz</code> command does. You need to install GraphViz and the ruby-graphviz gem.</p>\n<pre tabindex=\"0\"><code>$ brew install graphviz\n$ gem install ruby-graphviz\n$ bundle viz\n</code></pre><p>Once you&rsquo;ve done that, though, you get a pretty picture that&rsquo;s a lot of fun to look at. Here&rsquo;s the graph for a Gemfile that just contains the Rails gem.</p>\n<img src=\"https://indirect.micro.blog/uploads/2025/df9aaa5a9b.jpg\">\n<h3 id=\"irb-in-your-bundle\">IRB in your bundle</h3>\n<p>I have one final handy tip before the big finale: the console command. Running <code>bundle console</code> will open an IRB prompt for you, but it will also load your entire bundle and all the gems in it beforehand. If you want to try expirimenting with the gems you use, but don&rsquo;t have the Rails gem to give you a Rails console, this is a great alternative.</p>\n<pre><code>$ bundle console\n&gt;&gt; Rack::Server.new\n=&gt; #&lt;Rack::Server:0x007fb439037970 @options=nil&gt;\n</code></pre>\n<h3 id=\"creating-a-new-gem\">Creating a new gem</h3>\n<p>Finally, what I think is the biggest and most useful feature of Bundler after installing things. Since Bundler exists to manage gems, the Bundler team is very motivated to make it easy to create and manage gems. It&rsquo;s really, really easy. You can create a directory with the skeleton of a new gem just by running <code>bundle gem NAME</code>. You&rsquo;ll get a directory with a gemspec, readme, and lib file to drop your code into. Once you&rsquo;ve added your code, you can install the gem on your own system to try it out just by running <code>rake install</code>. Once you&rsquo;re happy with the gem and want to share it with others, pushing a new version of your gem to rubygems.org is as easy as <code>rake release</code>. As a side benefit, gems created this way can also easily be used as git gems. That means you (and anyone else using your gem) can fork, edit, and bundle any commit they want to.</p>\n<h3 id=\"step-3-profit\">Step 3: Profit</h3>\n<p>Now that you know all of the handy stuff Bundler will do for you, I suggest trying it out! Search your bundle, create a gem, edit it with git locals, and release it to rubygems.org. As far as I&rsquo;m concerned, the absolute best thing about Bundler is that it makes it easier for everyone to share useful code, and collaborate to make Ruby better for everyone.</p>\n<p class=\"aside\">This post was also given as a lightinng talk at <a href=\"http://2013.la-conf.org/#eclair\">La Conf</a>, and the slides for that talk are <a href=\"https://speakerdeck.com/indirect/hack-your-bundle-for-fun-and-profit-la-conf-2013\">posted to SpeakerDeck</a>.</p>\n<p class=\"aside\">This post was originally written for, and posted to, the <a href=\"http://www.engineyard.com/blog/2010/homebrew-os-xs-missing-package-manager/\">Engine Yard Blog</a>.</p>\n",
				"content_text": "\nBundler has turned out to be an amazingly useful tool for installing and tracking gems a Ruby project needs. So useful, in fact, that nearly every Ruby project uses it. Even though it shows up practically everywhere, most people don’t know about Bundler’s built-in tools and helpers. In an attempt to increase awareness (and Ruby developer productivity), I’m going to tell you about them.\n\n### Install, update, and outdated\n\nYou probably already know this, but I’m going to summarize for the people who are just getting started and don’t know yet. Run `bundle install` to install the bundle that’s requested by your project. If you’ve just run `git pull` and there are new gems? `bundle install`. If you’ve just added new gems or changed gem versions in the Gemfile? `bundle install`. It might seem like you want to `bundle update`, but that won’t just install gems — it will try to upgrade every single gem in your bundle. That’s usually a disaster unless you really meant to do it.\n\nThe update command is for when gems you use has been updated, and you want your bundle to have the newest version that your Gemfile will allow. Run `bundle outdated` to print a list of gems that could be upgraded. If you want to upgrade a specific gem, run `bundle update GEM`, or run `bundle update` to update everything. After the update finishes, make sure all your tests pass before you commit your new Gemfile.lock!\n\n### Show and open\n\nMost people know about `bundle show`, which prints the full path to the location where a gem is installed (probably because it’s called out in the success message after installing!). Far more useful, however, is the `bundle open` command, which will open the gem itself directly into your EDITOR. Here’s a minimalist demo:\n\n```\n$ bundle install\nFetching gem metadata from https://rubygems.org/..........\nResolving dependencies...\nInstalling rack (1.5.2)\nUsing bundler (1.3.1)\nYour bundle is complete! Use `bundle show [gemname]` to see where a bundled gem is installed.\n$ echo $EDITOR\nmate -w\n$ bundle open rack\n```\n\nThat’s all you need to get the installed copy of rack open in your editor. Being able to edit gems without having to look for them can be an amazing debugging tool. It makes it possible to insert print or debugger statements in a few seconds. If you do change your gems, though, be sure to reset them afterwards! There will be a pristine command soon, but for now, just run `bundle exec gem pristine` to restore the gems that you edited.\n\n### Searching\n\nThe show command still has one more trick up it’s sleeve, though: `bundle show --paths`. Printing a list of paths may not sound terribly useful, but it makes it trivial to search through the source of every gem in your bundle. Want to know where ActionDispatch::RemoteIp is defined? It’s a one-liner:\n\n    $ grep ActionDispatch::RemoteIp `bundle show --paths`\n\nWhether you use `grep`, `ack`, or `ag`, it’s very easy to set up a shell function that allows you to search the current bundle in just a few characters. Here’s mine:\n\n    function back () {\n      ack \"$@\" `bundle show --paths`\n    }\n\nWith that function, searching becomes even easier:\n\n    $ back ActionDispatch::RemoteIp\n\n### Binstubs\n\nOne of the most annoying things about using Bundler is the way that you (probably) have to run `bundle exec whatever` anytime you want to run a command. One of the easiest ways around that is installing Bundler binstubs. By running `bundle binstubs GEM`, you can generate stubs in the `bin/` directory. Those stubs will load your bundle, and the correct version of the gem, before running the command. Here's an example of setting up a binstub for rspec.\n\n```\n$ bundle binstubs rspec-core\n$ bin/rspec spec\nNo examples found.\nFinished in 0.00006 seconds\n0 examples, 0 failures\n```\n\nUse binstubs for commands that you run often, or for commands that you might want to run from (say) a cronjob. Since the binstubs don't have to load as much code, they even run faster. Rails 4 adopts binstubs as an official convention, and ships with `bin/rails` and `bin/rake`, both set up to always run for that specific application.\n\n### Creating a Gemfile\n\nI've seen some complaints recently that it's too much work to type `source 'https://rubygems.org'` when creating a new Gemfile. Happily, Bundler will do that for you! When you're starting a new project, you can create a new Gemfile with Rubygems.org as the source by running a single command:\n\n    $ bundle init\n\nAt that point, you're ready to add gems and install away!\n\n### Git local gems\n\nA lot of people ask how they can use Bundler to modify and commit to a gem in their Gemfile. Thanks to work lead by José Valim, Bundler 1.2 allows this, in a pretty elegant way. With one setting, you can load your own git clone in development, but deploying to production will simply check out the last commit you used.\n\nHere's how to set up a git local copy of rack:\n\n    $ echo \"gem 'rack', :github => 'rack/rack', :branch => 'master'\" >> Gemfile\n    $ bundle config local.rack ~/sw/gems/rack\n    $ bundle show rack\n    /Users/andre/sw/gems/rack\n\nNow that it's set up, you can edit the code your application will use, but still commit in that repository as often as you like. Pretty sweet.\n\n### Ruby versions\n\nAnother feature of Bundler 1.2 is ruby version requirements. If you know that your application only works with one version of ruby, you can require that version.  Just add one line to your Gemfile specifying the version number as a string.\n\n```ruby\nruby '1.9.3'\n```\n\nNow Bundler will raise an exception if you try to run your application on a different version of ruby. Never worry about accidentally using the wrong version while developing again!\n\n### Dependency graph\n\nBundler uses your Gemfile to create what is technically called a \"dependency graph\", where there are many gems that have various dependencies on eachother. It can be pretty cool to see that dependency graph drawn as a literal graph, and that's what the `bundle viz` command does. You need to install GraphViz and the ruby-graphviz gem.\n\n```\n$ brew install graphviz\n$ gem install ruby-graphviz\n$ bundle viz\n```\n\nOnce you've done that, though, you get a pretty picture that's a lot of fun to look at. Here's the graph for a Gemfile that just contains the Rails gem.\n\n<img src=\"https://indirect.micro.blog/uploads/2025/df9aaa5a9b.jpg\">\n\n### IRB in your bundle\n\nI have one final handy tip before the big finale: the console command. Running `bundle console` will open an IRB prompt for you, but it will also load your entire bundle and all the gems in it beforehand. If you want to try expirimenting with the gems you use, but don't have the Rails gem to give you a Rails console, this is a great alternative.\n\n    $ bundle console\n    >> Rack::Server.new\n    => #<Rack::Server:0x007fb439037970 @options=nil>\n\n### Creating a new gem\n\nFinally, what I think is the biggest and most useful feature of Bundler after installing things. Since Bundler exists to manage gems, the Bundler team is very motivated to make it easy to create and manage gems. It's really, really easy. You can create a directory with the skeleton of a new gem just by running `bundle gem NAME`. You'll get a directory with a gemspec, readme, and lib file to drop your code into. Once you've added your code, you can install the gem on your own system to try it out just by running `rake install`. Once you're happy with the gem and want to share it with others, pushing a new version of your gem to rubygems.org is as easy as `rake release`. As a side benefit, gems created this way can also easily be used as git gems. That means you (and anyone else using your gem) can fork, edit, and bundle any commit they want to.\n\n###  Step 3: Profit\n\nNow that you know all of the handy stuff Bundler will do for you, I suggest trying it out! Search your bundle, create a gem, edit it with git locals, and release it to rubygems.org. As far as I'm concerned, the absolute best thing about Bundler is that it makes it easier for everyone to share useful code, and collaborate to make Ruby better for everyone.\n\n<p class=\"aside\">This post was also given as a lightinng talk at <a href=\"http://2013.la-conf.org/#eclair\">La Conf</a>, and the slides for that talk are <a href=\"https://speakerdeck.com/indirect/hack-your-bundle-for-fun-and-profit-la-conf-2013\">posted to SpeakerDeck</a>.</p>\n\n<p class=\"aside\">This post was originally written for, and posted to, the <a href=\"http://www.engineyard.com/blog/2010/homebrew-os-xs-missing-package-manager/\">Engine Yard Blog</a>.</p>\n",
				"date_published": "2013-05-10T00:00:00-08:00",
				"url": "https://andre.arko.net/2013/05/10/hack-your-bundle-for-fun/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2013/03/29/rubygems-openssl-and-you/",
				"title": "Rubygems, OpenSSL, and you",
				"content_html": "<p>The last month or two has been a pretty tumultuous time for Rubygems, the Rubygems.org servers, and Bundler itself. This post is my attempt to summarize the history of what happened and make some concrete suggestions for rubyists that care about using Rubygems or Bundler (which is probably most of them). The practical upshot is that you should upgrade your installation of Rubygems to version 2.0.3 or later, and your Bundler gem to at least 1.3.4. If you&rsquo;re interested to hear why, keep reading.</p>\n<p>This year has been a rough for Ruby and Rails developers. The last three months have included almost as many security issues as all of 2012 and 2011 combined. Just a couple of months ago, the Rubygems.org server was compromised and had to be rebuilt from scratch. In the midst of all the security hullabaloo, the Rubygems and Bundler teams have also been working to support all of the changes made in Ruby 2.0 by the time it reached its first release on February 24.</p>\n<p>Developers began voicing concerns about security shortly before Bundler 1.3 and Rubygems 2.0 were released in order to support Ruby 2. In addition to Ruby 2 support, Bundler 1.3 began to verify the SSL certificate provided by rubygems.org, which 1.2 did not do. It also deprecated the default <code>:rubygems</code> source from version 1.0 in an attempt to increase visibility into the connection method Bundler uses, and to encourage HTTPS instead of HTTP without breaking backwards compatibility.</p>\n<p>Unfortunately, the initial 1.3 release was not fully compatible with installations of Ruby compiled without OpenSSL. Even worse, it was unable to verify the rubygems.org SSL certificate on machines with older OpenSSL CA certificates, which includes many &ldquo;stable&rdquo; OS releases used on production servers. I wrote error-handling code that should have at very least explained what the problem was, but one if my assumptions about exceptions was wrong, and so the explanatory message was not even displayed. Resolving these issues took time and cooperation from helpful users, and that left a lot of people in the frustrating position of seeing unexplained OpenSSL certificate verification errors every time they tried to run <code>bundle install</code>.</p>\n<p>Compounding these issues, a Rubygems bugfix release around the same time included a bug in certificate validation, causing OpenSSL exceptions similar to those seen in Bundler. The ultimate solution that both Rubygems and Bundler settled on is to include the well-known CA certificates needed to verify SSL connections to rubygems.org. Those built-in certificates are used as a fallback on machines with older CA certificates installed, preventing the OpenSSL errors that people were seeing.</p>\n<p>Finally, Rubygems 2.0.2 included, with the best intentions, a change to the code that connected to gem servers. When a server was configured to use HTTP, Rubygems would first attempt to connect via HTTPS, and only use HTTP if that failed. That change resulted in additional intermittent failures in Bundler, but only for those running the very latest Rubygems. Confusing things even more, the certificate for api.rubygems.org actually expired while all this was going on. That caused a rash of legitimate OpenSSL certificate validation exceptions, but they were mostly drowned out by the noise of everything else that was going on simultaneously.</p>\n<p>Eventually, api.rubygems.org got a valid certificate, Bundler 1.3.4 was released with improved HTTPS support and built-in CA certificates, and Rubygems 2.0.3 was released with built-in CA certificates but without automatic HTTPS. Things should now be functional whether your Ruby was built against OpenSSL or not. Additionally, connections to rubygems.org should not cause certificate validation exceptions, even if your system has older CA certificates.</p>\n<p>Making sweeping changes to fundamental Ruby infrastructure is hard, especially when we need to keep things compatible for users who haven’t been able to upgrade other parts of their stack yet. It’s easy to make helpful changes without realizing the potential problems, especially when prereleases and release candidates aren’t tested by very many people. If you care about this stuff, we’d love to have your help. Talk to us in #bundler and #rubygems on irc.freenode.net, or read the <a href=\"https://github.com/carlhuda/bundler/blob/master/CONTRIBUTE.md\">Bundler contributing guide</a> for a tour of the helpful things that anyone can do.</p>\n<p><i>Special thanks to <a href=\"http://twitter.com/j2h\">Josh Kalderimis</a> for inspiring me to write this post with his confusion about what the hell was going on, anyway.</i></p>\n",
				"content_text": "The last month or two has been a pretty tumultuous time for Rubygems, the Rubygems.org servers, and Bundler itself. This post is my attempt to summarize the history of what happened and make some concrete suggestions for rubyists that care about using Rubygems or Bundler (which is probably most of them). The practical upshot is that you should upgrade your installation of Rubygems to version 2.0.3 or later, and your Bundler gem to at least 1.3.4. If you're interested to hear why, keep reading.\n\nThis year has been a rough for Ruby and Rails developers. The last three months have included almost as many security issues as all of 2012 and 2011 combined. Just a couple of months ago, the Rubygems.org server was compromised and had to be rebuilt from scratch. In the midst of all the security hullabaloo, the Rubygems and Bundler teams have also been working to support all of the changes made in Ruby 2.0 by the time it reached its first release on February 24.\n\nDevelopers began voicing concerns about security shortly before Bundler 1.3 and Rubygems 2.0 were released in order to support Ruby 2. In addition to Ruby 2 support, Bundler 1.3 began to verify the SSL certificate provided by rubygems.org, which 1.2 did not do. It also deprecated the default `:rubygems` source from version 1.0 in an attempt to increase visibility into the connection method Bundler uses, and to encourage HTTPS instead of HTTP without breaking backwards compatibility.\n\nUnfortunately, the initial 1.3 release was not fully compatible with installations of Ruby compiled without OpenSSL. Even worse, it was unable to verify the rubygems.org SSL certificate on machines with older OpenSSL CA certificates, which includes many \"stable\" OS releases used on production servers. I wrote error-handling code that should have at very least explained what the problem was, but one if my assumptions about exceptions was wrong, and so the explanatory message was not even displayed. Resolving these issues took time and cooperation from helpful users, and that left a lot of people in the frustrating position of seeing unexplained OpenSSL certificate verification errors every time they tried to run `bundle install`.\n\nCompounding these issues, a Rubygems bugfix release around the same time included a bug in certificate validation, causing OpenSSL exceptions similar to those seen in Bundler. The ultimate solution that both Rubygems and Bundler settled on is to include the well-known CA certificates needed to verify SSL connections to rubygems.org. Those built-in certificates are used as a fallback on machines with older CA certificates installed, preventing the OpenSSL errors that people were seeing.\n\nFinally, Rubygems 2.0.2 included, with the best intentions, a change to the code that connected to gem servers. When a server was configured to use HTTP, Rubygems would first attempt to connect via HTTPS, and only use HTTP if that failed. That change resulted in additional intermittent failures in Bundler, but only for those running the very latest Rubygems. Confusing things even more, the certificate for api.rubygems.org actually expired while all this was going on. That caused a rash of legitimate OpenSSL certificate validation exceptions, but they were mostly drowned out by the noise of everything else that was going on simultaneously.\n\nEventually, api.rubygems.org got a valid certificate, Bundler 1.3.4 was released with improved HTTPS support and built-in CA certificates, and Rubygems 2.0.3 was released with built-in CA certificates but without automatic HTTPS. Things should now be functional whether your Ruby was built against OpenSSL or not. Additionally, connections to rubygems.org should not cause certificate validation exceptions, even if your system has older CA certificates.\n\nMaking sweeping changes to fundamental Ruby infrastructure is hard, especially when we need to keep things compatible for users who haven’t been able to upgrade other parts of their stack yet. It’s easy to make helpful changes without realizing the potential problems, especially when prereleases and release candidates aren’t tested by very many people. If you care about this stuff, we’d love to have your help. Talk to us in #bundler and #rubygems on irc.freenode.net, or read the [Bundler contributing guide](https://github.com/carlhuda/bundler/blob/master/CONTRIBUTE.md) for a tour of the helpful things that anyone can do.\n\n<i>Special thanks to [Josh Kalderimis](http://twitter.com/j2h) for inspiring me to write this post with his confusion about what the hell was going on, anyway.</i>\n",
				"date_published": "2013-03-29T00:00:00-08:00",
				"url": "https://andre.arko.net/2013/03/29/rubygems-openssl-and-you/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2013/03/24/falsehoods-programmers-believe/",
				"title": "Falsehoods programmers believe",
				"content_html": "<p>Everyone&rsquo;s <a href=\"http://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/\">names will be like mine</a>.</p>\n<p>Gender <a href=\"http://www.cscyphers.com/blog/2012/06/28/falsehoods-programmers-believe-about-gender/\">is binary</a>.<br>\nWell, at least <a href=\"http://smarterware.org/7388/the-case-against-drop-down-identities\">gender can be enumerated</a>.</p>\n<p>Time for programs is <a href=\"http://infiniteundo.com/post/25326999628/falsehoods-programmers-believe-about-time\">like time for people</a>.<br>\nWell, at least time has <a href=\"http://infiniteundo.com/post/25509354022/more-falsehoods-programmers-believe-about-time-wisdom\">always been like it is now</a>.</p>\n<p>The network will <a href=\"http://erratasec.blogspot.com/2012/06/falsehoods-programmers-believe-about.html\">always be up (or sometimes down)</a>.<br>\nWell, at least <a href=\"http://inessential.com/2013/03/18/brians_stupid_feed_tricks\">network responses will make sense</a>.</p>\n<p>Users will only provide <a href=\"http://infiniteundo.com/post/25230828820/things-you-should-test\">input that makes sense</a>.<br>\nWell, at least <a href=\"http://www.rinkworks.com/stupid/\">users will only do things I expect</a>.</p>\n<p>Cryptography <a href=\"http://www.schneier.com/book-sandl-pref.html\">makes encrypted data secure from attackers</a>.<br>\nWell, at least it&rsquo;s pretty safe if <a href=\"http://tonyarcieri.com/all-the-crypto-code-youve-ever-written-is-probably-broken\">I can&rsquo;t break it</a>.</p>\n<p>My security questions won&rsquo;t be <a href=\"http://www.wired.com/gadgetlab/2012/08/apple-amazon-mat-honan-hacking/all/\">answered by Amazon</a>.<br>\nWell, at least security answers won&rsquo;t be <a href=\"http://www.theblaze.com/stories/2011/11/07/your-facebook-profile-could-be-giving-away-answers-to-your-online-security-questions/\">public on Facebook</a>.</p>\n<p>People <a href=\"http://www.splashdata.com/press/PR121023.htm\">won&rsquo;t use &lsquo;password&rsquo; as their password</a><br>\nWell, at least <a href=\"http://web.archive.org/web/20130113055957/http://chargen.matasano.com/chargen/2007/9/7/enough-with-the-rainbow-tables-what-you-need-to-know-about-s.html\">random passwords will be secure</a>.</p>\n<p>You can believe the <a href=\"http://www.google.com/search?q=site:drupal.org+api+documentation+incorrect\">API documentation</a>.<br>\nWell, at least you <a href=\"http://stackoverflow.com/questions/2249740/getlasterror-returning-error-success-after-calling-connectnamedpipe?rq=1\">can handle errors</a>.</p>\n<p>It&rsquo;s <a href=\"http://griffin.oobleyboo.com/archive/on-pycon2013-and-equality/\">not harassment if I&rsquo;m okay with it</a>.<br>\nWell, <a href=\"http://braythwayt.com/2013/03/21/unjust.html\">trolls are unstoppable, so my conscience is clear</a>.</p>\n<p>Now please excuse me, I&rsquo;m off to find somewhere to sob quietly for a while without disturbing anyone. 😭</p>\n",
				"content_text": "\nEveryone's [names will be like mine][kalzumeus].\n\nGender [is binary][cscyphers].  \nWell, at least [gender can be enumerated][smarterware].\n\nTime for programs is [like time for people][infiniteundo].  \nWell, at least time has [always been like it is now][infiniteundo 2].\n\nThe network will [always be up (or sometimes down)][blogspot].  \nWell, at least [network responses will make sense][inessential].\n\nUsers will only provide [input that makes sense][infiniteundo 3].  \nWell, at least [users will only do things I expect][rinkworks].\n\nCryptography [makes encrypted data secure from attackers][schneier].  \nWell, at least it's pretty safe if [I can't break it][tonyarcieri].\n\nMy security questions won't be [answered by Amazon][wired].  \nWell, at least security answers won't be [public on Facebook][theblaze].\n\nPeople [won't use 'password' as their password][splashdata]  \nWell, at least [random passwords will be secure][archive].\n\nYou can believe the [API documentation][google].  \nWell, at least you [can handle errors][stackoverflow].\n\nIt's [not harassment if I'm okay with it][oobleyboo].  \nWell, [trolls are unstoppable, so my conscience is clear][braythwayt].\n\nNow please excuse me, I'm off to find somewhere to sob quietly for a while without disturbing anyone. 😭\n\n[archive]: http://web.archive.org/web/20130113055957/http://chargen.matasano.com/chargen/2007/9/7/enough-with-the-rainbow-tables-what-you-need-to-know-about-s.html\n[blogspot]: http://erratasec.blogspot.com/2012/06/falsehoods-programmers-believe-about.html\n[braythwayt]: http://braythwayt.com/2013/03/21/unjust.html\n[c2]: http://c2.com/cgi/wiki?WeirdErrorMessages\n[cscyphers]: http://www.cscyphers.com/blog/2012/06/28/falsehoods-programmers-believe-about-gender/\n[google]: http://www.google.com/search?q=site:drupal.org+api+documentation+incorrect\n[inessential]: http://inessential.com/2013/03/18/brians_stupid_feed_tricks\n[infiniteundo]: http://infiniteundo.com/post/25326999628/falsehoods-programmers-believe-about-time\n[infiniteundo 2]: http://infiniteundo.com/post/25509354022/more-falsehoods-programmers-believe-about-time-wisdom\n[infiniteundo 3]: http://infiniteundo.com/post/25230828820/things-you-should-test\n[kalzumeus]: http://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/\n[oobleyboo]: http://griffin.oobleyboo.com/archive/on-pycon2013-and-equality/\n[rinkworks]: http://www.rinkworks.com/stupid/\n[schneier]: http://www.schneier.com/book-sandl-pref.html\n[smarterware]: http://smarterware.org/7388/the-case-against-drop-down-identities\n[splashdata]: http://www.splashdata.com/press/PR121023.htm\n[stackoverflow]: http://stackoverflow.com/questions/2249740/getlasterror-returning-error-success-after-calling-connectnamedpipe?rq=1\n[theblaze]: http://www.theblaze.com/stories/2011/11/07/your-facebook-profile-could-be-giving-away-answers-to-your-online-security-questions/\n[tonyarcieri]: http://tonyarcieri.com/all-the-crypto-code-youve-ever-written-is-probably-broken\n[wired]: http://www.wired.com/gadgetlab/2012/08/apple-amazon-mat-honan-hacking/all/\n",
				"date_published": "2013-03-24T00:00:00-08:00",
				"url": "https://andre.arko.net/2013/03/24/falsehoods-programmers-believe/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2013/03/23/adding-arrays-in-ruby-quickly/",
				"title": "Adding arrays in Ruby quickly",
				"content_html": "<p>So&hellip; this is quite possibly so esoteric that it&rsquo;s not really going to be useful, but I was curious, even if it doesn&rsquo;t really matter in the end, thanks to modern processor speeds. The other day as I was working on my <a href=\"http://github.com/indirect/daneel\">silly chat bot</a>, I discovered that I had an array in a read-only attribute, and another array that I wanted to merge into the first array. Without a setter, it&rsquo;s not possible to something like <code>a = a + b</code>. As I was thinking about that, I realized that all of the <code>Array</code> addition operators create a third array to contain the result, and I didn&rsquo;t really want that either. So I looked up all of the <code>Array</code> operations that just add an element to the end of an existing array. The two mehtods I found are <code>#push</code> and <code>#&lt;&lt;</code>. I (maybe) could have also tested <code>a.insert(item, a.size)</code>, but I was trying to compare things of roughly equal complexity.</p>\n<p>For reasons not entirely clear to me, <code>&lt;&lt;</code> only ever takes a single argument, so adding an entire array means looping over the array, passing each element to <code>&lt;&lt;</code> one at a time. Push takes more than one argument, but I wasn&rsquo;t sure if it would be faster to pass in a lot of arguments for push to loop over, or just look over them myself. In the end, I decided to test both ways of calling <code>push</code>, one to compare the speed of the methods themselves, and one to compare the fastest possible way to add a large number of elements to an array. The results looked like this:</p>\n<pre tabindex=\"0\"><code>            user       system     total       real\npush *   339.270000  20.320000 359.590000 (360.085974)\npush     501.850000  17.810000 519.660000 (519.570627)\n&lt;&lt;       402.900000  16.670000 419.570000 (419.433473)\n</code></pre><p>For those of you playing along at home, I&rsquo;ve included the <a href=\"%22#add_arrays.rb%22\">code I used to benchmark each method</a> at the end of this post. The results weren&rsquo;t terribly surprising to me, but it does at least confirm that using the <code>&lt;&lt;</code> operator isn&rsquo;t the fastest way to add things to an array that already exists. It also confirms that looping over arrays in MRI&rsquo;s C implementation is faster than doing it ourselves in Ruby. So, in the end, it was pretty much just another way of finding out what we already knew. But it&rsquo;s nice to be sure, isn&rsquo;t it?</p>\n<p><a id=\"add_arrays.rb\"></a></p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># add_arrays.rb</span>\n</span></span><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#39;benchmark&#39;</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">TIMES</span> <span style=\"color:#f92672\">=</span> <span style=\"color:#ae81ff\">10_000</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">Benchmark</span><span style=\"color:#f92672\">.</span>bm <span style=\"color:#66d9ef\">do</span> <span style=\"color:#f92672\">|</span>b<span style=\"color:#f92672\">|</span>\n</span></span><span style=\"display:flex;\"><span>  b<span style=\"color:#f92672\">.</span>report(<span style=\"color:#e6db74\">&#34;push * &#34;</span>) <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">TIMES</span><span style=\"color:#f92672\">.</span>times <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>      a <span style=\"color:#f92672\">=</span> (<span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">..</span><span style=\"color:#ae81ff\">100_000</span>)<span style=\"color:#f92672\">.</span>to_a\n</span></span><span style=\"display:flex;\"><span>      a<span style=\"color:#f92672\">.</span>push(<span style=\"color:#f92672\">*</span>(<span style=\"color:#ae81ff\">100_000</span><span style=\"color:#f92672\">..</span><span style=\"color:#ae81ff\">200_000</span>)<span style=\"color:#f92672\">.</span>to_a)\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>  b<span style=\"color:#f92672\">.</span>report(<span style=\"color:#e6db74\">&#34;push   &#34;</span>) <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">TIMES</span><span style=\"color:#f92672\">.</span>times <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>      a <span style=\"color:#f92672\">=</span> (<span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">..</span><span style=\"color:#ae81ff\">100_000</span>)<span style=\"color:#f92672\">.</span>to_a\n</span></span><span style=\"display:flex;\"><span>      (<span style=\"color:#ae81ff\">100_000</span><span style=\"color:#f92672\">..</span><span style=\"color:#ae81ff\">200_000</span>)<span style=\"color:#f92672\">.</span>each { <span style=\"color:#f92672\">|</span>n<span style=\"color:#f92672\">|</span> a<span style=\"color:#f92672\">.</span>push n }\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>  b<span style=\"color:#f92672\">.</span>report(<span style=\"color:#e6db74\">&#34;&lt;&lt;     &#34;</span>) <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">TIMES</span><span style=\"color:#f92672\">.</span>times <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>      a <span style=\"color:#f92672\">=</span> (<span style=\"color:#ae81ff\">1</span><span style=\"color:#f92672\">..</span><span style=\"color:#ae81ff\">100_000</span>)<span style=\"color:#f92672\">.</span>to_a\n</span></span><span style=\"display:flex;\"><span>      (<span style=\"color:#ae81ff\">100_000</span><span style=\"color:#f92672\">..</span><span style=\"color:#ae81ff\">200_000</span>)<span style=\"color:#f92672\">.</span>each { <span style=\"color:#f92672\">|</span>n<span style=\"color:#f92672\">|</span> a <span style=\"color:#f92672\">&lt;&lt;</span> n }\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span>\n</span></span></code></pre></div>",
				"content_text": "So... this is quite possibly so esoteric that it's not really going to be useful, but I was curious, even if it doesn't really matter in the end, thanks to modern processor speeds. The other day as I was working on my [silly chat bot](http://github.com/indirect/daneel), I discovered that I had an array in a read-only attribute, and another array that I wanted to merge into the first array. Without a setter, it's not possible to something like `a = a + b`. As I was thinking about that, I realized that all of the `Array` addition operators create a third array to contain the result, and I didn't really want that either. So I looked up all of the `Array` operations that just add an element to the end of an existing array. The two mehtods I found are `#push` and `#<<`. I (maybe) could have also tested `a.insert(item, a.size)`, but I was trying to compare things of roughly equal complexity.\n\nFor reasons not entirely clear to me, `<<` only ever takes a single argument, so adding an entire array means looping over the array, passing each element to `<<` one at a time. Push takes more than one argument, but I wasn't sure if it would be faster to pass in a lot of arguments for push to loop over, or just look over them myself. In the end, I decided to test both ways of calling `push`, one to compare the speed of the methods themselves, and one to compare the fastest possible way to add a large number of elements to an array. The results looked like this:\n\n```\n            user       system     total       real\npush *   339.270000  20.320000 359.590000 (360.085974)\npush     501.850000  17.810000 519.660000 (519.570627)\n<<       402.900000  16.670000 419.570000 (419.433473)\n```\n\nFor those of you playing along at home, I've included the [code I used to benchmark each method](\"#add_arrays.rb\") at the end of this post. The results weren't terribly surprising to me, but it does at least confirm that using the `<<` operator isn't the fastest way to add things to an array that already exists. It also confirms that looping over arrays in MRI's C implementation is faster than doing it ourselves in Ruby. So, in the end, it was pretty much just another way of finding out what we already knew. But it's nice to be sure, isn't it?\n\n\n<a id=\"add_arrays.rb\"></a>\n\n``` ruby\n# add_arrays.rb\nrequire 'benchmark'\n\nTIMES = 10_000\n\nBenchmark.bm do |b|\n  b.report(\"push * \") do\n    TIMES.times do\n      a = (1..100_000).to_a\n      a.push(*(100_000..200_000).to_a)\n    end\n  end\n\n  b.report(\"push   \") do\n    TIMES.times do\n      a = (1..100_000).to_a\n      (100_000..200_000).each { |n| a.push n }\n    end\n  end\n\n  b.report(\"<<     \") do\n    TIMES.times do\n      a = (1..100_000).to_a\n      (100_000..200_000).each { |n| a << n }\n    end\n  end\n\nend\n```\n",
				"date_published": "2013-03-23T00:00:00-08:00",
				"url": "https://andre.arko.net/2013/03/23/adding-arrays-in-ruby-quickly/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2013/03/09/security-is-hard/",
				"title": "Security is hard",
				"content_html": "<p>This week, I gave a talk at <a href=\"http://ruby.onales.com/\">Ruby on Ales</a> about CVEs, responsible disclosure, and security process and best practices. The conference was awesome, and it was a blast to be able to give a talk there. I hope that rubyists will be able to embrace and extend the best practices discovered by years of community experimentation!</p>\n<p>I&rsquo;ll update this post with a link to the video once it&rsquo;s posted. In the meantime, you can check out the slides here, <a href=\"https://speakerdeck.com/indirect/security-is-hard-but-we-cant-go-shopping\">over on SpeakerDeck</a>, or <a href=\"/2013/08/22/security-is-hard-but-we-cant-go-shopping/security-is-hard.pdf\">as a PDF file</a>.</p>\n<script async class=\"speakerdeck-embed\" data-id=\"f7d88d006b7b0130f9cd123139183c09\" data-ratio=\"1.33333333333333\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n",
				"content_text": "\nThis week, I gave a talk at [Ruby on Ales](http://ruby.onales.com/) about CVEs, responsible disclosure, and security process and best practices. The conference was awesome, and it was a blast to be able to give a talk there. I hope that rubyists will be able to embrace and extend the best practices discovered by years of community experimentation!\n\nI'll update this post with a link to the video once it's posted. In the meantime, you can check out the slides here, [over on SpeakerDeck](https://speakerdeck.com/indirect/security-is-hard-but-we-cant-go-shopping), or [as a PDF file](/2013/08/22/security-is-hard-but-we-cant-go-shopping/security-is-hard.pdf).\n\n<script async class=\"speakerdeck-embed\" data-id=\"f7d88d006b7b0130f9cd123139183c09\" data-ratio=\"1.33333333333333\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n",
				"date_published": "2013-03-09T00:00:00-08:00",
				"url": "https://andre.arko.net/2013/03/09/security-is-hard/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2013/02/21/enumerablemapfind/",
				"title": "Enumerable#map_find",
				"content_html": "<p>It doesn’t happen very often, but the other day I wanted a method on <code>Enumerable</code> that doesn’t exist. I had a list of regular expressions in <code>patterns</code>, and I was trying to find the expression that matched a given <code>string</code>. Rather than just return the pattern, though, I wanted to get the <code>MatchData</code> object returned by running <code>#match</code>. That’s not possible using <code>#find</code>, since it just returns the object itself, rather than the result of the code that ran inside the block passed to <code>#find</code>. The code looked like this:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">find_match</span>(string, patterns)\n</span></span><span style=\"display:flex;\"><span>  patterns<span style=\"color:#f92672\">.</span>find{<span style=\"color:#f92672\">|</span>p<span style=\"color:#f92672\">|</span> p<span style=\"color:#f92672\">.</span>match(s) }\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span>\n</span></span></code></pre></div><p>While reading the standard library documentation, I did manage to discover one way to get what I actually wanted. <code>Rexexp.last_match</code> is a thread-local global that returns the <code>MatchData</code> object from the last regular expression match method that was run. The global-ish access made me feel dirty, but the implementation worked:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">find_match</span>(string, patterns)\n</span></span><span style=\"display:flex;\"><span>  patterns<span style=\"color:#f92672\">.</span>find{<span style=\"color:#f92672\">|</span>p<span style=\"color:#f92672\">|</span> p<span style=\"color:#f92672\">.</span>match(s) }\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">Rexegp</span><span style=\"color:#f92672\">.</span>last_match\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span>\n</span></span></code></pre></div><p>I wasn’t very satisfied with that, so I asked some other Rubyists as well, but none of us could come up with anything that already existed. In the process of discussing the problem, though, I realized there was a general solution that would provide the result I wanted. It doesn’t use globals, and it works for any code, not just regular expressions matches.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">find_match</span>(string, patterns)\n</span></span><span style=\"display:flex;\"><span>  match_data <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">nil</span>\n</span></span><span style=\"display:flex;\"><span>  patterns<span style=\"color:#f92672\">.</span>find{<span style=\"color:#f92672\">|</span>p<span style=\"color:#f92672\">|</span> match_data <span style=\"color:#f92672\">=</span> p<span style=\"color:#f92672\">.</span>match(s) }\n</span></span><span style=\"display:flex;\"><span>  match_data\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span>\n</span></span></code></pre></div><p>It works using a lesser-known and slightly sketchy Ruby trick: if you declare a variable before you call a block, and the block assigns a value to the same variable, the value set by the block will be available after the block is done running. It’s not very intuitive, and potentially dangerous in other situations where it happens by accident, but it works great in this case.</p>\n<p>Once I had a working solution, I realized I could easily abstract that solution into a method on <code>Enumerable</code>. I wasn’t sure what to call it for a while, but then I realized that it combines both <code>#map</code> (returning the result of the block) and <code>#find</code> (returning the first result that is not false or nil). So I decided to call it <code>#map_find</code>, and hope that’s enough for Rubyists to successfully guess what it might do. Here’s the implementation:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">module</span> Enumerable\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">map_find</span>\n</span></span><span style=\"display:flex;\"><span>    result <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">nil</span>\n</span></span><span style=\"display:flex;\"><span>    find { <span style=\"color:#f92672\">|</span>e<span style=\"color:#f92672\">|</span> result <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">yield</span> e }\n</span></span><span style=\"display:flex;\"><span>    result\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span>\n</span></span></code></pre></div><p>And it works just as advertised:</p>\n<pre tabindex=\"0\"><code>&gt;&gt; [“a”, &#34;b&#34;, &#34;c&#34;].map_find{|l| l.match(/b/) }\n=&gt; #&lt;MatchData &#34;b&#34;&gt;\n</code></pre><p>After adding that method to <code>Enumerable</code>, it’s extremely easy to get the result that I actually wanted. The method I had been trying to implement is back down to just one line, but this time it works exactly how I was hoping it would:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">find_match</span>(string, patterns)\n</span></span><span style=\"display:flex;\"><span>  patterns<span style=\"color:#f92672\">.</span>map_find{<span style=\"color:#f92672\">|</span>p<span style=\"color:#f92672\">|</span> p<span style=\"color:#f92672\">.</span>match(s) }\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span>\n</span></span></code></pre></div>",
				"content_text": "It doesn’t happen very often, but the other day I wanted a method on `Enumerable` that doesn’t exist. I had a list of regular expressions in `patterns`, and I was trying to find the expression that matched a given `string`. Rather than just return the pattern, though, I wanted to get the `MatchData` object returned by running `#match`. That’s not possible using `#find`, since it just returns the object itself, rather than the result of the code that ran inside the block passed to `#find`. The code looked like this:\n\n```ruby\ndef find_match(string, patterns)\n  patterns.find{|p| p.match(s) }\nend\n```\n\nWhile reading the standard library documentation, I did manage to discover one way to get what I actually wanted. `Rexexp.last_match` is a thread-local global that returns the `MatchData` object from the last regular expression match method that was run. The global-ish access made me feel dirty, but the implementation worked:\n\n```ruby\ndef find_match(string, patterns)\n  patterns.find{|p| p.match(s) }\n  Rexegp.last_match\nend\n```\n\nI wasn’t very satisfied with that, so I asked some other Rubyists as well, but none of us could come up with anything that already existed. In the process of discussing the problem, though, I realized there was a general solution that would provide the result I wanted. It doesn’t use globals, and it works for any code, not just regular expressions matches.\n\n```ruby\ndef find_match(string, patterns)\n  match_data = nil\n  patterns.find{|p| match_data = p.match(s) }\n  match_data\nend\n```\n\nIt works using a lesser-known and slightly sketchy Ruby trick: if you declare a variable before you call a block, and the block assigns a value to the same variable, the value set by the block will be available after the block is done running. It’s not very intuitive, and potentially dangerous in other situations where it happens by accident, but it works great in this case.\n\nOnce I had a working solution, I realized I could easily abstract that solution into a method on `Enumerable`. I wasn’t sure what to call it for a while, but then I realized that it combines both `#map` (returning the result of the block) and `#find` (returning the first result that is not false or nil). So I decided to call it `#map_find`, and hope that’s enough for Rubyists to successfully guess what it might do. Here’s the implementation:\n\n```ruby\nmodule Enumerable\n  def map_find\n    result = nil\n    find { |e| result = yield e }\n    result\n  end\nend\n```\n\nAnd it works just as advertised:\n\n```\n>> [“a”, \"b\", \"c\"].map_find{|l| l.match(/b/) }\n=> #<MatchData \"b\">\n```\n\n\nAfter adding that method to `Enumerable`, it’s extremely easy to get the result that I actually wanted. The method I had been trying to implement is back down to just one line, but this time it works exactly how I was hoping it would:\n\n```ruby\ndef find_match(string, patterns)\n  patterns.map_find{|p| p.match(s) }\nend\n```\n",
				"date_published": "2013-02-21T00:00:00-08:00",
				"url": "https://andre.arko.net/2013/02/21/enumerablemapfind/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2013/02/02/nested-layouts-on-rails/",
				"title": "Nested layouts on Rails ~\u003e 3.1",
				"content_html": "<h3 id=\"tldr\">tl;dr</h3>\n<p>Put this in your ApplicationHelper:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">inside_layout</span>(parent_layout)\n</span></span><span style=\"display:flex;\"><span>  view_flow<span style=\"color:#f92672\">.</span>set <span style=\"color:#e6db74\">:layout</span>, capture { <span style=\"color:#66d9ef\">yield</span> }\n</span></span><span style=\"display:flex;\"><span>  render <span style=\"color:#e6db74\">template</span>: <span style=\"color:#e6db74\">&#34;layouts/</span><span style=\"color:#e6db74\">#{</span>parent_layout<span style=\"color:#e6db74\">}</span><span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span>\n</span></span></code></pre></div><p>Then create inner templates (mine are usually for a controller) that look like this:</p>\n<pre tabindex=\"0\"><code class=\"language-erb\" data-lang=\"erb\">&lt;%= inside_layout &#34;application&#34; do %&gt;\n  &lt;p&gt;before template&lt;/p&gt;\n  &lt;%= yield %&gt;\n  &lt;p&gt;after template&lt;/p&gt;\n&lt;% end %&gt;\n</code></pre><p>Works in Haml, too.</p>\n<h3 id=\"yet-another-way-to-nest-your-layouts\">Yet another way to nest your layouts</h3>\n<p>Nested layouts is a topic that has been discussed to death, both in the <a href=\"http://guides.rubyonrails.org/layouts_and_rendering.html\">official Rails views guide</a> and appears to have confused <a href=\"http://stackoverflow.com/questions/6539239/multiple-level-nested-layout-in-rails-3\">many</a> <a href=\"http://stackoverflow.com/questions/741945/nested-layouts-in-ruby-on-rails\">different</a> <a href=\"http://stackoverflow.com/questions/4208380/confused-on-advanced-rails-layout-nesting\">people</a> on Stack Overflow.</p>\n<p>The “nested layouts” from the Rails view guide have always baffled me. Ultimately, the idea that they claim is best practice boils down to setting up a bunch of custom <code>yield</code> calls in your application layout and then using the child layouts to call <code>content_for</code> a bunch of times before rendering the application layout.</p>\n<p>This week, I got asked about setting up nested layouts by someone who tried to read the Rails guide and just got confused, so I went to see if there was a better way. I ran across <a href=\"http://m.onkey.org/nested-layouts-in-rails-3\">this blog post</a>, and that led to a gist demonstrating <a href=\"https://gist.github.com/740835\">layout nesting via partials</a>. While those options did actually work, I didn’t like the semi-magic call to <code>parent_template</code> at the end of the first option, and I didn’t like the requirement of a partial in the second.</p>\n<p>After beating my head against nested yield calls for a while, I managed to come up with a two-line helper method that takes a block with the “inner” layout inside it. I’m pretty happy with how it came out.</p>\n<p>The <code>inside_layout</code> method just takes the name of the parent layout and a block with the inner layout contents. The inner layout must call <code>yield</code>, as usual, where the action template should be inserted. Since you read past the tl;dr, I’ll even explain what’s going on for you! :D</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">inside_layout</span>(parent_layout)\n</span></span><span style=\"display:flex;\"><span>    view_flow<span style=\"color:#f92672\">.</span>set <span style=\"color:#e6db74\">:layout</span>, capture { <span style=\"color:#66d9ef\">yield</span> }\n</span></span><span style=\"display:flex;\"><span>    render <span style=\"color:#e6db74\">template</span>: <span style=\"color:#e6db74\">&#34;layouts/</span><span style=\"color:#e6db74\">#{</span>parent_layout<span style=\"color:#e6db74\">}</span><span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">end</span>\n</span></span></code></pre></div><p>I didn’t know this before, but Rails 3.1+ views (and helpers) all have <a href=\"https://github.com/rails/rails/blob/master/actionpack/lib/action_view/flows.rb#L4\">a <code>view_flow</code> object</a> available to them that manages all of the various <code>content_for</code> sections. Using that object, we swap out what will be inserted when the next template rendered calls <code>yield</code>. What we put in instead is the result of calling <code>capture</code> with the block that was passed to our helper. That method returns a string with the result of rendering the block. Finally, we simply render the parent layout, knowing that when it calls <code>yield</code>, it will get the result of rendering our entire inner layout’s block. Pretty sweet.</p>\n",
				"content_text": "\n### tl;dr\n\nPut this in your ApplicationHelper:\n\n```ruby\ndef inside_layout(parent_layout)\n  view_flow.set :layout, capture { yield }\n  render template: \"layouts/#{parent_layout}\"\nend\n```\n\nThen create inner templates (mine are usually for a controller) that look like this:\n\n```erb\n<%= inside_layout \"application\" do %>\n  <p>before template</p>\n  <%= yield %>\n  <p>after template</p>\n<% end %>\n```\n\nWorks in Haml, too.\n\n### Yet another way to nest your layouts\n\nNested layouts is a topic that has been discussed to death, both in the [official Rails views guide](http://guides.rubyonrails.org/layouts_and_rendering.html) and appears to have confused [many](http://stackoverflow.com/questions/6539239/multiple-level-nested-layout-in-rails-3) [different](http://stackoverflow.com/questions/741945/nested-layouts-in-ruby-on-rails) [people](http://stackoverflow.com/questions/4208380/confused-on-advanced-rails-layout-nesting) on Stack Overflow.\n\nThe “nested layouts” from the Rails view guide have always baffled me. Ultimately, the idea that they claim is best practice boils down to setting up a bunch of custom `yield` calls in your application layout and then using the child layouts to call `content_for` a bunch of times before rendering the application layout.\n\nThis week, I got asked about setting up nested layouts by someone who tried to read the Rails guide and just got confused, so I went to see if there was a better way. I ran across [this blog post](http://m.onkey.org/nested-layouts-in-rails-3), and that led to a gist demonstrating [layout nesting via partials](https://gist.github.com/740835). While those options did actually work, I didn’t like the semi-magic call to `parent_template` at the end of the first option, and I didn’t like the requirement of a partial in the second.\n\nAfter beating my head against nested yield calls for a while, I managed to come up with a two-line helper method that takes a block with the “inner” layout inside it. I’m pretty happy with how it came out.\n\nThe `inside_layout` method just takes the name of the parent layout and a block with the inner layout contents. The inner layout must call `yield`, as usual, where the action template should be inserted. Since you read past the tl;dr, I’ll even explain what’s going on for you! :D\n\n```ruby\n  def inside_layout(parent_layout)\n    view_flow.set :layout, capture { yield }\n    render template: \"layouts/#{parent_layout}\"\n  end\n```\n\nI didn’t know this before, but Rails 3.1+ views (and helpers) all have [a `view_flow` object](https://github.com/rails/rails/blob/master/actionpack/lib/action_view/flows.rb#L4) available to them that manages all of the various `content_for` sections. Using that object, we swap out what will be inserted when the next template rendered calls `yield`. What we put in instead is the result of calling `capture` with the block that was passed to our helper. That method returns a string with the result of rendering the block. Finally, we simply render the parent layout, knowing that when it calls `yield`, it will get the result of rendering our entire inner layout’s block. Pretty sweet.\n",
				"date_published": "2013-02-02T00:00:00-08:00",
				"url": "https://andre.arko.net/2013/02/02/nested-layouts-on-rails/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2013/01/07/a-tale-of-two-ys/",
				"title": "A Tale of Two ‘Y’s",
				"content_html": "<p>Today marks a momentous occasion: today, <a href=\"http://support.apple.com/kb/TS4510\">scheduled Do Not Disturb mode on iOS starts working again</a>! While the exact reasons that the feature stopped working between January 1 and January 7 are unlikely to ever be explained, investigating date bugs during the new year led me to discover a fascinating (and horrifying) aspect of dates on modern computers: the <a href=\"http://en.wikipedia.org/wiki/ISO_8601\">ISO 8601</a> <a href=\"http://en.wikipedia.org/wiki/ISO_week_date\">Week Date</a>.</p>\n<p>The best (or worst, really, if you&rsquo;re a programmer) part of the Week Date is the Week Year &ndash; it&rsquo;s a four-digit year that looks identical to the regular year. Except between December 28th and January 4th, when it might be one year later or one year earlier than the regular year. Awesome, right?</p>\n<p>In Ruby, or C, or any language that uses <code>strftime()</code> to format dates, it&rsquo;s not that hard to make sure that you get the regular year instead of the week year. According to the <a href=\"http://pubs.opengroup.org/onlinepubs/009695399/functions/strftime.html\">IEEE standard</a>, <code>%Y</code> and <code>%y</code> are both the regular year (in four and two digits, respectively), while <code>%G</code> and <code>%g</code> are the week year. Where it gets tricky is in other languages, like Objective-C, that use <a href=\"http://www.unicode.org/reports/tr35/tr35-25.html#Date_Format_Patterns\">Unicode date format patterns</a>. In those strings, <code>mm</code> represents the minute, and <code>MM</code> represents the month. <code>DD</code> will print the two-digit day of the year, while <code>dd</code> will print the two-digit day of the month. So, to get the month and day, you use a string like <code>MM-dd</code>. With that precedent, you might think that adding the year would mean using <code>YYYY-MM-dd</code>. Unfortunately, in a case of horrible ambiguity that is extremely hard to catch in advance, <code>YYYY</code> is the week year, while <code>yyyy</code> is the regular year.</p>\n<p>At that point, the main way to find out that you&rsquo;re using the wrong capitalization in your pattern is to notice that your application thinks it&rsquo;s a different year for a few days around the new year. And that seems bad. So everybody that uses date format patterns: write a script that will complain about uses of <code>YYYY</code>. Seriously. You&rsquo;ll probably need it later.</p>\n<p class=\"aside\">Reposted from the <a href=\"http://dev.mavenlink.com/blog/2013/1/9/a-tale-of-two-ys\">Mavenlink Dev Blog</a>.</p>\n",
				"content_text": "\nToday marks a momentous occasion: today, [scheduled Do Not Disturb mode on iOS starts working again](http://support.apple.com/kb/TS4510)! While the exact reasons that the feature stopped working between January 1 and January 7 are unlikely to ever be explained, investigating date bugs during the new year led me to discover a fascinating (and horrifying) aspect of dates on modern computers: the [ISO 8601](http://en.wikipedia.org/wiki/ISO_8601) [Week Date](http://en.wikipedia.org/wiki/ISO_week_date).\n\nThe best (or worst, really, if you're a programmer) part of the Week Date is the Week Year -- it's a four-digit year that looks identical to the regular year. Except between December 28th and January 4th, when it might be one year later or one year earlier than the regular year. Awesome, right?\n\nIn Ruby, or C, or any language that uses `strftime()` to format dates, it's not that hard to make sure that you get the regular year instead of the week year. According to the [IEEE standard](http://pubs.opengroup.org/onlinepubs/009695399/functions/strftime.html), `%Y` and `%y` are both the regular year (in four and two digits, respectively), while `%G` and `%g` are the week year. Where it gets tricky is in other languages, like Objective-C, that use [Unicode date format patterns](http://www.unicode.org/reports/tr35/tr35-25.html#Date_Format_Patterns). In those strings, `mm` represents the minute, and `MM` represents the month. `DD` will print the two-digit day of the year, while `dd` will print the two-digit day of the month. So, to get the month and day, you use a string like `MM-dd`. With that precedent, you might think that adding the year would mean using `YYYY-MM-dd`. Unfortunately, in a case of horrible ambiguity that is extremely hard to catch in advance, `YYYY` is the week year, while `yyyy` is the regular year.\n\nAt that point, the main way to find out that you're using the wrong capitalization in your pattern is to notice that your application thinks it's a different year for a few days around the new year. And that seems bad. So everybody that uses date format patterns: write a script that will complain about uses of `YYYY`. Seriously. You'll probably need it later.\n\n<p class=\"aside\">Reposted from the <a href=\"http://dev.mavenlink.com/blog/2013/1/9/a-tale-of-two-ys\">Mavenlink Dev Blog</a>.</p>\n",
				"date_published": "2013-01-07T00:00:00-08:00",
				"url": "https://andre.arko.net/2013/01/07/a-tale-of-two-ys/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2012/11/04/replacing-skitch-with-cloud-and/",
				"title": "Replacing Skitch with Cloud and Alfred",
				"content_html": "<p>Recently, I was sad to discover that Evernote&rsquo;s purchase of Skitch has finally resulted in the dreaded &ldquo;discontinuation of service&rdquo; for the truly great Skitch.com service. While searching for a replacement, I briefly contemplated downgrading Skitch and setting up my own server for images. After a little more thought, though, I remembered how terrible the uptime was on the last personal service I ran, and decided to go with something hosted instead. <a href=\"http://cl.ly\">Cloud</a> is a neat service that provides super easy upload of text, audio, files, and even images. The screenshot integration, though, is what got me to take a serious look.</p>\n<p>By combining CloudApp&rsquo;s upload hotkey with an Alfred extension that extracts direct image links, I was able to recreate my Skitch Pro workflow pretty closely. It doesn&rsquo;t provide the nice &ldquo;scribble on your screenshot&rdquo; functionality that the Skitch app does, but the workflow I settled on means I can use the new Skitch for that without having to upload to Evernote.</p>\n<p>So, without further ado, here&rsquo;s the setup:</p>\n<ol>\n<li>Install the Cloud Mac app. It&rsquo;s on the Mac App Store or free to download at <a href=\"http://cl.ly\">cl.ly</a>.</li>\n<li>Create a Cloud account (the paid accounts can even use a custom subdomain for links, if you like.)</li>\n<li>Using the Keyboard sytem preference pane, change the keyboard shortcut ⌘⇧4 to copy the screenshot to the clipboard instead of saving it to a file on your desktop. Or just always use ⌘⇧⌃4 to take a screenshot.</li>\n<li>Set a keyboard shortcut to upload the clipboard in CloudApp. I use the default, ⌘⌃C.</li>\n<li>Install <a href=\"https://cl.ly/0Q05361j243G/download/CloudApp%20Image.alfredextension\">this Alfred extension</a>.</li>\n</ol>\n<p>Now you&rsquo;re set! To share a screenshot, push take your screenshot, hit ⌥⌃C and then run the Alfred action. Now you can paste a direct link to the image into Campfire or wherever you like.</p>\n",
				"content_text": "Recently, I was sad to discover that Evernote's purchase of Skitch has finally resulted in the dreaded \"discontinuation of service\" for the truly great Skitch.com service. While searching for a replacement, I briefly contemplated downgrading Skitch and setting up my own server for images. After a little more thought, though, I remembered how terrible the uptime was on the last personal service I ran, and decided to go with something hosted instead. [Cloud](http://cl.ly) is a neat service that provides super easy upload of text, audio, files, and even images. The screenshot integration, though, is what got me to take a serious look.\n\nBy combining CloudApp's upload hotkey with an Alfred extension that extracts direct image links, I was able to recreate my Skitch Pro workflow pretty closely. It doesn't provide the nice \"scribble on your screenshot\" functionality that the Skitch app does, but the workflow I settled on means I can use the new Skitch for that without having to upload to Evernote.\n\nSo, without further ado, here's the setup:\n\n1. Install the Cloud Mac app. It's on the Mac App Store or free to download at [cl.ly](http://cl.ly).\n2. Create a Cloud account (the paid accounts can even use a custom subdomain for links, if you like.)\n3. Using the Keyboard sytem preference pane, change the keyboard shortcut ⌘⇧4 to copy the screenshot to the clipboard instead of saving it to a file on your desktop. Or just always use ⌘⇧⌃4 to take a screenshot.\n4. Set a keyboard shortcut to upload the clipboard in CloudApp. I use the default, ⌘⌃C.\n5. Install [this Alfred extension](https://cl.ly/0Q05361j243G/download/CloudApp%20Image.alfredextension).\n\nNow you're set! To share a screenshot, push take your screenshot, hit ⌥⌃C and then run the Alfred action. Now you can paste a direct link to the image into Campfire or wherever you like.\n",
				"date_published": "2012-11-04T00:00:00-08:00",
				"url": "https://andre.arko.net/2012/11/04/replacing-skitch-with-cloud-and/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2012/11/01/open-source-at-mavenlink/",
				"title": "Open Source at Mavenlink",
				"content_html": "<p>I&rsquo;ve started a new job, and I&rsquo;m pretty excited about it. <a href=\"http://www.mavenlink.com\">Mavenlink</a> has hired me to be their new open source lead. Part of my new job is going to be working with their engineers to give open source a bigger presence at Mavenlink, and that is pretty cool. Another part will include improving Mavenlink&rsquo;s API and making it easier for developers to use. The part I am most excited about, though, is that I&rsquo;m going to be working directly on improving the Ruby and Rails open source ecosystem. That will include spending more time improving Bundler, jquery-rails, and my other OSS projects, as well as extracting and open sourcing libraries and components that Mavenlink has built.</p>\n<p>I have <a href=\"/2012/07/23/towards-a-bundler-plugin-system/\">big plans</a> for the future of Bundler. I&rsquo;m excited to have Mavenlink supporting those efforts, and proud to be working with a team that values contributing back to the Ruby community.</p>\n<p>🎉🎊😸🎈🎉</p>\n",
				"content_text": "I've started a new job, and I'm pretty excited about it. [Mavenlink](http://www.mavenlink.com) has hired me to be their new open source lead. Part of my new job is going to be working with their engineers to give open source a bigger presence at Mavenlink, and that is pretty cool. Another part will include improving Mavenlink's API and making it easier for developers to use. The part I am most excited about, though, is that I'm going to be working directly on improving the Ruby and Rails open source ecosystem. That will include spending more time improving Bundler, jquery-rails, and my other OSS projects, as well as extracting and open sourcing libraries and components that Mavenlink has built.\n\nI have [big plans](/2012/07/23/towards-a-bundler-plugin-system/) for the future of Bundler. I'm excited to have Mavenlink supporting those efforts, and proud to be working with a team that values contributing back to the Ruby community.\n\n🎉🎊😸🎈🎉\n",
				"date_published": "2012-11-01T00:00:00-08:00",
				"url": "https://andre.arko.net/2012/11/01/open-source-at-mavenlink/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2012/09/18/force-encoding-in-javascript/",
				"title": "Force encoding in JavaScript",
				"content_html": "<p>So first, I should probably set the scene: in a special-purpose browser, automated and driven by JavaScript, there was a string that made no sense. Anytime it should have contained an apostrophe, it produced invalid JSON. Why would it do that, you ask? Well, after far too long digging into the way that smart quotes are represented in Unicode, I finally figured it out. A single left apostrophe, also known as \\u2019, takes up three bytes. The third byte happens to be \\x19, the obscure ^Y control character. Control characters aren&rsquo;t allowed in JSON strings, and so things exploded.</p>\n<p>Some investigation of the website in question turned up the encoding problem, which was a <code>meta</code> tag that declared the page was encoded as ISO-8859-1. The actual code writing out the contents of the page, however, was apparently set to output UTF-8. As it turns out, if those three bytes that represent an apostrophe are decoded as ISO-8859-1, they look like <code>â€˜</code> in a browser.</p>\n<p>Finally knowing what the problem was, I was still pretty stumped &ndash; how can I take the string containing UTF-8 bytes, but interpreted as ISO-8859-1, and force the encoding to UTF-8 instead? It would be trivial in Ruby 1.9, where the String class has a <code>force_encoding</code> method, or in Ruby 1.8, where I could just use the Iconv library. But JavaScript, as a language, doesn&rsquo;t even have a concept of string encodings! Strings are all stored in UTF-16 or UCS-2 format, and so there&rsquo;s no way to manually force a string to be interpreted as a certain encoding.</p>\n<p>Right as I was getting ready to despair, though, I had an inkling of an idea. I realized that one of the blog posts I had just read, about <a href=\"http://ecmanaut.blogspot.com/2006/07/encoding-decoding-utf8-in-javascript.html\">encoding and decoding UTF-8 in JavaScript</a>, might provide me with a way to decode the bytes I had as if they were UTF-8 bytes. Pretty surprisingly (at least to me), you can use <code>encodeURIComponent()</code> to turn a stream of bytes into a percent-escaped stream of bytes, and then use <code>unescape()</code> to interpret the stream of percent escaped bytes as UTF-8. So I tried it, and it worked perfectly.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-javascript\" data-lang=\"javascript\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">function</span> <span style=\"color:#a6e22e\">forceUnicodeEncoding</span>(<span style=\"color:#a6e22e\">string</span>) {\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">return</span> <span style=\"color:#a6e22e\">unescape</span>(encodeURIComponent(<span style=\"color:#a6e22e\">string</span>));\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><p>So after all that, I can happily inform you that you <em>can</em> force the encoding of a JavaScript string! But only to UTF-8, and only when the encoding has mistakenly been declared to be something else. Lucky for me, that&rsquo;s exactly what I needed.</p>\n",
				"content_text": "So first, I should probably set the scene: in a special-purpose browser, automated and driven by JavaScript, there was a string that made no sense. Anytime it should have contained an apostrophe, it produced invalid JSON. Why would it do that, you ask? Well, after far too long digging into the way that smart quotes are represented in Unicode, I finally figured it out. A single left apostrophe, also known as \\u2019, takes up three bytes. The third byte happens to be \\x19, the obscure ^Y control character. Control characters aren't allowed in JSON strings, and so things exploded.\n\nSome investigation of the website in question turned up the encoding problem, which was a `meta` tag that declared the page was encoded as ISO-8859-1. The actual code writing out the contents of the page, however, was apparently set to output UTF-8. As it turns out, if those three bytes that represent an apostrophe are decoded as ISO-8859-1, they look like `â€˜` in a browser.\n\nFinally knowing what the problem was, I was still pretty stumped -- how can I take the string containing UTF-8 bytes, but interpreted as ISO-8859-1, and force the encoding to UTF-8 instead? It would be trivial in Ruby 1.9, where the String class has a `force_encoding` method, or in Ruby 1.8, where I could just use the Iconv library. But JavaScript, as a language, doesn't even have a concept of string encodings! Strings are all stored in UTF-16 or UCS-2 format, and so there's no way to manually force a string to be interpreted as a certain encoding.\n\nRight as I was getting ready to despair, though, I had an inkling of an idea. I realized that one of the blog posts I had just read, about [encoding and decoding UTF-8 in JavaScript](http://ecmanaut.blogspot.com/2006/07/encoding-decoding-utf8-in-javascript.html), might provide me with a way to decode the bytes I had as if they were UTF-8 bytes. Pretty surprisingly (at least to me), you can use `encodeURIComponent()` to turn a stream of bytes into a percent-escaped stream of bytes, and then use `unescape()` to interpret the stream of percent escaped bytes as UTF-8. So I tried it, and it worked perfectly.\n\n```javascript\nfunction forceUnicodeEncoding(string) {\n  return unescape(encodeURIComponent(string));\n}\n```\n\nSo after all that, I can happily inform you that you _can_ force the encoding of a JavaScript string! But only to UTF-8, and only when the encoding has mistakenly been declared to be something else. Lucky for me, that's exactly what I needed.\n",
				"date_published": "2012-09-18T00:00:00-08:00",
				"url": "https://andre.arko.net/2012/09/18/force-encoding-in-javascript/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2012/07/23/towards-a-bundler-plugin-system/",
				"title": "Towards a Bundler plugin system",
				"content_html": "<h3 id=\"where-we-are-now\">Where we are now</h3>\n<p>Bundler 1.2 has reached <a href=\"https://github.com/carlhuda/bundler/blob/4f022aa998ee642c61740f1a011798aaf3a05cc7/CHANGELOG.md#120rc-jul-17-2012\">release candidate</a> status, and as we wind down that release, I’ve been thinking a lot about how Bundler can grow into a flexible underpinning for dependency-based tools. As it stands today, Bundler provides a relatively small set of tools to help you manage dependencies. The Ruby community has shown that they are interested in adding to those tools. We’ve been supplied with pull requests that add commands like <code>bundle viz</code>, <code>bundle outdated</code>, <code>bundle grep</code>, <code>bundle ack</code>, <code>bundle do</code>, and others.</p>\n<p>While it’s fantastic that the community is actively interested in expanding the functionality of a tool that they use frequently, we’ve discovered two main downsides to collecting everything in the core Bundler gem and repository. The first downside is that accepting features into Bundler core means that the people maintaining Bundler (currently just <a href=\"http://github.com/hone\">hone</a> and myself) have to take over maintenance of that code. With the notable exception of <a href=\"http://github.com/joelmoss\">joelmoss</a>, the author of the <code>outdated</code> command, contributors have tended to not maintain their code in Bundler.</p>\n<p>As the surface area of Bundler has grown, a larger and larger portion of our time working on it has been spent just providing support for and maintenance on features that already exist — this unfortunately slows down both bugfixes and new features. Because of this, I’ve become more and more hesitant to accept contributed commands and features into Bundler. That hesitance makes me sad. The community has created some awesome things, and I’m sure they would be extremely useful to some users. The unpleasant reality is that every new feature reduces the amount of time available to improve the core of Bundler (unless the new feature is both universally applicable and well-maintained by its contributor, like <code>outdated</code>).</p>\n<p>In an effort to have my cake and eat it too, I’m hoping to come up with a system for Bundler plugins that will allow contributors to create new commands and features, and then provide them to anyone in the community who wants them. Plugins that have a solid track record would then become excellent candidates for inclusion into core. Plugins allow developers to show that they’re willing to fix bugs and keep their code running, and allow users to demonstrate which features are the most universally useful. So, now that I’ve talked your ear off, how will these plugins work? After mulling things over for a while, here’s what I’ve come up with.</p>\n<h3 id=\"an-opt-in-extension-system\">An opt-in extension system</h3>\n<p>Plugins for Bundler need to do a few main things. They should be easy to install and easy to manage. I also think it’s important that they aren’t enabled automatically — just downloading a plugin onto your system shouldn’t modify Bundler’s behaviour unless you explicitly opt-in to using the plugin. Enabling plugins that way has a convenient side-effect: one user can enable a plugin without enabling it for other users who share their system gems. So, how can we accomplish those goals?</p>\n<h4 id=\"distribution\">Distribution</h4>\n<p>Plugins can be written and distributed as gems, using the extension namespace convention. For example, the <code>bundler-grep</code> gem could provide the command <code>bundle grep</code>. However, simply installing a gem plugin can’t activate it, so we need a way to track which plugins are enabled.</p>\n<p>Happily, Bundler already ships with a config system to handle per-user settings. Activating plugins can be managed using the <code>bundle config</code> command (or possibly using a direct <code>bundle plugin</code> command that wraps config). When Bundler starts, it can check the user’s config to see if there are any plugins that it should load. If there are, it can load those plugins using the standard Rubygems require system.</p>\n<h4 id=\"capabilities\">Capabilities</h4>\n<p>So what will plugins be able to do? Based on pull requests so far, the most common request is to add a new command. A great example of this are the recent proposals to add the commands <a href=\"https://github.com/carlhuda/bundler/pull/1898\"><code>licenses</code></a> and <a href=\"https://github.com/carlhuda/bundler/pull/2024\"><code>grep</code></a>. Past commands that would have been much easier to ship with a plugin system include <code>viz</code> and <code>outdated</code>. With the ability to release plugins, direct oversight isn’t needed for additional functionality to be available to those who want to write it or use it.</p>\n<p>Along with new commands, the plugin system should also provide hooks for typical activites. For example, some users want to be able to generate documentation (or even generate ctags) for any gems that are installed in their bundle. Callbacks for the gem installation process would allow that to happen. The events that I’m sure could use callbacks are installing gems, updating gem versions, and resolving a Gemfile for the first time. I’m open to suggestions about other times when callbacks would be useful, though.</p>\n<h3 id=\"so-what-now\">So what now?</h3>\n<p>I’m going to start implementing the plugin system that I outlined in this post. If I’m very lucky, I might even finish it in time for Bundler 1.3. Since things are just getting going, I’m very interested in feedback, questions, and concerns. I’ve opened a <a href=\"https://github.com/carlhuda/bundler/issues/1945\">plugins ticket</a> for discussion. I would like to make sure that the plugin system that gets implemented can address the needs of most users immediately. If you’re interested in writing a plugin, maybe we can work together. I’m looking forward to seeing what the Ruby community will come up with once they have more room to experiment.</p>\n",
				"content_text": "\n### Where we are now\n\nBundler 1.2 has reached [release candidate][rc] status, and as we wind down that release, I’ve been thinking a lot about how Bundler can grow into a flexible underpinning for dependency-based tools. As it stands today, Bundler provides a relatively small set of tools to help you manage dependencies. The Ruby community has shown that they are interested in adding to those tools. We’ve been supplied with pull requests that add commands like `bundle viz`, `bundle outdated`, `bundle grep`, `bundle ack`, `bundle do`, and others.\n\nWhile it’s fantastic that the community is actively interested in expanding the functionality of a tool that they use frequently, we’ve discovered two main downsides to collecting everything in the core Bundler gem and repository. The first downside is that accepting features into Bundler core means that the people maintaining Bundler (currently just [hone][hone] and myself) have to take over maintenance of that code. With the notable exception of [joelmoss][joelmoss], the author of the `outdated` command, contributors have tended to not maintain their code in Bundler.\n\nAs the surface area of Bundler has grown, a larger and larger portion of our time working on it has been spent just providing support for and maintenance on features that already exist — this unfortunately slows down both bugfixes and new features. Because of this, I’ve become more and more hesitant to accept contributed commands and features into Bundler. That hesitance makes me sad. The community has created some awesome things, and I’m sure they would be extremely useful to some users. The unpleasant reality is that every new feature reduces the amount of time available to improve the core of Bundler (unless the new feature is both universally applicable and well-maintained by its contributor, like `outdated`).\n\nIn an effort to have my cake and eat it too, I’m hoping to come up with a system for Bundler plugins that will allow contributors to create new commands and features, and then provide them to anyone in the community who wants them. Plugins that have a solid track record would then become excellent candidates for inclusion into core. Plugins allow developers to show that they’re willing to fix bugs and keep their code running, and allow users to demonstrate which features are the most universally useful. So, now that I’ve talked your ear off, how will these plugins work? After mulling things over for a while, here’s what I’ve come up with.\n\n### An opt-in extension system\n\nPlugins for Bundler need to do a few main things. They should be easy to install and easy to manage. I also think it’s important that they aren’t enabled automatically — just downloading a plugin onto your system shouldn’t modify Bundler’s behaviour unless you explicitly opt-in to using the plugin. Enabling plugins that way has a convenient side-effect: one user can enable a plugin without enabling it for other users who share their system gems. So, how can we accomplish those goals?\n\n#### Distribution\n\nPlugins can be written and distributed as gems, using the extension namespace convention. For example, the `bundler-grep` gem could provide the command `bundle grep`. However, simply installing a gem plugin can’t activate it, so we need a way to track which plugins are enabled.\n\nHappily, Bundler already ships with a config system to handle per-user settings. Activating plugins can be managed using the `bundle config` command (or possibly using a direct `bundle plugin` command that wraps config). When Bundler starts, it can check the user’s config to see if there are any plugins that it should load. If there are, it can load those plugins using the standard Rubygems require system.\n\n#### Capabilities\n\nSo what will plugins be able to do? Based on pull requests so far, the most common request is to add a new command. A great example of this are the recent proposals to add the commands [`licenses`][license] and [`grep`][grep]. Past commands that would have been much easier to ship with a plugin system include `viz` and `outdated`. With the ability to release plugins, direct oversight isn’t needed for additional functionality to be available to those who want to write it or use it.\n\nAlong with new commands, the plugin system should also provide hooks for typical activites. For example, some users want to be able to generate documentation (or even generate ctags) for any gems that are installed in their bundle. Callbacks for the gem installation process would allow that to happen. The events that I’m sure could use callbacks are installing gems, updating gem versions, and resolving a Gemfile for the first time. I’m open to suggestions about other times when callbacks would be useful, though.\n\n### So what now?\n\nI’m going to start implementing the plugin system that I outlined in this post. If I’m very lucky, I might even finish it in time for Bundler 1.3. Since things are just getting going, I’m very interested in feedback, questions, and concerns. I’ve opened a [plugins ticket][ticket] for discussion. I would like to make sure that the plugin system that gets implemented can address the needs of most users immediately. If you’re interested in writing a plugin, maybe we can work together. I’m looking forward to seeing what the Ruby community will come up with once they have more room to experiment.\n\n[rc]: https://github.com/carlhuda/bundler/blob/4f022aa998ee642c61740f1a011798aaf3a05cc7/CHANGELOG.md#120rc-jul-17-2012\n[hone]: http://github.com/hone\n[joelmoss]: http://github.com/joelmoss\n[license]: https://github.com/carlhuda/bundler/pull/1898\n[grep]: https://github.com/carlhuda/bundler/pull/2024\n[ticket]: https://github.com/carlhuda/bundler/issues/1945\n",
				"date_published": "2012-07-23T00:00:00-08:00",
				"url": "https://andre.arko.net/2012/07/23/towards-a-bundler-plugin-system/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2012/06/24/detached-git-status-line/",
				"title": "Detached git status line",
				"content_html": "<p>It&rsquo;s been <a href=\"/2007/12/19/git-branch-in-prompt-with-svn-support/\">a really long time</a> since I posted about a prompt with git status support built in. I don&rsquo;t care so much about svn these days, but something that has bothered me for quite a while about the default git status line is that it&rsquo;s pretty useless if you aren&rsquo;t at the tip of a branch. The default <code>__git_ps1</code> function simply returns the sha of the current commit. It&rsquo;s really unhelpful to simply see <code>(abc1234...)</code>, especially when you&rsquo;re doing a git bisect or something like that.</p>\n<p>Instead of just using <code>__git_ps1</code>, you can spruce up your git prompt to tell you exactly where you are! Git knows that commit <code>abc1234</code> is actually <code>master~2</code> or <code>feature_branch~25</code>. Wouldn&rsquo;t it be more helpful to see that?</p>\n<p>After some agonizing, I&rsquo;ve managed to glue together a couple of git commands that actually provide that extremely useful information. There was a minor bug that put &ldquo;master&rdquo; in the prompt if you had just created a new branch, but I got that fixed too.</p>\n<p>Implemented in glorious, horrifying bash script, here is the somewhat more informative git prompt:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">function</span> parse_git_dirty <span style=\"color:#f92672\">{</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#f92672\">[[</span> <span style=\"color:#66d9ef\">$(</span>git status 2&gt; /dev/null | tail -n1<span style=\"color:#66d9ef\">)</span> !<span style=\"color:#f92672\">=</span> <span style=\"color:#e6db74\">&#34;nothing to commit (working directory clean)&#34;</span> <span style=\"color:#f92672\">]]</span> <span style=\"color:#f92672\">&amp;&amp;</span> echo <span style=\"color:#e6db74\">&#34;⚡&#34;</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#f92672\">}</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">function</span> parse_git_branch <span style=\"color:#f92672\">{</span>\n</span></span><span style=\"display:flex;\"><span>  local b<span style=\"color:#f92672\">=</span><span style=\"color:#e6db74\">&#34;</span><span style=\"color:#66d9ef\">$(</span>git branch --no-color 2&gt; /dev/null | sed -e <span style=\"color:#e6db74\">&#39;/^[^*]/d&#39;</span> -e <span style=\"color:#e6db74\">&#39;s/^* //&#39;</span><span style=\"color:#66d9ef\">)</span><span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">if</span> <span style=\"color:#f92672\">[</span> -n <span style=\"color:#e6db74\">&#34;</span>$b<span style=\"color:#e6db74\">&#34;</span> <span style=\"color:#f92672\">]</span> <span style=\"color:#f92672\">&amp;&amp;</span> <span style=\"color:#f92672\">[</span> <span style=\"color:#e6db74\">&#34;</span>$b<span style=\"color:#e6db74\">&#34;</span> <span style=\"color:#f92672\">=</span> <span style=\"color:#e6db74\">&#34;(no branch)&#34;</span> <span style=\"color:#f92672\">]</span>; <span style=\"color:#66d9ef\">then</span>\n</span></span><span style=\"display:flex;\"><span>    local b<span style=\"color:#f92672\">=</span><span style=\"color:#e6db74\">&#34;</span><span style=\"color:#66d9ef\">$(</span>git name-rev --name-only HEAD 2&gt; /dev/null<span style=\"color:#66d9ef\">)</span><span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">fi</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">if</span> <span style=\"color:#f92672\">[</span> -n <span style=\"color:#e6db74\">&#34;</span>$b<span style=\"color:#e6db74\">&#34;</span> <span style=\"color:#f92672\">]</span>; <span style=\"color:#66d9ef\">then</span>\n</span></span><span style=\"display:flex;\"><span>    printf <span style=\"color:#e6db74\">&#34;(</span>$b<span style=\"color:#66d9ef\">$(</span>parse_git_dirty<span style=\"color:#66d9ef\">)</span><span style=\"color:#e6db74\">)&#34;</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">fi</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#f92672\">}</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>export PS1<span style=\"color:#f92672\">=</span><span style=\"color:#e6db74\">&#39;\\[\\033k\\033\\\\\\]\\[\\e[0;34m\\][\\u \\w]$(parse_git_branch)\\$\\[\\e[0;39m\\] &#39;</span>\n</span></span></code></pre></div><p>I&rsquo;ve also <a href=\"https://gist.github.com/631628\">posted this as a gist</a> with some examples if you&rsquo;d like to comment or fork my code.</p>\n",
				"content_text": "It's been [a really long time](/2007/12/19/git-branch-in-prompt-with-svn-support/) since I posted about a prompt with git status support built in. I don't care so much about svn these days, but something that has bothered me for quite a while about the default git status line is that it's pretty useless if you aren't at the tip of a branch. The default `__git_ps1` function simply returns the sha of the current commit. It's really unhelpful to simply see `(abc1234...)`, especially when you're doing a git bisect or something like that.\n\nInstead of just using `__git_ps1`, you can spruce up your git prompt to tell you exactly where you are! Git knows that commit `abc1234` is actually `master~2` or `feature_branch~25`. Wouldn't it be more helpful to see that?\n\nAfter some agonizing, I've managed to glue together a couple of git commands that actually provide that extremely useful information. There was a minor bug that put \"master\" in the prompt if you had just created a new branch, but I got that fixed too.\n\nImplemented in glorious, horrifying bash script, here is the somewhat more informative git prompt:\n\n```bash\nfunction parse_git_dirty {\n  [[ $(git status 2> /dev/null | tail -n1) != \"nothing to commit (working directory clean)\" ]] && echo \"⚡\"\n}\n\nfunction parse_git_branch {\n  local b=\"$(git branch --no-color 2> /dev/null | sed -e '/^[^*]/d' -e 's/^* //')\"\n  if [ -n \"$b\" ] && [ \"$b\" = \"(no branch)\" ]; then\n    local b=\"$(git name-rev --name-only HEAD 2> /dev/null)\"\n  fi\n\n  if [ -n \"$b\" ]; then\n    printf \"($b$(parse_git_dirty))\"\n  fi\n}\n\nexport PS1='\\[\\033k\\033\\\\\\]\\[\\e[0;34m\\][\\u \\w]$(parse_git_branch)\\$\\[\\e[0;39m\\] '\n```\n\nI've also [posted this as a gist](https://gist.github.com/631628) with some examples if you'd like to comment or fork my code.\n",
				"date_published": "2012-06-24T00:00:00-08:00",
				"url": "https://andre.arko.net/2012/06/24/detached-git-status-line/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2012/06/14/rails-on-ruby/",
				"title": "Rails 2.3.14 on Ruby 1.9.3",
				"content_html": "<p>Rails 2 hasn&rsquo;t been maintained for quite a while, and it last officially supported Ruby 1.9.1. I was working on getting an old Rails 2 app upgraded to Rails 3 and discovered something frustrating: Rails 2.3.14 don&rsquo;t boot under Ruby 1.9.3 unless every single controller has a helper class defined in a helper file. I didn&rsquo;t want to sit around creating 60 helper files, so I did the expedient thing, patching the app&rsquo;s <code>boot.rb</code> file to not raise exceptions on missing helper files.</p>\n<p>It turned out to not be a terribly large amount of code, but here&rsquo;s a diff just in case anyone else is upgrading a really old Rails app and runs into the same issue:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-diff\" data-lang=\"diff\"><span style=\"display:flex;\"><span>diff --git a/config/boot.rb b/config/boot.rb\n</span></span><span style=\"display:flex;\"><span>index 69b1a51..2c979cf 100644\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#f92672\">--- a/config/boot.rb\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#f92672\"></span><span style=\"color:#a6e22e\">+++ b/config/boot.rb\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\"></span><span style=\"color:#75715e\">@@ -61,12 +61,30 @@ module Rails\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#75715e\"></span>       require &#39;initializer&#39;\n</span></span><span style=\"display:flex;\"><span>     end\n</span></span><span style=\"display:flex;\"><span> \n</span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\">+    def monkeypatch_helpers\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\">+      require &#34;active_support&#34;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\">+      require &#39;action_controller/helpers&#39;\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\">+      ActionController::Helpers::ClassMethods.send(:define_method, :inherited_with_helper) do |child|\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\">+        inherited_without_helper(child)\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\">+\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\">+        begin\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\">+          child.master_helper_module = Module.new\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\">+          child.master_helper_module.__send__ :include, master_helper_module\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\">+          child.__send__ :default_helper_module!\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\">+        rescue MissingSourceFile =&gt; e\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\">+          raise unless e.is_missing?(&#34;helpers/#{child.controller_path}_helper&#34;)\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\">+        rescue LoadError\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\">+        end\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\">+      end\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\">+    end\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\">+\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\"></span>     def load_rails_gem\n</span></span><span style=\"display:flex;\"><span>       if version = self.class.gem_version\n</span></span><span style=\"display:flex;\"><span>         gem &#39;rails&#39;, version\n</span></span><span style=\"display:flex;\"><span>       else\n</span></span><span style=\"display:flex;\"><span>         gem &#39;rails&#39;\n</span></span><span style=\"display:flex;\"><span>       end\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\">+      monkeypatch_helpers\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#a6e22e\"></span>     rescue Gem::LoadError =&gt; load_error\n</span></span><span style=\"display:flex;\"><span>       $stderr.puts %(Missing the Rails #{version} gem. Please `gem install -v=#{version} rails`, update your RAILS_GEM_VERSION setting in config/environment.rb for the Rails version you do have installed, or comment out RAILS_GEM_VERSION to use the latest version installed.)\n</span></span><span style=\"display:flex;\"><span>       exit 1\n</span></span></code></pre></div><p>I&rsquo;ve also posted it <a href=\"https://gist.github.com/2913560\">as a gist</a> if that&rsquo;s more up your alley.</p>\n",
				"content_text": "Rails 2 hasn't been maintained for quite a while, and it last officially supported Ruby 1.9.1. I was working on getting an old Rails 2 app upgraded to Rails 3 and discovered something frustrating: Rails 2.3.14 don't boot under Ruby 1.9.3 unless every single controller has a helper class defined in a helper file. I didn't want to sit around creating 60 helper files, so I did the expedient thing, patching the app's `boot.rb` file to not raise exceptions on missing helper files.\n\nIt turned out to not be a terribly large amount of code, but here's a diff just in case anyone else is upgrading a really old Rails app and runs into the same issue:\n\n```diff\ndiff --git a/config/boot.rb b/config/boot.rb\nindex 69b1a51..2c979cf 100644\n--- a/config/boot.rb\n+++ b/config/boot.rb\n@@ -61,12 +61,30 @@ module Rails\n       require 'initializer'\n     end\n \n+    def monkeypatch_helpers\n+      require \"active_support\"\n+      require 'action_controller/helpers'\n+      ActionController::Helpers::ClassMethods.send(:define_method, :inherited_with_helper) do |child|\n+        inherited_without_helper(child)\n+\n+        begin\n+          child.master_helper_module = Module.new\n+          child.master_helper_module.__send__ :include, master_helper_module\n+          child.__send__ :default_helper_module!\n+        rescue MissingSourceFile => e\n+          raise unless e.is_missing?(\"helpers/#{child.controller_path}_helper\")\n+        rescue LoadError\n+        end\n+      end\n+    end\n+\n     def load_rails_gem\n       if version = self.class.gem_version\n         gem 'rails', version\n       else\n         gem 'rails'\n       end\n+      monkeypatch_helpers\n     rescue Gem::LoadError => load_error\n       $stderr.puts %(Missing the Rails #{version} gem. Please `gem install -v=#{version} rails`, update your RAILS_GEM_VERSION setting in config/environment.rb for the Rails version you do have installed, or comment out RAILS_GEM_VERSION to use the latest version installed.)\n       exit 1\n```\n\nI've also posted it [as a gist](https://gist.github.com/2913560) if that's more up your alley.\n",
				"date_published": "2012-06-14T00:00:00-08:00",
				"url": "https://andre.arko.net/2012/06/14/rails-on-ruby/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2012/05/29/track-remote-git-branches-with/",
				"title": "Track remote git branches with ease",
				"content_html": "<h2 id=\"tldr\">tl;dr</h2>\n<pre><code># ~/.gitconfig\n[aliases]\n  track = &quot;!f(){ branch=$(git name-rev --name-only HEAD); cmd=\\&quot;git branch --set-upstream $branch ${1:-origin}/${2:-$branch}\\&quot;; echo $cmd; $cmd; }; f&quot;\n</code></pre>\n<h2 id=\"the-problem\">The problem</h2>\n<p>Git is really great. That said, I find myself frequently annoyed by trying to manage upstream tracking branches. If you&rsquo;re creating a new branch for the first time, it&rsquo;s incredibly easy because git does the work for you.</p>\n<p>If you already have a branch, though, and you&rsquo;re trying to change the upstream branch, it’s incredibly annoying. I expect to be able to use the same arguments that I use with <code>git pull</code>. That means I invoke <code>git branch --set-upstream</code> with invalid arguments, get an error message, start to read the <code>git branch</code> manpage, give up, and then edit <code>.git/config</code> directly to fix my problem.</p>\n<p>I kept thinking that there must is a better way. At some point in the past, I wrote a git alias named “track” that attempted to make it easier. The problem with my initial attempt was that I still had to remember the exact syntax, and pass the local and remote branch names in a form like <code>master origin/master</code>. That was pretty much impossible for me to remember on the fly, and so things weren’t really any easier.</p>\n<h3 id=\"the-solution\">The solution</h3>\n<p>This week, I needed to change tracking branches again, and I decided that I would just keep going until I had something that I wanted. Worst case, I could always write a ruby script, right? Happily, things turned out to be much simpler than that. I discovered that you can define shell functions inside git aliases, and then figured out that bash provides a special syntax for “variable or constant value”. Combine those two together, and bam, awesome <code>git track</code>.</p>\n<p>The best part of this particular alias is that all the arguments are optional. You can invoke it with the remote and branch, just the remote, or neither one. Here are some examples:</p>\n<pre tabindex=\"0\"><code># create and check out a branch named &#34;feature&#34;\ngit co -b feature\n\n# make the current branch track origin/feature\ngit track\n\n# make the current branch track indirect/feature\ngit track indirect\n\n# make the current branch track indirect/master\ngit track indirect master\n</code></pre><p>You too can use this particular git alias. Just copy this line into your <code>.gitconfig</code> file, in the <code>[aliases]</code> section:</p>\n<pre tabindex=\"0\"><code>track = &#34;!f(){ branch=$(git name-rev --name-only HEAD); cmd=\\&#34;git branch --set-upstream $branch ${1:-origin}/${2:-$branch}\\&#34;; echo $cmd; $cmd; }; f&#34;\n</code></pre><p>Enjoy!</p>\n",
				"content_text": "## tl;dr\n\n    # ~/.gitconfig\n    [aliases]\n      track = \"!f(){ branch=$(git name-rev --name-only HEAD); cmd=\\\"git branch --set-upstream $branch ${1:-origin}/${2:-$branch}\\\"; echo $cmd; $cmd; }; f\"\n\n## The problem\n\nGit is really great. That said, I find myself frequently annoyed by trying to manage upstream tracking branches. If you're creating a new branch for the first time, it's incredibly easy because git does the work for you.\n\nIf you already have a branch, though, and you're trying to change the upstream branch, it’s incredibly annoying. I expect to be able to use the same arguments that I use with `git pull`. That means I invoke `git branch --set-upstream` with invalid arguments, get an error message, start to read the `git branch` manpage, give up, and then edit `.git/config` directly to fix my problem.\n\nI kept thinking that there must is a better way. At some point in the past, I wrote a git alias named “track” that attempted to make it easier. The problem with my initial attempt was that I still had to remember the exact syntax, and pass the local and remote branch names in a form like `master origin/master`. That was pretty much impossible for me to remember on the fly, and so things weren’t really any easier.\n\n### The solution\n\nThis week, I needed to change tracking branches again, and I decided that I would just keep going until I had something that I wanted. Worst case, I could always write a ruby script, right? Happily, things turned out to be much simpler than that. I discovered that you can define shell functions inside git aliases, and then figured out that bash provides a special syntax for “variable or constant value”. Combine those two together, and bam, awesome `git track`.\n\nThe best part of this particular alias is that all the arguments are optional. You can invoke it with the remote and branch, just the remote, or neither one. Here are some examples:\n\n```\n# create and check out a branch named \"feature\"\ngit co -b feature\n\n# make the current branch track origin/feature\ngit track\n\n# make the current branch track indirect/feature\ngit track indirect\n\n# make the current branch track indirect/master\ngit track indirect master\n```\n\nYou too can use this particular git alias. Just copy this line into your `.gitconfig` file, in the `[aliases]` section:\n\n```\ntrack = \"!f(){ branch=$(git name-rev --name-only HEAD); cmd=\\\"git branch --set-upstream $branch ${1:-origin}/${2:-$branch}\\\"; echo $cmd; $cmd; }; f\"\n```\n\nEnjoy!\n",
				"date_published": "2012-05-29T00:00:00-08:00",
				"url": "https://andre.arko.net/2012/05/29/track-remote-git-branches-with/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2012/05/15/middleman-buildpack-for-heroku/",
				"title": "Middleman Buildpack for Heroku",
				"content_html": "<p><em>Update</em>: Custom buildpacks aren&rsquo;t needed to host Middleman sites on Heroku anymore, making this blog post partially obsolete. I&rsquo;ve updated my <a href=\"https://github.com/indirect/middleman-heroku-static-app\">example of a Middleman site hosted on Heroku</a> using the standard Ruby buildpack.</p>\n<p>When I make a static site, <a href=\"http://middlemanapp.com/\">Middleman</a> is my default tool. Middleman makes building static sites as easy as Rails makes building dynamic sites. It&rsquo;s <a href=\"https://github.com/middleman/middleman/\">activately maintained</a> by <a href=\"http://twitter.com/tdreyno\">Thomas Reynolds</a>, and keeps up with the latest changes coming out of Rails and elsewhere.</p>\n<p>While it&rsquo;s an absolute pleasure to use, deploying it can be somewhat more annoying. Hosting a static site directly on S3 is an option, but it both a) costs money, and b) has extremely cumbersome permissions management. [Heroku][http://heroku.com], in contrast, is free for small sites and has fantastic permissions control. The catch is, Heroku only runs Rack-based applications.</p>\n<p>Although Middleman ships with a Rack-compatible server, it seemed kind of silly to create a static site that gets generated over and over again with every page view. I wanted to generate my static pages once, and then just serve them directly. While it&rsquo;s possible to check the entire static site into git and use <a href=\"http://rack.rubyforge.org/doc/classes/Rack/Static.html\">Rack::Static</a> to serve each page, I wasn&rsquo;t really into the idea of committing build products, either.</p>\n<p>The final solution that I settled leverages a lesser-known feature of Heroku&rsquo;s <a href=\"https://devcenter.heroku.com/articles/cedar\">cedar stack</a>, called <a href=\"https://devcenter.heroku.com/articles/buildpacks\">buildpacks</a>. Buildpacks are sort of like Heroku&rsquo;s deploy scripts, since they let you run arbitrary code at deploy time. My <a href=\"https://github.com/indirect/heroku-buildpack-middleman\">middleman buildpack</a> automatically runs Middleman to build your static site into the <code>./build</code> directory as part of the deploy process. You can create an app on Heroku using my custom buildpack by running this command:</p>\n<pre><code>$ heroku create --stack cedar --buildpack http://github.com/indirect/heroku-buildpack-middleman.git\n</code></pre>\n<p>Once you have the buildpack set up, you can just <code>git push heroku master</code>, and watch the output to see your site get built. Serving up the static pages is easy using Rack. I like to use Rack::TryStatic, since it allows nicer 404 pages. To make things as easy as possible, I&rsquo;ve created a <a href=\"https://github.com/indirect/middleman-heroku-static-app\">minimal app</a> that is pre-configured to work with the Middleman buildpack. You can just fork that app, run <code>heroku create</code>, and start deploying your fully-static site to Heroku. Check out the <a href=\"http://middleman-heroku-static-app.herokuapp.com/\">example site</a>, deployed to Heroku, or the <a href=\"https://github.com/indirect/middleman-heroku-static-app\">app source and readme</a> to learn more.</p>\n",
				"content_text": "\n*Update*: Custom buildpacks aren't needed to host Middleman sites on Heroku anymore, making this blog post partially obsolete. I've updated my [example of a Middleman site hosted on Heroku](https://github.com/indirect/middleman-heroku-static-app) using the standard Ruby buildpack.\n\nWhen I make a static site, [Middleman][mm] is my default tool. Middleman makes building static sites as easy as Rails makes building dynamic sites. It's [activately maintained][mmgithub] by [Thomas Reynolds][tdreyno], and keeps up with the latest changes coming out of Rails and elsewhere.\n\n[mm]: http://middlemanapp.com/\n[mmgithub]: https://github.com/middleman/middleman/\n[tdreyno]: http://twitter.com/tdreyno\n\nWhile it's an absolute pleasure to use, deploying it can be somewhat more annoying. Hosting a static site directly on S3 is an option, but it both a) costs money, and b) has extremely cumbersome permissions management. [Heroku][http://heroku.com], in contrast, is free for small sites and has fantastic permissions control. The catch is, Heroku only runs Rack-based applications.\n\nAlthough Middleman ships with a Rack-compatible server, it seemed kind of silly to create a static site that gets generated over and over again with every page view. I wanted to generate my static pages once, and then just serve them directly. While it's possible to check the entire static site into git and use [Rack::Static][rackstatic] to serve each page, I wasn't really into the idea of committing build products, either.\n\n[rackstatic]: http://rack.rubyforge.org/doc/classes/Rack/Static.html\n\nThe final solution that I settled leverages a lesser-known feature of Heroku's [cedar stack][cedar], called [buildpacks][buildpacks]. Buildpacks are sort of like Heroku's deploy scripts, since they let you run arbitrary code at deploy time. My [middleman buildpack][mmbp] automatically runs Middleman to build your static site into the `./build` directory as part of the deploy process. You can create an app on Heroku using my custom buildpack by running this command:\n\n    $ heroku create --stack cedar --buildpack http://github.com/indirect/heroku-buildpack-middleman.git\n\n[cedar]: https://devcenter.heroku.com/articles/cedar\n[buildpacks]: https://devcenter.heroku.com/articles/buildpacks\n[mmbp]: https://github.com/indirect/heroku-buildpack-middleman\n\nOnce you have the buildpack set up, you can just `git push heroku master`, and watch the output to see your site get built. Serving up the static pages is easy using Rack. I like to use Rack::TryStatic, since it allows nicer 404 pages. To make things as easy as possible, I've created a [minimal app][mmapp] that is pre-configured to work with the Middleman buildpack. You can just fork that app, run `heroku create`, and start deploying your fully-static site to Heroku. Check out the [example site][ex], deployed to Heroku, or the [app source and readme][mmapp] to learn more.\n\n[mmapp]: https://github.com/indirect/middleman-heroku-static-app\n[ex]: http://middleman-heroku-static-app.herokuapp.com/\n",
				"date_published": "2012-05-15T00:00:00-08:00",
				"url": "https://andre.arko.net/2012/05/15/middleman-buildpack-for-heroku/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2012/04/02/joining-tilde/",
				"title": "Joining Tilde",
				"content_html": "<p>I&rsquo;m really excited to announce that I&rsquo;ve joined <a href=\"http://tilde.io\">Tilde</a>. Today is my first day, and I&rsquo;m really excited to be working with <a href=\"http://twitter.com/wycats\">Yehuda</a>, <a href=\"http://twitter.com/tomdale\">Tom</a>, <a href=\"http://twitter.com/carllerche\">Carl</a>, <a href=\"http://twitter.com/wifelette\">Leah</a>, <a href=\"http://twitter.com/patr1ck\">Patrick</a> and the rest of the Tilde team.</p>\n<p>I&rsquo;ve been working with Yehuda, Carl, and Leah since I was at Engine Yard. While I was at <a href=\"http://plexapp.com\">Plex</a>, we continued to work together on open source projects like <a href=\"http://gembundler.com\">Bundler</a> and <a href=\"http://rubyonrails.com\">Rails</a>. More recently, I&rsquo;ve been following the development of <a href=\"http://handlebarsjs.com/\">Handlebars</a> and <a href=\"http://emberjs.com/\">Ember</a> with great interest.</p>\n<p>I&rsquo;m looking forward to working with the latest in client-side web development using Ember, as well as helping with <a href=\"http://www.kickstarter.com/projects/1397300529/railsapp\">the next generation of Rails development for new users</a>. Tilde is a very young company, but it&rsquo;s already a very exciting place to work, and I&rsquo;m expecting great things in the future.</p>\n",
				"content_text": "I'm really excited to announce that I've joined [Tilde][1]. Today is my first day, and I'm really excited to be working with [Yehuda][2], [Tom][3], [Carl][4], [Leah][6], [Patrick][5] and the rest of the Tilde team.\n\n[1]: http://tilde.io\n[2]: http://twitter.com/wycats\n[3]: http://twitter.com/tomdale\n[4]: http://twitter.com/carllerche\n[5]: http://twitter.com/patr1ck\n[6]: http://twitter.com/wifelette\n\nI've been working with Yehuda, Carl, and Leah since I was at Engine Yard. While I was at [Plex][11], we continued to work together on open source projects like [Bundler][9] and [Rails][10]. More recently, I've been following the development of [Handlebars][7] and [Ember][8] with great interest. \n\n[7]: http://handlebarsjs.com/\n[8]: http://emberjs.com/\n[9]: http://gembundler.com\n[10]: http://rubyonrails.com\n[11]: http://plexapp.com\n\nI'm looking forward to working with the latest in client-side web development using Ember, as well as helping with [the next generation of Rails development for new users][12]. Tilde is a very young company, but it's already a very exciting place to work, and I'm expecting great things in the future.\n\n[12]: http://www.kickstarter.com/projects/1397300529/railsapp\n",
				"date_published": "2012-04-02T00:00:00-08:00",
				"url": "https://andre.arko.net/2012/04/02/joining-tilde/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2012/03/07/bridge-mode-on-zyxel-phn/",
				"title": "Bridge mode on ZyXEL P-663HN-51 DSL modems",
				"content_html": "<p>My internet service is provided by the illustrious <a href=\"http://sonic.net\">Sonic.net</a>, and I just upgraded my <a href=\"http://sonic.net/solutions/home/internet/fusion/\">Fusion Broadband</a> from a single DSL line to two DSL lines. This increases my overall speeds to around 30mbits down and 5mbits up, which is great.</p>\n<p>As part of the upgrade, I switched from a ZTE DSL modem to a ZyXEL P-663HN-51 modem. After a few hours of significantly faster internet speeds, I discovered a problem &ndash; my old router was configured to run in &ldquo;bridge&rdquo; mode, and the new router wasn&rsquo;t. Without bridge mode, the Mac Mini I keep at home and use as a server was stuck behind a double-NAT. Since I need to be able to reach my Mini for IRC, among other things, this was a bit upsetting.</p>\n<p>It got a lot more upsetting when I couldn&rsquo;t figure out how to enable bridging on the new modem. The old modem let me enable bridging on the DMZ configuration page, but the new modem didn&rsquo;t seem to have any settings related to bridging. After going through every single admin page and searching the internet for almost an hour, I gave up and called Sonic.net support.</p>\n<p>The extremely helpful technical support rep that I talked to guided me through editing the routing configuration on the new DSL modem to enable bridging. Since I would have loved to just find a blog post explaining how to do it instead of having to call, here&rsquo;s a blog post explaining how to do it.</p>\n<p>It&rsquo;s pretty simple, once you know what you&rsquo;re doing. Browse to the P-663HN-51 admin website. If you haven&rsquo;t changed any settings, that will mean opening <a href=\"http://192.168.1.1\">http://192.168.1.1</a> and logging in with the username &ldquo;admin&rdquo; and the password &ldquo;1234&rdquo;. Once you&rsquo;re there, click on &ldquo;Advanced Setup&rdquo; in the left-hand column, and then on &ldquo;WAN&rdquo;. It should look like this:</p>\n<img src=\"https://indirect.micro.blog/uploads/2025/c65c2c4797.jpg\" style=\"width: auto\">\n<p>Then click the &ldquo;Edit&rdquo; button on the right-hand side of that page:</p>\n<img src=\"https://indirect.micro.blog/uploads/2025/8b2b859e5f.jpg\" style=\"width: auto\">\n<p>Be sure to turn off NAT, DHCP, and IP address assignment now, before you turn on bridge mode. Apparently, they aren&rsquo;t turned off, but the UI to disable them disappears once bridging is enabled. (Thanks, Craig Paxton.)</p>\n<p>You&rsquo;ll want to click &ldquo;Next&rdquo; at least once, until you eventually end up at a page that lets you choose bridge mode. Click the radio button labelled &ldquo;Bridging&rdquo;, and then push &ldquo;Next&rdquo; some more:</p>\n<img src=\"https://indirect.micro.blog/uploads/2025/286b8f02e1.jpg\" style=\"width: auto\">\n<p>Once you reach the last page, you&rsquo;ll be able to save your change. After that, you should restart the router to disable the DHCP server.</p>\n<p>Now you&rsquo;re set. Yay.</p>\n<p><em>8 months later:</em></p>\n<p>It&rsquo;s not entirely obvious, but one big thing changes once bridge mode is on. You can&rsquo;t connect to the router&rsquo;s web admin interface. It turns out that the DHCP address that you get assigned, when the modem is in bridge mode, only knows how to talk to the outside internet. Here&rsquo;s how to connect to the modem again, in case you ever need to: manually configure your ethernet interface to have a static IP address in the 192.168.1.x range, with the final value being higher than 1. Set the router to 192.168.1.1. Open a web browser, and navigate to <a href=\"http://192.168.1.1\">http://192.168.1.1</a>. There you go.</p>\n",
				"content_text": "My internet service is provided by the illustrious [Sonic.net](http://sonic.net), and I just upgraded my [Fusion Broadband](http://sonic.net/solutions/home/internet/fusion/) from a single DSL line to two DSL lines. This increases my overall speeds to around 30mbits down and 5mbits up, which is great.\n\nAs part of the upgrade, I switched from a ZTE DSL modem to a ZyXEL P-663HN-51 modem. After a few hours of significantly faster internet speeds, I discovered a problem -- my old router was configured to run in \"bridge\" mode, and the new router wasn't. Without bridge mode, the Mac Mini I keep at home and use as a server was stuck behind a double-NAT. Since I need to be able to reach my Mini for IRC, among other things, this was a bit upsetting.\n\nIt got a lot more upsetting when I couldn't figure out how to enable bridging on the new modem. The old modem let me enable bridging on the DMZ configuration page, but the new modem didn't seem to have any settings related to bridging. After going through every single admin page and searching the internet for almost an hour, I gave up and called Sonic.net support.\n\nThe extremely helpful technical support rep that I talked to guided me through editing the routing configuration on the new DSL modem to enable bridging. Since I would have loved to just find a blog post explaining how to do it instead of having to call, here's a blog post explaining how to do it.\n\nIt's pretty simple, once you know what you're doing. Browse to the P-663HN-51 admin website. If you haven't changed any settings, that will mean opening [http://192.168.1.1](http://192.168.1.1) and logging in with the username \"admin\" and the password \"1234\". Once you're there, click on \"Advanced Setup\" in the left-hand column, and then on \"WAN\". It should look like this:\n\n<img src=\"https://indirect.micro.blog/uploads/2025/c65c2c4797.jpg\" style=\"width: auto\">\n\nThen click the \"Edit\" button on the right-hand side of that page:\n\n<img src=\"https://indirect.micro.blog/uploads/2025/8b2b859e5f.jpg\" style=\"width: auto\">\n\nBe sure to turn off NAT, DHCP, and IP address assignment now, before you turn on bridge mode. Apparently, they aren't turned off, but the UI to disable them disappears once bridging is enabled. (Thanks, Craig Paxton.)\n\nYou'll want to click \"Next\" at least once, until you eventually end up at a page that lets you choose bridge mode. Click the radio button labelled \"Bridging\", and then push \"Next\" some more:\n\n<img src=\"https://indirect.micro.blog/uploads/2025/286b8f02e1.jpg\" style=\"width: auto\">\n\nOnce you reach the last page, you'll be able to save your change. After that, you should restart the router to disable the DHCP server.\n\nNow you're set. Yay.\n\n*8 months later:*\n\nIt's not entirely obvious, but one big thing changes once bridge mode is on. You can't connect to the router's web admin interface. It turns out that the DHCP address that you get assigned, when the modem is in bridge mode, only knows how to talk to the outside internet. Here's how to connect to the modem again, in case you ever need to: manually configure your ethernet interface to have a static IP address in the 192.168.1.x range, with the final value being higher than 1. Set the router to 192.168.1.1. Open a web browser, and navigate to [http://192.168.1.1](http://192.168.1.1). There you go.\n",
				"date_published": "2012-03-07T00:00:00-08:00",
				"url": "https://andre.arko.net/2012/03/07/bridge-mode-on-zyxel-phn/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2012/02/19/sslerror-certificate-verify-failed-on/",
				"title": "SSLError certificate verify failed on Engine Yard",
				"content_html": "<p>The other day, I deployed a new feature for the first time: connect your <a href=\"https://github.com/plataformatec/devise\">Devise</a> account to Facebook via <a href=\"https://github.com/intridea/omniauth\">OmniAuth</a>. I tested it out on my laptop, and everything seemed swell, but then I deployed to production.</p>\n<p>Unfortunately, the ruby installed by Engine Yard&rsquo;s stack doesn&rsquo;t seem to be able to find the CA file that it needs to verify certificates when creating HTTPS connections. As a result, connecting to Facebook in production would throw an exception: <code>SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed (OpenSSL::SSL::SSLError)</code>.</p>\n<p>Fortunately, the solution turns out to be pretty straightforward. All you need to do is tell ruby where the CA file is location on all the Engine Yard boxes. After some digging around, I discovered that it&rsquo;s located at <code>/etc/ssl/certs/ca-certificates.crt</code>. Now that you know where the CA file is, all you have to do is tell ruby where it is so it can use it.</p>\n<p>Setting up OmniAuth is pretty easy. In your <code>devise.rb</code> file, just adjust the OmniAuth configuration to look like this:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>config<span style=\"color:#f92672\">.</span>omniauth <span style=\"color:#e6db74\">:facebook</span>, <span style=\"color:#e6db74\">&#34;APP_ID&#34;</span>, <span style=\"color:#e6db74\">&#34;APP_SECRET&#34;</span>, {\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#e6db74\">:scope</span> <span style=\"color:#f92672\">=&gt;</span> <span style=\"color:#e6db74\">&#39;publish_stream&#39;</span>, <span style=\"color:#e6db74\">:client_options</span> <span style=\"color:#f92672\">=&gt;</span> {\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#e6db74\">:ssl</span> <span style=\"color:#f92672\">=&gt;</span> {<span style=\"color:#e6db74\">:ca_file</span> <span style=\"color:#f92672\">=&gt;</span> <span style=\"color:#e6db74\">&#39;/etc/ssl/certs/ca-certificates.crt&#39;</span>}\n</span></span><span style=\"display:flex;\"><span>  }\n</span></span><span style=\"display:flex;\"><span>}\n</span></span></code></pre></div><p>Once that&rsquo;s done, you just need to track down any other places where you&rsquo;re using SSL, and tell Net::HTTP (or your favourite equivalent) where the CA file is. For Net::HTTP, that just means doing this before you make any requests:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>https <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">Net</span><span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">HTTP</span><span style=\"color:#f92672\">.</span>new(<span style=\"color:#e6db74\">&#34;somehost.org&#34;</span>, <span style=\"color:#ae81ff\">443</span>)\n</span></span><span style=\"display:flex;\"><span>https<span style=\"color:#f92672\">.</span>ca_file <span style=\"color:#f92672\">=</span> <span style=\"color:#e6db74\">&#39;/usr/share/curl/curl-ca-bundle.crt&#39;</span>\n</span></span></code></pre></div><p>And that is how to make your HTTPS requests work in production on Engine Yard.</p>\n",
				"content_text": "The other day, I deployed a new feature for the first time: connect your [Devise](https://github.com/plataformatec/devise) account to Facebook via [OmniAuth](https://github.com/intridea/omniauth). I tested it out on my laptop, and everything seemed swell, but then I deployed to production.\n\nUnfortunately, the ruby installed by Engine Yard's stack doesn't seem to be able to find the CA file that it needs to verify certificates when creating HTTPS connections. As a result, connecting to Facebook in production would throw an exception: `SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed (OpenSSL::SSL::SSLError)`.\n\nFortunately, the solution turns out to be pretty straightforward. All you need to do is tell ruby where the CA file is location on all the Engine Yard boxes. After some digging around, I discovered that it's located at `/etc/ssl/certs/ca-certificates.crt`. Now that you know where the CA file is, all you have to do is tell ruby where it is so it can use it.\n\nSetting up OmniAuth is pretty easy. In your `devise.rb` file, just adjust the OmniAuth configuration to look like this:\n\n```ruby\nconfig.omniauth :facebook, \"APP_ID\", \"APP_SECRET\", {\n  :scope => 'publish_stream', :client_options => {\n    :ssl => {:ca_file => '/etc/ssl/certs/ca-certificates.crt'}\n  }\n}\n```\n\nOnce that's done, you just need to track down any other places where you're using SSL, and tell Net::HTTP (or your favourite equivalent) where the CA file is. For Net::HTTP, that just means doing this before you make any requests:\n\n```ruby\nhttps = Net::HTTP.new(\"somehost.org\", 443)\nhttps.ca_file = '/usr/share/curl/curl-ca-bundle.crt'\n```\n\nAnd that is how to make your HTTPS requests work in production on Engine Yard.\n",
				"date_published": "2012-02-19T00:00:00-08:00",
				"url": "https://andre.arko.net/2012/02/19/sslerror-certificate-verify-failed-on/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2012/01/24/fix-newrelicyml-symlink-errors-on/",
				"title": "Fix newrelic.yml symlink errors on Engine Yard",
				"content_html": "<p>Perhaps you, like me, occasionally deploy Ruby applications to [Engine Yard Cloud][ey]. And perhaps you, also like me, use the lovely [NewRelic][nr] analytics package provided to all Engine Yard customers. If you do, you have probably noticed that the [<code>newrelic_rpm</code> gem][rpm] complains in development if you do not have a <code>config/newrelic.yml</code> file.</p>\n<p>Naively, I created this file and added it to git, thinking that that would make the warnings go away and make everything wonderful. For a short time, I even thought it had. But then I noticed that every time I deployed my application, the deploy output contained a new error message. This error message repeated several times, possibly even once per server that I was deploying onto, and made me very sad. The error looks like this in the deploy output:</p>\n<pre tabindex=\"0\"><code>~&gt; Symlink other shared config files\nln: creating symbolic link/data/appname/releases/3000102030405/config/newrelic.yml&#39;: File exists\n~&gt; Symlink mongrel_cluster.yml\n</code></pre><p>Today, I finally figured out how to stop that error from appearing. Logically enough, the abstract idea is to just remove the <code>newrelic.yml</code> file before the engineyard gem attempts to create a symlink with the same name. In practice, that is tougher than it sounds. It turns out that the &ldquo;Symlink other shared config files&rdquo; step takes place very early in the deploy process.</p>\n<p>I first tried the <code>before_symlink.rb</code> hook, but that refers to when the release directory is symlinked to <code>current</code>. That is much too late in the deploy process to fix this. Next, I tried the <code>before_migrate.rb</code> hook, which is suggested in the [deploy hook documentation][doc] for people who are confused by the symlink hook. That wasn&rsquo;t early enough either. I finally discovered the <code>after_bundle.rb</code> hook while perusing the docs again, and that worked! So here is the code you should add to <code>deploy/after_bundle.rb</code> in your application:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># remove the NewRelic config to avoid a warning when it is symlinked</span>\n</span></span><span style=\"display:flex;\"><span>run <span style=\"color:#e6db74\">&#34;rm -f </span><span style=\"color:#e6db74\">#{</span>release_path<span style=\"color:#e6db74\">}</span><span style=\"color:#e6db74\">/config/newrelic.yml&#34;</span>\n</span></span></code></pre></div><p>That&rsquo;s it. Once the <code>after_bundle.rb</code> file is checked in to your repo, your deploys should be warning free.</p>\n",
				"content_text": "Perhaps you, like me, occasionally deploy Ruby applications to [Engine Yard Cloud][ey]. And perhaps you, also like me, use the lovely [NewRelic][nr] analytics package provided to all Engine Yard customers. If you do, you have probably noticed that the [`newrelic_rpm` gem][rpm] complains in development if you do not have a `config/newrelic.yml` file.\n\nNaively, I created this file and added it to git, thinking that that would make the warnings go away and make everything wonderful. For a short time, I even thought it had. But then I noticed that every time I deployed my application, the deploy output contained a new error message. This error message repeated several times, possibly even once per server that I was deploying onto, and made me very sad. The error looks like this in the deploy output:\n\n```\n~> Symlink other shared config files\nln: creating symbolic link/data/appname/releases/3000102030405/config/newrelic.yml': File exists\n~> Symlink mongrel_cluster.yml\n```\n\nToday, I finally figured out how to stop that error from appearing. Logically enough, the abstract idea is to just remove the `newrelic.yml` file before the engineyard gem attempts to create a symlink with the same name. In practice, that is tougher than it sounds. It turns out that the \"Symlink other shared config files\" step takes place very early in the deploy process.\n\nI first tried the `before_symlink.rb` hook, but that refers to when the release directory is symlinked to `current`. That is much too late in the deploy process to fix this. Next, I tried the `before_migrate.rb` hook, which is suggested in the [deploy hook documentation][doc] for people who are confused by the symlink hook. That wasn't early enough either. I finally discovered the `after_bundle.rb` hook while perusing the docs again, and that worked! So here is the code you should add to `deploy/after_bundle.rb` in your application:\n\n```ruby\n# remove the NewRelic config to avoid a warning when it is symlinked\nrun \"rm -f #{release_path}/config/newrelic.yml\"\n```\n\nThat's it. Once the `after_bundle.rb` file is checked in to your repo, your deploys should be warning free.\n",
				"date_published": "2012-01-24T00:00:00-08:00",
				"url": "https://andre.arko.net/2012/01/24/fix-newrelicyml-symlink-errors-on/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/12/26/repeated-headers-and-ruby-web/",
				"title": "Repeated headers and Ruby web servers",
				"content_html": "<p>A few weeks ago, I ran into an interesting problem with my Rails app. For some reason, the <code>request.remote_ip</code> value inside my app didn&rsquo;t contain the correct value. Instead, it simply contained the internal address of the EC2 instance I was using as a load balancer. I started noticing the problem when I set up a stack consisting of stunnel, HAProxy, Nginx, and Passenger.</p>\n<p>After a huge amount of testing (and checking a lot of headers), I discovered that the raw request being delivered to Passenger contained two separate X-Forwarded-For headers. At first, I thought the problem must be in Rack, overriding the value in the headers hash with a new value when it saw the same header a second time. I dove into the Rack code thinking I had just found a giant bug&hellip; but the Rack code seemed to be doing the right thing, so I had to keep looking.</p>\n<p>I was especially confused by the way that my local development environment (using the Pow server) didn&rsquo;t seem to have the problem. Eventually, I figured out that the problem isn&rsquo;t Rack. According to the relevant HTTP spec (namely RFC 2616), two headers can simply be interpreted as a single header with two comma-separated values. Many Rack servers do this, and so Rack gets a single header with two values. Some Rack servers don&rsquo;t do this, and apps running in those servers merely see the value of the last header in the request.</p>\n<p>In order to narrow things down, I tested all of the current production-use Rack servers I could think of off the top of my head. Here&rsquo;s what I found:</p>\n<pre><code># Pow \n$ curl -s -H&quot;X-Header: 1&quot; -H&quot;X-Header: 2&quot; app.dev/headers | grep X_HEADER\nHTTP_X_HEADER: 1, 2\n\n# Unicorn\n$ curl -s -H&quot;X-Header: 1&quot; -H&quot;X-Header: 2&quot; localhost:8080/headers | grep X_HEADER\nHTTP_X_HEADER: 1,2\n\n# Thin\n$ curl -s -H&quot;X-Header: 1&quot; -H&quot;X-Header: 2&quot; localhost:3000/headers | grep X_HEADER\nHTTP_X_HEADER: 2\n\n# Passenger-standalone (which is nginx+passenger)\n$ curl -s -H&quot;X-Header: 1&quot; -H&quot;X-Header: 2&quot; localhost:3000/headers | grep X_HEADER\nHTTP_X_HEADER: 2\n\n# HAProxy+nginx+passenger\n$ curl -s -H&quot;X-Header: 1&quot; -H&quot;X-Header: 2&quot; app.prod/headers | grep X_HEADER\nHTTP_X_HEADER: 2\n</code></pre>\n<p>As you can see in the results above, Pow and Unicorn both combine repeated headers into a comma-separated list that is passed to Rack. Thin and Passenger, on the other hand, take a simple &ldquo;last header wins&rdquo; approach.</p>\n<p>In the end, I wound up changing my stack to use Unicorn instead of Passenger, so that I could get the values from all of the headers instead of just the last one. Hopefully the next guy with the same problem will just find this when they search, and save them from having to repeat that testing work.</p>\n",
				"content_text": "A few weeks ago, I ran into an interesting problem with my Rails app. For some reason, the `request.remote_ip` value inside my app didn't contain the correct value. Instead, it simply contained the internal address of the EC2 instance I was using as a load balancer. I started noticing the problem when I set up a stack consisting of stunnel, HAProxy, Nginx, and Passenger.\n\nAfter a huge amount of testing (and checking a lot of headers), I discovered that the raw request being delivered to Passenger contained two separate X-Forwarded-For headers. At first, I thought the problem must be in Rack, overriding the value in the headers hash with a new value when it saw the same header a second time. I dove into the Rack code thinking I had just found a giant bug... but the Rack code seemed to be doing the right thing, so I had to keep looking.\n\nI was especially confused by the way that my local development environment (using the Pow server) didn't seem to have the problem. Eventually, I figured out that the problem isn't Rack. According to the relevant HTTP spec (namely RFC 2616), two headers can simply be interpreted as a single header with two comma-separated values. Many Rack servers do this, and so Rack gets a single header with two values. Some Rack servers don't do this, and apps running in those servers merely see the value of the last header in the request.\n\nIn order to narrow things down, I tested all of the current production-use Rack servers I could think of off the top of my head. Here's what I found:\n\n    # Pow \n    $ curl -s -H\"X-Header: 1\" -H\"X-Header: 2\" app.dev/headers | grep X_HEADER\n    HTTP_X_HEADER: 1, 2\n\n    # Unicorn\n    $ curl -s -H\"X-Header: 1\" -H\"X-Header: 2\" localhost:8080/headers | grep X_HEADER\n    HTTP_X_HEADER: 1,2\n\n    # Thin\n    $ curl -s -H\"X-Header: 1\" -H\"X-Header: 2\" localhost:3000/headers | grep X_HEADER\n    HTTP_X_HEADER: 2\n\n    # Passenger-standalone (which is nginx+passenger)\n    $ curl -s -H\"X-Header: 1\" -H\"X-Header: 2\" localhost:3000/headers | grep X_HEADER\n    HTTP_X_HEADER: 2\n\n    # HAProxy+nginx+passenger\n    $ curl -s -H\"X-Header: 1\" -H\"X-Header: 2\" app.prod/headers | grep X_HEADER\n    HTTP_X_HEADER: 2\n\nAs you can see in the results above, Pow and Unicorn both combine repeated headers into a comma-separated list that is passed to Rack. Thin and Passenger, on the other hand, take a simple \"last header wins\" approach.\n\nIn the end, I wound up changing my stack to use Unicorn instead of Passenger, so that I could get the values from all of the headers instead of just the last one. Hopefully the next guy with the same problem will just find this when they search, and save them from having to repeat that testing work.\n",
				"date_published": "2011-12-26T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/12/26/repeated-headers-and-ruby-web/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/12/10/make-rails-stop-trying-to/",
				"title": "Make Rails 3 stop trying to serve HTML",
				"content_html": "<p>Something kind of surreal happened today. I noticed that one of my Rails 3 apps was logging <code>ActionView::MissingTemplate</code> errors. When I looked into it, the error was coming from an HTML template that didn&rsquo;t exist. The problem was, that action wasn&rsquo;t supposed to serve HTML at all, ever. I had even dutifully called <code>clear_respond_to; respond_to :xml</code> in my controller, and I thought that would fix everything. Unfortunately, googling and checking Stack Overflow turned up nothing relevant to this particular version of the error, so I decided I had better just dig in.</p>\n<p>The request was strange. It came from an IP address in China, and claimed to be asking for <code>http://www.google.com/index.html</code>, even though the request was sent to my server&rsquo;s IP. After some experimentation, I figured out that there was a problem with my routes: Rails 3 defaults to allowing any regular request to have its format specified with a file extension, like <code>.html</code>. So even though I was responding with XML when the format wasn&rsquo;t specified, my controller was trying to return HTML when it was explicitly requested.</p>\n<p>The <code>resource</code> routes allow you to supply a <code>:format</code> parameter that sets the format for all requests to that resource. I thought that regular routes had a <code>:format</code> parameter that worked the same way. It turns out they don&rsquo;t. Regular routes (set by calling <code>match</code>, <code>get</code>, <code>root</code>, and the like) will take <code>:format =&gt; :xml</code> as an argument. But it turns out that argument is just a shortcut for <code>:defaults =&gt; {:format =&gt; :xml}</code>. So while the format was XML if you didn&rsquo;t ask for anything else, explicitly requesting HTML would still get you HTML.</p>\n<p>The solution turned out to be adding <code>:constraints =&gt; {:format =&gt; :xml}</code> to the route as well as setting the default format. That means that the routes you still want, like <code>/index</code> and <code>/index.xml</code>, will still work. Even better, it will reject requests for <code>/index.html</code> as an invalid route, since the format no longer matches the constraint. Problem solved, but man that took more work than I was expecting to figure out.</p>\n",
				"content_text": "Something kind of surreal happened today. I noticed that one of my Rails 3 apps was logging `ActionView::MissingTemplate` errors. When I looked into it, the error was coming from an HTML template that didn't exist. The problem was, that action wasn't supposed to serve HTML at all, ever. I had even dutifully called `clear_respond_to; respond_to :xml` in my controller, and I thought that would fix everything. Unfortunately, googling and checking Stack Overflow turned up nothing relevant to this particular version of the error, so I decided I had better just dig in.\n\nThe request was strange. It came from an IP address in China, and claimed to be asking for `http://www.google.com/index.html`, even though the request was sent to my server's IP. After some experimentation, I figured out that there was a problem with my routes: Rails 3 defaults to allowing any regular request to have its format specified with a file extension, like `.html`. So even though I was responding with XML when the format wasn't specified, my controller was trying to return HTML when it was explicitly requested.\n\nThe `resource` routes allow you to supply a `:format` parameter that sets the format for all requests to that resource. I thought that regular routes had a `:format` parameter that worked the same way. It turns out they don't. Regular routes (set by calling `match`, `get`, `root`, and the like) will take `:format => :xml` as an argument. But it turns out that argument is just a shortcut for `:defaults => {:format => :xml}`. So while the format was XML if you didn't ask for anything else, explicitly requesting HTML would still get you HTML.\n\nThe solution turned out to be adding `:constraints => {:format => :xml}` to the route as well as setting the default format. That means that the routes you still want, like `/index` and `/index.xml`, will still work. Even better, it will reject requests for `/index.html` as an invalid route, since the format no longer matches the constraint. Problem solved, but man that took more work than I was expecting to figure out.\n",
				"date_published": "2011-12-10T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/12/10/make-rails-stop-trying-to/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/12/06/change-postgres-database-owner/",
				"title": "Change Postgres database owner",
				"content_html": "<p>Since this took some digging to find, I&rsquo;m just going to post it for posterity (and myself in the future):</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-sql\" data-lang=\"sql\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">UPDATE</span> pg_class <span style=\"color:#66d9ef\">SET</span> relowner <span style=\"color:#f92672\">=</span> (<span style=\"color:#66d9ef\">SELECT</span> oid \n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">FROM</span> pg_roles <span style=\"color:#66d9ef\">WHERE</span> rolname <span style=\"color:#f92672\">=</span> <span style=\"color:#e6db74\">&#39;$USER&#39;</span>) \n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">WHERE</span> relname <span style=\"color:#66d9ef\">IN</span> (<span style=\"color:#66d9ef\">SELECT</span> relname\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">FROM</span> pg_class, pg_namespace \n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">WHERE</span> pg_namespace.oid <span style=\"color:#f92672\">=</span> pg_class.relnamespace\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">AND</span> pg_namespace.nspname <span style=\"color:#f92672\">=</span> <span style=\"color:#e6db74\">&#39;public&#39;</span>);\n</span></span></code></pre></div><p>Change $USER to be the name of the user you want to be the new owner of the DB.</p>\n",
				"content_text": "Since this took some digging to find, I'm just going to post it for posterity (and myself in the future):\n\n```sql\nUPDATE pg_class SET relowner = (SELECT oid \n    FROM pg_roles WHERE rolname = '$USER') \n  WHERE relname IN (SELECT relname\n    FROM pg_class, pg_namespace \n    WHERE pg_namespace.oid = pg_class.relnamespace\n    AND pg_namespace.nspname = 'public');\n```\n\nChange $USER to be the name of the user you want to be the new owner of the DB.\n",
				"date_published": "2011-12-06T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/12/06/change-postgres-database-owner/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/11/12/use-newrelic-rpm-developer-mode/",
				"title": "Use NewRelic RPM developer mode with Pow",
				"content_html": "<h3 id=\"tldr\">tl;dr</h3>\n<pre tabindex=\"0\"><code>echo &#39;export NEWRELIC_DISPATCHER=pow&#39; &gt;&gt; ~/.powconfig\necho &#39;export POW_WORKERS=1&#39; &gt;&gt; ~/.powconfig\n</code></pre><h3 id=\"what-now\">What now?</h3>\n<p>I have used the excellent <a href=\"http://www.fngtps.com/passenger-preference-pane\">Passenger Pane</a> for years without complaint. But then I upgraded to Lion, and that disabled all of my apps&rsquo; <code>.dev</code> domains. Since the new local hotness seems to be <a href=\"pow.cx\">Pow</a>, I tried it. Lo and behold, my <code>.dev</code> domains worked again!</p>\n<p>However, trouble was lurking in my newly-functional paradise. NewRelic developer mode was disabled in all my Pow-run processes! It turns out that the NewRelic RPM gem doesn&rsquo;t include Pow on its list of known dispatchers. Googling around for a solution, I discovered that this was <a href=\"https://twitter.com/#!/thomasfuchs/status/76920868302897152\">not a new problem</a>, and it supposedly <a href=\"http://stevendaniels.net/2011/04/pow-and-new-relic-rpm/\">already had a solution</a>.</p>\n<p>With hope in my heart, I tried the suggested solution of forcing developer mode to always be on. While that seemed to uselessly enable developer mode in the rails console, it still wasn&rsquo;t working when I tried to load <code>/newrelic</code>. Stymied, I gave up and moved on to other problems.</p>\n<p>A week later, I was browsing the NewRelic docs for something else, and discovered the <a href=\"http://newrelic.com/docs/ruby/how-do-i-make-sure-the-ruby-agent-starts\">holy grail</a>. The solution is incredibly simple: just tell NewRelic what your dispatcher is named in an environment variable. Since Pow has built-in support for setting environment variables, you can do this by adding just one line to the file <code>~/.powconfig</code>:</p>\n<pre tabindex=\"0\"><code>export NEWRELIC_DISPATCHER=pow\n</code></pre><p>There is one other consideration. By default, Pow runs two workers per Rack app, balancing connections between them. That is not optimal for NewRelic development mode, where request statistics are simply kept in memory for each process. To work around this, you can simply instruct Pow to run only one worker for each app, and all your requests will go to the same process:</p>\n<pre tabindex=\"0\"><code>export POW_WORKERS=1\n</code></pre><p>With that, and a quick <code>killall pow</code>, NewRelic development mode was MINE YET AGAIN. And now I can get back to profiling my slow actions and sequel statements without the bother of running <code>rails server</code>. Hurrah.</p>\n",
				"content_text": "### tl;dr\n\n```\necho 'export NEWRELIC_DISPATCHER=pow' >> ~/.powconfig\necho 'export POW_WORKERS=1' >> ~/.powconfig\n```\n\n### What now?\n\nI have used the excellent [Passenger Pane][pp] for years without complaint. But then I upgraded to Lion, and that disabled all of my apps' `.dev` domains. Since the new local hotness seems to be [Pow](pow.cx), I tried it. Lo and behold, my `.dev` domains worked again!\n\n[pp]: http://www.fngtps.com/passenger-preference-pane\n\nHowever, trouble was lurking in my newly-functional paradise. NewRelic developer mode was disabled in all my Pow-run processes! It turns out that the NewRelic RPM gem doesn't include Pow on its list of known dispatchers. Googling around for a solution, I discovered that this was [not a new problem][tf], and it supposedly [already had a solution][sd].\n\n[tf]: https://twitter.com/#!/thomasfuchs/status/76920868302897152\n[sd]: http://stevendaniels.net/2011/04/pow-and-new-relic-rpm/\n\nWith hope in my heart, I tried the suggested solution of forcing developer mode to always be on. While that seemed to uselessly enable developer mode in the rails console, it still wasn't working when I tried to load `/newrelic`. Stymied, I gave up and moved on to other problems.\n\nA week later, I was browsing the NewRelic docs for something else, and discovered the [holy grail][hg]. The solution is incredibly simple: just tell NewRelic what your dispatcher is named in an environment variable. Since Pow has built-in support for setting environment variables, you can do this by adding just one line to the file `~/.powconfig`:\n\n```\nexport NEWRELIC_DISPATCHER=pow\n```\n\n[hg]: http://newrelic.com/docs/ruby/how-do-i-make-sure-the-ruby-agent-starts\n\nThere is one other consideration. By default, Pow runs two workers per Rack app, balancing connections between them. That is not optimal for NewRelic development mode, where request statistics are simply kept in memory for each process. To work around this, you can simply instruct Pow to run only one worker for each app, and all your requests will go to the same process:\n\n```\nexport POW_WORKERS=1\n```\n\nWith that, and a quick `killall pow`, NewRelic development mode was MINE YET AGAIN. And now I can get back to profiling my slow actions and sequel statements without the bother of running `rails server`. Hurrah.\n",
				"date_published": "2011-11-12T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/11/12/use-newrelic-rpm-developer-mode/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/11/09/the-request-is-doing-what/",
				"title": "The request is doing WHAT?",
				"content_html": "<h3 id=\"tldr\">tl;dr</h3>\n<p>Don&rsquo;t use the <code>with_deleted</code> method added by <a href=\"https://github.com/goncalossilva/rails3_acts_as_paranoid\">rails3_acts_as_paranoid</a>. Seriously. Don&rsquo;t.</p>\n<h3 id=\"launch-time\">launch time!</h3>\n<p>A little over a week ago, Plex launched the site I&rsquo;ve been working on, <a href=\"https://my.plexapp.com\">myPlex</a>. At the same time, we <a href=\"http://elan.plexapp.com/2011/10/29/plex-v0-9-5-brave-new-world/\">launched</a> updated Mac, iOS, and Android apps as well as new Windows and GoogleTV apps. It&rsquo;s pretty damn cool, if I do say so myself, and a lot of people signed up and started using it.</p>\n<h3 id=\"uhoh\">uhoh</h3>\n<p>I noticed a problem, though &ndash; one of the most common requests to the app was using tons of CPU and DB time. In order to keep up with demand, we had to keep adding additional hardware. It was very strange how disproportionately bad the slow action was, though. According to NewRelic, the average response time for other requests was 400ms, and this slow request was averaging 9,600ms. Totally crazy, right?</p>\n<h3 id=\"rails-how-does-it-work\">rails, how does it work</h3>\n<p>After employing some benchmarking, NewRelic in development mode, and a profiler, I eventually found several ways performance could be improved. The real killer, though, was a specific type of query: <code>SELECT * FROM table_name</code>. Every row in an entire table was getting queried, more than once per request! &ldquo;How is this possible&rdquo;, I thought. &ldquo;Doesn&rsquo;t Rails have a query cache?&rdquo;</p>\n<h3 id=\"the-thrilling-conclusion\">the thrilling conclusion</h3>\n<p>Well, as it turns out, the query cache can be overridden by the <code>#reload</code> method. And when I checked the scary query backtraces, I ran into a method provided by the rails3_acts_as_paranoid gem:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">with_deleted</span>\n</span></span><span style=\"display:flex;\"><span>  self<span style=\"color:#f92672\">.</span>unscoped<span style=\"color:#f92672\">.</span>reload\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span>\n</span></span></code></pre></div><p>So every time that I tried to include items that were marked deleted in a query using scopes, <code>with_deleted</code> would fire off a full-table select before running my query! ZOMG.</p>\n<h3 id=\"aftermath\">aftermath</h3>\n<p>Happily, between that fix and several other tweaks and indexes, I was able to speed up the slow request! How much, you ask? Well, the average response time on that slow API action went from 12,792ms to 1,137ms. Oh, and I was able to cut the amount of hardware serving the site by 90%. That&rsquo;s always nice.</p>\n",
				"content_text": "### tl;dr\n\nDon't use the `with_deleted` method added by [rails3_acts_as_paranoid][aap]. Seriously. Don't.\n\n[aap]: https://github.com/goncalossilva/rails3_acts_as_paranoid\n\n### launch time!\n\nA little over a week ago, Plex launched the site I've been working on, [myPlex][myplex]. At the same time, we [launched][ann] updated Mac, iOS, and Android apps as well as new Windows and GoogleTV apps. It's pretty damn cool, if I do say so myself, and a lot of people signed up and started using it.\n\n[myplex]: https://my.plexapp.com\n[ann]: http://elan.plexapp.com/2011/10/29/plex-v0-9-5-brave-new-world/\n\n### uhoh\n\nI noticed a problem, though -- one of the most common requests to the app was using tons of CPU and DB time. In order to keep up with demand, we had to keep adding additional hardware. It was very strange how disproportionately bad the slow action was, though. According to NewRelic, the average response time for other requests was 400ms, and this slow request was averaging 9,600ms. Totally crazy, right?\n\n### rails, how does it work\n\nAfter employing some benchmarking, NewRelic in development mode, and a profiler, I eventually found several ways performance could be improved. The real killer, though, was a specific type of query: `SELECT * FROM table_name`. Every row in an entire table was getting queried, more than once per request! \"How is this possible\", I thought. \"Doesn't Rails have a query cache?\"\n\n### the thrilling conclusion\n\nWell, as it turns out, the query cache can be overridden by the `#reload` method. And when I checked the scary query backtraces, I ran into a method provided by the rails3_acts_as_paranoid gem:\n\n```ruby\ndef with_deleted\n  self.unscoped.reload\nend\n```\n\nSo every time that I tried to include items that were marked deleted in a query using scopes, `with_deleted` would fire off a full-table select before running my query! ZOMG.\n\n### aftermath\n\nHappily, between that fix and several other tweaks and indexes, I was able to speed up the slow request! How much, you ask? Well, the average response time on that slow API action went from 12,792ms to 1,137ms. Oh, and I was able to cut the amount of hardware serving the site by 90%. That's always nice.\n",
				"date_published": "2011-11-09T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/11/09/the-request-is-doing-what/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/10/12/omnifocus-locationbased-alerts/",
				"title": "OmniFocus location-based alerts",
				"content_html": "<p>iOS 5 is released! While I&rsquo;m happy about a lot of new features, the one that has me the most excited at the moment is background location monitoring. Apps can now register with the OS to get notified when your location has significantly changed. The most obvious example of this, included with iOS 5 itself, is the new Reminders app. When you create a todo item, you can add a location to the item, and tell the app to alert you either when you arrive at or leave that location. It&rsquo;s pretty cool.</p>\n<p>OmniFocus 1.12 takes the new API and integrates it beautifully. In previous versions, you could already assign a location or search (like &ldquo;walgreens OR CVS&rdquo;) to a context. Once you&rsquo;ve set up locations, OmniFocus can provide you with a map showing nearby places with tasks assigned.</p>\n<p>Now that you can add alerts, though, things get really interesting. You can get an alert two ways: when you arrive at a location or when you leave a location. Arrival alerts are great when you need something from a specific place, or questions to ask someone in person when you see them. Departure alerts are great for general errands that you don&rsquo;t want to think about until you go out.</p>\n<p>You can also set how close you need to be to trigger the alert. The distance options are about 500 feet, 1500 feet, and 6 miles. The closest option is great for home, work, or any context that depends on you being at a specific street address. The middle option is useful for grouping stores in a single neighborhood, reminding you that you also wanted to do something a few blocks away. The 6 mile option is aimed at entire cities. It lets you do things like remind yourself of everything you wanted to do when you arrive somewhere you don&rsquo;t visit that often.</p>\n<p>Although I&rsquo;ve only been using them for a few days, I&rsquo;ve already found the location-based alerts very useful. It just makes me happy to see my grocery list pop up automatically when I pull into the parking lot.</p>\n",
				"content_text": "iOS 5 is released! While I'm happy about a lot of new features, the one that has me the most excited at the moment is background location monitoring. Apps can now register with the OS to get notified when your location has significantly changed. The most obvious example of this, included with iOS 5 itself, is the new Reminders app. When you create a todo item, you can add a location to the item, and tell the app to alert you either when you arrive at or leave that location. It's pretty cool.\n\nOmniFocus 1.12 takes the new API and integrates it beautifully. In previous versions, you could already assign a location or search (like \"walgreens OR CVS\") to a context. Once you've set up locations, OmniFocus can provide you with a map showing nearby places with tasks assigned.\n\nNow that you can add alerts, though, things get really interesting. You can get an alert two ways: when you arrive at a location or when you leave a location. Arrival alerts are great when you need something from a specific place, or questions to ask someone in person when you see them. Departure alerts are great for general errands that you don't want to think about until you go out.\n\nYou can also set how close you need to be to trigger the alert. The distance options are about 500 feet, 1500 feet, and 6 miles. The closest option is great for home, work, or any context that depends on you being at a specific street address. The middle option is useful for grouping stores in a single neighborhood, reminding you that you also wanted to do something a few blocks away. The 6 mile option is aimed at entire cities. It lets you do things like remind yourself of everything you wanted to do when you arrive somewhere you don't visit that often.\n\nAlthough I've only been using them for a few days, I've already found the location-based alerts very useful. It just makes me happy to see my grocery list pop up automatically when I pull into the parking lot.\n",
				"date_published": "2011-10-12T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/10/12/omnifocus-locationbased-alerts/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/09/26/from-address-keyboard-shortcuts-in/",
				"title": "''From'' address keyboard shortcuts in Lion",
				"content_html": "<p>Mac OS X contains a truly helpful feature in the Keyboard system preference pane that allows you to set a keyboard shortcut for any menu item in any application by name. In Snow Leopard, I used this feature to add keyboard shortcuts for selecting the &ldquo;From&rdquo; address while I was composing an email in Mail.app. I had configured it so that ⌃1, ⌃2, etc. would switch between my personal, work, and other email accounts. The menu items in Mail.app are titled things like &ldquo;Andre Arko <a href=\"mailto:andre@arko.net\">andre@arko.net</a>&rdquo;, so it was easy to assign a keyboard shortcut to that item by name. Unfortunately, Lion&rsquo;s Keyboard preference pane adds a bug with menu item titles that contain angle brackets. The bug escapes angle brackets incorrectly when inserting them into the application&rsquo;s preferences plist file. As a result, the keyboard shortcuts are never activated.</p>\n<p>After some manual investigation, I figured out that Mail&rsquo;s plist file was getting entries that looked like &ldquo;\\eAndre Arko &lt;andre@arko.net\\e&rdquo; instead. I <a href=\"http://openradar.appspot.com/radar?id=1288404\">filed a bug with Apple</a>, but I haven&rsquo;t gotten a response, so I figure I&rsquo;m stuck with it for a while. With some experimentation, I discovered that it&rsquo;s possible to manually edit the plist to fix the shortcuts, but the bug is re-triggered whenever you open the Keyboard prefpane. To automate fixing the Mail shortcuts after changing other settings, I wrote a ruby script:</p>\n<pre tabindex=\"0\"><code>#!/usr/bin/env ruby\nrequire &#39;rubygems&#39;\nrequire &#39;plist&#39;\n\npath = File.expand_path(&#34;~/Library/Preferences/com.apple.mail.plist&#34;)\n# Convert to XML plist so the plist gem can read it\nsystem(&#34;plutil -convert xml1 &#39;#{path}&#39;&#34;)\n\n# Parse the plist into a hash\nprefs = Plist.parse_xml(path)\n\n# Gsub away the buggy values from the Lion Keyboard.prefpane\nuser_keys = prefs[&#34;NSUserKeyEquivalents&#34;]\nuser_keys.keys.each do |key|\n  shortcut = user_keys.delete(key)\n  fixed_key = key.gsub(/\\e(.*?)\\e/, &#39;\\1&gt;&#39;)\n  user_keys[fixed_key] = shortcut\nend\n\n# Write out the fixed plist as XML\nFile.open(path, &#34;w&#34;){|f| f.write(prefs.to_plist) }\n\n# Convert the XML plist back to binary for Mail.app\nsystem(&#34;plutil -convert binary1 &#39;#{path}&#39;&#34;)\n</code></pre><p>I also posted <a href=\"https://gist.github.com/1131361\">the code as a gist</a> if you&rsquo;d like to fork it or comment on it. So far, there is <a href=\"https://gist.github.com/1225867\">a fork that adds support for Sparrow</a>, the standalone Gmail client, as well.</p>\n",
				"content_text": "\nMac OS X contains a truly helpful feature in the Keyboard system preference pane that allows you to set a keyboard shortcut for any menu item in any application by name. In Snow Leopard, I used this feature to add keyboard shortcuts for selecting the \"From\" address while I was composing an email in Mail.app. I had configured it so that ⌃1, ⌃2, etc. would switch between my personal, work, and other email accounts. The menu items in Mail.app are titled things like \"Andre Arko <andre@arko.net>\", so it was easy to assign a keyboard shortcut to that item by name. Unfortunately, Lion's Keyboard preference pane adds a bug with menu item titles that contain angle brackets. The bug escapes angle brackets incorrectly when inserting them into the application's preferences plist file. As a result, the keyboard shortcuts are never activated.\n\nAfter some manual investigation, I figured out that Mail's plist file was getting entries that looked like \"\\eAndre Arko <andre@arko.net\\e\" instead. I [filed a bug with Apple](http://openradar.appspot.com/radar?id=1288404), but I haven't gotten a response, so I figure I'm stuck with it for a while. With some experimentation, I discovered that it's possible to manually edit the plist to fix the shortcuts, but the bug is re-triggered whenever you open the Keyboard prefpane. To automate fixing the Mail shortcuts after changing other settings, I wrote a ruby script:\n\n```\n#!/usr/bin/env ruby\nrequire 'rubygems'\nrequire 'plist'\n\npath = File.expand_path(\"~/Library/Preferences/com.apple.mail.plist\")\n# Convert to XML plist so the plist gem can read it\nsystem(\"plutil -convert xml1 '#{path}'\")\n\n# Parse the plist into a hash\nprefs = Plist.parse_xml(path)\n\n# Gsub away the buggy values from the Lion Keyboard.prefpane\nuser_keys = prefs[\"NSUserKeyEquivalents\"]\nuser_keys.keys.each do |key|\n  shortcut = user_keys.delete(key)\n  fixed_key = key.gsub(/\\e(.*?)\\e/, '\\1>')\n  user_keys[fixed_key] = shortcut\nend\n\n# Write out the fixed plist as XML\nFile.open(path, \"w\"){|f| f.write(prefs.to_plist) }\n\n# Convert the XML plist back to binary for Mail.app\nsystem(\"plutil -convert binary1 '#{path}'\")\n```\n\nI also posted [the code as a gist](https://gist.github.com/1131361) if you'd like to fork it or comment on it. So far, there is [a fork that adds support for Sparrow](https://gist.github.com/1225867), the standalone Gmail client, as well.\n",
				"date_published": "2011-09-26T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/09/26/from-address-keyboard-shortcuts-in/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/09/04/rvm-system-gems-without-sudo/",
				"title": "RVM system gems without sudo",
				"content_html": "<p>Somewhat unusually (I guess?) I sometimes use system Ruby on OS X. I use RVM for other versions of Ruby, like 1.9.2 and JRuby. System Ruby is still the easiest Ruby version to use on OS X, however. Furthermore, because it is so common with developers, I need to make sure that each new version of <a href=\"https://github.com/carlhuda/bundler\">Bundler</a> is compatible with OS X system Ruby.</p>\n<p>System Ruby has a problem, though: it is hardcoded to install gem executables into <code>/usr/bin</code>. This is very awkward because it means you need to run <code>sudo gem install</code> so Rubygems can install the binaries. A less awkward setup (IMHO), is to leave /Library/Ruby/Gems/1.8 owned by the staff group, as it is in a fresh OS X install. In order to make that setup work, however, you have to override the Rubygems binary directory by adding <code>-n/Library/Ruby/Gems/1.8/bin</code> to your <code>~/.gemrc</code> file, or pass that to the <code>gem</code> command every time you run it. Once you add that directory to the front of your <code>$PATH</code> environment variable, you&rsquo;re set. Install gems without sudo, and everything is good.</p>\n<p>This is all well and good until you install RVM and it tries to install gems for other versions of Ruby. Those other versions of Ruby will honor your <code>~/.gemrc</code> file and install your gem binaries into the system gems binary directory. In order to work around this, I have written an after_use hook for RVM. It redefines the <code>gem</code> function that RVM sets up when you change between Ruby versions to include the -n option or not, depending on the version of Ruby that you have switched to. Here&rsquo;s the code:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\">#!/usr/bin/env bash\n</span></span></span><span style=\"display:flex;\"><span><span style=\"color:#75715e\"></span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">if</span> <span style=\"color:#f92672\">[</span> $rvm_ruby_string <span style=\"color:#f92672\">==</span> <span style=\"color:#e6db74\">&#34;system&#34;</span> <span style=\"color:#f92672\">]</span>; <span style=\"color:#66d9ef\">then</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">function</span> gem <span style=\"color:#f92672\">{</span>\n</span></span><span style=\"display:flex;\"><span>    local result\n</span></span><span style=\"display:flex;\"><span>    command gem <span style=\"color:#e6db74\">&#34;-n/Library/Ruby/Gems/1.8/bin </span>$@<span style=\"color:#e6db74\">&#34;</span> ; result<span style=\"color:#f92672\">=</span><span style=\"color:#e6db74\">&#34;</span>$?<span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>    hash -r\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">return</span> $result\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#f92672\">}</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">else</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">function</span> gem <span style=\"color:#f92672\">{</span>\n</span></span><span style=\"display:flex;\"><span>    local result\n</span></span><span style=\"display:flex;\"><span>    command gem <span style=\"color:#e6db74\">&#34;</span>$@<span style=\"color:#e6db74\">&#34;</span> ; result<span style=\"color:#f92672\">=</span><span style=\"color:#e6db74\">&#34;</span>$?<span style=\"color:#e6db74\">&#34;</span>\n</span></span><span style=\"display:flex;\"><span>    hash -r\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">return</span> $result\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#f92672\">}</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">fi</span>\n</span></span></code></pre></div><p>In order to install it, run these commands:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-bash\" data-lang=\"bash\"><span style=\"display:flex;\"><span>curl http://bit.ly/after_use_system_binstubs &gt; ~/.rvm/hooks/after_use_system_binstubs\n</span></span><span style=\"display:flex;\"><span>chmod +x ~/.rvm/hooks/after_use_system_binstubs\n</span></span></code></pre></div><p>Once the hook is installed, RVM will redefine the <code>gem</code> function to include the -n option for system gem installs, and not include the -n option for non-system gem installs. It&rsquo;s really pretty nice, since you no longer have to use <code>sudo</code> to install any gems, system or not.</p>\n",
				"content_text": "Somewhat unusually (I guess?) I sometimes use system Ruby on OS X. I use RVM for other versions of Ruby, like 1.9.2 and JRuby. System Ruby is still the easiest Ruby version to use on OS X, however. Furthermore, because it is so common with developers, I need to make sure that each new version of [Bundler][1] is compatible with OS X system Ruby.\n\n[1]: https://github.com/carlhuda/bundler\n\nSystem Ruby has a problem, though: it is hardcoded to install gem executables into `/usr/bin`. This is very awkward because it means you need to run `sudo gem install` so Rubygems can install the binaries. A less awkward setup (IMHO), is to leave /Library/Ruby/Gems/1.8 owned by the staff group, as it is in a fresh OS X install. In order to make that setup work, however, you have to override the Rubygems binary directory by adding `-n/Library/Ruby/Gems/1.8/bin` to your `~/.gemrc` file, or pass that to the `gem` command every time you run it. Once you add that directory to the front of your `$PATH` environment variable, you're set. Install gems without sudo, and everything is good.\n\nThis is all well and good until you install RVM and it tries to install gems for other versions of Ruby. Those other versions of Ruby will honor your `~/.gemrc` file and install your gem binaries into the system gems binary directory. In order to work around this, I have written an after_use hook for RVM. It redefines the `gem` function that RVM sets up when you change between Ruby versions to include the -n option or not, depending on the version of Ruby that you have switched to. Here's the code:\n\n``` bash\n#!/usr/bin/env bash\n\nif [ $rvm_ruby_string == \"system\" ]; then\n  function gem {\n    local result\n    command gem \"-n/Library/Ruby/Gems/1.8/bin $@\" ; result=\"$?\"\n    hash -r\n    return $result\n  }\nelse\n  function gem {\n    local result\n    command gem \"$@\" ; result=\"$?\"\n    hash -r\n    return $result\n  }\nfi\n```\n\nIn order to install it, run these commands:\n\n``` bash\ncurl http://bit.ly/after_use_system_binstubs > ~/.rvm/hooks/after_use_system_binstubs\nchmod +x ~/.rvm/hooks/after_use_system_binstubs\n```\n\nOnce the hook is installed, RVM will redefine the `gem` function to include the -n option for system gem installs, and not include the -n option for non-system gem installs. It's really pretty nice, since you no longer have to use `sudo` to install any gems, system or not.\n",
				"date_published": "2011-09-04T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/09/04/rvm-system-gems-without-sudo/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/08/29/view-github-issues-opened-by/",
				"title": "View Github issues opened by a user",
				"content_html": "<p>Since I keep wanting this and forgetting how to do it, I&rsquo;m posting it for posterity (and myself, later). In Github Issues 2.0, there is an undocumented URL that lets you see all the tickets opened by a specific person:</p>\n<pre><code>http://github.com/owner/reponame/issues/created_by/user\n</code></pre>\n<p>I&rsquo;ve been using it for things like <a href=\"http://github.com/carlhuda/bundler/issues/created_by/indirect\">Bundler issues I have opened</a>.</p>\n",
				"content_text": "Since I keep wanting this and forgetting how to do it, I'm posting it for posterity (and myself, later). In Github Issues 2.0, there is an undocumented URL that lets you see all the tickets opened by a specific person:\n\n    http://github.com/owner/reponame/issues/created_by/user\n\nI've been using it for things like [Bundler issues I have opened](http://github.com/carlhuda/bundler/issues/created_by/indirect).\n",
				"date_published": "2011-08-29T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/08/29/view-github-issues-opened-by/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/08/18/pid-numbers-for-rails-logs/",
				"title": "PID numbers for Rails 3 logs",
				"content_html": "<p><strong>tl;dr</strong>: Copy <a href=\"https://gist.github.com/1091527\">the code</a> into <code>lib/pid_logger.rb</code> and edit your <code>application.rb</code>. Now your Rails 3 app has PID numbers on each log line. Tada.</p>\n<p>I recently signed up for <a href=\"http://papertrailapp.com\">Papertrail</a>, and starting sending all of my production Rails logs to a single consolidated place for tailing and searching. Papertrail is pretty great, but having all my logs in one place revealed a pretty big problem with default Passenger configurations. All the Passenger processes log to the same <code>production.log</code> file. This means that log lines from completely unrelated requests can get intermingled, making it impossible to tell which log lines belong to which requests. This makes the production log nearly useless for reconstructing what happened while investigating something.</p>\n<p>A typical solution to this issue, especially for Mongrel or Unicorn setups, is to have each Rails process log to a separate file. Since I&rsquo;m using Passenger, though, I came up with a simpler solution: prepend the PID of the current process to each log line. This means I can use Papertrail to filter the production log by PID number, and then I can focus on a single request as needed.</p>\n<p>There are a lot of patches floating around the internet that change how Rails logs things, and I wasn&rsquo;t feeling great about copying someone else&rsquo;s monkepatch into my app. Ultimately, I wrote a subclass of ActiveSupport::BufferedLogger that uses some of the tricks from patches floating around the web and some of my own code. Once you install it, you&rsquo;ll be able to see both the PID and the log level of each line written to the production logs, greatly improving things. I suggest putting this file into <code>lib/pid_logger.rb</code>.</p>\n<pre><code># You must require this file in application.rb, above the Application\n# definition, for this to work. For example:\n#\n#   # PIDs prepended to logs\n#   if Rails.env.production?\n#     require File.expand_path('../../lib/pid_logger', __FILE__)\n#   end\n#\n#   module MyApp\n#     class Application &lt; Rails::Application\n\nrequire 'active_support/buffered_logger'\n\nclass PidLogger &lt; ActiveSupport::BufferedLogger\n\n  SEVERITIES = Severity.constants.sort_by{|c| Severity.const_get(c) }\n\n  def add(severity, message = nil, progname = nil, &amp;block)\n    return if @level &gt; severity\n    message = (message || (block &amp;&amp; block.call) || progname).to_s\n    # Prepend pid and severity to the written message\n    log = &quot;[#{$$}] #{SEVERITIES[severity]} #{message.gsub(/^\\n+/, '')}&quot;\n    # If a newline is necessary then create a new message ending with a newline.\n    log &lt;&lt; &quot;\\n&quot; unless log[-1] == ?\\n\n    buffer &lt;&lt; log\n    auto_flush\n    message\n  end\n\n  class Railtie &lt; ::Rails::Railtie\n    initializer &quot;swap in PidLogger&quot; do\n      Rails.logger = PidLogger.new(Rails.application.config.paths.log.first)\n      ActiveSupport::Dependencies.logger = Rails.logger\n      Rails.cache.logger = Rails.logger\n      ActiveSupport.on_load(:active_record) do\n        ActiveRecord::Base.logger = Rails.logger\n      end\n      ActiveSupport.on_load(:action_controller) do\n        ActionController::Base.logger = Rails.logger\n      end\n      ActiveSupport.on_load(:action_mailer) do\n        ActionMailer::Base.logger = Rails.logger\n      end\n    end\n  end\n\nend\n</code></pre>\n<p>I&rsquo;ve also posted the code on Github <a href=\"https://gist.github.com/1091527\">in a gist</a>, for forking, commentary, and improvement. Thanks!</p>\n",
				"content_text": "**tl;dr**: Copy [the code][1] into `lib/pid_logger.rb` and edit your `application.rb`. Now your Rails 3 app has PID numbers on each log line. Tada.\n\nI recently signed up for [Papertrail](http://papertrailapp.com), and starting sending all of my production Rails logs to a single consolidated place for tailing and searching. Papertrail is pretty great, but having all my logs in one place revealed a pretty big problem with default Passenger configurations. All the Passenger processes log to the same `production.log` file. This means that log lines from completely unrelated requests can get intermingled, making it impossible to tell which log lines belong to which requests. This makes the production log nearly useless for reconstructing what happened while investigating something.\n\nA typical solution to this issue, especially for Mongrel or Unicorn setups, is to have each Rails process log to a separate file. Since I'm using Passenger, though, I came up with a simpler solution: prepend the PID of the current process to each log line. This means I can use Papertrail to filter the production log by PID number, and then I can focus on a single request as needed.\n\nThere are a lot of patches floating around the internet that change how Rails logs things, and I wasn't feeling great about copying someone else's monkepatch into my app. Ultimately, I wrote a subclass of ActiveSupport::BufferedLogger that uses some of the tricks from patches floating around the web and some of my own code. Once you install it, you'll be able to see both the PID and the log level of each line written to the production logs, greatly improving things. I suggest putting this file into `lib/pid_logger.rb`.\n\n    # You must require this file in application.rb, above the Application\n    # definition, for this to work. For example:\n    #\n    #   # PIDs prepended to logs\n    #   if Rails.env.production?\n    #     require File.expand_path('../../lib/pid_logger', __FILE__)\n    #   end\n    #\n    #   module MyApp\n    #     class Application < Rails::Application\n\n    require 'active_support/buffered_logger'\n\n    class PidLogger < ActiveSupport::BufferedLogger\n\n      SEVERITIES = Severity.constants.sort_by{|c| Severity.const_get(c) }\n\n      def add(severity, message = nil, progname = nil, &block)\n        return if @level > severity\n        message = (message || (block && block.call) || progname).to_s\n        # Prepend pid and severity to the written message\n        log = \"[#{$$}] #{SEVERITIES[severity]} #{message.gsub(/^\\n+/, '')}\"\n        # If a newline is necessary then create a new message ending with a newline.\n        log << \"\\n\" unless log[-1] == ?\\n\n        buffer << log\n        auto_flush\n        message\n      end\n\n      class Railtie < ::Rails::Railtie\n        initializer \"swap in PidLogger\" do\n          Rails.logger = PidLogger.new(Rails.application.config.paths.log.first)\n          ActiveSupport::Dependencies.logger = Rails.logger\n          Rails.cache.logger = Rails.logger\n          ActiveSupport.on_load(:active_record) do\n            ActiveRecord::Base.logger = Rails.logger\n          end\n          ActiveSupport.on_load(:action_controller) do\n            ActionController::Base.logger = Rails.logger\n          end\n          ActiveSupport.on_load(:action_mailer) do\n            ActionMailer::Base.logger = Rails.logger\n          end\n        end\n      end\n\n    end\n\nI've also posted the code on Github [in a gist][1], for forking, commentary, and improvement. Thanks!\n\n[1]:https://gist.github.com/1091527\n",
				"date_published": "2011-08-18T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/08/18/pid-numbers-for-rails-logs/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/08/07/fixing-mac-shutdown-delays/",
				"title": "Fixing Mac Shutdown Delays",
				"content_html": "<h3 id=\"or-how-mysql-is-broken-on-os-x\">or, How MySQL is broken on OS X</h3>\n<p>Ever since I saw James Edward Grey II <a href=\"http://twitter.com/#!/JEG2/status/49261212801839104\">point out</a> the delay at shutdown caused by having MySQL installed on OS X, it has bugged me. A lot. I even spent a few hours trying to figure out what was going on back then, but to no avail. The only thing I was able to figure out at the time was that running <code>launchctl unload com.mysql.mysqld.plist</code> always took 20 seconds.</p>\n<p>This last week, however, I installed MySQL 5.5 on a new MacBook Air. The first thing I discovered is that it is <a href=\"http://lightyearsoftware.com/2011/02/mysql-5-5-on-mac-os-x/\">deeply broken</a>. Not only is the path to the shared library incomplete, the provided OS X StartupItem doesn&rsquo;t work.</p>\n<p>Since the startup item was broken, I resolved to create a plist that would allow <a href=\"http://developer.apple.com/library/mac/#documentation/Darwin/Reference/ManPages/man8/launchd.8.html\">launchd</a> to control MySQL. While I was creating and testing that plist, I discovered that <code>mysqld</code> completely ignores interrupts, and will happily run forever while you press ⌃C to no avail. The only way to tell the server process to stop is to send it a <code>SIGTERM</code>, via <code>kill</code> or what have you. As I made that discovery, I suddenly developed a hunch that the shutdown delay was caused by the <code>mysqld</code> process refusing to shut down when interrupted.</p>\n<p>After a bit of digging through the search results for <code>launchd 20 seconds</code>, I discovered the <a href=\"http://developer.apple.com/library/mac/#documentation/Darwin/Reference/ManPages/man5/launchd.plist.5.html\">launchd.plist</a> man page. On that page, I learned about a completely new (to me) option: <code>ExitTimeOut</code>.</p>\n<pre><code>ExitTimeOut &lt;integer&gt;\nThe amount of time launchd waits before sending a SIGKILL signal. The default value is 20 seconds. The value zero is interpreted as infinity.\n</code></pre>\n<p>The mention of a 20 second default stood out like a red flag to me, so I tried adding an entry to the MySQL plist that changed <code>ExitTimeOut</code> to one second. As soon as I did that, <code>launchctl unload com.mysql.mysqld.plist</code> took only one second to run. With great excitement, I tested out restarting my computer, and discovered that that single change completely resolved the long delays that had been irritating me.</p>\n<p>Here is the complete code for the plist, located at <code>/Library/LaunchDaemons/com.mysql.mysqld.plist</code>.</p>\n<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;!DOCTYPE plist PUBLIC &quot;-//Apple//DTD PLIST 1.0//EN&quot;\n&quot;http://www.apple.com/DTDs/PropertyList-1.0.dtd&quot;&gt;\n&lt;plist version=&quot;1.0&quot;&gt;\n&lt;dict&gt;\n\t&lt;key&gt;KeepAlive&lt;/key&gt;\n\t&lt;true/&gt;\n\t&lt;key&gt;Label&lt;/key&gt;\n\t&lt;string&gt;com.mysql.mysqld&lt;/string&gt;\n\t&lt;key&gt;Program&lt;/key&gt;\n\t&lt;string&gt;/usr/local/mysql/bin/mysqld_safe&lt;/string&gt;\n\t&lt;key&gt;RunAtLoad&lt;/key&gt;\n\t&lt;true/&gt;\n    &lt;key&gt;UserName&lt;/key&gt;\n    &lt;string&gt;_mysql&lt;/string&gt;\n\t&lt;key&gt;WorkingDirectory&lt;/key&gt;\n    &lt;string&gt;/usr/local/mysql&lt;/string&gt;\n\t&lt;key&gt;ExitTimeOut&lt;/key&gt;\n\t&lt;integer&gt;1&lt;/integer&gt;\n&lt;/dict&gt;\n&lt;/plist&gt;\n</code></pre>\n<p>If you want to load it without restarting your machine, use the command <code>sudo launchctl load /Library/LaunchDaemons/com.mysql.mysqld.plist</code>.</p>\n<p>Enjoy!</p>\n",
				"content_text": "### or, How MySQL is broken on OS X\n\nEver since I saw James Edward Grey II [point out][1] the delay at shutdown caused by having MySQL installed on OS X, it has bugged me. A lot. I even spent a few hours trying to figure out what was going on back then, but to no avail. The only thing I was able to figure out at the time was that running `launchctl unload com.mysql.mysqld.plist` always took 20 seconds.\n\nThis last week, however, I installed MySQL 5.5 on a new MacBook Air. The first thing I discovered is that it is [deeply broken][2]. Not only is the path to the shared library incomplete, the provided OS X StartupItem doesn't work.\n\n[1]: http://twitter.com/#!/JEG2/status/49261212801839104\n[2]: http://lightyearsoftware.com/2011/02/mysql-5-5-on-mac-os-x/\n\nSince the startup item was broken, I resolved to create a plist that would allow [launchd][3] to control MySQL. While I was creating and testing that plist, I discovered that `mysqld` completely ignores interrupts, and will happily run forever while you press ⌃C to no avail. The only way to tell the server process to stop is to send it a `SIGTERM`, via `kill` or what have you. As I made that discovery, I suddenly developed a hunch that the shutdown delay was caused by the `mysqld` process refusing to shut down when interrupted.\n\n[3]: http://developer.apple.com/library/mac/#documentation/Darwin/Reference/ManPages/man8/launchd.8.html\n\nAfter a bit of digging through the search results for `launchd 20 seconds`, I discovered the [launchd.plist][4] man page. On that page, I learned about a completely new (to me) option: `ExitTimeOut`.\n\n    ExitTimeOut <integer>\n    The amount of time launchd waits before sending a SIGKILL signal. The default value is 20 seconds. The value zero is interpreted as infinity.\n\n[4]: http://developer.apple.com/library/mac/#documentation/Darwin/Reference/ManPages/man5/launchd.plist.5.html\n\nThe mention of a 20 second default stood out like a red flag to me, so I tried adding an entry to the MySQL plist that changed `ExitTimeOut` to one second. As soon as I did that, `launchctl unload com.mysql.mysqld.plist` took only one second to run. With great excitement, I tested out restarting my computer, and discovered that that single change completely resolved the long delays that had been irritating me.\n\nHere is the complete code for the plist, located at `/Library/LaunchDaemons/com.mysql.mysqld.plist`.\n\n    <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\"\n    \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n    <plist version=\"1.0\">\n    <dict>\n    \t<key>KeepAlive</key>\n    \t<true/>\n    \t<key>Label</key>\n    \t<string>com.mysql.mysqld</string>\n    \t<key>Program</key>\n    \t<string>/usr/local/mysql/bin/mysqld_safe</string>\n    \t<key>RunAtLoad</key>\n    \t<true/>\n        <key>UserName</key>\n        <string>_mysql</string>\n    \t<key>WorkingDirectory</key>\n        <string>/usr/local/mysql</string>\n    \t<key>ExitTimeOut</key>\n    \t<integer>1</integer>\n    </dict>\n    </plist>\n\nIf you want to load it without restarting your machine, use the command `sudo launchctl load /Library/LaunchDaemons/com.mysql.mysqld.plist`.\n\nEnjoy!\n",
				"date_published": "2011-08-07T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/08/07/fixing-mac-shutdown-delays/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/07/23/safari-extensions-in-lion/",
				"title": "Safari extensions in Lion",
				"content_html": "<h3 id=\"tldr\">tl;dr</h3>\n<p>Install <a href=\"https://github.com/rs/SafariOmnibar\">Safari Omnibar</a> from the provided <a href=\"https://github.com/downloads/rs/SafariOmnibar/Safari%20Omnibar-1.1.pkg\">installer package</a>, and then download <a href=\"http://cl.ly/3o023c4026201S060p27\">my customized version</a> and install it to <code>/Library/Application Support/SIMBL/Plugins/</code>. Edit keywords in the file <code>SafariOmnibar.bundle/Resources/SearchProviders.plist</code>. Tada, now you have a single location bar for URLs, searches, and keyword searches.</p>\n<p><strong>Update</strong>: So as of today, you can just <a href=\"https://github.com/downloads/rs/SafariOmnibar/Safari%20Omnibar-1.2.pkg\">install SafariOmnibar 1.2</a> now and then update SearchProviders.plist yourself. If you&rsquo;d like to, you can <a href=\"https://gist.github.com/1101586\">copy my plist</a>.</p>\n<p>Install <a href=\"http://hoyois.github.com/safariextensions/clicktoplugin/\">the Safari extension version</a> of ClickToFlash, get video downloading for free.</p>\n<h3 id=\"the-state-of-safari-extensions-in-lion\">The state of Safari extensions in Lion</h3>\n<p>I have been a dedicated user of <a href=\"http://haoli.dnsalias.com/saft/\">Saft</a>, <a href=\"http://alexstaubo.github.com/keywurl/\">Keywurl</a>, <a href=\"http://glimmerblocker.org/\">GlimmerBlocker</a>, and <a href=\"http://clicktoflash.com/\">ClickToFlash</a> for years. When I upgraded to <a href=\"http://www.apple.com/macosx/\">Lion</a>, however, I discovered that none of them work.</p>\n<p>Saft mainly provided automatic restoration of open windows when quitting and restarting Safari. Keywurl provided searching directly in the location bar, and keyword searches to limit the search to specific sites. GlimmerBlocker provided ad-blocking and page transformations like a &ldquo;download video&rdquo; link on YouTube pages.</p>\n<p>When I upgraded to Lion, none of them worked.</p>\n<p>GlimmerBlocker was updated pretty quickly, but it turns out that YouTube download links download corrupted video files. Keywurl and Saft haven&rsquo;t been updated yet, and will probably take time. ClickToFlash likewise only supports Snow Leopard, and tickets reporting issues with Lion haven&rsquo;t been responded to for months. So, we need replacements.</p>\n<p>Happily, Saft&rsquo;s biggest feature (restore open windows) is now built in to Safari on Lion. Theoretically, GlimmerBlocker supports keyword searches in the location bar, but I couldn&rsquo;t figure out how to configure it and eventually gave up. I was really excited to find <a href=\"https://github.com/rs/SafariOmnibar\">SafariOmnibar</a>, which merges the search and location bars into a single bar that supports both. At that point, I only needed keyword searching to replace Keywurl completely.</p>\n<p>I was pretty excited to discover that keyword searches were already implemented in git, so I checked it out and compiled <a href=\"http://cl.ly/3o023c4026201S060p27\">my own release</a> of version 1.1 with keyword support. Then my only task was to import the Keywurl keyword plist into the SafariOmnibar keyword plist format. Now I have a complete replacement for Keywurl, albeit without a UI for adding new keywords.</p>\n<p>Unable to discover any obvious options for downloading YouTube videos with a quick Google search, I gave up on that and went to replace ClickToFlash. There is <a href=\"http://hoyois.github.com/safariextensions/clicktoplugin/\">a Safari extension version</a> that works quite well in Safari 5.1. It not only replaces Flash videos with HTML5 videos if possible, it even provides a &ldquo;Download video&rdquo; contextual menu item on those videos.</p>\n<p>So at this point I&rsquo;m pretty happy. The combination of SafariOmnibar and ClickToFlash replaced all of the extension functionality that I had previously been using, albeit coming from entirely different software. Woo.</p>\n",
				"content_text": "\n### tl;dr\n\nInstall [Safari Omnibar][so] from the provided [installer package][ip], and then download [my customized version][mcv] and install it to `/Library/Application Support/SIMBL/Plugins/`. Edit keywords in the file `SafariOmnibar.bundle/Resources/SearchProviders.plist`. Tada, now you have a single location bar for URLs, searches, and keyword searches.\n\n[so]: https://github.com/rs/SafariOmnibar\n[ip]: https://github.com/downloads/rs/SafariOmnibar/Safari%20Omnibar-1.1.pkg\n[mcv]: http://cl.ly/3o023c4026201S060p27\n\n**Update**: So as of today, you can just [install SafariOmnibar 1.2][12] now and then update SearchProviders.plist yourself. If you'd like to, you can [copy my plist][sp].\n\n[12]: https://github.com/downloads/rs/SafariOmnibar/Safari%20Omnibar-1.2.pkg\n[sp]: https://gist.github.com/1101586\n\nInstall [the Safari extension version][sectf] of ClickToFlash, get video downloading for free.\n\n[sectf]: http://hoyois.github.com/safariextensions/clicktoplugin/\n\n### The state of Safari extensions in Lion\n\nI have been a dedicated user of [Saft][saft], [Keywurl][kw], [GlimmerBlocker][gb], and [ClickToFlash][ctf] for years. When I upgraded to [Lion][lion], however, I discovered that none of them work.\n\nSaft mainly provided automatic restoration of open windows when quitting and restarting Safari. Keywurl provided searching directly in the location bar, and keyword searches to limit the search to specific sites. GlimmerBlocker provided ad-blocking and page transformations like a \"download video\" link on YouTube pages.\n\nWhen I upgraded to Lion, none of them worked.\n\nGlimmerBlocker was updated pretty quickly, but it turns out that YouTube download links download corrupted video files. Keywurl and Saft haven't been updated yet, and will probably take time. ClickToFlash likewise only supports Snow Leopard, and tickets reporting issues with Lion haven't been responded to for months. So, we need replacements.\n\nHappily, Saft's biggest feature (restore open windows) is now built in to Safari on Lion. Theoretically, GlimmerBlocker supports keyword searches in the location bar, but I couldn't figure out how to configure it and eventually gave up. I was really excited to find [SafariOmnibar][so], which merges the search and location bars into a single bar that supports both. At that point, I only needed keyword searching to replace Keywurl completely.\n\nI was pretty excited to discover that keyword searches were already implemented in git, so I checked it out and compiled [my own release][mcv] of version 1.1 with keyword support. Then my only task was to import the Keywurl keyword plist into the SafariOmnibar keyword plist format. Now I have a complete replacement for Keywurl, albeit without a UI for adding new keywords.\n\nUnable to discover any obvious options for downloading YouTube videos with a quick Google search, I gave up on that and went to replace ClickToFlash. There is [a Safari extension version][sectf] that works quite well in Safari 5.1. It not only replaces Flash videos with HTML5 videos if possible, it even provides a \"Download video\" contextual menu item on those videos.\n\nSo at this point I'm pretty happy. The combination of SafariOmnibar and ClickToFlash replaced all of the extension functionality that I had previously been using, albeit coming from entirely different software. Woo.\n\n[saft]: http://haoli.dnsalias.com/saft/\n[kw]: http://alexstaubo.github.com/keywurl/\n[gb]: http://glimmerblocker.org/\n[ctf]: http://clicktoflash.com/\n[lion]: http://www.apple.com/macosx/\n",
				"date_published": "2011-07-23T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/07/23/safari-extensions-in-lion/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/07/16/smoothing-rails-development-in-bash/",
				"title": "Smoothing Rails development in Bash",
				"content_html": "<p>This post covers a few ways that I&rsquo;ve automated the repetitive tasks that are required while building and deploying Rails apps.</p>\n<h3 id=\"rails-console\">Rails console</h3>\n<p>In Rails 2 apps, you get a console by running <code>./script/console</code>, but in Rails 3, you get a console by running <code>./script/rails console</code>. Because I want to open a console without having to remember which version of Rails I&rsquo;m using, I wrote an alias to figure it out and do the right thing for me.</p>\n<pre><code># Rails 2 and Rails 3 console\n# usage: rc [ENVIRONMENT]\nfunction rc {\n  if [ -e &quot;./script/console&quot; ]; then\n    ./script/console $@\n  else\n    rails console $@\n  fi\n}\n</code></pre>\n<h3 id=\"ssh-host-keys\">SSH Host Keys</h3>\n<p>Deploying apps to EC2 or something similar means that you will inevitably run into SSH telling you that the host key has changed and you could be experiencing a man-in-the-middle attack. Making the error go away requires opening your <code>known_hosts</code> file, finding the right line, deleting it, saving the file, and then trying again. It&rsquo;s a giant pain in the ass.</p>\n<p>The most common &ldquo;fix&rdquo; that I have seen is to disable host key checking entirely, but that removes the security provided by host keys. My fix is to be able to remove stale host keys quickly and then get back to what I was doing.</p>\n<pre><code># Remove known host entries\n# usage: ssh-rm-host -n LINENUM\n#        ssh-rm-host HOSTNAME\nfunction ssh-rm-host () {\n  if [ &quot;$1&quot; == &quot;-n&quot; ]; then\n    sed -i old $2d ~/.ssh/known_hosts\n  else\n    ssh-keygen -R $1\n  fi\n}\n</code></pre>\n<h3 id=\"running-specs\">Running specs</h3>\n<p>Similar to the console, running specs in Rails 3 apps requires RSpec 2 and the <code>rspec</code> binary. Specs in Rails 2 require RSpec 1 and the <code>spec</code> binary. Fortunately, the <code>spec_helper.rb</code> helps us distinguish between versions of RSpec, so we can just run the tests without stressing about versions.</p>\n<pre><code># easy spec running\n# usage: s [RSPEC_OPTS]\nfunction s {\n  if grep -q -i &quot;RSpec.configure do&quot; spec/spec_helper.rb; then\n    # echo &quot;rspec2!&quot;\n    if [ -z &quot;$*&quot; ]; then\n      rspec -fs -c spec\n    else\n      rspec -fs -c $*\n    fi\n  else\n    # echo &quot;rspec1!&quot;\n    if [ -z &quot;$*&quot; ]; then\n      spec _1.3.2_ -fs -c spec\n    else\n      spec _1.3.2_ -fs -c $*\n    fi\n  fi\n}\n</code></pre>\n<h3 id=\"rails-logs\">Rails logs</h3>\n<p>Tailing logs isn&rsquo;t actually that hard, but I type it so frequently that I got tired of doing it. While I&rsquo;m at it, I&rsquo;ll throw in my less-related configuration that comes in handy.</p>\n<pre><code>export PAGER='less' # less is more (than more)\nexport LESSEDIT='mate -l %lm %f' # open in textmate from less\nexport LESS='-XFRf' # don't clear screen on exit, show colors\nalias rl='less log/development.log'\n</code></pre>\n<h3 id=\"other-handy-stuff\">Other handy stuff</h3>\n<p>It&rsquo;s not directly related to Rails development, but I highly endorse <a href=\"https://github.com/joelthelion/autojump/\">autojump</a> for getting around quickly and my <a href=\"https://github.com/indirect/tm\">tm</a> tool for opening TextMate projects from the command line.</p>\n<p>That&rsquo;s all I&rsquo;ve got at the moment, but if you have any suggestions or additions I&rsquo;d be interested to hear about them.</p>\n",
				"content_text": "This post covers a few ways that I've automated the repetitive tasks that are required while building and deploying Rails apps.\n\n### Rails console\n\nIn Rails 2 apps, you get a console by running `./script/console`, but in Rails 3, you get a console by running `./script/rails console`. Because I want to open a console without having to remember which version of Rails I'm using, I wrote an alias to figure it out and do the right thing for me.\n\n    # Rails 2 and Rails 3 console\n    # usage: rc [ENVIRONMENT]\n    function rc {\n      if [ -e \"./script/console\" ]; then\n        ./script/console $@\n      else\n        rails console $@\n      fi\n    }\n\n### SSH Host Keys\n\nDeploying apps to EC2 or something similar means that you will inevitably run into SSH telling you that the host key has changed and you could be experiencing a man-in-the-middle attack. Making the error go away requires opening your `known_hosts` file, finding the right line, deleting it, saving the file, and then trying again. It's a giant pain in the ass.\n\nThe most common \"fix\" that I have seen is to disable host key checking entirely, but that removes the security provided by host keys. My fix is to be able to remove stale host keys quickly and then get back to what I was doing.\n\n    # Remove known host entries\n    # usage: ssh-rm-host -n LINENUM\n    #        ssh-rm-host HOSTNAME\n    function ssh-rm-host () {\n      if [ \"$1\" == \"-n\" ]; then\n        sed -i old $2d ~/.ssh/known_hosts\n      else\n        ssh-keygen -R $1\n      fi\n    }\n\n### Running specs\n\nSimilar to the console, running specs in Rails 3 apps requires RSpec 2 and the `rspec` binary. Specs in Rails 2 require RSpec 1 and the `spec` binary. Fortunately, the `spec_helper.rb` helps us distinguish between versions of RSpec, so we can just run the tests without stressing about versions.\n\n    # easy spec running\n    # usage: s [RSPEC_OPTS]\n    function s {\n      if grep -q -i \"RSpec.configure do\" spec/spec_helper.rb; then\n        # echo \"rspec2!\"\n        if [ -z \"$*\" ]; then\n          rspec -fs -c spec\n        else\n          rspec -fs -c $*\n        fi\n      else\n        # echo \"rspec1!\"\n        if [ -z \"$*\" ]; then\n          spec _1.3.2_ -fs -c spec\n        else\n          spec _1.3.2_ -fs -c $*\n        fi\n      fi\n    }\n\n### Rails logs\n\nTailing logs isn't actually that hard, but I type it so frequently that I got tired of doing it. While I'm at it, I'll throw in my less-related configuration that comes in handy.\n\n    export PAGER='less' # less is more (than more)\n    export LESSEDIT='mate -l %lm %f' # open in textmate from less\n    export LESS='-XFRf' # don't clear screen on exit, show colors\n    alias rl='less log/development.log'\n\n### Other handy stuff\n\nIt's not directly related to Rails development, but I highly endorse [autojump](https://github.com/joelthelion/autojump/) for getting around quickly and my [tm](https://github.com/indirect/tm) tool for opening TextMate projects from the command line.\n\nThat's all I've got at the moment, but if you have any suggestions or additions I'd be interested to hear about them.\n",
				"date_published": "2011-07-16T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/07/16/smoothing-rails-development-in-bash/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/07/02/sending-delayed-job-exceptions-to/",
				"title": "Sending Delayed Job Exceptions to Hoptoad",
				"content_html": "<p>I&rsquo;ve been using <a href=\"https://github.com/collectiveidea/delayed_job\">delayed_job</a> to handle background tasks recently. Unfortunately, my background jobs have sometimes had bugs (either in my code or in the code of the web service I usually talk to). While DJ records the last exception in the database between retries, the job itself is deleted completely on failure. Since I don&rsquo;t want my jobs table to fill up with every job I&rsquo;ve ever had that failed, I decided to send exceptions that occur during DJ jobs to Hoptoad. The error is saved for posterity at Hoptoad, I get an email notifying me there was a problem, and anyone on the team can investigate the issue. Wins all around. How do you do this black magic, you ask? It&rsquo;s actually not too bad in the end. Create an initializer in your Rails app (I creatively name mine <code>delayed_job_hoptoad.rb</code>), and then add this code:</p>\n<pre><code># Monkeypatch Delayed::Job to send Hoptoad notifications when there are exceptions\nrequire 'hoptoad_notifier'\n\nmodule Delayed\n  class Worker\n\n    def handle_failed_job_with_hoptoad(job, error)\n      HoptoadNotifier.notify_or_ignore(error, :cgi_data =&gt; job.attributes)\n      handle_failed_job_without_hoptoad(job, error)\n    end\n    alias_method_chain :handle_failed_job, :hoptoad\n\n  end\nend\n</code></pre>\n<p>Not too bad in the end. I originally called <code>handle_failed_job</code> instead of <code>handle_failed_job_without_hoptoad</code>, though, and that resulted in a stack overflow every time there was an error. Way to make things better, I know.</p>\n",
				"content_text": "I've been using [delayed_job][1] to handle background tasks recently. Unfortunately, my background jobs have sometimes had bugs (either in my code or in the code of the web service I usually talk to). While DJ records the last exception in the database between retries, the job itself is deleted completely on failure. Since I don't want my jobs table to fill up with every job I've ever had that failed, I decided to send exceptions that occur during DJ jobs to Hoptoad. The error is saved for posterity at Hoptoad, I get an email notifying me there was a problem, and anyone on the team can investigate the issue. Wins all around. How do you do this black magic, you ask? It's actually not too bad in the end. Create an initializer in your Rails app (I creatively name mine `delayed_job_hoptoad.rb`), and then add this code:\n\n    # Monkeypatch Delayed::Job to send Hoptoad notifications when there are exceptions\n    require 'hoptoad_notifier'\n\n    module Delayed\n      class Worker\n\n        def handle_failed_job_with_hoptoad(job, error)\n          HoptoadNotifier.notify_or_ignore(error, :cgi_data => job.attributes)\n          handle_failed_job_without_hoptoad(job, error)\n        end\n        alias_method_chain :handle_failed_job, :hoptoad\n\n      end\n    end\n\nNot too bad in the end. I originally called `handle_failed_job` instead of `handle_failed_job_without_hoptoad`, though, and that resulted in a stack overflow every time there was an error. Way to make things better, I know.\n\n[1]: https://github.com/collectiveidea/delayed_job\n",
				"date_published": "2011-07-02T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/07/02/sending-delayed-job-exceptions-to/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/06/30/rails-page-caching-nginx-and/",
				"title": "Rails page caching, Nginx, and RESTful routes",
				"content_html": "<h3 id=\"or-why-am-i-getting-these-405-not-allowed-errors\">Or, Why Am I Getting These 405 Not Allowed Errors</h3>\n<p>There are some situations where Rails&rsquo; page caching is really great. Usually big pages that take a while to generate and are publicly available to everyone. If you&rsquo;re using Rails page caching, chances are good that you&rsquo;re also using Nginx, because everyone seems to be doing that nowadays. Suddenly, a <s>challenger</s> problem appears! If you are also using <a href=\"http://guides.rubyonrails.org/routing.html\">RESTful routes</a> as per the Rails defaults, Nginx suddenly starts throwing inexplicable <code>405 Not Allowed</code> errors.</p>\n<p>Let&rsquo;s assume you have cached a page for a certain URL, like <code>/posts</code>. Once the cached file is generated, Nginx will serve that file instead of passing the request to the Rails backend. Creating a new post involves a POST request to the very same url <code>/posts</code>. Unfortunately, Nginx notices the cached file and helpfully informs you that it&rsquo;s not possible to issue a POST request against a static file (that 405 that is probably driving you crazy). Fortunately, there is a solution!</p>\n<p>The solution has two steps:</p>\n<ol>\n<li>\n<p>Set your page_cache directory to <code>public/cache</code> by editing <code>production.rb</code> to have a line like this:</p>\n<p>config.action_controller.page_cache_directory = Rails.root.join(&ldquo;public/cache&rdquo;).to_s</p>\n</li>\n<li>\n<p>Edit your nginx config file, adding these two conditionals to your server block:</p>\n<p>server {\n# &hellip; your other nginx and rails configuration stuff &hellip;</p>\n<pre><code>   # Don't try to serve the cache for POST, PUT, or DELETE\n   # if you do try, nginx returns a 405 Not Allowed error\n   if ($request_method != GET) {\n       break;\n   }\n\n   # If it is a GET after all, try to serve the cache before\n   # passing the request off to passenger\n   if (-f $document_root/cache$uri) {\n       rewrite (.*) /cache$1 break;\n   }\n</code></pre>\n<p>}</p>\n</li>\n</ol>\n<p>It&rsquo;s not that simple, and required lots of trial and error to get working, but you can just copy and paste my solution and be done. Go internet.</p>\n",
				"content_text": "### Or, Why Am I Getting These 405 Not Allowed Errors\n\nThere are some situations where Rails' page caching is really great. Usually big pages that take a while to generate and are publicly available to everyone. If you're using Rails page caching, chances are good that you're also using Nginx, because everyone seems to be doing that nowadays. Suddenly, a <s>challenger</s> problem appears! If you are also using [RESTful routes](http://guides.rubyonrails.org/routing.html) as per the Rails defaults, Nginx suddenly starts throwing inexplicable `405 Not Allowed` errors.\n\nLet's assume you have cached a page for a certain URL, like `/posts`. Once the cached file is generated, Nginx will serve that file instead of passing the request to the Rails backend. Creating a new post involves a POST request to the very same url `/posts`. Unfortunately, Nginx notices the cached file and helpfully informs you that it's not possible to issue a POST request against a static file (that 405 that is probably driving you crazy). Fortunately, there is a solution!\n\nThe solution has two steps:\n\n  1. Set your page_cache directory to `public/cache` by editing `production.rb` to have a line like this:\n\n        config.action_controller.page_cache_directory = Rails.root.join(\"public/cache\").to_s\n\n  2. Edit your nginx config file, adding these two conditionals to your server block:\n\n        server {\n            # ... your other nginx and rails configuration stuff ...\n\n            # Don't try to serve the cache for POST, PUT, or DELETE\n            # if you do try, nginx returns a 405 Not Allowed error\n            if ($request_method != GET) {\n                break;\n            }\n\n            # If it is a GET after all, try to serve the cache before\n            # passing the request off to passenger\n            if (-f $document_root/cache$uri) {\n                rewrite (.*) /cache$1 break;\n            }\n        }\n\nIt's not that simple, and required lots of trial and error to get working, but you can just copy and paste my solution and be done. Go internet.\n",
				"date_published": "2011-06-30T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/06/30/rails-page-caching-nginx-and/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/06/11/deploying-with-bundler-notes/",
				"title": "Deploying With Bundler Notes",
				"content_html": "<p><em>I gave a talk at RailsConf 2011 about deploying with Bundler. These are my notes for the talk. I&rsquo;ve also posted my <a href=\"/2011/06/11/deploying-with-bundler-slides\">slides for the talk</a>. Combined, they probably serve as a somewhat reasonable replacement for having seen the talk itself.</em></p>\n<p>Bundler exists to make booting your application consistent and repeatable, guaranteed. It does this by having you write a Gemfile listing of all the gems that your application needs to boot. When it installs those gems, it records the exact version of every gem into the Gemfile.lock file. Other developers are guaranteed to have the same gems you do. Your production servers are guaranteed to have the same gems you do.</p>\n<p>This is a big deal.</p>\n<p>It seems kind of easy and obvious now in retrospect, but in the bad old days, it could take anywhere from hours to days to get a new developer or a new server up and running your app with all the right versions of the gems that you needed. That entire process has been boiled down to running <code>bundle install</code> and waiting for a minute or two.</p>\n<p>So! You start developing a beautiful new application. At some point, someone else starts working on it with you, and they only have to run <code>bundle install</code> and then they can start developing, too. Later, you realize that you want to deploy this application to a server in production so that the entire world can see it. Fantastic.</p>\n<p>Now that Ruby application servers all use Rack to communicate with your underlying application, Rails, Sinatra, and every other web framework can be treated identically for deployment. Just put the library you use into your Gemfile, and your app will have all the gems that it needs to run on the server.</p>\n<p>I&rsquo;m going to cover the various deployment options that are available to apps that use Bundler, starting with the simplest and easiest, and expanding to include the more unusual and more complicated scenarios at the end.</p>\n<p>The most drop-dead simple thing that you can possibly do to get your bundled application up and running on a server on the public internet is to have someone else do it. If you use Heroku, you can <code>git push heroku master</code>, and if you use Engine Yard, you can run <code>ey deploy</code>. In either case, their infrastructure will take care of everything. They check and see that you have a Gemfile, they run Bundler with settings tuned for their specific environments, and then they boot your application and you can start using it right away.</p>\n<p>The next level of complexity is running your own server, installing Phusion Passenger, and having Passenger manage each of your app server instances. It&rsquo;s not that much more complicated, but this is the point at which you run into the first potential Bundler deployment issue: installing system gems requires root permissions.</p>\n<p>You might be tempted to add <code>sudo bundle install</code> to your deployment script and call it a day, but that turns out to be a terrible idea. The crux of the problem is that <code>bundle install</code> saves configuration information that is specific to this application &ndash; what groups of gems to ignore (like :development or :test), where the gems are installed to, all of those sorts of things. If you run <code>bundle install</code> as root, that file is created and owned by root. That means the Passenger app server (which runs as the nobody user by default) can&rsquo;t read or write to the bundle configuration.</p>\n<p>Bundler contains an &ldquo;emergency workaround&rdquo; for this situation where Bundler will invoke sudo itself to install the gems. It&rsquo;s a hacky workaround at best, though, because entering your sudo password isn&rsquo;t scriptable. Even more awkwardly, many system administrators don&rsquo;t give regular users the ability to sudo at all. This means installing to system gems is ruled out completely. The much more effective solution is to install the application&rsquo;s gems into a path owned by the web server user. This is simple to do with the command <code>bundle install --path vendor/bundle</code>.</p>\n<p>Okay, so now we have our gems installed on the server into an application-specific path. There is one potential issue remaining: what happens if a developer changes the Gemfile, forgets to run <code>bundle install</code>, and then tries to deploy? The deploy script will install the bundle on the production server, but the server will be running against gems that have never been tested at all. You can avoid the potential disaster by using <code>bundle install --frozen</code>. Frozen mode means that Bundler checks the Gemfile against the Gemfile.lock file, and refuses to install if they don&rsquo;t match. The error message instructs you to run <code>bundle install</code> on your development machine, make sure everything works, and then check in a new Gemfile.lock with the versions that you are now sure work with your application.</p>\n<p>Now your application works in production, you can deploy without superuser permissions, and you will never run with gems in production that haven&rsquo;t been tested yet! Since this is the most common case, Bundler wraps all of this up into a single option that you can pass to the install command: <code>bundle install --deployment</code>.</p>\n<p>But wait, you say, what if I use Unicorn or Rainbows or Mongrel or something like that? Well, it&rsquo;s pretty simple as well. Just add the server to your Gemfile. When you want to run your server, use e.g. <code>bundle exec unicorn</code>. This will make sure that the exact version in your Gemfile will be started, regardless of other versions on your system.</p>\n<p>The final complication that we sometimes see while deploying to production environments is servers that have been heavily firewalled, and cannot connect to rubygems.org to download gems for installation at deploy-time. Sometimes it&rsquo;s possible to use a proxy server, but when even that isn&rsquo;t an option, Bundler has your back. Run the command <code>bundle pack</code> to download the .gem file for every gem that your application needs into the <code>vendor/cache</code> directory. When you run <code>bundle install</code> later, Bundler is smart enough to automatically look in <code>vendor/cache</code>. It will install your bundle using those gems as a source, and completely avoid contacting rubygems.org, allowing you to deploy into the most restrictive server setup.</p>\n<p>At this point, you&rsquo;re probably thinking that this sounds like a lot to remember, and a lot of effort to get set up. Fortunately, Bundler even goes beyond providing the &ndash;deployment flag. It integrates directly with Capistrano and Vlad, the two most popular deployment systems. Adding Bundler to a working deploy setup is incredibly easy. With Capistrano, just add <code>require 'bundler/capistrano'</code> to your deploy.rb, and everything will work. With Vlad, just <code>require 'bundler/vlad'</code> in your deploy.rb, and make sure that your deploy task also runs the task <code>vlad:bundle:install</code>. That&rsquo;s it !If you need to adjust Bundler options, like the location the gems are installed to, or the list of groups that shouldn&rsquo;t be installed, you can. Just set some variables in your deploy script before Bundler runs. The details of how to configure everything are available at gembundler.com/deploying.</p>\n<p>The next feature release of Bundler, version 1.1, will include several features that make deploying easier in specific scenarios. The biggest change, which everyone will be able to use, is that Bundler no longer downloads the entire Rubygems source index before installing. Nick Quaranto has added a new API to rubygems.org that allows Bundler to download only the gem specs that it needs to install your Gemfile. This is a great improvement. In my tests, it cut down the time to install a small Gemfile from 30 seconds down to under 10 seconds. Another new feature that can help with deployments is &ndash;standalone mode. When you install your bundle with standalone mode enabled, Bundler is no longer needed at the system level. Your application becomes self-contained, and is able to load its installed gems itself. If you want to try out these features now, and help us finish testing them, you can get a prerelease of Bundler 1.1 today by running <code>gem install Bundler --pre</code>.</p>\n<p>So that&rsquo;s how to deploy your application and all its dependencies reliably and repeatably using Bundler.</p>\n",
				"content_text": "_I gave a talk at RailsConf 2011 about deploying with Bundler. These are my notes for the talk. I've also posted my [slides for the talk](/2011/06/11/deploying-with-bundler-slides). Combined, they probably serve as a somewhat reasonable replacement for having seen the talk itself._\n\nBundler exists to make booting your application consistent and repeatable, guaranteed. It does this by having you write a Gemfile listing of all the gems that your application needs to boot. When it installs those gems, it records the exact version of every gem into the Gemfile.lock file. Other developers are guaranteed to have the same gems you do. Your production servers are guaranteed to have the same gems you do.\n\nThis is a big deal.\n\nIt seems kind of easy and obvious now in retrospect, but in the bad old days, it could take anywhere from hours to days to get a new developer or a new server up and running your app with all the right versions of the gems that you needed. That entire process has been boiled down to running `bundle install` and waiting for a minute or two.\n\nSo! You start developing a beautiful new application. At some point, someone else starts working on it with you, and they only have to run `bundle install` and then they can start developing, too. Later, you realize that you want to deploy this application to a server in production so that the entire world can see it. Fantastic.\n\nNow that Ruby application servers all use Rack to communicate with your underlying application, Rails, Sinatra, and every other web framework can be treated identically for deployment. Just put the library you use into your Gemfile, and your app will have all the gems that it needs to run on the server.\n\nI'm going to cover the various deployment options that are available to apps that use Bundler, starting with the simplest and easiest, and expanding to include the more unusual and more complicated scenarios at the end.\n\nThe most drop-dead simple thing that you can possibly do to get your bundled application up and running on a server on the public internet is to have someone else do it. If you use Heroku, you can `git push heroku master`, and if you use Engine Yard, you can run `ey deploy`. In either case, their infrastructure will take care of everything. They check and see that you have a Gemfile, they run Bundler with settings tuned for their specific environments, and then they boot your application and you can start using it right away.\n\nThe next level of complexity is running your own server, installing Phusion Passenger, and having Passenger manage each of your app server instances. It's not that much more complicated, but this is the point at which you run into the first potential Bundler deployment issue: installing system gems requires root permissions.\n\nYou might be tempted to add `sudo bundle install` to your deployment script and call it a day, but that turns out to be a terrible idea. The crux of the problem is that `bundle install` saves configuration information that is specific to this application -- what groups of gems to ignore (like :development or :test), where the gems are installed to, all of those sorts of things. If you run `bundle install` as root, that file is created and owned by root. That means the Passenger app server (which runs as the nobody user by default) can't read or write to the bundle configuration.\n\nBundler contains an \"emergency workaround\" for this situation where Bundler will invoke sudo itself to install the gems. It's a hacky workaround at best, though, because entering your sudo password isn't scriptable. Even more awkwardly, many system administrators don't give regular users the ability to sudo at all. This means installing to system gems is ruled out completely. The much more effective solution is to install the application's gems into a path owned by the web server user. This is simple to do with the command `bundle install --path vendor/bundle`.\n\nOkay, so now we have our gems installed on the server into an application-specific path. There is one potential issue remaining: what happens if a developer changes the Gemfile, forgets to run `bundle install`, and then tries to deploy? The deploy script will install the bundle on the production server, but the server will be running against gems that have never been tested at all. You can avoid the potential disaster by using `bundle install --frozen`. Frozen mode means that Bundler checks the Gemfile against the Gemfile.lock file, and refuses to install if they don't match. The error message instructs you to run `bundle install` on your development machine, make sure everything works, and then check in a new Gemfile.lock with the versions that you are now sure work with your application.\n\nNow your application works in production, you can deploy without superuser permissions, and you will never run with gems in production that haven't been tested yet! Since this is the most common case, Bundler wraps all of this up into a single option that you can pass to the install command: `bundle install --deployment`.\n\nBut wait, you say, what if I use Unicorn or Rainbows or Mongrel or something like that? Well, it's pretty simple as well. Just add the server to your Gemfile. When you want to run your server, use e.g. `bundle exec unicorn`. This will make sure that the exact version in your Gemfile will be started, regardless of other versions on your system.\n\nThe final complication that we sometimes see while deploying to production environments is servers that have been heavily firewalled, and cannot connect to rubygems.org to download gems for installation at deploy-time. Sometimes it's possible to use a proxy server, but when even that isn't an option, Bundler has your back. Run the command `bundle pack` to download the .gem file for every gem that your application needs into the `vendor/cache` directory. When you run `bundle install` later, Bundler is smart enough to automatically look in `vendor/cache`. It will install your bundle using those gems as a source, and completely avoid contacting rubygems.org, allowing you to deploy into the most restrictive server setup.\n\nAt this point, you're probably thinking that this sounds like a lot to remember, and a lot of effort to get set up. Fortunately, Bundler even goes beyond providing the --deployment flag. It integrates directly with Capistrano and Vlad, the two most popular deployment systems. Adding Bundler to a working deploy setup is incredibly easy. With Capistrano, just add `require 'bundler/capistrano'` to your deploy.rb, and everything will work. With Vlad, just `require 'bundler/vlad'` in your deploy.rb, and make sure that your deploy task also runs the task `vlad:bundle:install`. That's it !If you need to adjust Bundler options, like the location the gems are installed to, or the list of groups that shouldn't be installed, you can. Just set some variables in your deploy script before Bundler runs. The details of how to configure everything are available at gembundler.com/deploying.\n\nThe next feature release of Bundler, version 1.1, will include several features that make deploying easier in specific scenarios. The biggest change, which everyone will be able to use, is that Bundler no longer downloads the entire Rubygems source index before installing. Nick Quaranto has added a new API to rubygems.org that allows Bundler to download only the gem specs that it needs to install your Gemfile. This is a great improvement. In my tests, it cut down the time to install a small Gemfile from 30 seconds down to under 10 seconds. Another new feature that can help with deployments is --standalone mode. When you install your bundle with standalone mode enabled, Bundler is no longer needed at the system level. Your application becomes self-contained, and is able to load its installed gems itself. If you want to try out these features now, and help us finish testing them, you can get a prerelease of Bundler 1.1 today by running `gem install Bundler --pre`.\n\nSo that's how to deploy your application and all its dependencies reliably and repeatably using Bundler.\n",
				"date_published": "2011-06-11T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/06/11/deploying-with-bundler-notes/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/06/11/deploying-with-bundler-slides/",
				"title": "Deploying With Bundler Slides",
				"content_html": "<p><em>I gave a talk at RailsConf 2011 about deploying with Bundler. These are slides for the talk. I&rsquo;ve also posted my <a href=\"/2011/06/11/deploying-with-bundler-notes\">notes for the talk</a>. Combined, they probably serve as a somewhat reasonable replacement for having seen the talk itself.</em></p>\n<img src=\"uploads/2025/013df41266.jpg\" alt=\"deploying.001\">\n<img src=\"uploads/2025/b8c91bb502.jpg\" alt=\"deploying.002\">\n<img src=\"uploads/2025/0a197ce2f7.jpg\" alt=\"deploying.003\">\n<img src=\"uploads/2025/18a14555b4.jpg\" alt=\"deploying.004\">\n<img src=\"uploads/2025/d374418ec8.jpg\" alt=\"deploying.005\">\n<img src=\"uploads/2025/00d01a3b32.jpg\" alt=\"deploying.006\">\n<img src=\"uploads/2025/0db0fd149e.jpg\" alt=\"deploying.007\">\n<img src=\"uploads/2025/567144b709.jpg\" alt=\"deploying.008\">\n<img src=\"uploads/2025/37feb3d6ac.jpg\" alt=\"deploying.009\">\n<img src=\"uploads/2025/8416b1d6fb.jpg\" alt=\"deploying.010\">\n<img src=\"uploads/2025/6ca270fc93.jpg\" alt=\"deploying.011\">\n<img src=\"uploads/2025/ae1dcc051a.jpg\" alt=\"deploying.012\">\n<img src=\"uploads/2025/b8cd4b5159.jpg\" alt=\"deploying.013\">\n<img src=\"uploads/2025/3630f0df2a.jpg\" alt=\"deploying.014\">\n<img src=\"uploads/2025/4da9d5f47c.jpg\" alt=\"deploying.015\">\n<img src=\"uploads/2025/2b9f5da61a.jpg\" alt=\"deploying.016\">\n<img src=\"uploads/2025/bdb27ffa84.jpg\" alt=\"deploying.017\">\n<img src=\"uploads/2025/d57cf8b0f5.jpg\" alt=\"deploying.018\">\n<img src=\"uploads/2025/17ad901338.jpg\" alt=\"deploying.019\">\n<img src=\"uploads/2025/4d45fa1ff5.jpg\" alt=\"deploying.020\">\n<img src=\"uploads/2025/2482ae32f3.jpg\" alt=\"deploying.021\">\n<img src=\"uploads/2025/786a7bc042.jpg\" alt=\"deploying.022\">\n<img src=\"uploads/2025/56917d6124.jpg\" alt=\"deploying.023\">\n<img src=\"uploads/2025/96462ca1f9.jpg\" alt=\"deploying.024\">\n<img src=\"uploads/2025/671d1e9675.jpg\" alt=\"deploying.025\">\n<img src=\"uploads/2025/48c88484da.jpg\" alt=\"deploying.026\">\n<img src=\"uploads/2025/2b4338b9c9.jpg\" alt=\"deploying.027\">\n<img src=\"uploads/2025/cec0d3db1c.jpg\" alt=\"deploying.028\">\n<img src=\"uploads/2025/7560bdf8f4.jpg\" alt=\"deploying.029\">\n<img src=\"uploads/2025/f1dff125b3.jpg\" alt=\"deploying.030\">\n<img src=\"uploads/2025/cc49ba1ea2.jpg\" alt=\"deploying.031\">\n<img src=\"uploads/2025/c5151b7bcd.jpg\" alt=\"deploying.032\">\n<img src=\"uploads/2025/fb4adc0e4d.jpg\" alt=\"deploying.033\">\n<img src=\"uploads/2025/c353023107.jpg\" alt=\"deploying.034\">\n<img src=\"uploads/2025/4ebfd880df.jpg\" alt=\"deploying.035\">\n<img src=\"uploads/2025/0b6809c330.jpg\" alt=\"deploying.036\">\n<img src=\"uploads/2025/79f21fd0f8.jpg\" alt=\"deploying.037\">\n<img src=\"uploads/2025/dbb9676e3e.jpg\" alt=\"deploying.038\">\n<img src=\"uploads/2025/058fa4bbc0.jpg\" alt=\"deploying.039\">\n<img src=\"uploads/2025/5d898abe9a.jpg\" alt=\"deploying.040\">\n<img src=\"uploads/2025/916e1e5a3c.jpg\" alt=\"deploying.041\">\n<img src=\"uploads/2025/de8ad7ec2e.jpg\" alt=\"deploying.042\">\n<img src=\"uploads/2025/d099d65b4a.jpg\" alt=\"deploying.043\">\n<img src=\"uploads/2025/757cdf9c75.jpg\" alt=\"deploying.044\">\n<img src=\"uploads/2025/52322fb721.jpg\" alt=\"deploying.045\">\n<img src=\"uploads/2025/79bea0c3f2.jpg\" alt=\"deploying.046\">\n<img src=\"uploads/2025/7c7cc6a03f.jpg\" alt=\"deploying.047\">\n<img src=\"uploads/2025/67b81597da.jpg\" alt=\"deploying.048\">\n<img src=\"uploads/2025/726047d392.jpg\" alt=\"deploying.049\">\n<img src=\"uploads/2025/6cdcbad24d.jpg\" alt=\"deploying.050\">\n<img src=\"uploads/2025/1a4e718108.jpg\" alt=\"deploying.051\">\n<img src=\"uploads/2025/8f1717b152.jpg\" alt=\"deploying.052\">\n<img src=\"uploads/2025/8f8893f1e1.jpg\" alt=\"deploying.053\">\n<img src=\"uploads/2025/2aa529a8d2.jpg\" alt=\"deploying.054\">\n<img src=\"uploads/2025/5b98ab21f9.jpg\" alt=\"deploying.055\">\n<img src=\"uploads/2025/dc84aac937.jpg\" alt=\"deploying.056\">\n<img src=\"uploads/2025/fd8dbb8387.jpg\" alt=\"deploying.057\">\n<img src=\"uploads/2025/93361ffb27.jpg\" alt=\"deploying.058\">\n<img src=\"uploads/2025/b7b8b72f15.jpg\" alt=\"deploying.059\">\n<img src=\"uploads/2025/762451126f.jpg\" alt=\"deploying.060\">\n<img src=\"uploads/2025/d1332ac564.jpg\" alt=\"deploying.061\">\n<img src=\"uploads/2025/0ec812b1ce.jpg\" alt=\"deploying.062\">\n<img src=\"uploads/2025/43bdccdb8d.jpg\" alt=\"deploying.063\">\n<img src=\"uploads/2025/ebdbe976b2.jpg\" alt=\"deploying.064\">\n<img src=\"uploads/2025/984fad8bff.jpg\" alt=\"deploying.065\">\n<img src=\"uploads/2025/842b1dca55.jpg\" alt=\"deploying.066\">\n<img src=\"uploads/2025/6256a2e8b1.jpg\" alt=\"deploying.067\">\n<img src=\"uploads/2025/b7ab3aa7c6.jpg\" alt=\"deploying.068\">\n<img src=\"uploads/2025/04abb9306b.jpg\" alt=\"deploying.069\">\n<img src=\"uploads/2025/cfe692b0d5.jpg\" alt=\"deploying.070\">\n<img src=\"uploads/2025/aa96bfd835.jpg\" alt=\"deploying.071\">\n<img src=\"uploads/2025/25a49a56cc.jpg\" alt=\"deploying.072\">\n<img src=\"uploads/2025/74409c317f.jpg\" alt=\"deploying.073\">\n<img src=\"uploads/2025/e6b9935327.jpg\" alt=\"deploying.074\">\n<img src=\"uploads/2025/a95c10c471.jpg\" alt=\"deploying.075\">\n<img src=\"uploads/2025/3e92368042.jpg\" alt=\"deploying.076\">\n<img src=\"uploads/2025/7f9a24a992.jpg\" alt=\"deploying.077\">\n<img src=\"uploads/2025/f8b15a16f2.jpg\" alt=\"deploying.078\">\n<img src=\"uploads/2025/d2b46711e8.jpg\" alt=\"deploying.079\">\n<img src=\"uploads/2025/91f03753b3.jpg\" alt=\"deploying.080\">\n<img src=\"uploads/2025/a359ab79d0.jpg\" alt=\"deploying.081\">\n<img src=\"uploads/2025/dc6ad16557.jpg\" alt=\"deploying.082\">\n<img src=\"uploads/2025/3ab1d57b59.jpg\" alt=\"deploying.083\">\n<img src=\"uploads/2025/df7e44d96f.jpg\" alt=\"deploying.084\">\n<img src=\"uploads/2025/4d5584245f.jpg\" alt=\"deploying.085\">\n<img src=\"uploads/2025/18508c9bf5.jpg\" alt=\"deploying.086\">\n<img src=\"uploads/2025/0e43701c74.jpg\" alt=\"deploying.087\">\n<img src=\"uploads/2025/0b2c5d3c82.jpg\" alt=\"deploying.088\">\n<img src=\"uploads/2025/956c7bc5cf.jpg\" alt=\"deploying.089\">\n<img src=\"uploads/2025/f2b12e9293.jpg\" alt=\"deploying.090\">\n<img src=\"uploads/2025/748c6bd275.jpg\" alt=\"deploying.091\">\n<img src=\"uploads/2025/dc8a19d1ca.jpg\" alt=\"deploying.092\">\n<img src=\"uploads/2025/78be4d89c7.jpg\" alt=\"deploying.093\">\n",
				"content_text": "_I gave a talk at RailsConf 2011 about deploying with Bundler. These are slides for the talk. I've also posted my [notes for the talk](/2011/06/11/deploying-with-bundler-notes). Combined, they probably serve as a somewhat reasonable replacement for having seen the talk itself._\n\n<img src=\"uploads/2025/013df41266.jpg\" alt=\"deploying.001\">\n<img src=\"uploads/2025/b8c91bb502.jpg\" alt=\"deploying.002\">\n<img src=\"uploads/2025/0a197ce2f7.jpg\" alt=\"deploying.003\">\n<img src=\"uploads/2025/18a14555b4.jpg\" alt=\"deploying.004\">\n<img src=\"uploads/2025/d374418ec8.jpg\" alt=\"deploying.005\">\n<img src=\"uploads/2025/00d01a3b32.jpg\" alt=\"deploying.006\">\n<img src=\"uploads/2025/0db0fd149e.jpg\" alt=\"deploying.007\">\n<img src=\"uploads/2025/567144b709.jpg\" alt=\"deploying.008\">\n<img src=\"uploads/2025/37feb3d6ac.jpg\" alt=\"deploying.009\">\n<img src=\"uploads/2025/8416b1d6fb.jpg\" alt=\"deploying.010\">\n<img src=\"uploads/2025/6ca270fc93.jpg\" alt=\"deploying.011\">\n<img src=\"uploads/2025/ae1dcc051a.jpg\" alt=\"deploying.012\">\n<img src=\"uploads/2025/b8cd4b5159.jpg\" alt=\"deploying.013\">\n<img src=\"uploads/2025/3630f0df2a.jpg\" alt=\"deploying.014\">\n<img src=\"uploads/2025/4da9d5f47c.jpg\" alt=\"deploying.015\">\n<img src=\"uploads/2025/2b9f5da61a.jpg\" alt=\"deploying.016\">\n<img src=\"uploads/2025/bdb27ffa84.jpg\" alt=\"deploying.017\">\n<img src=\"uploads/2025/d57cf8b0f5.jpg\" alt=\"deploying.018\">\n<img src=\"uploads/2025/17ad901338.jpg\" alt=\"deploying.019\">\n<img src=\"uploads/2025/4d45fa1ff5.jpg\" alt=\"deploying.020\">\n<img src=\"uploads/2025/2482ae32f3.jpg\" alt=\"deploying.021\">\n<img src=\"uploads/2025/786a7bc042.jpg\" alt=\"deploying.022\">\n<img src=\"uploads/2025/56917d6124.jpg\" alt=\"deploying.023\">\n<img src=\"uploads/2025/96462ca1f9.jpg\" alt=\"deploying.024\">\n<img src=\"uploads/2025/671d1e9675.jpg\" alt=\"deploying.025\">\n<img src=\"uploads/2025/48c88484da.jpg\" alt=\"deploying.026\">\n<img src=\"uploads/2025/2b4338b9c9.jpg\" alt=\"deploying.027\">\n<img src=\"uploads/2025/cec0d3db1c.jpg\" alt=\"deploying.028\">\n<img src=\"uploads/2025/7560bdf8f4.jpg\" alt=\"deploying.029\">\n<img src=\"uploads/2025/f1dff125b3.jpg\" alt=\"deploying.030\">\n<img src=\"uploads/2025/cc49ba1ea2.jpg\" alt=\"deploying.031\">\n<img src=\"uploads/2025/c5151b7bcd.jpg\" alt=\"deploying.032\">\n<img src=\"uploads/2025/fb4adc0e4d.jpg\" alt=\"deploying.033\">\n<img src=\"uploads/2025/c353023107.jpg\" alt=\"deploying.034\">\n<img src=\"uploads/2025/4ebfd880df.jpg\" alt=\"deploying.035\">\n<img src=\"uploads/2025/0b6809c330.jpg\" alt=\"deploying.036\">\n<img src=\"uploads/2025/79f21fd0f8.jpg\" alt=\"deploying.037\">\n<img src=\"uploads/2025/dbb9676e3e.jpg\" alt=\"deploying.038\">\n<img src=\"uploads/2025/058fa4bbc0.jpg\" alt=\"deploying.039\">\n<img src=\"uploads/2025/5d898abe9a.jpg\" alt=\"deploying.040\">\n<img src=\"uploads/2025/916e1e5a3c.jpg\" alt=\"deploying.041\">\n<img src=\"uploads/2025/de8ad7ec2e.jpg\" alt=\"deploying.042\">\n<img src=\"uploads/2025/d099d65b4a.jpg\" alt=\"deploying.043\">\n<img src=\"uploads/2025/757cdf9c75.jpg\" alt=\"deploying.044\">\n<img src=\"uploads/2025/52322fb721.jpg\" alt=\"deploying.045\">\n<img src=\"uploads/2025/79bea0c3f2.jpg\" alt=\"deploying.046\">\n<img src=\"uploads/2025/7c7cc6a03f.jpg\" alt=\"deploying.047\">\n<img src=\"uploads/2025/67b81597da.jpg\" alt=\"deploying.048\">\n<img src=\"uploads/2025/726047d392.jpg\" alt=\"deploying.049\">\n<img src=\"uploads/2025/6cdcbad24d.jpg\" alt=\"deploying.050\">\n<img src=\"uploads/2025/1a4e718108.jpg\" alt=\"deploying.051\">\n<img src=\"uploads/2025/8f1717b152.jpg\" alt=\"deploying.052\">\n<img src=\"uploads/2025/8f8893f1e1.jpg\" alt=\"deploying.053\">\n<img src=\"uploads/2025/2aa529a8d2.jpg\" alt=\"deploying.054\">\n<img src=\"uploads/2025/5b98ab21f9.jpg\" alt=\"deploying.055\">\n<img src=\"uploads/2025/dc84aac937.jpg\" alt=\"deploying.056\">\n<img src=\"uploads/2025/fd8dbb8387.jpg\" alt=\"deploying.057\">\n<img src=\"uploads/2025/93361ffb27.jpg\" alt=\"deploying.058\">\n<img src=\"uploads/2025/b7b8b72f15.jpg\" alt=\"deploying.059\">\n<img src=\"uploads/2025/762451126f.jpg\" alt=\"deploying.060\">\n<img src=\"uploads/2025/d1332ac564.jpg\" alt=\"deploying.061\">\n<img src=\"uploads/2025/0ec812b1ce.jpg\" alt=\"deploying.062\">\n<img src=\"uploads/2025/43bdccdb8d.jpg\" alt=\"deploying.063\">\n<img src=\"uploads/2025/ebdbe976b2.jpg\" alt=\"deploying.064\">\n<img src=\"uploads/2025/984fad8bff.jpg\" alt=\"deploying.065\">\n<img src=\"uploads/2025/842b1dca55.jpg\" alt=\"deploying.066\">\n<img src=\"uploads/2025/6256a2e8b1.jpg\" alt=\"deploying.067\">\n<img src=\"uploads/2025/b7ab3aa7c6.jpg\" alt=\"deploying.068\">\n<img src=\"uploads/2025/04abb9306b.jpg\" alt=\"deploying.069\">\n<img src=\"uploads/2025/cfe692b0d5.jpg\" alt=\"deploying.070\">\n<img src=\"uploads/2025/aa96bfd835.jpg\" alt=\"deploying.071\">\n<img src=\"uploads/2025/25a49a56cc.jpg\" alt=\"deploying.072\">\n<img src=\"uploads/2025/74409c317f.jpg\" alt=\"deploying.073\">\n<img src=\"uploads/2025/e6b9935327.jpg\" alt=\"deploying.074\">\n<img src=\"uploads/2025/a95c10c471.jpg\" alt=\"deploying.075\">\n<img src=\"uploads/2025/3e92368042.jpg\" alt=\"deploying.076\">\n<img src=\"uploads/2025/7f9a24a992.jpg\" alt=\"deploying.077\">\n<img src=\"uploads/2025/f8b15a16f2.jpg\" alt=\"deploying.078\">\n<img src=\"uploads/2025/d2b46711e8.jpg\" alt=\"deploying.079\">\n<img src=\"uploads/2025/91f03753b3.jpg\" alt=\"deploying.080\">\n<img src=\"uploads/2025/a359ab79d0.jpg\" alt=\"deploying.081\">\n<img src=\"uploads/2025/dc6ad16557.jpg\" alt=\"deploying.082\">\n<img src=\"uploads/2025/3ab1d57b59.jpg\" alt=\"deploying.083\">\n<img src=\"uploads/2025/df7e44d96f.jpg\" alt=\"deploying.084\">\n<img src=\"uploads/2025/4d5584245f.jpg\" alt=\"deploying.085\">\n<img src=\"uploads/2025/18508c9bf5.jpg\" alt=\"deploying.086\">\n<img src=\"uploads/2025/0e43701c74.jpg\" alt=\"deploying.087\">\n<img src=\"uploads/2025/0b2c5d3c82.jpg\" alt=\"deploying.088\">\n<img src=\"uploads/2025/956c7bc5cf.jpg\" alt=\"deploying.089\">\n<img src=\"uploads/2025/f2b12e9293.jpg\" alt=\"deploying.090\">\n<img src=\"uploads/2025/748c6bd275.jpg\" alt=\"deploying.091\">\n<img src=\"uploads/2025/dc8a19d1ca.jpg\" alt=\"deploying.092\">\n<img src=\"uploads/2025/78be4d89c7.jpg\" alt=\"deploying.093\">\n",
				"date_published": "2011-06-11T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/06/11/deploying-with-bundler-slides/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/05/27/undefined-method-merge-for-nilnilclass/",
				"title": "undefined method `merge' for nil:NilClass",
				"content_html": "<h3 id=\"tldr\">tl;dr</h3>\n<p>If you are using Nginx and Passenger to serve a Rails app, your filesystem is full. Delete some stuff.</p>\n<h3 id=\"the-story\">The story</h3>\n<p>In what is definitely the craziest error that I have run into recently, my staging site suddenly started returning 500 errors to some requests. It even took me a while to figure that the app was erroring because I wasn&rsquo;t getting any exceptions reported by Hoptoad.</p>\n<p>When I finally looked in the Rails log myself, this is all that was there:</p>\n<pre><code>Started POST &quot;/users/sign_in&quot; for 70.36.143.76 at Fri May 27 17:03:22 -0500 2011\n\nNoMethodError (undefined method `merge' for nil:NilClass):\n</code></pre>\n<p>That&rsquo;s… not exactly helpful. I kept digging, and eventually figured out that the error only occurred on POST requests. Worse, everything was fine if I booted up a server using <code>rails server -e staging</code>. For that process, POST requests worked and everything was fine.</p>\n<p>Hoping it was something transient, I restarted Nginx, only to discover this (even scarier) error:</p>\n<pre><code>[alert]: Unable to start the Phusion Passenger watchdog: it seems to have crashed during startup for an unknown reason, with exit code 1 (-1: Unknown error 18446744073709551615)\n</code></pre>\n<p>While that may be the longest error number that I have ever seen, it still didn&rsquo;t give me any idea what was going on.</p>\n<p>Finally, while trying to boot passenger by itself using <code>passenger start</code>, I saw this error:</p>\n<pre><code>/usr/local/lib/ruby/1.8/fileutils.rb:243:in `mkdir': Too many links - /tmp/root-passenger-standalone-30121 (Errno::EMLINK)\n</code></pre>\n<p>Too many links! That&rsquo;s when it finally occurred to me that the filesystem might be out of handles, and suddenly that made sense out of my vague recollection that Nginx writes POST bodies to files to pass them to the app server.</p>\n<p>Sure enough, /tmp was full of stuff:</p>\n<pre><code>$ ls /tmp | wc -l\n31998\n</code></pre>\n<p>Most of those weren&rsquo;t used at all, so I started deleting them. With that, everything started working again. Magic. :P</p>\n",
				"content_text": "\n### tl;dr\n\nIf you are using Nginx and Passenger to serve a Rails app, your filesystem is full. Delete some stuff.\n\n### The story\n\nIn what is definitely the craziest error that I have run into recently, my staging site suddenly started returning 500 errors to some requests. It even took me a while to figure that the app was erroring because I wasn't getting any exceptions reported by Hoptoad.\n\nWhen I finally looked in the Rails log myself, this is all that was there:\n\n    Started POST \"/users/sign_in\" for 70.36.143.76 at Fri May 27 17:03:22 -0500 2011\n\n    NoMethodError (undefined method `merge' for nil:NilClass):\n\nThat's… not exactly helpful. I kept digging, and eventually figured out that the error only occurred on POST requests. Worse, everything was fine if I booted up a server using `rails server -e staging`. For that process, POST requests worked and everything was fine.\n\nHoping it was something transient, I restarted Nginx, only to discover this (even scarier) error:\n\n    [alert]: Unable to start the Phusion Passenger watchdog: it seems to have crashed during startup for an unknown reason, with exit code 1 (-1: Unknown error 18446744073709551615)\n\nWhile that may be the longest error number that I have ever seen, it still didn't give me any idea what was going on.\n\nFinally, while trying to boot passenger by itself using `passenger start`, I saw this error:\n\n    /usr/local/lib/ruby/1.8/fileutils.rb:243:in `mkdir': Too many links - /tmp/root-passenger-standalone-30121 (Errno::EMLINK)\n\nToo many links! That's when it finally occurred to me that the filesystem might be out of handles, and suddenly that made sense out of my vague recollection that Nginx writes POST bodies to files to pass them to the app server.\n\nSure enough, /tmp was full of stuff:\n\n    $ ls /tmp | wc -l\n    31998\n\nMost of those weren't used at all, so I started deleting them. With that, everything started working again. Magic. :P\n",
				"date_published": "2011-05-27T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/05/27/undefined-method-merge-for-nilnilclass/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2011/05/26/add-true-readline-to-ruby/",
				"title": "Add true Readline to Ruby on OS X",
				"content_html": "<p>I normally use OS X system Ruby. As of OS X 10.6.4, that is Ruby 1.8.7-p174. Sadly, system Ruby links against libedit instead of libreadline, which means I can&rsquo;t use any of my nice readline setup.</p>\n<p>To fix that, you can install readline <a href=\"http://henrik.nyh.se/2008/03/irb-readline\">with MacPorts</a> or <a href=\"http://www.jorgebernal.info/development/fixing-snow-leopard-ruby-readline\">by hand</a> and then compile a new <code>readline.bundle</code> for Ruby to use. But I use <a href=\"https://github.com/mxcl/homebrew\">Homebrew</a> to manage my unix packages, so those instructions weren&rsquo;t quite enough for me.</p>\n<p>Here&rsquo;s how to add true Readline compatibility to Ruby 1.8.7-p174 on OS X 10.6 using Homebrew.</p>\n<pre><code># Install readline\nbrew install readline\n\n# Download the readline extension\ncd /tmp\nsvn co http://svn.ruby-lang.org/repos/ruby/tags/v1_8_7_174/ext/readline/\n\n# Compile the bundle against homebrewed readline\nmake readline.o CFLAGS='-I/usr/local/Cellar/readline/6.1/include -DHAVE_RL_USERNAME_COMPLETION_FUNCTION'\ncc -arch i386 -arch x86_64 -pipe -bundle -undefined dynamic_lookup -o readline.bundle readline.o -L/usr/local/Cellar/readline/6.1/lib -L/System/Library/Frameworks/Ruby.framework/Versions/1.8/usr/lib -L. -arch i386 -arch x86_64 -lruby -lreadline -lncurses -lpthread -ldl\n\n# Move the bundle into the right place\ncd /Library/Ruby/Site/1.8/universal-darwin10.0/\nsudo mv readline.bundle readline.bundle.libedit\nsudo mv /tmp/readline/readline.bundle ./readline.bundle\n</code></pre>\n<p>Tada! Working Readline support in Ruby, including IRB.</p>\n",
				"content_text": "I normally use OS X system Ruby. As of OS X 10.6.4, that is Ruby 1.8.7-p174. Sadly, system Ruby links against libedit instead of libreadline, which means I can't use any of my nice readline setup.\n\nTo fix that, you can install readline [with MacPorts](http://henrik.nyh.se/2008/03/irb-readline) or [by hand](http://www.jorgebernal.info/development/fixing-snow-leopard-ruby-readline) and then compile a new `readline.bundle` for Ruby to use. But I use [Homebrew](https://github.com/mxcl/homebrew) to manage my unix packages, so those instructions weren't quite enough for me.\n\nHere's how to add true Readline compatibility to Ruby 1.8.7-p174 on OS X 10.6 using Homebrew.\n\n    # Install readline\n    brew install readline\n\n    # Download the readline extension\n    cd /tmp\n    svn co http://svn.ruby-lang.org/repos/ruby/tags/v1_8_7_174/ext/readline/\n\n    # Compile the bundle against homebrewed readline\n    make readline.o CFLAGS='-I/usr/local/Cellar/readline/6.1/include -DHAVE_RL_USERNAME_COMPLETION_FUNCTION'\n    cc -arch i386 -arch x86_64 -pipe -bundle -undefined dynamic_lookup -o readline.bundle readline.o -L/usr/local/Cellar/readline/6.1/lib -L/System/Library/Frameworks/Ruby.framework/Versions/1.8/usr/lib -L. -arch i386 -arch x86_64 -lruby -lreadline -lncurses -lpthread -ldl\n\n    # Move the bundle into the right place\n    cd /Library/Ruby/Site/1.8/universal-darwin10.0/\n    sudo mv readline.bundle readline.bundle.libedit\n    sudo mv /tmp/readline/readline.bundle ./readline.bundle\n\nTada! Working Readline support in Ruby, including IRB.\n",
				"date_published": "2011-05-26T00:00:00-08:00",
				"url": "https://andre.arko.net/2011/05/26/add-true-readline-to-ruby/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2010/10/15/extending-rails-with-railties/",
				"title": "Extending Rails 3 with Railties",
				"content_html": "<p>Rails 3.0 is finally released, and with it comes a fantastic new way to extend Rails: Railties. Railties are the basis of the core components of Rails 3, and are the result of months of careful refactoring by Carlhuda. It is easier to extend and expand Rails than it has ever been before, without an <code>alias_method_chain</code> in sight.</p>\n<p>Unfortunately, while the system for extending and expanding Rails has been completely overhauled, the documentation hasn&rsquo;t been updated yet. The <a href=\"http://guides.rubyonrails.org/plugins.html\">Rails Plugins Guide</a> only covers writing plugins in the old Rails 2 style. Ilya Grigorik wrote a <a href=\"http://www.igvita.com/2010/08/04/rails-3-internals-railtie-creating-plugins/\">Railtie &amp; Creating Plugins</a> blog post, but just scratched the surface of what is possible with a Railtie plugin. This post covers writing Railtie plugins, hooking into the Rails initialization process, packaging Railtie plugins as gems, and using gem plugins in a Rails 3 application.</p>\n<h2 id=\"creating-railtie-plugins\">Creating Railtie plugins</h2>\n<p>Creating a Railtie is easy. Just create a class that inherits from <a href=\"http://api.rubyonrails.org/classes/Rails/Railtie.html\"><code>::Rails::Railtie</code></a>. Every subclass of Railtie is used to initialize your Rails application. Since ActionController, ActionView, and the other Rails components are also Railties, your plugin can function as a first-class member of the Rails application. It will have access to the same methods and context that are used by the official Rails components. Here is a sample minimal Railtie that will be loaded when your Rails application boots.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#39;rails&#39;</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">class</span> <span style=\"color:#a6e22e\">MyCoolRailtie</span> <span style=\"color:#f92672\">&lt;</span> <span style=\"color:#66d9ef\">Rails</span><span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">Railtie</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#75715e\"># railtie code goes here</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span></span></span></code></pre></div>\n<p>The <a href=\"http://api.rubyonrails.org/classes/Rails/Railtie.html\">Railtie documentation</a> lists all of the methods that are available inside each Railtie class, but doesn&rsquo;t really go into depth about what you can use Railties to do. Here are some example Railties explaining how to use the Railtie methods (in alphabetical order) to customize and extend Rails.</p>\n<h3 id=\"console\"><code>console</code></h3>\n<p>The <code>console</code> method allows your Railtie to add code that will be run when a Rails console is started.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>console <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">Foo</span><span style=\"color:#f92672\">.</span>console_mode!\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span></span></span></code></pre></div>\n<h3 id=\"generators\"><code>generators</code></h3>\n<p>Rails will require any generators defined in <code>lib/generators/*.rb</code> automatically. If you ship <a href=\"http://api.rubyonrails.org/classes/Rails/Generators.html\">Rails::Generators</a> with your Railtie in some other directory, you can require them using this method.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>generators <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>  require <span style=\"color:#e6db74\">&#39;path/to/generator&#39;</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span></span></span></code></pre></div>\n<h3 id=\"rake_tasks\"><code>rake_tasks</code></h3>\n<p>If you ship rake tasks for apps with your Railtie, load them using this method.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>rake_tasks <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>  require <span style=\"color:#e6db74\">&#39;path/to/railtie.tasks&#39;</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span></span></span></code></pre></div>\n<h3 id=\"initializer\"><code>initializer</code></h3>\n<p>The <code>initializer</code> method provides Railties with a lot of power. They create initializers that will be run during the Rails boot process, like the files put into <code>config/initializers</code> in the app directory. The <code>initializer</code> method takes two options, <code>:after</code> or <code>:before</code>, if there are specific initializers that you want to run before or after yours.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>initializer <span style=\"color:#e6db74\">&#34;my_cool_railtie.boot_foo&#34;</span> <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">Foo</span><span style=\"color:#f92672\">.</span>boot(<span style=\"color:#66d9ef\">Bar</span>)\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>initializer <span style=\"color:#e6db74\">&#34;my_cool_railtie.boot_bar&#34;</span>,\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#e6db74\">:before</span> <span style=\"color:#f92672\">=&gt;</span> <span style=\"color:#e6db74\">&#34;my_cool_railtie.boot_foo&#34;</span> <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">Bar</span><span style=\"color:#f92672\">.</span>boot!\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span></span></span></code></pre></div>\n<h2 id=\"rails-configuration-hooks\">Rails configuration hooks</h2>\n<p>The biggest extension hook that Railties provide is somewhat unassuming: the <code>config</code> method. That method returns the instance of <a href=\"http://api.rubyonrails.org/classes/Rails/Railtie/Configuration.html\"><code>Railtie::Configuration</code></a> that belongs to the application being booted. This opens up all sorts of interesting possibilities, since the <code>config</code> object is the same one that is made available inside a Rails application&rsquo;s <code>environment.rb</code> file. Here are some annotated examples of using <code>config</code> to change how a Rails application is initialized and configured.</p>\n<h3 id=\"after_initialize\"><code>after_initialize</code></h3>\n<p>This method takes a block that will be run after Rails is is completely initialized, and all of the application&rsquo;s initializers have run.</p>\n<h3 id=\"app_middleware\"><code>app_middleware</code></h3>\n<p>This method exposes the <a href=\"http://api.rubyonrails.org/classes/ActionDispatch/MiddlewareStack.html\">MiddlewareStack</a> that will be used to handle requests to your Rails application. You can use any of the methods defined on MiddlewareStack, including <code>use</code> and <code>swap</code>, to manage the Rails application&rsquo;s Rack middlewares. For example, if your Railtie included the Rack middleware <code>MyRailtie::Middleware</code>, you could add it to the Rails application middleware stack like this.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>config<span style=\"color:#f92672\">.</span>middlewares<span style=\"color:#f92672\">.</span>use <span style=\"color:#66d9ef\">MyRailtie</span><span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">Middleware</span></span></span></code></pre></div>\n<h3 id=\"before_configuration\"><code>before_configuration</code></h3>\n<p>Code passed in a block to this method will be run after immediately before the application configuration block inside <code>application.rb</code> is run. This is usually the best place to set default options that users of your plugin should be able to override themselves, as in the <code>jquery-rails</code> example below.</p>\n<h3 id=\"before_eager_load\"><code>before_eager_load</code></h3>\n<p>The block passed to <code>before_eager_load</code> will be run before Rails requires the application’s classes. Eager load is never run in development mode. However, if you need to run code after Rails loads but before any application code loads, this is the place to put it.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>config<span style=\"color:#f92672\">.</span>before_eager_load <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">SomeClass</span><span style=\"color:#f92672\">.</span>set_important_value <span style=\"color:#f92672\">=</span> <span style=\"color:#e6db74\">&#34;RoboCop&#34;</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span></span></span></code></pre></div>\n<h3 id=\"before_initialize\"><code>before_initialize</code></h3>\n<p>This method takes a block to be run before the Rails initialization process happens &ndash; this is basically equivalent to creating an initializer, and setting it to run :before the first initializer the app has.</p>\n<h3 id=\"generators-1\"><code>generators</code></h3>\n<p>This object holds the configuration for the generators that are invoked when you run the <code>rails generate</code> command.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>config<span style=\"color:#f92672\">.</span>generators <span style=\"color:#66d9ef\">do</span> <span style=\"color:#f92672\">|</span>g<span style=\"color:#f92672\">|</span>\n</span></span><span style=\"display:flex;\"><span>  g<span style=\"color:#f92672\">.</span>orm             <span style=\"color:#e6db74\">:datamapper</span>, <span style=\"color:#e6db74\">:migration</span> <span style=\"color:#f92672\">=&gt;</span> <span style=\"color:#66d9ef\">true</span>\n</span></span><span style=\"display:flex;\"><span>  g<span style=\"color:#f92672\">.</span>template_engine <span style=\"color:#e6db74\">:haml</span>\n</span></span><span style=\"display:flex;\"><span>  g<span style=\"color:#f92672\">.</span>test_framework  <span style=\"color:#e6db74\">:rspec</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span></span></span></code></pre></div>\n<p>You can also use it to disable colorized logging in the console.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span>config<span style=\"color:#f92672\">.</span>generators<span style=\"color:#f92672\">.</span>colorize_logging <span style=\"color:#f92672\">=</span> <span style=\"color:#66d9ef\">false</span></span></span></code></pre></div>\n<h3 id=\"to_prepare\"><code>to_prepare</code></h3>\n<p>Last, but quite importantly, <code>to_prepare</code> allows you the chance to do one-time setup. The block you pass to this method will be run for every request in development mode, but only once in production. Use it when you need to set something up once before the app starts serving requests.</p>\n<h2 id=\"examples\">Examples</h2>\n<p>At this point, you&rsquo;re probably thinking &ldquo;why would I actually want to do any of that stuff?&rdquo;. So, here are a few select examples of Railtie plugins packaged as gems.</p>\n<h3 id=\"rspec-railsrspec-rails\"><a href=\"http://github.com/rspec/rspec-rails\">rspec-rails</a></h3>\n<p>The rspec-rails plugin ships with a set of rake tasks and generators that integrate the <a href=\"http://github.com/rspec/rspec\">RSpec</a> gem with Rails.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">module</span> RSpec\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">module</span> Rails\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">class</span> <span style=\"color:#a6e22e\">Railtie</span> <span style=\"color:#f92672\">&lt;</span> <span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">Rails</span><span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">Railtie</span>\n</span></span><span style=\"display:flex;\"><span>      config<span style=\"color:#f92672\">.</span>generators<span style=\"color:#f92672\">.</span>integration_tool <span style=\"color:#e6db74\">:rspec</span>\n</span></span><span style=\"display:flex;\"><span>      config<span style=\"color:#f92672\">.</span>generators<span style=\"color:#f92672\">.</span>test_framework   <span style=\"color:#e6db74\">:rspec</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>      rake_tasks <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>        load <span style=\"color:#e6db74\">&#34;rspec/rails/tasks/rspec.rake&#34;</span>\n</span></span><span style=\"display:flex;\"><span>      <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span></span></span></code></pre></div>\n<p>This Railtie just does three things: First, it sets the generators that will be used for integration tests via the <code>integration_tool</code> method. Next, it sets the generators that will be used to generate model, controller, and view tests (via the <code>test_framework</code> method). Last, it loads the RSpec rake tasks to run RSpec tests instead of test-unit tests.</p>\n<h3 id=\"jquery-railsjquery\"><a href=\"http://github.com/indirect/jquery-rails\">jquery-rails</a></h3>\n<p>The jquery-rails plugin ships with a generator that downloads and installs jQuery, the jquery-ujs script that enables Rails helpers with jQuery, and optionally installs jQueryUI as well.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">module</span> Jquery\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">module</span> Rails\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">class</span> <span style=\"color:#a6e22e\">Railtie</span> <span style=\"color:#f92672\">&lt;</span> <span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">Rails</span><span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">Railtie</span>\n</span></span><span style=\"display:flex;\"><span>      config<span style=\"color:#f92672\">.</span>before_configuration <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>        <span style=\"color:#66d9ef\">if</span> <span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">Rails</span><span style=\"color:#f92672\">.</span>root<span style=\"color:#f92672\">.</span>join(\n</span></span><span style=\"display:flex;\"><span>          <span style=\"color:#e6db74\">&#34;public/javascripts/jquery-ui.min.js&#34;</span>)<span style=\"color:#f92672\">.</span>exist?\n</span></span><span style=\"display:flex;\"><span>          config<span style=\"color:#f92672\">.</span>action_view<span style=\"color:#f92672\">.</span>javascript_expansions<span style=\"color:#f92672\">[</span><span style=\"color:#e6db74\">:defaults</span><span style=\"color:#f92672\">]</span> <span style=\"color:#f92672\">=</span>\n</span></span><span style=\"display:flex;\"><span>            <span style=\"color:#e6db74\">%w(jquery.min jquery-ui.min rails)</span>\n</span></span><span style=\"display:flex;\"><span>        <span style=\"color:#66d9ef\">else</span>\n</span></span><span style=\"display:flex;\"><span>          config<span style=\"color:#f92672\">.</span>action_view<span style=\"color:#f92672\">.</span>javascript_expansions<span style=\"color:#f92672\">[</span><span style=\"color:#e6db74\">:defaults</span><span style=\"color:#f92672\">]</span> <span style=\"color:#f92672\">=</span>\n</span></span><span style=\"display:flex;\"><span>            <span style=\"color:#e6db74\">%w(jquery.min rails)</span>\n</span></span><span style=\"display:flex;\"><span>        <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>      <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span></span></span></code></pre></div>\n<p>This Railtie only sets one setting, but checks for the jQueryUI library to determine the value to set. By using the <code>config.before_configuration</code> hook, it runs right before the <code>application.rb</code> config block runs. That means it has access to the Rails.root, which is needed to check for jQueryUI, and it means that users can still override <code>javascript_expansion[:defaults]</code> in their <code>application.rb</code> if they want something different than the new defaults that the plugin provides.</p>\n<h3 id=\"haml-railshaml\"><a href=\"http://github.com/indirect/haml-rails\">haml-rails</a></h3>\n<p>The haml-rails gem provides generators for views written in Haml instead of the default generated views that are written in ERB.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">module</span> Haml\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">module</span> Rails\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">class</span> <span style=\"color:#a6e22e\">Railtie</span> <span style=\"color:#f92672\">&lt;</span> <span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">Rails</span><span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">Railtie</span>\n</span></span><span style=\"display:flex;\"><span>      config<span style=\"color:#f92672\">.</span>generators<span style=\"color:#f92672\">.</span>template_engine <span style=\"color:#e6db74\">:haml</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>      config<span style=\"color:#f92672\">.</span>before_initialize <span style=\"color:#66d9ef\">do</span>\n</span></span><span style=\"display:flex;\"><span>        <span style=\"color:#66d9ef\">Haml</span><span style=\"color:#f92672\">.</span>init_rails(binding)\n</span></span><span style=\"display:flex;\"><span>        <span style=\"color:#66d9ef\">Haml</span><span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">Template</span><span style=\"color:#f92672\">.</span>options<span style=\"color:#f92672\">[</span><span style=\"color:#e6db74\">:format</span><span style=\"color:#f92672\">]</span> <span style=\"color:#f92672\">=</span> <span style=\"color:#e6db74\">:html5</span>\n</span></span><span style=\"display:flex;\"><span>      <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span></span></span></code></pre></div>\n<p>This Railtie simply changes the template engine that Rails invokes when you run <code>rails generate</code>, and then initializes Haml for Rails, and sets the Haml output format to HTML5.</p>\n<h2 id=\"packaging-up-gem-plugins\">Packaging up gem plugins</h2>\n<p>Railtie plugins are easy to turn into gem plugins for Rails. This makes them easy to distribute, manage, and upgrade. The first thing you need is a gem. If you don&rsquo;t have a gem yet, you can create a new gem easily using <a href=\"http://gembundler.com\">Bundler</a>. Just run <code>bundle gem my_new_gem</code> and Bundler will generate a skeleton gem and gemspec that follow gem best practices. Once you have a gem, just make sure that your Railtie subclass is defined when <code>lib/my_new_gem.rb</code> is loaded. You can define the Railtie in a separate file and require that file, or define it directly. Last, add a dependency on the Rails gem (~&gt;3.0) to your gemspec.</p>\n<p>If your gem is also a plain Ruby library, and you don&rsquo;t want to depend on the Rails gem, then you can put your Railtie in a separate file, and conditionally require that file inside your main library file.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># lib/my_new_gem/my_cool_railtie.rb</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">module</span> MyNewGem\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">class</span> <span style=\"color:#a6e22e\">MyCoolRailtie</span> <span style=\"color:#f92672\">&lt;</span> <span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">Rails</span><span style=\"color:#f92672\">::</span><span style=\"color:#66d9ef\">Railtie</span>\n</span></span><span style=\"display:flex;\"><span>    <span style=\"color:#75715e\"># Railtie code here</span>\n</span></span><span style=\"display:flex;\"><span>  <span style=\"color:#66d9ef\">end</span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">end</span></span></span></code></pre></div>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># lib/my_new_gem.rb</span>\n</span></span><span style=\"display:flex;\"><span>require <span style=\"color:#e6db74\">&#39;lib/my_cool_railtie.rb&#39;</span> <span style=\"color:#66d9ef\">if</span> defined?(<span style=\"color:#66d9ef\">Rails</span>)</span></span></code></pre></div>\n<p>This ensures that your gem can be loaded (without the Railtie) if it is loaded outside the context of a Rails application.</p>\n<p>Now that your gem has a Railtie, you can build it and release it to <a href=\"http://gemcutter.org\">Gemcutter</a>. Once your gem is on Gemcutter, using it with Rails 3 applications is extremely easy &ndash; just add the gem to your <code>Gemfile</code>. Bundler will download and install your gem when you run <code>bundle install</code>, Rails will load it, and the <code>Rails::Railtie</code> class takes care of the rest!</p>\n<p class=\"aside\">This post was originally written for, and posted to, the <a href=\"http://www.engineyard.com/blog/2010/extending-rails-3-with-railties/\">Engine Yard Blog</a>.</p>\n",
				"content_text": "\nRails 3.0 is finally released, and with it comes a fantastic new way to extend Rails: Railties. Railties are the basis of the core components of Rails 3, and are the result of months of careful refactoring by Carlhuda. It is easier to extend and expand Rails than it has ever been before, without an `alias_method_chain` in sight.\n\nUnfortunately, while the system for extending and expanding Rails has been completely overhauled, the documentation hasn't been updated yet. The [Rails Plugins Guide][plugins] only covers writing plugins in the old Rails 2 style. Ilya Grigorik wrote a [Railtie & Creating Plugins][ilya] blog post, but just scratched the surface of what is possible with a Railtie plugin. This post covers writing Railtie plugins, hooking into the Rails initialization process, packaging Railtie plugins as gems, and using gem plugins in a Rails 3 application.\n\n[plugins]: http://guides.rubyonrails.org/plugins.html\n[ilya]: http://www.igvita.com/2010/08/04/rails-3-internals-railtie-creating-plugins/\n\n\n## Creating Railtie plugins\n\nCreating a Railtie is easy. Just create a class that inherits from [`::Rails::Railtie`][railtie]. Every subclass of Railtie is used to initialize your Rails application. Since ActionController, ActionView, and the other Rails components are also Railties, your plugin can function as a first-class member of the Rails application. It will have access to the same methods and context that are used by the official Rails components. Here is a sample minimal Railtie that will be loaded when your Rails application boots.\n\n{{< highlight ruby >}}\nrequire 'rails'\nclass MyCoolRailtie < Rails::Railtie\n  # railtie code goes here\nend\n{{< / highlight >}}\n\nThe [Railtie documentation][railtie] lists all of the methods that are available inside each Railtie class, but doesn't really go into depth about what you can use Railties to do. Here are some example Railties explaining how to use the Railtie methods (in alphabetical order) to customize and extend Rails.\n\n[railtie]: http://api.rubyonrails.org/classes/Rails/Railtie.html\n\n### `console`\n\nThe `console` method allows your Railtie to add code that will be run when a Rails console is started.\n\n{{< highlight ruby >}}\nconsole do\n  Foo.console_mode!\nend\n{{< / highlight >}}\n\n### `generators`\n\nRails will require any generators defined in `lib/generators/*.rb` automatically. If you ship [Rails::Generators][generators] with your Railtie in some other directory, you can require them using this method.\n\n{{< highlight ruby >}}\ngenerators do\n  require 'path/to/generator'\nend\n{{< / highlight >}}\n\n[generators]: http://api.rubyonrails.org/classes/Rails/Generators.html\n\n### `rake_tasks`\n\nIf you ship rake tasks for apps with your Railtie, load them using this method.\n\n{{< highlight ruby >}}\nrake_tasks do\n  require 'path/to/railtie.tasks'\nend\n{{< / highlight >}}\n\n### `initializer`\n\nThe `initializer` method provides Railties with a lot of power. They create initializers that will be run during the Rails boot process, like the files put into `config/initializers` in the app directory. The `initializer` method takes two options, `:after` or `:before`, if there are specific initializers that you want to run before or after yours.\n\n{{< highlight ruby >}}\ninitializer \"my_cool_railtie.boot_foo\" do\n  Foo.boot(Bar)\nend\n\ninitializer \"my_cool_railtie.boot_bar\",\n  :before => \"my_cool_railtie.boot_foo\" do\n    Bar.boot!\nend\n{{< / highlight >}}\n\n\n## Rails configuration hooks\n\nThe biggest extension hook that Railties provide is somewhat unassuming: the `config` method. That method returns the instance of [`Railtie::Configuration`][config] that belongs to the application being booted. This opens up all sorts of interesting possibilities, since the `config` object is the same one that is made available inside a Rails application's `environment.rb` file. Here are some annotated examples of using `config` to change how a Rails application is initialized and configured.\n\n[config]: http://api.rubyonrails.org/classes/Rails/Railtie/Configuration.html\n\n### `after_initialize`\n\nThis method takes a block that will be run after Rails is is completely initialized, and all of the application's initializers have run.\n\n### `app_middleware`\n\nThis method exposes the [MiddlewareStack][middleware] that will be used to handle requests to your Rails application. You can use any of the methods defined on MiddlewareStack, including `use` and `swap`, to manage the Rails application's Rack middlewares. For example, if your Railtie included the Rack middleware `MyRailtie::Middleware`, you could add it to the Rails application middleware stack like this.\n\n{{< highlight ruby >}}\nconfig.middlewares.use MyRailtie::Middleware\n{{< / highlight >}}\n\n[middleware]: http://api.rubyonrails.org/classes/ActionDispatch/MiddlewareStack.html\n\n### `before_configuration`\n\nCode passed in a block to this method will be run after immediately before the application configuration block inside `application.rb` is run. This is usually the best place to set default options that users of your plugin should be able to override themselves, as in the `jquery-rails` example below.\n\n### `before_eager_load`\n\nThe block passed to `before_eager_load` will be run before Rails requires the application’s classes. Eager load is never run in development mode. However, if you need to run code after Rails loads but before any application code loads, this is the place to put it.\n\n{{< highlight ruby >}}\nconfig.before_eager_load do\n  SomeClass.set_important_value = \"RoboCop\"\nend\n{{< / highlight >}}\n\n### `before_initialize`\n\nThis method takes a block to be run before the Rails initialization process happens -- this is basically equivalent to creating an initializer, and setting it to run :before the first initializer the app has.\n\n### `generators`\n\nThis object holds the configuration for the generators that are invoked when you run the `rails generate` command.\n\n{{< highlight ruby >}}\nconfig.generators do |g|\n  g.orm             :datamapper, :migration => true\n  g.template_engine :haml\n  g.test_framework  :rspec\nend\n{{< / highlight >}}\n\nYou can also use it to disable colorized logging in the console.\n\n{{< highlight ruby >}}\nconfig.generators.colorize_logging = false\n{{< / highlight >}}\n\n### `to_prepare`\n\nLast, but quite importantly, `to_prepare` allows you the chance to do one-time setup. The block you pass to this method will be run for every request in development mode, but only once in production. Use it when you need to set something up once before the app starts serving requests.\n\n\n## Examples\n\nAt this point, you're probably thinking \"why would I actually want to do any of that stuff?\". So, here are a few select examples of Railtie plugins packaged as gems.\n\n### [rspec-rails][rspec-rails]\n\nThe rspec-rails plugin ships with a set of rake tasks and generators that integrate the [RSpec][rspec] gem with Rails.\n\n{{< highlight ruby >}}\nmodule RSpec\n  module Rails\n    class Railtie < ::Rails::Railtie\n      config.generators.integration_tool :rspec\n      config.generators.test_framework   :rspec\n\n      rake_tasks do\n        load \"rspec/rails/tasks/rspec.rake\"\n      end\n    end\n  end\nend\n{{< / highlight >}}\n\nThis Railtie just does three things: First, it sets the generators that will be used for integration tests via the `integration_tool` method. Next, it sets the generators that will be used to generate model, controller, and view tests (via the `test_framework` method). Last, it loads the RSpec rake tasks to run RSpec tests instead of test-unit tests.\n\n[rspec-rails]: http://github.com/rspec/rspec-rails\n[rspec]: http://github.com/rspec/rspec\n\n### [jquery-rails][jquery]\n\nThe jquery-rails plugin ships with a generator that downloads and installs jQuery, the jquery-ujs script that enables Rails helpers with jQuery, and optionally installs jQueryUI as well.\n\n{{< highlight ruby >}}\nmodule Jquery\n  module Rails\n    class Railtie < ::Rails::Railtie\n      config.before_configuration do\n        if ::Rails.root.join(\n          \"public/javascripts/jquery-ui.min.js\").exist?\n          config.action_view.javascript_expansions[:defaults] =\n            %w(jquery.min jquery-ui.min rails)\n        else\n          config.action_view.javascript_expansions[:defaults] =\n            %w(jquery.min rails)\n        end\n      end\n    end\n  end\nend\n{{< / highlight >}}\n\nThis Railtie only sets one setting, but checks for the jQueryUI library to determine the value to set. By using the `config.before_configuration` hook, it runs right before the `application.rb` config block runs. That means it has access to the Rails.root, which is needed to check for jQueryUI, and it means that users can still override `javascript_expansion[:defaults]` in their `application.rb` if they want something different than the new defaults that the plugin provides.\n\n[jquery]: http://github.com/indirect/jquery-rails\n\n### [haml-rails][haml]\n\nThe haml-rails gem provides generators for views written in Haml instead of the default generated views that are written in ERB.\n\n{{< highlight ruby >}}\nmodule Haml\n  module Rails\n    class Railtie < ::Rails::Railtie\n      config.generators.template_engine :haml\n\n      config.before_initialize do\n        Haml.init_rails(binding)\n        Haml::Template.options[:format] = :html5\n      end\n    end\n  end\nend\n{{< / highlight >}}\n\nThis Railtie simply changes the template engine that Rails invokes when you run `rails generate`, and then initializes Haml for Rails, and sets the Haml output format to HTML5.\n\n[haml]: http://github.com/indirect/haml-rails\n\n\n## Packaging up gem plugins\n\nRailtie plugins are easy to turn into gem plugins for Rails. This makes them easy to distribute, manage, and upgrade. The first thing you need is a gem. If you don't have a gem yet, you can create a new gem easily using [Bundler][bundler]. Just run `bundle gem my_new_gem` and Bundler will generate a skeleton gem and gemspec that follow gem best practices. Once you have a gem, just make sure that your Railtie subclass is defined when `lib/my_new_gem.rb` is loaded. You can define the Railtie in a separate file and require that file, or define it directly. Last, add a dependency on the Rails gem (~>3.0) to your gemspec.\n\n[bundler]: http://gembundler.com\n\nIf your gem is also a plain Ruby library, and you don't want to depend on the Rails gem, then you can put your Railtie in a separate file, and conditionally require that file inside your main library file.\n\n{{< highlight ruby >}}\n# lib/my_new_gem/my_cool_railtie.rb\nmodule MyNewGem\n  class MyCoolRailtie < ::Rails::Railtie\n    # Railtie code here\n  end\nend\n{{< / highlight >}}\n\n{{< highlight ruby >}}\n# lib/my_new_gem.rb\nrequire 'lib/my_cool_railtie.rb' if defined?(Rails)\n{{< / highlight >}}\n\nThis ensures that your gem can be loaded (without the Railtie) if it is loaded outside the context of a Rails application.\n\nNow that your gem has a Railtie, you can build it and release it to [Gemcutter][gemcutter]. Once your gem is on Gemcutter, using it with Rails 3 applications is extremely easy -- just add the gem to your `Gemfile`. Bundler will download and install your gem when you run `bundle install`, Rails will load it, and the `Rails::Railtie` class takes care of the rest!\n\n[gemcutter]: http://gemcutter.org\n\n<p class=\"aside\">This post was originally written for, and posted to, the <a href=\"http://www.engineyard.com/blog/2010/extending-rails-3-with-railties/\">Engine Yard Blog</a>.</p>\n",
				"date_published": "2010-10-15T00:00:00-08:00",
				"url": "https://andre.arko.net/2010/10/15/extending-rails-with-railties/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2010/09/06/jekyll-postfiles-plugin/",
				"title": "Jekyll postfiles plugin",
				"content_html": "<p>Through the years, I&rsquo;ve used a lot of blogging engines. Nowadays, I use <a href=\"http://jekyllrb.com/\">Jekyll</a> to generate a static version of my blog. Jekyll has had one annoying shortcoming for me, though &ndash; it doesn&rsquo;t support including files in a blog post. This has bitten me more than once, especially as I move between blogging engines (or just try to clean up random files on my web server).</p>\n<p>About a year ago, I decided to try to fix that. So, I forked Jekyll and patched it to allow files associated with posts in a <code>_postfiles</code> directory. That worked for a while (although I had to install my fork onto my server, which was a little bit annoying). This week, I realized that my RSS feed had broken images because I was using relative paths to reference postfiles. To solve that, I decided to write a <code>&amp;#123;&amp;#123; '{% postfile foo ' }}%}</code> liquid tag that expanded file references out to absolute paths.</p>\n<p>While working on the liquid tag, I discovered that Jekyll 0.7 supports plugins, and that I could stop forking Jekyll and just write a plugin instead. So I did. Presenting <a href=\"http://github.com/indirect/jekyll-postfiles\">jekyll-postfiles</a>, a plugin for Jekyll that (optionally) adds files to each post. Just create a folder named <code>_postfiles</code>, next to the folder named <code>_posts</code>. When you have a file that you want to include in a post, create a folder with the same name as the post, and put the file in there, with a directory structure like this:</p>\n<pre><code>_posts/\n  2010-09-06-jekyll-postfiles-plugin.md\n_postfiles/\n  2010-09-06-jekyll-postfiles-plugin/\n    file.zip\n</code></pre>\n<p>Reference the file inside the post using the liquid tag, like <code>&amp;#123;&amp;#123; '{% postfile file.zip ' }}%}</code>, and you&rsquo;re all set.</p>\n<h3 id=\"tldr\">tl;dr</h3>\n<p>I wrote a Jekyll plugin to let you include files with your posts! You can get it on github at <a href=\"http://github.com/indirect/jekyll-postfiles\">indirect/jekyll-postfiles</a></p>\n",
				"content_text": "Through the years, I've used a lot of blogging engines. Nowadays, I use [Jekyll](http://jekyllrb.com/) to generate a static version of my blog. Jekyll has had one annoying shortcoming for me, though -- it doesn't support including files in a blog post. This has bitten me more than once, especially as I move between blogging engines (or just try to clean up random files on my web server).\n\nAbout a year ago, I decided to try to fix that. So, I forked Jekyll and patched it to allow files associated with posts in a `_postfiles` directory. That worked for a while (although I had to install my fork onto my server, which was a little bit annoying). This week, I realized that my RSS feed had broken images because I was using relative paths to reference postfiles. To solve that, I decided to write a `&#123;&#123; '{% postfile foo ' }}%}` liquid tag that expanded file references out to absolute paths.\n\nWhile working on the liquid tag, I discovered that Jekyll 0.7 supports plugins, and that I could stop forking Jekyll and just write a plugin instead. So I did. Presenting [jekyll-postfiles](http://github.com/indirect/jekyll-postfiles), a plugin for Jekyll that (optionally) adds files to each post. Just create a folder named `_postfiles`, next to the folder named `_posts`. When you have a file that you want to include in a post, create a folder with the same name as the post, and put the file in there, with a directory structure like this:\n\n    _posts/\n      2010-09-06-jekyll-postfiles-plugin.md\n    _postfiles/\n      2010-09-06-jekyll-postfiles-plugin/\n        file.zip\n\nReference the file inside the post using the liquid tag, like `&#123;&#123; '{% postfile file.zip ' }}%}`, and you're all set.\n\n### tl;dr\n\nI wrote a Jekyll plugin to let you include files with your posts! You can get it on github at [indirect/jekyll-postfiles](http://github.com/indirect/jekyll-postfiles)\n",
				"date_published": "2010-09-06T00:00:00-08:00",
				"url": "https://andre.arko.net/2010/09/06/jekyll-postfiles-plugin/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2010/08/16/using-passengerpane-with-gemhome-set/",
				"title": "Using PassengerPane with GEM_HOME set",
				"content_html": "<p>I use the excellent <a href=\"http://mxcl.github.com/homebrew\">homebrew</a> to manage my unix software on OS X, and as a result my bash profile includes <code>export GEM_HOME /usr/local/Cellar/Gems/1.8</code>. Unfortunately, that makes Passenger unable to find any of my gems, which is a bummer.</p>\n<p>However, it turns out the fix is just a one-line change to the .vhost files that PassengerPane creates in <code>/etc/apache2/passenger_pane_vhosts</code>. Just add the line starting with <code>SetEnv</code> below, and restart Apache.</p>\n<pre><code>&lt;VirtualHost *:80&gt;\n  ServerName app.dev\n  DocumentRoot &quot;/Users/andre/Sites/app/public&quot;\n  RackEnv development\n  SetEnv GEM_HOME /usr/local/Cellar/Gems/1.8\n  &lt;Directory &quot;/Users/andre/Sites/app/public&quot;&gt;\n    Order allow,deny\n    Allow from all\n  &lt;/Directory&gt;\n&lt;/VirtualHost&gt;\n</code></pre>\n<p>(I use .dev as my development TLD so that my applications don&rsquo;t conflict with Bonjour domains on .local.)</p>\n<p>There. Now (hopefully) I will remember this post next time, and not print-debug Passenger&rsquo;s environment <em>again</em>.</p>\n",
				"content_text": "I use the excellent [homebrew](http://mxcl.github.com/homebrew) to manage my unix software on OS X, and as a result my bash profile includes `export GEM_HOME /usr/local/Cellar/Gems/1.8`. Unfortunately, that makes Passenger unable to find any of my gems, which is a bummer.\n\nHowever, it turns out the fix is just a one-line change to the .vhost files that PassengerPane creates in `/etc/apache2/passenger_pane_vhosts`. Just add the line starting with `SetEnv` below, and restart Apache.\n\n    <VirtualHost *:80>\n      ServerName app.dev\n      DocumentRoot \"/Users/andre/Sites/app/public\"\n      RackEnv development\n      SetEnv GEM_HOME /usr/local/Cellar/Gems/1.8\n      <Directory \"/Users/andre/Sites/app/public\">\n        Order allow,deny\n        Allow from all\n      </Directory>\n    </VirtualHost>\n\n(I use .dev as my development TLD so that my applications don't conflict with Bonjour domains on .local.)\n\nThere. Now (hopefully) I will remember this post next time, and not print-debug Passenger's environment _again_.\n",
				"date_published": "2010-08-16T00:00:00-08:00",
				"url": "https://andre.arko.net/2010/08/16/using-passengerpane-with-gemhome-set/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2010/08/15/hamlrails-gem-for-haml-with/",
				"title": "Haml-rails gem for Haml with Rails 3",
				"content_html": "<p>I&rsquo;m setting up a new Rails 3 app, and I discovered that the <a href=\"http://github.com/rspec/rspec-rails\">rspec-rails</a> plugin is a very well-behaved Rails 3 gem plugin. All you have to do is add it to your Gemfile, and then (via a Railtie) it adds RSpec rake tasks, adds some RSpec generators, and replaces Test/Unit as the testing framework for anything newly generated.</p>\n<p><a href=\"http://haml-lang.com\">Haml</a>, on the other hand, does not integrate with the new features that Rails 3 provides. You need to run <code>haml --rails .</code> to generate an initializer yourself, and it doesn&rsquo;t come with any generators. The current haml generators had been living in my <a href=\"http://github.com/indirect/rails3-generators\">rails3-generators</a> gem, but that repo was just supposed to be a stopgap measure while plugin authors got around to integrating with Rails3 themselves.</p>\n<p>Since Haml doesn&rsquo;t seem to have gotten around to integrating with Rails 3, I just did it myself: presenting <a href=\"http://gemcutter.org/gems/haml-rails\">haml-rails</a>, the gem that not only adds Haml generators, but hooks into Rails to activate Haml and replace ERB with Haml automatically. No configuration required.</p>\n<p>The source is <a href=\"http://github.com/indirect/haml-rails\">on github</a> if you want to check it out.</p>\n<p>Installation is pretty complicated, but I&rsquo;m sure you can get the hang of it:</p>\n<pre><code><div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-ruby\" data-lang=\"ruby\"><span style=\"display:flex;\"><span><span style=\"color:#75715e\"># Gemfile</span>\n</span></span><span style=\"display:flex;\"><span>gem <span style=\"color:#e6db74\">&#34;haml-rails&#34;</span></span></span></code></pre></div>\n</code></pre>\n<p>Enjoy!</p>\n",
				"content_text": "I'm setting up a new Rails 3 app, and I discovered that the [rspec-rails](http://github.com/rspec/rspec-rails) plugin is a very well-behaved Rails 3 gem plugin. All you have to do is add it to your Gemfile, and then (via a Railtie) it adds RSpec rake tasks, adds some RSpec generators, and replaces Test/Unit as the testing framework for anything newly generated.\n\n[Haml](http://haml-lang.com), on the other hand, does not integrate with the new features that Rails 3 provides. You need to run `haml --rails .` to generate an initializer yourself, and it doesn't come with any generators. The current haml generators had been living in my [rails3-generators](http://github.com/indirect/rails3-generators) gem, but that repo was just supposed to be a stopgap measure while plugin authors got around to integrating with Rails3 themselves.\n\nSince Haml doesn't seem to have gotten around to integrating with Rails 3, I just did it myself: presenting [haml-rails](http://gemcutter.org/gems/haml-rails), the gem that not only adds Haml generators, but hooks into Rails to activate Haml and replace ERB with Haml automatically. No configuration required.\n\nThe source is [on github](http://github.com/indirect/haml-rails) if you want to check it out.\n\nInstallation is pretty complicated, but I'm sure you can get the hang of it:\n\n    {{< highlight ruby >}}\n    # Gemfile\n    gem \"haml-rails\"\n    {{< / highlight >}}\n\nEnjoy!\n",
				"date_published": "2010-08-15T00:00:00-08:00",
				"url": "https://andre.arko.net/2010/08/15/hamlrails-gem-for-haml-with/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2010/08/04/bundler-webinar-slides/",
				"title": "Bundler webinar slides",
				"content_html": "<p>These are the slides for the <a href=\"http://www.engineyard.com/video/13917022\">Bundler webinar</a> I gave at <a href=\"http://engineyard.com\">Engine Yard</a>, right before the release of Bundler 1.0rc3.</p>\n<p>If you want to watch and listen to the entire thing,  Engine Yard has posted <a href=\"http://www.engineyard.com/video/13917022\">a video of the presentation</a>.</p>\n<img src=\"uploads/2025/9253181703.jpg\" alt=\"webinar.001\">\n<img src=\"uploads/2025/539e33e33a.jpg\" alt=\"webinar.002\">\n<img src=\"uploads/2025/612be12061.jpg\" alt=\"webinar.003\">\n<img src=\"uploads/2025/a7d3d12b6a.jpg\" alt=\"webinar.004\">\n<img src=\"uploads/2025/ed05b79466.jpg\" alt=\"webinar.005\">\n<img src=\"uploads/2025/3084899f94.jpg\" alt=\"webinar.006\">\n<img src=\"uploads/2025/3b5a61efd6.jpg\" alt=\"webinar.007\">\n<img src=\"uploads/2025/ee3e9bd756.jpg\" alt=\"webinar.008\">\n<img src=\"uploads/2025/449cbf96ec.jpg\" alt=\"webinar.009\">\n<img src=\"uploads/2025/b5ae229156.jpg\" alt=\"webinar.010\">\n<img src=\"uploads/2025/31ca11ae17.jpg\" alt=\"webinar.011\">\n<img src=\"uploads/2025/f7fc09bc98.jpg\" alt=\"webinar.012\">\n<img src=\"uploads/2025/e4b890cdfb.jpg\" alt=\"webinar.013\">\n<img src=\"uploads/2025/d9dc230cf1.jpg\" alt=\"webinar.014\">\n<img src=\"uploads/2025/ac01f77356.jpg\" alt=\"webinar.015\">\n<img src=\"uploads/2025/7b491c1ca5.jpg\" alt=\"webinar.016\">\n<img src=\"uploads/2025/ffed5df2d6.jpg\" alt=\"webinar.017\">\n<img src=\"uploads/2025/900f30a2f1.jpg\" alt=\"webinar.018\">\n<img src=\"uploads/2025/eb5b849e1e.jpg\" alt=\"webinar.019\">\n<img src=\"uploads/2025/ac1a80a7f2.jpg\" alt=\"webinar.020\">\n<img src=\"uploads/2025/197558e3ff.jpg\" alt=\"webinar.021\">\n<img src=\"uploads/2025/b8dd02d3b2.jpg\" alt=\"webinar.022\">\n<img src=\"uploads/2025/ec3d4815eb.jpg\" alt=\"webinar.023\">\n<img src=\"uploads/2025/30d0c6cfd0.jpg\" alt=\"webinar.024\">\n<img src=\"uploads/2025/318f088340.jpg\" alt=\"webinar.025\">\n<img src=\"uploads/2025/5d2780e704.jpg\" alt=\"webinar.026\">\n<img src=\"uploads/2025/53c98767e3.jpg\" alt=\"webinar.027\">\n<img src=\"uploads/2025/2e33854385.jpg\" alt=\"webinar.028\">\n<img src=\"uploads/2025/bfaae6ecb7.jpg\" alt=\"webinar.029\">\n<img src=\"uploads/2025/09b7468ec0.jpg\" alt=\"webinar.030\">\n<img src=\"uploads/2025/7ae27539cc.jpg\" alt=\"webinar.031\">\n<img src=\"uploads/2025/2ca0c5b7d7.jpg\" alt=\"webinar.032\">\n",
				"content_text": "These are the slides for the [Bundler webinar](http://www.engineyard.com/video/13917022) I gave at [Engine Yard](http://engineyard.com), right before the release of Bundler 1.0rc3.\n\nIf you want to watch and listen to the entire thing,  Engine Yard has posted [a video of the presentation](http://www.engineyard.com/video/13917022).\n\n<img src=\"uploads/2025/9253181703.jpg\" alt=\"webinar.001\">\n<img src=\"uploads/2025/539e33e33a.jpg\" alt=\"webinar.002\">\n<img src=\"uploads/2025/612be12061.jpg\" alt=\"webinar.003\">\n<img src=\"uploads/2025/a7d3d12b6a.jpg\" alt=\"webinar.004\">\n<img src=\"uploads/2025/ed05b79466.jpg\" alt=\"webinar.005\">\n<img src=\"uploads/2025/3084899f94.jpg\" alt=\"webinar.006\">\n<img src=\"uploads/2025/3b5a61efd6.jpg\" alt=\"webinar.007\">\n<img src=\"uploads/2025/ee3e9bd756.jpg\" alt=\"webinar.008\">\n<img src=\"uploads/2025/449cbf96ec.jpg\" alt=\"webinar.009\">\n<img src=\"uploads/2025/b5ae229156.jpg\" alt=\"webinar.010\">\n<img src=\"uploads/2025/31ca11ae17.jpg\" alt=\"webinar.011\">\n<img src=\"uploads/2025/f7fc09bc98.jpg\" alt=\"webinar.012\">\n<img src=\"uploads/2025/e4b890cdfb.jpg\" alt=\"webinar.013\">\n<img src=\"uploads/2025/d9dc230cf1.jpg\" alt=\"webinar.014\">\n<img src=\"uploads/2025/ac01f77356.jpg\" alt=\"webinar.015\">\n<img src=\"uploads/2025/7b491c1ca5.jpg\" alt=\"webinar.016\">\n<img src=\"uploads/2025/ffed5df2d6.jpg\" alt=\"webinar.017\">\n<img src=\"uploads/2025/900f30a2f1.jpg\" alt=\"webinar.018\">\n<img src=\"uploads/2025/eb5b849e1e.jpg\" alt=\"webinar.019\">\n<img src=\"uploads/2025/ac1a80a7f2.jpg\" alt=\"webinar.020\">\n<img src=\"uploads/2025/197558e3ff.jpg\" alt=\"webinar.021\">\n<img src=\"uploads/2025/b8dd02d3b2.jpg\" alt=\"webinar.022\">\n<img src=\"uploads/2025/ec3d4815eb.jpg\" alt=\"webinar.023\">\n<img src=\"uploads/2025/30d0c6cfd0.jpg\" alt=\"webinar.024\">\n<img src=\"uploads/2025/318f088340.jpg\" alt=\"webinar.025\">\n<img src=\"uploads/2025/5d2780e704.jpg\" alt=\"webinar.026\">\n<img src=\"uploads/2025/53c98767e3.jpg\" alt=\"webinar.027\">\n<img src=\"uploads/2025/2e33854385.jpg\" alt=\"webinar.028\">\n<img src=\"uploads/2025/bfaae6ecb7.jpg\" alt=\"webinar.029\">\n<img src=\"uploads/2025/09b7468ec0.jpg\" alt=\"webinar.030\">\n<img src=\"uploads/2025/7ae27539cc.jpg\" alt=\"webinar.031\">\n<img src=\"uploads/2025/2ca0c5b7d7.jpg\" alt=\"webinar.032\">\n",
				"date_published": "2010-08-04T00:00:00-08:00",
				"url": "https://andre.arko.net/2010/08/04/bundler-webinar-slides/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2010/07/10/moving-along/",
				"title": "Moving along",
				"content_html": "<p>I&rsquo;m a little bit surprised to be saying this, but the week that just ended was my last week at <a href=\"http://engineyard.com\">Engine Yard</a>. I&rsquo;ve had a great time working on <a href=\"http://gembundler.com\">Bundler</a> and EY&rsquo;s <a href=\"http://www.engineyard.com/products/appcloud\">Cloud</a>, and I am sure EY will continue to do great things.</p>\n<p>Even though I&rsquo;m leaving Engine Yard, I am staying on the Bundler team. I will be working closely with Carl and Yehuda in the coming weeks to resolve issues in the beta versions and finish the 1.0 release.</p>\n<p>That said, starting Monday, I&rsquo;m going to be working at <a href=\"http://plexapp.com\">Plex</a>. I&rsquo;ve already used Plex for a year or two myself, and I really like it. It&rsquo;s very exciting to be working on a project I personally like with a small team of very cool people. I&rsquo;m looking forward to it a lot.</p>\n",
				"content_text": "I'm a little bit surprised to be saying this, but the week that just ended was my last week at [Engine Yard](http://engineyard.com). I've had a great time working on [Bundler](http://gembundler.com) and EY's [Cloud](http://www.engineyard.com/products/appcloud), and I am sure EY will continue to do great things.\n\nEven though I'm leaving Engine Yard, I am staying on the Bundler team. I will be working closely with Carl and Yehuda in the coming weeks to resolve issues in the beta versions and finish the 1.0 release.\n\nThat said, starting Monday, I'm going to be working at [Plex](http://plexapp.com). I've already used Plex for a year or two myself, and I really like it. It's very exciting to be working on a project I personally like with a small team of very cool people. I'm looking forward to it a lot.\n",
				"date_published": "2010-07-10T00:00:00-08:00",
				"url": "https://andre.arko.net/2010/07/10/moving-along/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2010/06/09/railsconf-bundler-talk-slides/",
				"title": "RailsConf 2010 Bundler Talk Slides",
				"content_html": "<img src=\"uploads/2025/589b003f08.jpg\" alt=\"bundler.001\">\n<img src=\"uploads/2025/77645c5763.jpg\" alt=\"bundler.002\">\n<img src=\"uploads/2025/070adede3d.jpg\" alt=\"bundler.003\">\n<img src=\"uploads/2025/05cb43f19f.jpg\" alt=\"bundler.004\">\n<img src=\"uploads/2025/55c3e860f3.jpg\" alt=\"bundler.005\">\n<img src=\"uploads/2025/7350fd9a32.jpg\" alt=\"bundler.006\">\n<img src=\"uploads/2025/864343aa9d.jpg\" alt=\"bundler.007\">\n<img src=\"uploads/2025/cca64abf2e.jpg\" alt=\"bundler.008\">\n<img src=\"uploads/2025/b5e675186c.jpg\" alt=\"bundler.009\">\n<img src=\"uploads/2025/c9c56c7add.jpg\" alt=\"bundler.010\">\n<img src=\"uploads/2025/9182fd87e6.jpg\" alt=\"bundler.011\">\n<img src=\"uploads/2025/9be60b8138.jpg\" alt=\"bundler.012\">\n<img src=\"uploads/2025/ee71343745.jpg\" alt=\"bundler.013\">\n<img src=\"uploads/2025/b59dcc3087.jpg\" alt=\"bundler.014\">\n<img src=\"uploads/2025/d6eb66cf29.jpg\" alt=\"bundler.015\">\n<img src=\"uploads/2025/6169fcc25c.jpg\" alt=\"bundler.016\">\n<img src=\"uploads/2025/f0b15e9f02.jpg\" alt=\"bundler.017\">\n<img src=\"uploads/2025/01167e996c.jpg\" alt=\"bundler.018\">\n<img src=\"uploads/2025/76a56f0aa4.jpg\" alt=\"bundler.019\">\n<img src=\"uploads/2025/566fe18756.jpg\" alt=\"bundler.020\">\n<img src=\"uploads/2025/e49fe23870.jpg\" alt=\"bundler.021\">\n<img src=\"uploads/2025/2864f6eb6e.jpg\" alt=\"bundler.022\">\n<img src=\"uploads/2025/715bfbed7b.jpg\" alt=\"bundler.023\">\n<img src=\"uploads/2025/7a9d4f98db.jpg\" alt=\"bundler.024\">\n<img src=\"uploads/2025/d0d190db4a.jpg\" alt=\"bundler.025\">\n<img src=\"uploads/2025/b115a73705.jpg\" alt=\"bundler.026\">\n<img src=\"uploads/2025/21d4759cc7.jpg\" alt=\"bundler.027\">\n",
				"content_text": "<img src=\"uploads/2025/589b003f08.jpg\" alt=\"bundler.001\">\n<img src=\"uploads/2025/77645c5763.jpg\" alt=\"bundler.002\">\n<img src=\"uploads/2025/070adede3d.jpg\" alt=\"bundler.003\">\n<img src=\"uploads/2025/05cb43f19f.jpg\" alt=\"bundler.004\">\n<img src=\"uploads/2025/55c3e860f3.jpg\" alt=\"bundler.005\">\n<img src=\"uploads/2025/7350fd9a32.jpg\" alt=\"bundler.006\">\n<img src=\"uploads/2025/864343aa9d.jpg\" alt=\"bundler.007\">\n<img src=\"uploads/2025/cca64abf2e.jpg\" alt=\"bundler.008\">\n<img src=\"uploads/2025/b5e675186c.jpg\" alt=\"bundler.009\">\n<img src=\"uploads/2025/c9c56c7add.jpg\" alt=\"bundler.010\">\n<img src=\"uploads/2025/9182fd87e6.jpg\" alt=\"bundler.011\">\n<img src=\"uploads/2025/9be60b8138.jpg\" alt=\"bundler.012\">\n<img src=\"uploads/2025/ee71343745.jpg\" alt=\"bundler.013\">\n<img src=\"uploads/2025/b59dcc3087.jpg\" alt=\"bundler.014\">\n<img src=\"uploads/2025/d6eb66cf29.jpg\" alt=\"bundler.015\">\n<img src=\"uploads/2025/6169fcc25c.jpg\" alt=\"bundler.016\">\n<img src=\"uploads/2025/f0b15e9f02.jpg\" alt=\"bundler.017\">\n<img src=\"uploads/2025/01167e996c.jpg\" alt=\"bundler.018\">\n<img src=\"uploads/2025/76a56f0aa4.jpg\" alt=\"bundler.019\">\n<img src=\"uploads/2025/566fe18756.jpg\" alt=\"bundler.020\">\n<img src=\"uploads/2025/e49fe23870.jpg\" alt=\"bundler.021\">\n<img src=\"uploads/2025/2864f6eb6e.jpg\" alt=\"bundler.022\">\n<img src=\"uploads/2025/715bfbed7b.jpg\" alt=\"bundler.023\">\n<img src=\"uploads/2025/7a9d4f98db.jpg\" alt=\"bundler.024\">\n<img src=\"uploads/2025/d0d190db4a.jpg\" alt=\"bundler.025\">\n<img src=\"uploads/2025/b115a73705.jpg\" alt=\"bundler.026\">\n<img src=\"uploads/2025/21d4759cc7.jpg\" alt=\"bundler.027\">\n",
				"date_published": "2010-06-09T00:00:00-08:00",
				"url": "https://andre.arko.net/2010/06/09/railsconf-bundler-talk-slides/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2010/05/09/slides-from-my-bundler-talk/",
				"title": "Slides from my Bundler talk at the Red Dirt Ruby Conference",
				"content_html": "<img src=\"uploads/2025/f5a6c47dc7.jpg\" alt=\"bundler.001\">\n<img src=\"uploads/2025/95f892d874.jpg\" alt=\"bundler.002\">\n<img src=\"uploads/2025/3c424ecfa5.jpg\" alt=\"bundler.003\">\n<img src=\"uploads/2025/c8cd8c2def.jpg\" alt=\"bundler.004\">\n<img src=\"uploads/2025/5507d29ea1.jpg\" alt=\"bundler.005\">\n<img src=\"uploads/2025/72b684e481.jpg\" alt=\"bundler.006\">\n<img src=\"uploads/2025/c3a6474d6c.jpg\" alt=\"bundler.007\">\n<img src=\"uploads/2025/d13ac14aaf.jpg\" alt=\"bundler.008\">\n<img src=\"uploads/2025/95ae2b3d22.jpg\" alt=\"bundler.009\">\n<img src=\"uploads/2025/2edf4b802f.jpg\" alt=\"bundler.010\">\n<img src=\"uploads/2025/5b7f341567.jpg\" alt=\"bundler.011\">\n<img src=\"uploads/2025/806f723755.jpg\" alt=\"bundler.012\">\n<img src=\"uploads/2025/d3df85e89c.jpg\" alt=\"bundler.013\">\n<img src=\"uploads/2025/fadb7a76da.jpg\" alt=\"bundler.014\">\n<img src=\"uploads/2025/8d136fdda3.jpg\" alt=\"bundler.015\">\n<img src=\"uploads/2025/66193e26b7.jpg\" alt=\"bundler.016\">\n<img src=\"uploads/2025/b8af7c8dba.jpg\" alt=\"bundler.017\">\n<img src=\"uploads/2025/f7804d7b04.jpg\" alt=\"bundler.018\">\n<img src=\"uploads/2025/c7e3d905fa.jpg\" alt=\"bundler.019\">\n<img src=\"uploads/2025/c3e93079af.jpg\" alt=\"bundler.020\">\n<img src=\"uploads/2025/44e58b34f1.jpg\" alt=\"bundler.021\">\n",
				"content_text": "<img src=\"uploads/2025/f5a6c47dc7.jpg\" alt=\"bundler.001\">\n<img src=\"uploads/2025/95f892d874.jpg\" alt=\"bundler.002\">\n<img src=\"uploads/2025/3c424ecfa5.jpg\" alt=\"bundler.003\">\n<img src=\"uploads/2025/c8cd8c2def.jpg\" alt=\"bundler.004\">\n<img src=\"uploads/2025/5507d29ea1.jpg\" alt=\"bundler.005\">\n<img src=\"uploads/2025/72b684e481.jpg\" alt=\"bundler.006\">\n<img src=\"uploads/2025/c3a6474d6c.jpg\" alt=\"bundler.007\">\n<img src=\"uploads/2025/d13ac14aaf.jpg\" alt=\"bundler.008\">\n<img src=\"uploads/2025/95ae2b3d22.jpg\" alt=\"bundler.009\">\n<img src=\"uploads/2025/2edf4b802f.jpg\" alt=\"bundler.010\">\n<img src=\"uploads/2025/5b7f341567.jpg\" alt=\"bundler.011\">\n<img src=\"uploads/2025/806f723755.jpg\" alt=\"bundler.012\">\n<img src=\"uploads/2025/d3df85e89c.jpg\" alt=\"bundler.013\">\n<img src=\"uploads/2025/fadb7a76da.jpg\" alt=\"bundler.014\">\n<img src=\"uploads/2025/8d136fdda3.jpg\" alt=\"bundler.015\">\n<img src=\"uploads/2025/66193e26b7.jpg\" alt=\"bundler.016\">\n<img src=\"uploads/2025/b8af7c8dba.jpg\" alt=\"bundler.017\">\n<img src=\"uploads/2025/f7804d7b04.jpg\" alt=\"bundler.018\">\n<img src=\"uploads/2025/c7e3d905fa.jpg\" alt=\"bundler.019\">\n<img src=\"uploads/2025/c3e93079af.jpg\" alt=\"bundler.020\">\n<img src=\"uploads/2025/44e58b34f1.jpg\" alt=\"bundler.021\">\n",
				"date_published": "2010-05-09T00:00:00-08:00",
				"url": "https://andre.arko.net/2010/05/09/slides-from-my-bundler-talk/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2010/05/01/bundler-for-gem-development/",
				"title": "Bundler for gem development",
				"content_html": "<p><a href=\"http://gembundler.com\">Bundler</a> was written to manage the gem dependencies of ruby applications, and it has gotten to the point where it is pretty good at that. What you probably don&rsquo;t know is that it can manage the gem dependencies of any gem that you are working on, as well.</p>\n<p>Tools like <a href=\"http://github.com/technicalpickles/jeweler\">Jeweler</a> have attempted to help with this problem in the past. Jeweler comes with a rake task that tells you if some of your gem&rsquo;s dependencies aren&rsquo;t installed. Bundler takes this idea one step further, allowing you to install all of your gem&rsquo;s dependencies with a single command. This makes it very simple to check out a gem&rsquo;s source and start working on it right away.</p>\n<hr>\n<p><strong>Update</strong> While the stuff below worked in Bundler 0.9, it has been removed in 1.0. Integrating Bundler with gemspecs proved to be a fantastic idea, but the version that shipped with Bundler 0.9 had some issues. Gemspecs are executable Ruby code, and adding a dependency on the Bundler gem to your gemspec didn&rsquo;t work in practice, because many machines that needed to process gemspecs didn&rsquo;t have the right version of Bundler already installed.</p>\n<p>Replacing <code>Gem::Specification#add_bundler_dependencies</code> is the <code>gemspec</code> method for your Gemfile. You can use it like this:</p>\n<pre><code>source :rubygems\ngemspec\n</code></pre>\n<p>For more information about the <code>gemspec</code> method and managing your gems with Bundler, please see the official docs, written by myself and the incomparable Josh Hull over on <a href=\"http://gembundler.com/rubygems.html\">the official Bundler documentation site</a>.</p>\n<hr>\n<p>Specifying your gem&rsquo;s dependencies is simple: just like any other bundler project, you create a <a href=\"http://gembundler.com/gemfile.html\">Gemfile</a>. That file is some ruby code declaring the other gems that you gem needs to have installed in order to run. It will look something like this:</p>\n<pre><code>source :rubyforge\ngem &quot;json&quot;\n\ngroup :development do\n  gem &quot;rspec&quot;\nend\n</code></pre>\n<p>Once you have a Gemfile, you just add a couple of lines to your gemspec. Require the bundler library at the top, and then call the <code>add_bundler_dependencies</code> method on your gemspec object. In the simplest form, it looks like this:</p>\n<pre><code>require 'bundler'\nGem::Specification.new do |s|\n  s.add_bundler_dependencies\nend\n</code></pre>\n<p>Once you&rsquo;ve done that, all the gems in your gemfile that are in the default group will be added to your gem as dependencies. If you specify version requirements in the Gemfile, they will be reflected in the built gemspec. If you have gems that are only needed for development of your gem, put them in a group named :development and they will be added to your gemspec as development dependencies. Lastly, if you have gems that you want bundler to install, but not list inside the gemspec, put them in a group named anything besides :development.</p>\n<p>At this point, what I usually do is add the standard bundler snippet to the top of my spec_helper.rb file so that i can run my specs without bundle exec:</p>\n<pre><code>require 'rubygems'\nrequire 'bundler'\nBundler.setup\n</code></pre>\n<p>That&rsquo;s really all you need to use bundler while you develop a gem. If you have any questions, feel free to email, tweet, or ask me in #bundler on freenode.</p>\n",
				"content_text": "[Bundler](http://gembundler.com) was written to manage the gem dependencies of ruby applications, and it has gotten to the point where it is pretty good at that. What you probably don't know is that it can manage the gem dependencies of any gem that you are working on, as well.\n\nTools like [Jeweler](http://github.com/technicalpickles/jeweler) have attempted to help with this problem in the past. Jeweler comes with a rake task that tells you if some of your gem's dependencies aren't installed. Bundler takes this idea one step further, allowing you to install all of your gem's dependencies with a single command. This makes it very simple to check out a gem's source and start working on it right away.\n\n------------------\n\n**Update** While the stuff below worked in Bundler 0.9, it has been removed in 1.0. Integrating Bundler with gemspecs proved to be a fantastic idea, but the version that shipped with Bundler 0.9 had some issues. Gemspecs are executable Ruby code, and adding a dependency on the Bundler gem to your gemspec didn't work in practice, because many machines that needed to process gemspecs didn't have the right version of Bundler already installed.\n\nReplacing `Gem::Specification#add_bundler_dependencies` is the `gemspec` method for your Gemfile. You can use it like this:\n\n    source :rubygems\n    gemspec\n\nFor more information about the `gemspec` method and managing your gems with Bundler, please see the official docs, written by myself and the incomparable Josh Hull over on [the official Bundler documentation site](http://gembundler.com/rubygems.html).\n\n------------------\n\nSpecifying your gem's dependencies is simple: just like any other bundler project, you create a [Gemfile](http://gembundler.com/gemfile.html). That file is some ruby code declaring the other gems that you gem needs to have installed in order to run. It will look something like this:\n\n    source :rubyforge\n    gem \"json\"\n\n    group :development do\n      gem \"rspec\"\n    end\n\nOnce you have a Gemfile, you just add a couple of lines to your gemspec. Require the bundler library at the top, and then call the `add_bundler_dependencies` method on your gemspec object. In the simplest form, it looks like this:\n\n    require 'bundler'\n    Gem::Specification.new do |s|\n      s.add_bundler_dependencies\n    end\n\nOnce you've done that, all the gems in your gemfile that are in the default group will be added to your gem as dependencies. If you specify version requirements in the Gemfile, they will be reflected in the built gemspec. If you have gems that are only needed for development of your gem, put them in a group named :development and they will be added to your gemspec as development dependencies. Lastly, if you have gems that you want bundler to install, but not list inside the gemspec, put them in a group named anything besides :development.\n\nAt this point, what I usually do is add the standard bundler snippet to the top of my spec_helper.rb file so that i can run my specs without bundle exec:\n\n    require 'rubygems'\n    require 'bundler'\n    Bundler.setup\n\nThat's really all you need to use bundler while you develop a gem. If you have any questions, feel free to email, tweet, or ask me in #bundler on freenode.\n",
				"date_published": "2010-05-01T00:00:00-08:00",
				"url": "https://andre.arko.net/2010/05/01/bundler-for-gem-development/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2010/04/04/ipad-internet-via-iphone-without/",
				"title": "iPad internet via iPhone, without jailbreaking",
				"content_html": "<p>I am a big fan of the <a href=\"http://github.com/tcurdt/iProxy\">iProxy</a> project, which lets you get your laptop online via your iPhone without having to jailbreak it, provided you are (or at least know) someone in the iPhone developer program. I got my iPad today, and took it out to the park. Once I was there, I of course instantly wanted to look something up online. To solve this problem until the iPad 3G comes out, I figured out a mildly annoying hack that will suffice for the time being. So here&rsquo;s what to do:</p>\n<ol>\n<li>\n<p>Install iProxy onto your iPhone.</p>\n<p>I can&rsquo;t really help you with this part, but there&rsquo;s instructions in various parts of the internet.</p>\n</li>\n<li>\n<p>Create a file named <code>socks.pac</code>, and put this in it:</p>\n<pre><code>function FindProxyForURL(url, host) {\n  return &quot;SOCKS 10.0.0.1:8888&quot;;\n}\n</code></pre>\n</li>\n<li>\n<p>Upload that file to Air Sharing on your iPhone, so you can host it for the iPad later.</p>\n</li>\n<li>\n<p>Get your iPhone and iPad onto the same wifi network.</p>\n<p>If you don&rsquo;t have any networks available, you might have to create one using a laptop. Once one of them has joined, the laptop doesn&rsquo;t have to be involved anymore.</p>\n</li>\n<li>\n<p>Set up static IP addresses for the iPhone and iPad.</p>\n<p>I used 10.0.0.1 for the iPhone and 10.0.0.2 for the iPad.</p>\n</li>\n<li>\n<p>Launch Air Sharing on the iPhone, and configure the iPad to use your socks.pac file.</p>\n<p>On the iPad, under the IP Address section, is HTTP Proxy. Choose &ldquo;Auto&rdquo;, and type in the URL of the file you are sharing from the iPhone. For me, that was &ldquo;http://10.0.0.1/socks.pac&rdquo;. Open Safari on your iPad and try to browse to a website. It will fail.</p>\n</li>\n<li>\n<p>Run iProxy on your iPhone, and then try to browse to a website on your iPad again. Success!</p>\n</li>\n</ol>\n<p>For some reason, I didn&rsquo;t have very much luck getting applications other than Safari to use the SOCKS proxy that my iPhone was providing. But I was very happy to be able to surf the web on the bigger screen.</p>\n",
				"content_text": "I am a big fan of the [iProxy](http://github.com/tcurdt/iProxy) project, which lets you get your laptop online via your iPhone without having to jailbreak it, provided you are (or at least know) someone in the iPhone developer program. I got my iPad today, and took it out to the park. Once I was there, I of course instantly wanted to look something up online. To solve this problem until the iPad 3G comes out, I figured out a mildly annoying hack that will suffice for the time being. So here's what to do:\n\n  1.  Install iProxy onto your iPhone.\n  \n      I can't really help you with this part, but there's instructions in various parts of the internet.\n  \n  2.  Create a file named `socks.pac`, and put this in it:\n  \n          function FindProxyForURL(url, host) {\n            return \"SOCKS 10.0.0.1:8888\";\n          }\n\n  3.  Upload that file to Air Sharing on your iPhone, so you can host it for the iPad later.\n  \n  4.  Get your iPhone and iPad onto the same wifi network.\n  \n      If you don't have any networks available, you might have to create one using a laptop. Once one of them has joined, the laptop doesn't have to be involved anymore.\n  \n  5.  Set up static IP addresses for the iPhone and iPad.\n  \n      I used 10.0.0.1 for the iPhone and 10.0.0.2 for the iPad.\n  \n  6.  Launch Air Sharing on the iPhone, and configure the iPad to use your socks.pac file.\n  \n      On the iPad, under the IP Address section, is HTTP Proxy. Choose \"Auto\", and type in the URL of the file you are sharing from the iPhone. For me, that was \"http://10.0.0.1/socks.pac\". Open Safari on your iPad and try to browse to a website. It will fail.\n  \n  7.  Run iProxy on your iPhone, and then try to browse to a website on your iPad again. Success!\n\nFor some reason, I didn't have very much luck getting applications other than Safari to use the SOCKS proxy that my iPhone was providing. But I was very happy to be able to surf the web on the bigger screen.\n",
				"date_published": "2010-04-04T00:00:00-08:00",
				"url": "https://andre.arko.net/2010/04/04/ipad-internet-via-iphone-without/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2010/03/23/tm-textmate-project-management/",
				"title": "tm, TextMate project management",
				"content_html": "<p>I realized, the other day, that I always create a new TextMate project by running <code>mate .</code> whenever I want to edit code on a project. Unfortunately, that approach has frustrating downsides like forgetting per-project ignores, open files, window positions, and sidebar arrangement. To save all that stuff, you have to save a .tmproj file somewhere. But I could never figure out where to put them, and I never managed to remember to open them later since I was already in the habit of <code>mate .</code>.</p>\n<p>One day, I tried to run <code>mate bundler</code> and realized that I should just create a command that would open the .tmproj files for me. That way, I don&rsquo;t have to remember where they are and I still get all the benefits of saving .tmproj files. So I created <a href=\"http://github.com/indirect/tm\">tm</a>, a little tiny ruby script that opens .tmproj files by name if you save them into ~/.tmproj. (I symlink my dotfiles out of my Dropbox, so this works across my machines, too.) If you are so inclined, <code>tm</code> is even smart enough to wrap <code>mate</code> transparently, with the only additions being the ability to open (and tab complete) projects by name. Pretty handy.</p>\n",
				"content_text": "I realized, the other day, that I always create a new TextMate project by running `mate .` whenever I want to edit code on a project. Unfortunately, that approach has frustrating downsides like forgetting per-project ignores, open files, window positions, and sidebar arrangement. To save all that stuff, you have to save a .tmproj file somewhere. But I could never figure out where to put them, and I never managed to remember to open them later since I was already in the habit of `mate .`.\n\nOne day, I tried to run `mate bundler` and realized that I should just create a command that would open the .tmproj files for me. That way, I don't have to remember where they are and I still get all the benefits of saving .tmproj files. So I created [tm](http://github.com/indirect/tm), a little tiny ruby script that opens .tmproj files by name if you save them into ~/.tmproj. (I symlink my dotfiles out of my Dropbox, so this works across my machines, too.) If you are so inclined, `tm` is even smart enough to wrap `mate` transparently, with the only additions being the ability to open (and tab complete) projects by name. Pretty handy.\n",
				"date_published": "2010-03-23T00:00:00-08:00",
				"url": "https://andre.arko.net/2010/03/23/tm-textmate-project-management/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2010/03/05/igooglefight/",
				"title": "iGoogleFight",
				"content_html": "<p>I was horrified and dismayed to learn, the other week, that <a href=\"http://googlefight.com\">GoogleFight</a> uses flash to present the results of fights. This meant I couldn&rsquo;t use it on my iPhone, which was horrible. So I built a <a href=\"http://www.jqtouch.com/\">jQTouch</a> app, backed by <a href=\"http://www.sinatrarb.com/\">Sinatra</a>. It handles all of my Google-fighting needs in style, and without any Flash whatsoever.</p>\n<p>So, without any further ado, I present:</p>\n<h3 id=\"igooglefightcomhttpigooglefightcom\"><a href=\"http://igooglefight.com\">iGoogleFight.com</a></h3>\n",
				"content_text": "I was horrified and dismayed to learn, the other week, that [GoogleFight](http://googlefight.com) uses flash to present the results of fights. This meant I couldn't use it on my iPhone, which was horrible. So I built a [jQTouch](http://www.jqtouch.com/) app, backed by [Sinatra](http://www.sinatrarb.com/). It handles all of my Google-fighting needs in style, and without any Flash whatsoever.\n\nSo, without any further ado, I present:\n\n### [iGoogleFight.com](http://igooglefight.com)\n",
				"date_published": "2010-03-05T00:00:00-08:00",
				"url": "https://andre.arko.net/2010/03/05/igooglefight/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2010/02/13/bundler-and-rails/",
				"title": "Bundler 0.9 and Rails 2.3.5",
				"content_html": "<p><a href=\"http://github.com/carlhuda/bundler\">Bundler</a> 0.9 is out, and people have been having trouble getting it working with Rails 2.3.5 applications. In order to help with that, I&rsquo;ll be keeping this blog post updated with the latest instructions on converting a Rails 2.3.5 application over to use Bundler 0.9.</p>\n<p>The first step in converting is to tweak the configuration and startup files in your app. You&rsquo;ll need to add a monkeypatch to your app&rsquo;s <code>config/boot.rb</code> file:</p>\n<script src=\"https://gist.github.com/302406.js?file=boot.rb\"></script>\n<p>Next, you&rsquo;ll need to add a new file at <code>config/preinitializer.rb</code>, and put this into it:</p>\n<script src=\"https://gist.github.com/302406.js?file=preinitializer.rb\"></script>\n<p>Last, you&rsquo;ll need to move your gem requirements list from <code>config/environment.rb</code> to your <code>Gemfile</code>. Here&rsquo;s an example <code>Gemfile</code>:</p>\n<script src=\"https://gist.github.com/302406.js?file=Gemfile\"></script>\n<p>By the time you have finished, there shouldn&rsquo;t be any <code>config.gem</code> statements left in your <code>environment.rb</code> file. All the gems that your application depends on should be listed in your <code>Gemfile</code> instead. If you have gems that should only be loaded in certain environments, like development-only or test-only gems, you can put those in the &ldquo;development&rdquo; and &ldquo;test&rdquo; groups.</p>\n<p>When you&rsquo;re done, you can install your gems and record the specific versions that you are using, so that they won&rsquo;t change unexpectedly as you deploy:</p>\n<pre><code>bundle install\nbundle lock\n</code></pre>\n<p>After you have installed and locked your bundle, you can run Rails scripts directly. However, you <em>must</em> run all other commands via <code>bundle exec</code>. For example:</p>\n<pre><code>./script/server\nbundle exec rake db:migrate\n</code></pre>\n<p>When you deploy your application, you will need to run <code>bundle install</code> as part of your deploy process, typically after your code has been updated but before you restart your app servers. If you have development gems that you can&rsquo;t (or don&rsquo;t want to) install on your production machine, you can run <code>bundle install --without development</code>.</p>\n<p>There are a lot more things you can do with the Bundler. If you want to read about them, I suggest checking out the <a href=\"http://github.com/carlhuda/bundler/tree/master/README.markdown\">Bundler README</a>.</p>\n",
				"content_text": "[Bundler](http://github.com/carlhuda/bundler) 0.9 is out, and people have been having trouble getting it working with Rails 2.3.5 applications. In order to help with that, I'll be keeping this blog post updated with the latest instructions on converting a Rails 2.3.5 application over to use Bundler 0.9.\n\nThe first step in converting is to tweak the configuration and startup files in your app. You'll need to add a monkeypatch to your app's `config/boot.rb` file:\n\n<script src=\"https://gist.github.com/302406.js?file=boot.rb\"></script>\n\nNext, you'll need to add a new file at `config/preinitializer.rb`, and put this into it:\n\n<script src=\"https://gist.github.com/302406.js?file=preinitializer.rb\"></script>\n\nLast, you'll need to move your gem requirements list from `config/environment.rb` to your `Gemfile`. Here's an example `Gemfile`:\n\n<script src=\"https://gist.github.com/302406.js?file=Gemfile\"></script>\n\nBy the time you have finished, there shouldn't be any `config.gem` statements left in your `environment.rb` file. All the gems that your application depends on should be listed in your `Gemfile` instead. If you have gems that should only be loaded in certain environments, like development-only or test-only gems, you can put those in the \"development\" and \"test\" groups.\n\nWhen you're done, you can install your gems and record the specific versions that you are using, so that they won't change unexpectedly as you deploy:\n\n    bundle install\n    bundle lock\n\nAfter you have installed and locked your bundle, you can run Rails scripts directly. However, you _must_ run all other commands via `bundle exec`. For example:\n\n    ./script/server\n    bundle exec rake db:migrate\n\nWhen you deploy your application, you will need to run `bundle install` as part of your deploy process, typically after your code has been updated but before you restart your app servers. If you have development gems that you can't (or don't want to) install on your production machine, you can run `bundle install --without development`.\n\nThere are a lot more things you can do with the Bundler. If you want to read about them, I suggest checking out the [Bundler README](http://github.com/carlhuda/bundler/tree/master/README.markdown).\n",
				"date_published": "2010-02-13T00:00:00-08:00",
				"url": "https://andre.arko.net/2010/02/13/bundler-and-rails/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2010/02/02/homebrew-os-xs-missing-package/",
				"title": "Homebrew: OS X''s Missing Package Manager",
				"content_html": "<p>Managing software packages on unix is a giant pain. Most linux distributions are built around different ways to try to alleviate that pain. Over the years, there have been various attempts to create effective package managers for OS X. The two most popular efforts, <a href=\"http://finkproject.org\">Fink</a> and <a href=\"http://macports.org\">MacPorts</a>, but they certainly have their frustrations. In both cases, creating packages or portfiles can be complex and difficult.</p>\n<p>Fortunately for us, <a href=\"http://www.methylblue.com/\">Max Howell</a> decided that there should be a package manager that is easy to edit, and that makes creating new packages a breeze. <a href=\"http://github.com/mxcl/homebrew\">Homebrew</a> is that package manager.</p>\n<h2 id=\"what-does-it-do\">What does it do?</h2>\n<p>Homebrew alleviates the drudgery and repetition of downloading and installing unix software packages on OS X. If you&rsquo;re sick of <code>./configure &amp;&amp; make &amp;&amp; make install</code>, Homebrew can definitely help.</p>\n<h2 id=\"why-homebrew\">Why Homebrew?</h2>\n<p>OS X already has two package managers: <a href=\"http://finkproject.org\">Fink</a> and <a href=\"http://macports.org\">MacPorts</a>. If one of those is working for you, great. But if you&rsquo;ve been frustrated by them in the past, I strongly suggest you give Homebrew a try. It&rsquo;s very easy to create and edit formulae, and even to edit Homebrew itself, since the core is just a few hundred lines of Ruby code.</p>\n<p>It doesn&rsquo;t impose external structure on you &ndash; the default is to install it to <code>/usr/local</code>, but you can install it anywhere. Inside your Homebrew directory, software is installed into subdirectories inside Homebrew&rsquo;s cellar, like <code>Cellar/git/1.6.5.4/</code>. After installation, Homebrew symlinks the software into the regular unix directories. If you want to hand-install a package or version that isn&rsquo;t officially part of Homebrew yet, they can happily co-exist in the same location.</p>\n<p>That&rsquo;s usually not necessary, though, since formulae can install directly out of version control. If a package has a public git, svn, cvs, or mercurial repository, you can install the latest development version as often as you like with a simple <code>brew install</code>.</p>\n<p>Installing packages is faster, too, because Homebrew also works hard to avoid package duplication. No more installing yet another version of Perl as a package dependency when you already have a working install of Perl built in to OS X.</p>\n<p>Best of all, Homebrew has a basic philosophy that you shouldn&rsquo;t have to use sudo to install or manage software on your computer.</p>\n<h2 id=\"okay-it-sounds-pretty-great-how-do-i-get-it\">Okay, it sounds pretty great. How do I get it?</h2>\n<p>The first (and only) dependency that Homebrew has is the OS X Developer Tools, which are on the OS X installer disc, and can be downloaded from <a href=\"http://developer.apple.com\">developer.apple.com</a>.</p>\n<p>Unless you have a reason not to, the easiest place to install Homebrew is in <code>/usr/local</code>. You can do that in just a few steps on the command line:</p>\n<pre><code># Take ownership of /usr/local so you don't have to sudo\nsudo chown -R `whoami` /usr/local\n# Fix the permissions on your mysql installation, if you have one\nsudo chown -R mysql:mysql /usr/local/mysql\n# Download and install Homebrew from github\ncurl -L http://github.com/mxcl/homebrew/tarball/master | tar xz --strip 1 -C /usr/local\n</code></pre>\n<p>Once you&rsquo;ve done that, you&rsquo;re good to go! Assuming <code>/usr/local/bin</code> is in your PATH, feel free to try it out:</p>\n<pre><code>brew install wget\nbrew info git\n</code></pre>\n<p>The Homebrew wiki also has more about <a href=\"http://wiki.github.com/mxcl/homebrew/cpan-ruby-gems-and-python-disttools\">integrating with RubyGems, CPAN, and Python&rsquo;s EasyInstall</a>.</p>\n<p>Keeping your copy of Homebrew up to date is easy:</p>\n<pre><code>brew install git\nbrew update\n</code></pre>\n<p>Once you have git installed, you can just run <code>brew update</code> anytime you want to pull down the latest formulae.</p>\n<h2 id=\"contributing\">Contributing</h2>\n<p>Creating a new formula is almost that easy. If Homebrew didn&rsquo;t have a formula for wget, you could create one like this:</p>\n<pre><code>brew create http://ftp.gnu.org/gnu/wget/wget-1.12.tar.bz2\n</code></pre>\n<p>After you save your formula, you can test it out with <code>brew install -vd wget</code>, to enable verbose logging and debug mode. If you need help getting your formula working, there is more documentation on the <a href=\"http://wiki.github.com/mxcl/homebrew/contributing\">Homebrew wiki</a>. You can also learn by example from already existing formula, like <a href=\"http://github.com/mxcl/homebrew/tree/master/Library/Formula/git.rb\">git</a> or <a href=\"http://github.com/mxcl/homebrew/tree/master/Library/Formula/flac.rb\">flac</a>.</p>\n<p>You can check out lots of example formulae, and Homebrew&rsquo;s internals, by running <code>brew edit</code>. The code is pretty straightforward. If you have questions, or are interested in future plans, the contributors to Homebrew tend to hang out in the #machomebrew channel on Freenode.</p>\n<p>Once you have a working new formula, it&rsquo;s easy to create your own fork of Homebrew on github to push your new formula to, by using the github gem.</p>\n<pre><code>git add .\ngit commit -m &quot;Added a formula for wget&quot;\ngem install json github\ngithub fork\ngit push &lt;your github username&gt; mastergitx\n</code></pre>\n<p>After pushing your change to github, go to the <a href=\"http://github.com/mxcl/homebrew/issues\">Homebrew issue tracker</a> and create a ticket with the subject &ldquo;New formula: <software name>&rdquo;. Assuming everything checks out, your formula will be added to the main Homebrew repository and available for everyone else to use.</p>\n<h2 id=\"wrapping-up\">Wrapping up</h2>\n<p>Homebrew is a compelling alternative to MacPorts and Fink. Both the Homebrew core and all the formulae are written in ruby, so it&rsquo;s easy to add new packages or even new features. If you&rsquo;re looking for more control over the unix software you have installed on your Mac, or you&rsquo;ve been frustrated by other package managers in the past, check it out. I think you&rsquo;ll be happily surprised.</p>\n<p class=\"aside\">This post was originally written for, and posted to, the <a href=\"http://www.engineyard.com/blog/2010/homebrew-os-xs-missing-package-manager/\">Engine Yard Blog</a>.</p>\n",
				"content_text": "\n\nManaging software packages on unix is a giant pain. Most linux distributions are built around different ways to try to alleviate that pain. Over the years, there have been various attempts to create effective package managers for OS X. The two most popular efforts, [Fink](http://finkproject.org) and [MacPorts](http://macports.org), but they certainly have their frustrations. In both cases, creating packages or portfiles can be complex and difficult.\n\nFortunately for us, [Max Howell](http://www.methylblue.com/) decided that there should be a package manager that is easy to edit, and that makes creating new packages a breeze. [Homebrew](http://github.com/mxcl/homebrew) is that package manager.\n\n\n## What does it do?\n\nHomebrew alleviates the drudgery and repetition of downloading and installing unix software packages on OS X. If you're sick of `./configure && make && make install`, Homebrew can definitely help.\n\n\n## Why Homebrew?\n\nOS X already has two package managers: [Fink](http://finkproject.org) and [MacPorts](http://macports.org). If one of those is working for you, great. But if you've been frustrated by them in the past, I strongly suggest you give Homebrew a try. It's very easy to create and edit formulae, and even to edit Homebrew itself, since the core is just a few hundred lines of Ruby code.\n\nIt doesn't impose external structure on you -- the default is to install it to `/usr/local`, but you can install it anywhere. Inside your Homebrew directory, software is installed into subdirectories inside Homebrew's cellar, like `Cellar/git/1.6.5.4/`. After installation, Homebrew symlinks the software into the regular unix directories. If you want to hand-install a package or version that isn't officially part of Homebrew yet, they can happily co-exist in the same location.\n\nThat's usually not necessary, though, since formulae can install directly out of version control. If a package has a public git, svn, cvs, or mercurial repository, you can install the latest development version as often as you like with a simple `brew install`.\n\nInstalling packages is faster, too, because Homebrew also works hard to avoid package duplication. No more installing yet another version of Perl as a package dependency when you already have a working install of Perl built in to OS X.\n\nBest of all, Homebrew has a basic philosophy that you shouldn't have to use sudo to install or manage software on your computer.\n\n\n## Okay, it sounds pretty great. How do I get it?\n\nThe first (and only) dependency that Homebrew has is the OS X Developer Tools, which are on the OS X installer disc, and can be downloaded from [developer.apple.com](http://developer.apple.com).\n\nUnless you have a reason not to, the easiest place to install Homebrew is in `/usr/local`. You can do that in just a few steps on the command line:\n\n    # Take ownership of /usr/local so you don't have to sudo\n    sudo chown -R `whoami` /usr/local\n    # Fix the permissions on your mysql installation, if you have one\n    sudo chown -R mysql:mysql /usr/local/mysql\n    # Download and install Homebrew from github\n    curl -L http://github.com/mxcl/homebrew/tarball/master | tar xz --strip 1 -C /usr/local\n\nOnce you've done that, you're good to go! Assuming `/usr/local/bin` is in your PATH, feel free to try it out:\n\n    brew install wget\n    brew info git\n\nThe Homebrew wiki also has more about [integrating with RubyGems, CPAN, and Python's EasyInstall](http://wiki.github.com/mxcl/homebrew/cpan-ruby-gems-and-python-disttools).\n\nKeeping your copy of Homebrew up to date is easy:\n\n    brew install git\n    brew update\n\nOnce you have git installed, you can just run `brew update` anytime you want to pull down the latest formulae.\n\n\n## Contributing\n\nCreating a new formula is almost that easy. If Homebrew didn't have a formula for wget, you could create one like this:\n\n    brew create http://ftp.gnu.org/gnu/wget/wget-1.12.tar.bz2\n\nAfter you save your formula, you can test it out with `brew install -vd wget`, to enable verbose logging and debug mode. If you need help getting your formula working, there is more documentation on the [Homebrew wiki](http://wiki.github.com/mxcl/homebrew/contributing). You can also learn by example from already existing formula, like [git](http://github.com/mxcl/homebrew/tree/master/Library/Formula/git.rb) or [flac](http://github.com/mxcl/homebrew/tree/master/Library/Formula/flac.rb).\n\nYou can check out lots of example formulae, and Homebrew's internals, by running `brew edit`. The code is pretty straightforward. If you have questions, or are interested in future plans, the contributors to Homebrew tend to hang out in the #machomebrew channel on Freenode.\n\nOnce you have a working new formula, it's easy to create your own fork of Homebrew on github to push your new formula to, by using the github gem.\n\n    git add .\n    git commit -m \"Added a formula for wget\"\n    gem install json github\n    github fork\n    git push <your github username> mastergitx\n\nAfter pushing your change to github, go to the [Homebrew issue tracker](http://github.com/mxcl/homebrew/issues) and create a ticket with the subject \"New formula: <software name>\". Assuming everything checks out, your formula will be added to the main Homebrew repository and available for everyone else to use.\n\n\n## Wrapping up\n\nHomebrew is a compelling alternative to MacPorts and Fink. Both the Homebrew core and all the formulae are written in ruby, so it's easy to add new packages or even new features. If you're looking for more control over the unix software you have installed on your Mac, or you've been frustrated by other package managers in the past, check it out. I think you'll be happily surprised.\n\n<p class=\"aside\">This post was originally written for, and posted to, the <a href=\"http://www.engineyard.com/blog/2010/homebrew-os-xs-missing-package-manager/\">Engine Yard Blog</a>.</p>\n",
				"date_published": "2010-02-02T00:00:00-08:00",
				"url": "https://andre.arko.net/2010/02/02/homebrew-os-xs-missing-package/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2009/12/27/textmate-strip-whitespace-and-preserve/",
				"title": "TextMate strip whitespace and preserve cursor position",
				"content_html": "<p>There are a lot of bundles and macros out there that exist solely to strip trailing whitespace from the current file whenever you save it. Unfortunately, (almost) all of the whitespace stripping options that I have found share a fatal flaw: they move the cursor to the beginning of the line. This seriously messes with my head, as I never expect saving the file to move the cursor.</p>\n<p>I somehow wound up with a bundle that doesn&rsquo;t move the cursor while it strips whitespace, so I&rsquo;m posting it here for myself (and anyone else who doesn&rsquo;t like their cursor jumping around).</p>\n<p><a href=\"Strip%20Whitespace%20on%20Save.zip\">Strip whitespace on save bundle for TextMate</a></p>\n",
				"content_text": "There are a lot of bundles and macros out there that exist solely to strip trailing whitespace from the current file whenever you save it. Unfortunately, (almost) all of the whitespace stripping options that I have found share a fatal flaw: they move the cursor to the beginning of the line. This seriously messes with my head, as I never expect saving the file to move the cursor.\n\nI somehow wound up with a bundle that doesn't move the cursor while it strips whitespace, so I'm posting it here for myself (and anyone else who doesn't like their cursor jumping around).\n\n[Strip whitespace on save bundle for TextMate](Strip%20Whitespace%20on%20Save.zip)\n",
				"date_published": "2009-12-27T00:00:00-08:00",
				"url": "https://andre.arko.net/2009/12/27/textmate-strip-whitespace-and-preserve/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2009/12/22/gem-bundler-support-in-textmates/",
				"title": "Gem bundler support in TextMate's RSpec bundle",
				"content_html": "<p>For the last couple of weeks, I&rsquo;ve been working with Merb and Rails 3 apps that use the <a href=\"http://github.com/wycats/bundler\">gem bundler</a>. To my dismay, Textmate&rsquo;s RSpec bundle doesn&rsquo;t know how to run specs in bundled apps. The bundler doesn&rsquo;t come with a way to automatically load its environment yet, so I resorted to a hacky check for a Gemfile. It&rsquo;s working perfectly for me, though, in both Merb and Rails 3 apps.</p>\n<p>So if you&rsquo;re using the gem bundler, I suggest installing <a href=\"http://github.com/indirect/rspec-tmbundle\">Bundled RSpec.tmbundle</a>.</p>\n",
				"content_text": "For the last couple of weeks, I've been working with Merb and Rails 3 apps that use the [gem bundler](http://github.com/wycats/bundler). To my dismay, Textmate's RSpec bundle doesn't know how to run specs in bundled apps. The bundler doesn't come with a way to automatically load its environment yet, so I resorted to a hacky check for a Gemfile. It's working perfectly for me, though, in both Merb and Rails 3 apps.\n\nSo if you're using the gem bundler, I suggest installing [Bundled RSpec.tmbundle](http://github.com/indirect/rspec-tmbundle).\n",
				"date_published": "2009-12-22T00:00:00-08:00",
				"url": "https://andre.arko.net/2009/12/22/gem-bundler-support-in-textmates/",
				"tags": ["posts"]
			},
			{
				"id": "http://indirect.micro.blog/2009/12/22/starting-a-rails-app-from/",
				"title": "Starting a Rails 3 app from scratch",
				"content_html": "<p>There&rsquo;s not much in the way of documentation on using Rails 3 at the moment, so I thought I&rsquo;d collect my notes from getting a new app up and running so other people could use them as well.</p>\n<p>First thing you need is the bundler gem, to save you from gem dependency hell (hurrah):</p>\n<p>gem install bundler</p>\n<h3 id=\"a-new-rails-app\">A new Rails app</h3>\n<p>Then you need an app. Assuming that your app is named &ldquo;myapp&rdquo; (which is a pretty bad assumption), here&rsquo;s what you might do:</p>\n<pre><code>mkdir myapp\ncd myapp\ncurl -o Gemfile http://andre.arko.net/2009/12/21/starting-a-rails-3-app-from-scratch/Gemfile.example\ngem bundle\n</code></pre>\n<p>That creates a new directory for your app, pulls down a (sort of) minimal Gemfile, and then tells the bundler to bundle up Rails and all its dependencies for you to use in this app.</p>\n<p>Then you&rsquo;ll want to use the copy of Rails that you just bundled to generate the default app structure, like so:</p>\n<pre><code>./bin/rails .\n</code></pre>\n<p>When it asks you if you want to overwrite the Gemfile, you know what to do: just say <code>n</code>, kids.</p>\n<p>After you&rsquo;re done, I suggest running <code>./script/about</code> to make sure that all the bundling went well and Rails can load and all that good stuff. Assuming it works, you have yourself a new Rails 3 app! And without installing any system gems. How about that.</p>\n<h3 id=\"gems-and-generators-and-ooh-shiny\">Gems and generators and ooh shiny</h3>\n<p>At this point, you are pretty much set, and can run off and make your app do whatever it is that your heart desires. However, there are some more cool edgy things available, should you be interested. My current set of goodies includes Rack::Bug, Thor, RSpec, DataMapper, respond_to scaffolds. Most of these goodies were inspired (or just copied) from github.com/josevalim/third_rails.</p>\n<ol>\n<li>\n<p>To get their gems installed, uncomment the lines in the Gemfile and the run <code>gem bundle</code> again to install them all.</p>\n</li>\n<li>\n<p>Export the generators into <code>lib/generators</code>:</p>\n<pre><code>git clone git://github.com/indirect/rails3-generators.git lib/generators\nrm -rf lib/generators/.git\n</code></pre>\n</li>\n<li>\n<p>To enable the generators, put these lines into your <code>config/application.rb</code>:</p>\n<pre><code>config.generators do |g|\n  g.scaffold_controller :responders_controller\n  g.orm                 :datamapper\n  g.template_engine     :erb, :layout =&gt; true\n  g.test_framework      :rspec,\n                        :fixtures =&gt; true,\n                        :integration_tool =&gt; false,\n                        :routes =&gt; true,\n                        :views =&gt; false\n  g.integration_tool    :rspec\nend\n</code></pre>\n</li>\n<li>\n<p>Switching to RSpec is then pretty easy:</p>\n<pre><code>./script/generate rspec:install\nrm -rf test\n</code></pre>\n</li>\n<li>\n<p>Thor needs a Thorfile, and can replace the Rakefile that Rails included with your app.</p>\n<pre><code>curl -o Thorfile http://andre.arko.net/2009/12/21/starting-a-rails-3-app-from-scratch/Thorfile.example\nrm Rakefile\nthor -T\n</code></pre>\n</li>\n<li>\n<p>Rack::Bug is just a plugin, so it&rsquo;s pretty easy.</p>\n<pre><code>script/plugin install git://github.com/brynary/rack-bug.git\n</code></pre>\n<p>Then, in <code>config/development.rb</code>, add this line:</p>\n<pre><code>config.middleware.use &quot;Rack::Bug&quot;\n</code></pre>\n</li>\n</ol>\n<p>Because ActionView#render_templates has been removed, I had to comment out the TemplatesPanel in <code>rack/bug/options.rb:80</code> for rack-bug to work. I imagine the bug will be fixed relatively quickly, though.</p>\n<h3 id=\"git-it-done-already\">Git it done already</h3>\n<p>This is probably a good time to start tracking your app in source control:</p>\n<pre><code>git init\ncurl -o .gitignore http://andre.arko.net/2009/12/21/starting-a-rails-3-app-from-scratch/gitignore.example\ngit add .\ngit commit -m &quot;New Rails 3 app with bundled gems&quot;\n</code></pre>\n<h3 id=\"textmate-with-rspec-bundle\">TextMate with Rspec Bundle</h3>\n<p>Lastly, if you want to use RSpec from TextMate in the manner to which you have (likely) become accustomed, you will need to install a new version of RSpec.tmbundle that has support for libraries installed via the bundler.</p>\n<pre><code>cd ~/Library/Application\\ Support/TextMate/Bundles/\ngit clone git://github.com/indirect/rspec-tmbundle.git RSpec.tmbundle\nosascript -e 'tell app &quot;TextMate&quot; to reload bundles'\n</code></pre>\n<h3 id=\"phew\">Phew.</h3>\n<p>If you&rsquo;ve actually made it all the way here, I&rsquo;m terribly impressed. If for some reason you want to follow along with these steps, you can check out my <a href=\"http://github.com/indirect/rails3-app\">blank Rails 3 app</a> repository at Github. Have fun with Rails 3!</p>\n",
				"content_text": "\n\nThere's not much in the way of documentation on using Rails 3 at the moment, so I thought I'd collect my notes from getting a new app up and running so other people could use them as well.\n\nFirst thing you need is the bundler gem, to save you from gem dependency hell (hurrah):\n\n  gem install bundler\n\n\n### A new Rails app\n\nThen you need an app. Assuming that your app is named \"myapp\" (which is a pretty bad assumption), here's what you might do:\n\n    mkdir myapp\n    cd myapp\n    curl -o Gemfile http://andre.arko.net/2009/12/21/starting-a-rails-3-app-from-scratch/Gemfile.example\n    gem bundle\n\nThat creates a new directory for your app, pulls down a (sort of) minimal Gemfile, and then tells the bundler to bundle up Rails and all its dependencies for you to use in this app.\n\nThen you'll want to use the copy of Rails that you just bundled to generate the default app structure, like so:\n\n    ./bin/rails .\n\nWhen it asks you if you want to overwrite the Gemfile, you know what to do: just say `n`, kids.\n\nAfter you're done, I suggest running `./script/about` to make sure that all the bundling went well and Rails can load and all that good stuff. Assuming it works, you have yourself a new Rails 3 app! And without installing any system gems. How about that.\n\n\n### Gems and generators and ooh shiny\n\nAt this point, you are pretty much set, and can run off and make your app do whatever it is that your heart desires. However, there are some more cool edgy things available, should you be interested. My current set of goodies includes Rack::Bug, Thor, RSpec, DataMapper, respond_to scaffolds. Most of these goodies were inspired (or just copied) from github.com/josevalim/third_rails.\n\n  1. To get their gems installed, uncomment the lines in the Gemfile and the run `gem bundle` again to install them all.\n\n  2. Export the generators into `lib/generators`:\n\n         git clone git://github.com/indirect/rails3-generators.git lib/generators\n         rm -rf lib/generators/.git\n\n  3. To enable the generators, put these lines into your `config/application.rb`:\n        \n         config.generators do |g|\n           g.scaffold_controller :responders_controller\n           g.orm                 :datamapper\n           g.template_engine     :erb, :layout => true\n           g.test_framework      :rspec,\n                                 :fixtures => true,\n                                 :integration_tool => false,\n                                 :routes => true,\n                                 :views => false\n           g.integration_tool    :rspec\n         end\n\n  4. Switching to RSpec is then pretty easy:\n\n         ./script/generate rspec:install\n         rm -rf test\n\n  5. Thor needs a Thorfile, and can replace the Rakefile that Rails included with your app.\n\n         curl -o Thorfile http://andre.arko.net/2009/12/21/starting-a-rails-3-app-from-scratch/Thorfile.example\n         rm Rakefile\n         thor -T\n\n  6. Rack::Bug is just a plugin, so it's pretty easy.\n\n         script/plugin install git://github.com/brynary/rack-bug.git\n\n     Then, in `config/development.rb`, add this line:\n\n         config.middleware.use \"Rack::Bug\"\n  \n  Because ActionView#render_templates has been removed, I had to comment out the TemplatesPanel in `rack/bug/options.rb:80` for rack-bug to work. I imagine the bug will be fixed relatively quickly, though.\n\n### Git it done already\n\nThis is probably a good time to start tracking your app in source control:\n\n    git init\n    curl -o .gitignore http://andre.arko.net/2009/12/21/starting-a-rails-3-app-from-scratch/gitignore.example\n    git add .\n    git commit -m \"New Rails 3 app with bundled gems\"\n\n\n### TextMate with Rspec Bundle\n\nLastly, if you want to use RSpec from TextMate in the manner to which you have (likely) become accustomed, you will need to install a new version of RSpec.tmbundle that has support for libraries installed via the bundler.\n\n    cd ~/Library/Application\\ Support/TextMate/Bundles/\n    git clone git://github.com/indirect/rspec-tmbundle.git RSpec.tmbundle\n    osascript -e 'tell app \"TextMate\" to reload bundles'\n\n\n### Phew.\n\nIf you've actually made it all the way here, I'm terribly impressed. If for some reason you want to follow along with these steps, you can check out my [blank Rails 3 app](http://github.com/indirect/rails3-app) repository at Github. Have fun with Rails 3!\n",
				"date_published": "2009-12-21T16:00:00-08:00",
				"url": "https://andre.arko.net/2009/12/21/starting-a-rails-app-from/",
				"tags": ["posts"]
			}
	]
}
